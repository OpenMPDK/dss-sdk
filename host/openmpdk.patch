diff --git a/.github/ISSUE_TEMPLATE/bug_report.md b/.github/ISSUE_TEMPLATE/bug_report.md
index 4782774..b418ce2 100644
--- a/.github/ISSUE_TEMPLATE/bug_report.md
+++ b/.github/ISSUE_TEMPLATE/bug_report.md
@@ -4,10 +4,6 @@ about: Create a report to help us improve
 
 ---
 
-**Location (Korea, USA, China, India, etc.)**
-Put your location for prompt support.
-
-
 **Describe the bug**
 A clear and concise description of what the bug is.
 
diff --git a/PDK/core/CMakeLists.txt b/PDK/core/CMakeLists.txt
index 5558681..bfae9ee 100644
--- a/PDK/core/CMakeLists.txt
+++ b/PDK/core/CMakeLists.txt
@@ -5,6 +5,7 @@ option(WITH_EMU "Enable Emulator support" OFF)
 option(WITH_KDD "Enable Kernel Driver support" OFF)
 option(WITH_SPDK "Enable SPDK Driver support" OFF)
 option(SAMSUNG_API "Support SAMSUNG API not SNIA API" OFF)
+option(KVS_REMOTE "Support for SAMSUNG API in remote KV environment" OFF)
 
 set(CMAKE_C_FLAGS "-MMD -MP -Wall -DLINUX -D_FILE_OFFSET_BITS=64 -fPIC  -march=native")
 set(CMAKE_CXX_FLAGS "-O2 -g -std=c++11 -MMD -MP -Wall -DLINUX -D_FILE_OFFSET_BITS=64 -fPIC  -march=native")
@@ -20,6 +21,9 @@ endif()
 
 if(SAMSUNG_API)
   add_definitions(-DSAMSUNG_API)
+  if(KVS_REMOTE)
+    add_definitions(-DKVS_REMOTE)
+  endif()
 endif()
 
 if(WITH_KDD)
@@ -32,6 +36,7 @@ include_directories (${CMAKE_CURRENT_SOURCE_DIR}/src/device_abstract_layer/kerne
 # SOURCE CODE
 
 SET(HEADERS_API
+    ${CMAKE_CURRENT_SOURCE_DIR}/src/api/include/rdd_cl.h
     ${CMAKE_CURRENT_SOURCE_DIR}/src/device_abstract_layer/include/kvs_adi.h
     ${CMAKE_CURRENT_SOURCE_DIR}/src/device_abstract_layer/kernel_driver_adapter/kadi.h
     ${CMAKE_CURRENT_SOURCE_DIR}/src/device_abstract_layer/kernel_driver_adapter/kadi_debug.h
@@ -62,7 +67,7 @@ add_definitions(-DWITH_KDD)
 set(KVAPI_CXXFLAGS "-O2 -std=c++11 -MMD -MP -Wall -D_FILE_OFFSET_BITS=64 -fPIC -fpermissive -march=native")
 set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${KVAPI_CFLAGS}")
 set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} ${KVAPI_CXXFLAGS}")
-set(KVAPI_LIBS ${KVAPI_LIBS} -pthread -Wl,--no-as-needed -fPIC -lnuma -lrt -march=native)
+set(KVAPI_LIBS ${KVAPI_LIBS} -pthread -lrdmacm -libverbs -Wl,--no-as-needed -fPIC -lnuma -lrt -march=native)
 
 include_directories (${CMAKE_CURRENT_SOURCE_DIR}/src/api/include)
 include_directories (${CMAKE_CURRENT_SOURCE_DIR}/src/api/include/private)
@@ -77,7 +82,7 @@ target_link_libraries (kvapi_static ${KVAPI_LIBS})
 
 # ${SOURCES_API} 
 add_executable(sample_code_async ${CMAKE_CURRENT_SOURCE_DIR}/sample_code/test_async.cpp ${HEADERS_API})
-target_link_libraries(sample_code_async kvapi_static)
+target_link_libraries(sample_code_async kvapi)
 add_dependencies(sample_code_async kvapi_static)
 #${SOURCES_API} ${HEADERS_API}
 add_executable(sample_code_sync ${CMAKE_CURRENT_SOURCE_DIR}/sample_code/test_sync.cpp ${HEADERS_API})
@@ -170,7 +175,11 @@ elseif(WITH_SPDK)
   include_directories (${CMAKE_CURRENT_SOURCE_DIR}/src/api/include/private)
   include_directories (${CMAKE_CURRENT_SOURCE_DIR}/src/device_abstract_layer/include/private)
   
-  set(KVKUDD_LIBS -L${CMAKE_CURRENT_SOURCE_DIR}/lib -Wl,--no-as-needed -lkvnvmedd -ldl -luuid)
+  set(KVKUDD_LIBS -L${CMAKE_BINARY_DIR}/lib -Wl,--no-as-needed -ldl -luuid)
+
+  set(KV_UDD_LINK_LIBS -Wl,--start-group -Wl,--whole-archive -lrte_pci -lrte_bus_pci -lkv_interface -lspdk_env_dpdk -lspdk_log -lspdk_util -lspdk_json -lspdk_sock -lspdk_sock_posix -lspdk_thread -Wl,--end-group -Wl,--no-whole-archive -lkvnvmedd -lspdk_bdev_nvme -lrte_mempool -lrte_ring -lrte_eal -lrte_kvargs -lspdk_nvme)
+
+
   SET(SOURCES_API
     ${SOURCES_API}
     ${CMAKE_CURRENT_SOURCE_DIR}/src/api/src/uddenv.cpp
@@ -180,7 +189,7 @@ elseif(WITH_SPDK)
     ${CMAKE_CURRENT_SOURCE_DIR}/src/api/src/private_cfrontend.cpp
     ${CMAKE_CURRENT_SOURCE_DIR}/src/device_abstract_layer/emulator/src/kv_config.cpp
     )
-  set(KVAPI_LIBS ${KVAPI_LIBS} ${KVKUDD_LIBS} -lrt)
+  set(KVAPI_LIBS ${KV_UDD_LINK_LIBS} ${KVAPI_LIBS} ${KVKUDD_LIBS} -lrt)
   add_definitions(-DWITH_SPDK)
   message("SPDK is enabled")
 
diff --git a/PDK/core/include/kvs_api.h b/PDK/core/include/kvs_api.h
index 4ed900d..fe926eb 100644
--- a/PDK/core/include/kvs_api.h
+++ b/PDK/core/include/kvs_api.h
@@ -49,9 +49,9 @@ extern "C" {
 #endif
 
 // memory
-#define _FUNCTIONIZE(a, b)  a(b)
+#define _FUNCTIONIZE(a,b)  a(b)
 #define _STRINGIZE(a)      #a
-#define _INT2STRING(i)     _FUNCTIONIZE(_STRINGIZE, i)
+#define _INT2STRING(i)     _FUNCTIONIZE(_STRINGIZE,i)
 #define _LOCATION       __FILE__ ":" _INT2STRING(__LINE__)
 
 void *_kvs_zalloc(size_t size_bytes, size_t alignment, const char *file);
@@ -160,10 +160,7 @@ kvs_result kvs_delete_container (kvs_device_handle dev_hd, const char *cont_name
 kvs_result kvs_open_container (kvs_device_handle dev_hd, const char* name, kvs_container_handle *cont_hd);
   
 kvs_result kvs_close_container (kvs_container_handle cont_hd);
-
-kvs_result kvs_list_containers(kvs_device_handle dev_hd, uint32_t index,
-  uint32_t buffer_size, kvs_container_name *names, uint32_t *cont_cnt);
-
+    
 kvs_result kvs_get_container_info (kvs_container_handle cont_hd, kvs_container *cont);
 
 
@@ -246,6 +243,91 @@ kvs_result kvs_exist_tuples(kvs_container_handle cont_hd, uint32_t key_cnt, cons
   
 kvs_result kvs_exist_tuples_async(kvs_container_handle cont_hd, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, const kvs_exist_context *ctx, kvs_callback_function cbfn);
 
+#ifdef KVS_REMOTE
+
+/*! Lock a KV pair
+ * 
+ * This API tries to take a reader or writer lock on the key supplied
+ * \param cont_hd - container handle
+ * \param key - key to lock
+ * \param lock_duration  - Lock duration requested from application in sec
+ * \param ctx - lock options
+ * \ingroup KV_API
+ *
+ */
+
+  kvs_result kvs_lock_tuple(kvs_container_handle cont_hd, const kvs_key *key, uint64_t instance_uuid, const kvs_lock_context *ctx);
+
+/*! Unlock a KV pair
+ * 
+ * This API tries to unlock a reader or writer lock on the key supplied
+ * \param cont_hd - container handle
+ * \param key - key to lock
+ * \param ctx - lock options
+ * \ingroup KV_API
+ * 
+ */
+
+  kvs_result kvs_unlock_tuple(kvs_container_handle cont_hd, const kvs_key *key, uint64_t instance_uuid, const kvs_lock_context *ctx);
+
+ 
+
+/*! List key(s) 
+ *
+ * Finds and returns the key(s). The value is an input/output parameter. It needs to provide an address of 
+ * the value buffer and its size. The retreived list data will be copied to the buffer. If the buffer size is not enough 
+ * to store the results, it will return \ref KVS_ERR_VALUE.
+ * 
+ * Some memory constraints can be applied as described in \ref kvs_store_tuple. 
+ * 
+ * list from root, list from specified key, list from specified prefix, and list from specified perfix/key options are supported. 
+ * 
+ * \param cont_hd container handle
+ * \param key key to retrieve
+ * \param value a value buffer where the output will be stored  [in/out]
+ * \param ctx options 
+ * \ingroup KV_API
+ */
+kvs_result kvs_list_tuple(kvs_container_handle cont_hd, const kvs_key *prefix_key, const kvs_key *start_key, uint16_t max_keys_to_list, kvs_value *value, const kvs_list_context *ctx);
+
+kvs_result kvs_list_tuple_async(kvs_container_handle cont_hd, const kvs_key *prefix_key, const kvs_key *start_key, uint16_t max_keys_to_list, kvs_value *value, const kvs_list_context *ctx, kvs_callback_function cbfn);
+
+/*! Retrieve key(s) with direct data transfer to the client
+ *
+ *  This command is meant for giving a performance improvement over regular retrieve command where end clients are remote and openmpdk stack
+ *  is not installed on clients, for example, clients doing s3 api call to our dss stack. In such case, dss target will directly transfer payload value
+ *  to the remote client bypassing entire dss host stack on return.
+ *
+ *  \param cont_hd container handle
+ *  \param key key to retrieve
+ *  \param remote_address remote client address where value will be RDMA into
+ *  \param transfer_length value length it is expecting
+ *  \param remote_client_id remote client identifier for RDMA transfer
+ *  \param ctx options
+ *  \ingroup KV_API
+ */
+kvs_result kvs_retrieve_tuple_direct(kvs_container_handle cont_hd, const kvs_key *key, const kvs_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kvs_retrieve_context *ctx);
+
+/*! Store key(s) with direct data transfer to the client
+ *  
+ *  This command is meant for giving a performance improvement over regular retrieve command where end clients are remote and openmpdk stack
+ *  is not installed on clients, for example, clients doing s3 api call to our dss stack. In such case, dss target will directly transfer payload value
+ *  to the remote client bypassing entire dss host stack on return.
+ *  
+ *  \param cont_hd container handle
+ *  \param key key to retrieve
+ *  \param remote_address remote client address where value will be RDMA into
+ *  \param transfer_length value length it is expecting
+ *  \param remote_client_id remote client identifier for RDMA transfer
+ *  \param ctx options
+ *  \ingroup KV_API
+ *  
+ */
+kvs_result kvs_store_tuple_direct(kvs_container_handle cont_hd, const kvs_key *key, const kvs_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kvs_store_context *ctx);
+
+
+#endif
+  
 /*! Open an iterator
  *
  * \param cont_hd container handle
@@ -741,7 +823,7 @@ kvs_result kvs_retrieve_kvp(kvs_key_space_handle ks_hd, kvs_key *key, kvs_option
   As an input value.value contains the buffer to store the key value pair value and value.length contains the buffer size.
   The key value pair value is copied to value.value buffer and value.length is set to the retrieved value size. If the offset of value is not zero,
   the value of key value pair is copied into the buffer, skipping the first offset bytes of the value of key value pair.
-  That is, value.length is equal to the total size of (actual_value_size ¨C offset). The offset is required to align to KVS_ALIGNMENT_UNIT.
+  That is, value.length is equal to the total size of (actual_value_size â€“ offset). The offset is required to align to KVS_ALIGNMENT_UNIT.
   If the offset is not aligned, a KVS_ERR_VALUE_OFFSET_MISALIGNED error is returned. If an allocated value buffer is not big enough to hold the value,
   it will set value.actual_value_size to the actual value length and return KVS_ERR_BUFFER_SMALL.
 
@@ -982,7 +1064,7 @@ kvs_result kvs_exist_kv_pairs_async(kvs_key_space_handle ks_hd, uint32_t key_cnt
 *
   This API enables applications to set up a Key Group such that the keys in that Key Group may be iterated within a Key Space
   (i.e., kvs_crearte_iterator() enables a device to prepare a Key Group of keys for iteration by matching a given bit pattern (it_fltr.bit_pattern) to all keys in the Key Space
-  considering bits indicated by it_fltr.bitmask and the device sets up a Key Group of keys matching that ¡°(bitmask & key) == bit_pattern¡±.)
+  considering bits indicated by it_fltr.bitmask and the device sets up a Key Group of keys matching that â€œ(bitmask & key) == bit_patternâ€.)
 
   PARAMETERS
   IN ks_hd Key Space handle
@@ -1031,8 +1113,8 @@ kvs_result kvs_delete_iterator(kvs_key_space_handle ks_hd, kvs_iterator_handle i
   buffer_size is the iterator buffer (iter_list) size in bytes. The retrieved values (iter_list) are either keys or key-value pairs based on the iterator option
   which is specified by kvs_create_iterator().
   Output values (iter_list.it_list) are determined by the iterator option specified by an application.
-  KV_ITERATOR_OPT_KEY [MANDATORY]: a subset of keys are returned in iter_list.it_list data structure
-  KV_ITERATOR_OPT_KEY_VALUE; a subset of key-value pairs are returned in iter_list.it_list data structure
+  â€¢ KV_ITERATOR_OPT_KEY [MANDATORY]: a subset of keys are returned in iter_list.it_list data structure
+  â€¢ KV_ITERATOR_OPT_KEY_VALUE; a subset of key-value pairs are returned in iter_list.it_list data structure
 
   PARAMETERS
   IN ks_hd Key Space handle
@@ -1057,8 +1139,8 @@ kvs_result kvs_iterate_next(kvs_key_space_handle ks_hd, kvs_iterator_handle iter
   This API obtains a subset of key or key-value pairs from an iterator Key Group of iter_hd within a Key Space
   (i.e., kvs_iterator_next() retrieves a next Key Group of keys or key-value pairs in the iterator key group (iter_hd) that is set with kvs_create_iterator() command).
   Output values (iter_list.it_list) are determined by the iterator option set by an application.
-  KV_ITERATOR_OPT_KEY [MANDATORY]: a subset of keys are returned in iter_list.it_list data structure
-  KV_ITERATOR_OPT_KEY_VALUE; a subset of key-value pairs are returned in iter_list.it_list data structure
+  â€¢ KV_ITERATOR_OPT_KEY [MANDATORY]: a subset of keys are returned in iter_list.it_list data structure
+  â€¢ KV_ITERATOR_OPT_KEY_VALUE; a subset of key-value pairs are returned in iter_list.it_list data structure
 
   PARAMETERS
   IN ks_hd Key Space handle
diff --git a/PDK/core/include/kvs_const.h b/PDK/core/include/kvs_const.h
index 72c5406..21e5c8a 100644
--- a/PDK/core/include/kvs_const.h
+++ b/PDK/core/include/kvs_const.h
@@ -40,20 +40,16 @@ extern "C" {
 #endif
 
 #define KVS_MIN_KEY_LENGTH 4
-#define KVS_MAX_KEY_LENGTH 255
+#define KVS_MAX_KEY_LENGTH 1024
 #define KVS_MIN_VALUE_LENGTH 0
 #define KVS_MAX_VALUE_LENGTH (2*1024*1024)
 #define KVS_OPTIMAL_VALUE_LENGTH 4096
-#define KVS_ALIGNMENT_UNIT 512 /*value of KVS_ALIGNMENT_UNIT must be a power of 2 currently */
-#define KVS_VALUE_LENGTH_ALIGNMENT_UNIT 4 /*value of KV_VALUE_LENGTH_ALIGNMENT_UNIT must be a power of 2 currently */
+#define KVS_ALIGNMENT_UNIT 4
 #define KVS_MAX_ITERATE_HANDLE 16
 #define G_ITER_KEY_SIZE_FIXED 16
 #define KVS_MAX_KEY_GROUP_BYTES 4
 #define KVS_ITERATOR_BUFFER_SIZE (32*1024)
-#define MAX_CONT_PATH_LEN 255
-#define MAX_KEYSPACE_NAME_LEN MAX_CONT_PATH_LEN
-
-
+  
 #ifdef __cplusplus
 } // extern "C"
 #endif
diff --git a/PDK/core/include/kvs_result.h b/PDK/core/include/kvs_result.h
index 49832ef..a51d150 100644
--- a/PDK/core/include/kvs_result.h
+++ b/PDK/core/include/kvs_result.h
@@ -91,8 +91,16 @@ typedef enum {
   KVS_ERR_DEV_PATH_TOO_LONG=0x02A,     // the length of dev path more than 255
   KVS_ERR_ITERATOR_NUM_OUT_RANGE=0x02B, //the number of iterator required out of range(equal 0, or bigger than 16)
   KVS_ERR_DD_UNSUPPORTED=0x02C,   //device driver does not support.
-  KVS_ERR_ITERATOR_BUFFER_SIZE=0x02D, // buffer (iter_list->it_list) size in kvs_iterator_list error  
-  KVS_ERR_MEMORY_MALLOC_FAIL=0x032,  // memory malloc function error.
+  KVS_ERR_ITERATOR_BUFFER_SIZE=0x02D, // buffer (iter_list->it_list) size in kvs_iterator_list error
+  KVS_ERR_NONEXIST_PREFIX         = 0x02E,    // prefix doesnt exist for target list op
+  KVS_ERR_NONEXIST_STARTKEY       = 0x02F,    // start key doesnt exist for target list op
+  KVS_ERR_UNSUPPORTED_OPTION      = 0x030,    // unsupported option for target list op
+  KVS_ERR_END_OF_LIST             = 0x031,    // target list op completed successfully, no more keys to list
+  KVS_ERR_KEY_IS_LOCKED           = 0x032,    // faild to lock. key is already locked
+  KVS_ERR_LOCK_UUID_MISMATCH       = 0x033,    // UUID mismatch for lock
+  KVS_ERR_NONEXIST_WRITER         = 0x034,    // No writer exist for unlock
+  KVS_ERR_NONEXIST_READER         = 0x035,    // No reader exist for unlock
+  KVS_ERR_LOCK_EXPIRED            = 0x036,    // Lock expired
 
   // From user driver	
   KVS_ERR_CACHE_INVALID_PARAM=0x200	, // (kv cache) invalid parameters
@@ -131,8 +139,6 @@ typedef enum {
   KVS_ERR_CONT_NAME=0x405	, // container name is invalid
   KVS_ERR_CONT_NOT_EXIST=0x406	, // container does not existi
   KVS_ERR_CONT_OPEN=0x407	, // container is already opened
-  KVS_ERR_CONT_PATH_TOO_LONG=0x408, //the length of container more than 255
-  KVS_ERR_CONT_MAX = 0x409, //Exceeded max number of created container
 } kvs_result;	
 
 #else
@@ -164,7 +170,16 @@ typedef enum {
   KVS_ERR_VALUE_OFFSET_MISALIGNED = 0x016,    // offset of value is required to be aligned to KVS_ALIGNMENT_UNIT
   KVS_ERR_VALUE_UPDATE_NOT_ALLOWED = 0x017,   // key exists but value update is not allowed
   KVS_ERR_DEV_NOT_OPENED          = 0x018,    // device was not opened yet
-  KVS_ERR_UNSUPPORTED             = 0x019     // unsupported function
+  KVS_ERR_NONEXIST_PREFIX         = 0x019,    // prefix doesnt exist for target list op
+  KVS_ERR_NONEXIST_STARTKEY       = 0x01A,    // start key doesnt exist for target list op
+  KVS_ERR_UNSUPPORTED_OPTION      = 0x01B,    // unsupported option for target list op
+  KVS_ERR_END_OF_LIST             = 0x01C,    // target list op completed successfully, no more keys to list
+  KVS_ERR_KEY_IS_LOCKED           = 0x01D,    // faild to lock. key is already locked
+  KVS_ERR_LOCK_UUID_MISMATCH       = 0x01E,    // UUID mismatch for lock
+  KVS_ERR_NONEXIST_WRITER         = 0x01F,    // No writer exist for unlock
+  KVS_ERR_NONEXIST_READER         = 0x020,    // No reader exist for unlock
+  KVS_ERR_LOCK_EXPIRED            = 0x021,    // Lock expired
+  KVS_ERR_UNSUPPORTED             = 0x022     // unsupported function
 } kvs_result;
 
 #endif
diff --git a/PDK/core/include/kvs_struct.h b/PDK/core/include/kvs_struct.h
index 1577e82..cc9ab95 100644
--- a/PDK/core/include/kvs_struct.h
+++ b/PDK/core/include/kvs_struct.h
@@ -62,7 +62,6 @@ typedef struct {
   bool kvs_store_compress;     // compress value before storing it to media
 } kvs_store_option;
 
-
 typedef enum {
   KVS_ITERATOR_KEY = 0,    // [DEFAULT] iterator command gets only key entries without value
   KVS_ITERATOR_KEY_VALUE,  // [OPTION] iterator command gets key and value pairs
@@ -97,6 +96,9 @@ enum kvs_op {
   IOCB_ASYNC_ITER_OPEN_CMD=5,
   IOCB_ASYNC_ITER_CLOSE_CMD=6,
   IOCB_ASYNC_ITER_NEXT_CMD=7,
+  IOCB_ASYNC_LIST_CMD=8,
+  IOCB_ASYNC_LOCK_CMD=9,
+  IOCB_ASYNC_UNLOCK_CMD=10
 };
 
 /**
@@ -111,7 +113,7 @@ enum kvs_op {
 #endif
   
 
-typedef uint8_t kvs_key_t;
+typedef uint16_t kvs_key_t;
 typedef uint32_t kvs_value_t;
 typedef unsigned __int128 uint128_t;
 
@@ -249,13 +251,38 @@ typedef struct {
   void *private2;
 } kvs_exist_context;
 
+#ifdef KVS_REMOTE
+
+typedef struct {
+  bool kvs_reader_lock;      // [OPTION] reader or writer lock
+  bool kvs_blocking_lock;   // [OPTION] Blocking till get the lock or return immediately if didn't get
+  uint8_t lock_priority;        /*!< lock priority */
+  uint32_t lock_duration;       /*!< application instance wants to acquire lock */
+
+} kvs_lock_option;
+
+typedef struct {
+  kvs_lock_option option;       /*!< an option for a lock operation. */
+  void *private1;
+  void *private2;
+
+} kvs_lock_context;
+
+ 
+typedef struct {
+  void *private1;               /*!< a pointer to a user's I/O context information, which will be delivered to a callback function, unmodified */
+  void *private2;               /*!< the second pointer to a user's I/O context information */
+} kvs_list_context;
+
+#endif
+
 typedef struct {
   kvs_container_option option;
 } kvs_container_context;
 
 typedef struct {
   uint8_t  opcode;                /*!< operation opcode */
-  kvs_container_handle cont_hd;  /*!< container handle */
+  kvs_container_handle *cont_hd;  /*!< container handle */
   kvs_key *key;                   /*!< key data structure */
   kvs_value *value;               /*!< value data structure */
   uint32_t key_cnt;               /*!< kvs_exist_tuple_async */
@@ -264,7 +291,10 @@ typedef struct {
   void *private1;                 /*!< a pointer passed from a user */
   void *private2;                 /*!< a pointer passed from a user */
   kvs_result result;              /*!< IO result */
-  kvs_iterator_handle iter_hd;   /*!< iterator handle */
+  kvs_iterator_handle *iter_hd;   /*!< iterator handle */
+  kvs_key *prefix_key;            /*!< prefix_key data structure */
+  kvs_key *start_key;             /*!< start_key data structure */
+  uint16_t max_keys_to_list;
 } kvs_callback_context;
 
 
@@ -344,6 +374,7 @@ typedef enum {
   KVS_CMD_ITER_NEXT       =0x06,
   KVS_CMD_RETRIEVE        =0x07,
   KVS_CMD_STORE           =0x08,
+  KVS_CMD_LIST            =0x09,
 } kvs_context;
 
 typedef enum {
diff --git a/PDK/core/lib/libkvnvmedd.a b/PDK/core/lib/libkvnvmedd.a
index ec1bcee..7b4fbf9 100644
Binary files a/PDK/core/lib/libkvnvmedd.a and b/PDK/core/lib/libkvnvmedd.a differ
diff --git a/PDK/core/sample_code/test_async.cpp b/PDK/core/sample_code/test_async.cpp
index a319a24..028f01c 100644
--- a/PDK/core/sample_code/test_async.cpp
+++ b/PDK/core/sample_code/test_async.cpp
@@ -68,8 +68,7 @@ void usage(char *program)
   printf("-d      device_path  :  kvssd device path. e.g. emul: /dev/kvemul; kdd: /dev/nvme0n1; udd: 0000:06:00.0\n");
   printf("-n      num_ios      :  total number of ios (ignore this for iterator)\n");
   printf("-q      queue_depth  :  queue depth (ignore this for iterator)\n");
-  printf("-o      op_type      :  1: write; 2: read; 3: delete; 4: iterator;"
-                                  "5: check key exist;\n");
+  printf("-o      op_type      :  1: write; 2: read; 3: delete; 4: iterator; 5: check key exist\n");
   printf("-k      klen         :  key length (ignore this for iterator)\n");
   printf("-v      vlen         :  value length (ignore this for iterator)\n");
   printf("==============\n");
@@ -126,11 +125,11 @@ bool _malloc_str_pool(uint32_t pool_size, std::queue<char *> *pool,
   pthread_mutex_t *pool_lock, uint32_t str_len){
   for (uint32_t i = 0; i < pool_size; i++) {
     char *keymem = (char*)kvs_malloc(str_len, 4096);
+    memset(keymem, 0, str_len);
     if(!keymem) {
       _free_str_pool(pool, pool_lock);
       return false;
     }
-    memset(keymem, 0, str_len);  
     pool->push(keymem);
   }
   return true;
@@ -256,7 +255,7 @@ void _iterator_complete_handle(kvs_callback_context* ioctx) {
     fprintf(stderr, "ERROR io_complete: iterator result=0x%x, err= %s\n",
        ioctx->result, kvs_errstr(ioctx->result));
     return;
-  } 
+  }
   struct iterator_info *iter_info;
   iter_info = (struct iterator_info *)ioctx->private1;
 #if DEBUG_ON
@@ -361,7 +360,7 @@ int perform_iterator(kvs_container_handle cont_hd,
   char prefix_str[5] = "0000";
   unsigned int PREFIX_KV = 0;
   for (int i = 0; i < 4; i++){
-    PREFIX_KV |= (prefix_str[i] << (3-i)*8);
+    PREFIX_KV |= (prefix_str[i] << i*8);
   }
 
   iter_ctx_open.bit_pattern = PREFIX_KV;
@@ -494,7 +493,7 @@ int perform_read(kvs_container_handle cont_hd, int count, int maxdepth, kvs_key_
           ret = FAILED;
           goto exit;
         }
-        snprintf(key, klen, "%0*ld", klen - 1, seq++);
+        sprintf(key, "%0*ld", klen - 1, seq++);
         memset(value, 0, vlen);
 
         kvs_retrieve_option option;
@@ -568,8 +567,8 @@ int perform_insertion(kvs_container_handle cont_hd, int count, int maxdepth, kvs
         ret = FAILED;
         goto exit;
       }
-      snprintf(key,  klen, "%0*ld", klen - 1, seq++);
-      snprintf(value, vlen, "value%ld", seq);
+      sprintf(key, "%0*ld", klen - 1, seq++);
+      sprintf(value, "value%ld", seq);
 
       kvs_store_option option;
       memset(&option, 0, sizeof(kvs_store_option));
@@ -600,7 +599,7 @@ int perform_insertion(kvs_container_handle cont_hd, int count, int maxdepth, kvs
     }
   }
 
-exit: 
+exit:
   // wait until commands that has succussfully submitted finish
   while(completed < num_submitted) {
     usleep(1);
@@ -640,7 +639,7 @@ int perform_delete(kvs_container_handle cont_hd, int count, int maxdepth, kvs_ke
         ret = FAILED;
         goto exit;
       }
-      snprintf(key, klen, "%0*ld", klen - 1, seq++);
+      sprintf(key, "%0*ld", klen - 1, seq++);
       kvs_key *kvskey = _allocate_kvskey(key, klen);
       if(!kvskey) {
         _free_kv_buff(key, NULL, &keypool, NULL, &lock);
@@ -708,7 +707,7 @@ int perform_key_exist(kvs_container_handle cont_hd, int count, int maxdepth, kvs
         ret = FAILED;
         goto exit;
       }
-      snprintf(key,  klen, "%0*ld", klen - 1, seq++);
+      sprintf(key, "%0*ld", klen - 1, seq++);
       kvs_key *kvskey = _allocate_kvskey(key, klen);
       if(!kvskey) {
         _free_kv_buff(key, NULL, &keypool, NULL, &lock);
@@ -747,105 +746,6 @@ exit:
   return SUCCESS;
 }
 
-int _env_exit(kvs_device_handle dev, const char* cont_name,
-  kvs_container_handle cont_handle) {
-  int32_t dev_util = 0;
-  kvs_get_device_utilization(dev, &dev_util);
-  fprintf(stdout, "After: Total used is %d\n", dev_util);
-  kvs_close_container(cont_handle);
-  kvs_delete_container(dev, cont_name);
-  kvs_close_device(dev);
-  kvs_result ret = kvs_exit_env();
-  return ret;
-}
-
-int _env_init(kvs_init_options* options, char* dev_path, kvs_device_handle* dev,
-  const char *cont_name, kvs_container_handle* cont_handle) {
-  // initialize the environment
-  kvs_init_env(options);
-  kvs_result ret = kvs_open_device(dev_path, dev);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Device open failed\n");
-    return FAILED;
-  }
-
-  //container list before create "test"
-  uint32_t valid_cnt = 0;
-  const uint32_t retrieve_cnt = 2;
-  kvs_container_name names[retrieve_cnt];
-  for(uint8_t idx = 0; idx < retrieve_cnt; idx++) {
-    names[idx].name_len = MAX_CONT_PATH_LEN;
-    names[idx].name = (char*)malloc(MAX_CONT_PATH_LEN);
-  }
-  ret = kvs_list_containers(*dev, 1, retrieve_cnt*sizeof(kvs_container_name),
-    names, &valid_cnt);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "List current containers failed. error:0x%x.\n", ret);
-    kvs_close_device(*dev);
-    return FAILED;
-  }
-  for (uint8_t idx = 0; idx < valid_cnt; idx++) {
-    kvs_delete_container(*dev, names[idx].name);
-  }
-
-  kvs_container_context ctx;
-  // Initialize key order to KVS_KEY_ORDER_NONE
-  ctx.option.ordering = KVS_KEY_ORDER_NONE;
-  ret = kvs_create_container(*dev, cont_name, 0, &ctx);
-  if (ret != KVS_SUCCESS) {
-    fprintf(stderr, "Create containers failed. error:0x%x.\n", ret);
-    kvs_close_device(*dev);
-    return FAILED;
-  }
-
-  ret = kvs_open_container(*dev, cont_name, cont_handle);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Open containers %s failed. error:0x%x.\n", cont_name, ret);
-    kvs_delete_container(*dev, cont_name);
-    kvs_close_device(*dev);
-    return FAILED;
-  }
-
-  char *name_buff = (char*)malloc(MAX_CONT_PATH_LEN);
-  kvs_container_name name = {MAX_CONT_PATH_LEN, name_buff};
-  kvs_container cont = {false, 0, 0, 0, 0, &name};
-  ret = kvs_get_container_info(*cont_handle, &cont);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Get info of containers %s failed. error:0x%x.\n", name.name, ret);
-    _env_exit(*dev, cont_name, *cont_handle);
-    return FAILED;
-  }
-
-  fprintf(stdout, "Container information get name: %s\n", cont.name->name);
-  fprintf(stdout, "open:%d, scale:%d, capacity:%ld, free_size:%ld count:%ld.\n", 
-    cont.opened, cont.scale, cont.capacity, cont.free_size, cont.count);
-  free(name_buff);
-
-  int32_t dev_util = 0;
-  int64_t dev_capa = 0;
-  kvs_get_device_utilization(*dev, &dev_util);
-
-  float waf = 0.0;
-  kvs_get_device_waf(*dev, &waf);
-  kvs_get_device_capacity(*dev, &dev_capa);
-  fprintf(stdout, "Before: Total size is %ld bytes, used is %d, waf is %.2f\n", dev_capa, dev_util, waf);
-  
-  kvs_device *dev_info = (kvs_device*)malloc(sizeof(kvs_device));
-  if(dev_info){
-    kvs_get_device_info(*dev, dev_info);
-    fprintf(stdout, "Total size: %.2f GB\nMax value size: %d\nMax key size: %d\n"
-      "Optimal value size: %d\n", (float)dev_info->capacity/1000/1000/1000,
-    dev_info->max_value_len, dev_info->max_key_len, dev_info->optimal_value_len);
-    free(dev_info);
-  }else{
-    fprintf(stderr, "dev_info malloc failed\n");
-    _env_exit(*dev, cont_name, *cont_handle);
-    return FAILED;
-  }
-
-  return SUCCESS;
-}
-
 int main(int argc, char *argv[]) {
   char* dev_path = NULL;
   int num_ios = 10;
@@ -923,12 +823,40 @@ int main(int argc, char *argv[]) {
     options.memory.use_dpdk = 0;
   }
 
-  const char *cont_name = "test";
+  // initialize the environment
+  kvs_init_env(&options);
+
   kvs_device_handle dev;
-  kvs_container_handle cont_handle;
-  if(_env_init(&options, dev_path, &dev, cont_name, &cont_handle) != SUCCESS)
+  ret = kvs_open_device(dev_path, &dev);
+  if(ret != KVS_SUCCESS) {
+    fprintf(stderr, "Device open failed %s\n", kvs_errstr(ret));
     return FAILED;
+  }
+  
+  kvs_container_context ctx;
+  kvs_create_container(dev, "test", 4, &ctx);
+  
+  kvs_container_handle cont_handle;
+  kvs_open_container(dev, "test", &cont_handle);
+
+  int32_t dev_util = 0;
+  int64_t dev_capa = 0;
+  kvs_device *dev_info = (kvs_device*)malloc(sizeof(kvs_device));
+
+  kvs_get_device_utilization(dev, &dev_util);
+
+  float waf = 0.0;
+
+  kvs_get_device_waf(dev, &waf);
+  kvs_get_device_capacity(dev, &dev_capa);
+  fprintf(stdout, "Before: Total size is %ld bytes, used is %d, waf is %.2f\n", dev_capa, dev_util, waf);
 
+  kvs_get_device_info(dev, dev_info);
+  fprintf(stdout, "Total size: %.2f GB\nMax value size: %d\nMax key size: %d\nOptimal value size: %d\n",
+    (float)dev_info->capacity/1000/1000/1000, dev_info->max_value_len,
+    dev_info->max_key_len, dev_info->optimal_value_len);
+  free(dev_info);
+  
   switch(op_type) {
   case WRITE_OP:
     perform_insertion(cont_handle, num_ios, qdepth, klen, vlen);
@@ -951,12 +879,17 @@ int main(int argc, char *argv[]) {
     break;
   default:
     fprintf(stderr, "Please specify a correct op_type for testing\n");
-    ret = FAILED;
+    return FAILED;
   }
 
-  _env_exit(dev, cont_name, cont_handle);
+  kvs_get_device_utilization(dev, &dev_util);
+  fprintf(stdout, "After: Total size is %ld bytes, used is %d\n", dev_capa, dev_util);  
+  
+  kvs_close_container(cont_handle);
+  kvs_delete_container(dev, "test");
+  kvs_exit_env();
 
-  return ret;
+  return SUCCESS;
 }
 
 #else
@@ -1101,16 +1034,20 @@ int perform_iterator(kvs_key_space_handle ks_hd, kvs_option_iterator iter_op={KV
   kvs_key_group_filter iter_fltr;
   kvs_iterator_handle iter_hd;
 
-  iter_fltr.bitmask[0] = 0xff;
-  iter_fltr.bitmask[1] = 0xff;
-  iter_fltr.bitmask[2] = 0;
-  iter_fltr.bitmask[3] = 0;
-
-  iter_fltr.bit_pattern[0] = '0';
-  iter_fltr.bit_pattern[1] = '0';
-  iter_fltr.bit_pattern[2] = '0';
-  iter_fltr.bit_pattern[3] = '0';
+  iter_fltr.bitmask[0] = 0;
+  iter_fltr.bitmask[1] = 0;
+  iter_fltr.bitmask[2] = 0xff;
+  iter_fltr.bitmask[3] = 0xff;
+  char prefix_str[5] = "0000";
+  unsigned int PREFIX_KV = 0;
+  for (int i = 0; i < 4; i++){
+    PREFIX_KV |= (prefix_str[i] << i*8);
+  }
 
+  iter_fltr.bit_pattern[0] = PREFIX_KV & 0xff;
+  iter_fltr.bit_pattern[1] = PREFIX_KV & 0xff00 >> 8;
+  iter_fltr.bit_pattern[2] = PREFIX_KV & 0xff0000 >> 16;
+  iter_fltr.bit_pattern[3] = PREFIX_KV & 0xff000000 >> 24;
 
   submitted = completed = 0;
 
@@ -1123,9 +1060,6 @@ int perform_iterator(kvs_key_space_handle ks_hd, kvs_option_iterator iter_op={KV
   
   /* Do iteration */
   kvs_iterator_list* iter_list = (kvs_iterator_list*)malloc(sizeof(kvs_iterator_list));
-  if(iter_list == NULL){
-    return FAILED;
-  }
   iter_list->size = ITER_BUFFER_SIZE;
   uint8_t *buffer;
   buffer =(uint8_t*) kvs_malloc(ITER_BUFFER_SIZE, 4096);
@@ -1211,7 +1145,7 @@ int perform_read(kvs_key_space_handle ks_hd, int count, int maxdepth, uint16_t k
           ret = FAILED;
           goto exit;
         }
-        snprintf(key,  klen, "%0*ld", klen - 1, seq++);
+        sprintf(key, "%0*ld", klen - 1, seq++);
         memset(value, 0, vlen);
 
         kvs_option_retrieve option = {false};
@@ -1279,8 +1213,8 @@ int perform_insertion(kvs_key_space_handle ks_hd, int count, int maxdepth, uint1
         ret = FAILED;
         goto exit;
       }
-      snprintf(key,  klen, "%0*ld", klen - 1, seq++);
-      snprintf(value, vlen, "value%ld", seq);
+      sprintf(key, "%0*ld", klen - 1, seq++);
+      sprintf(value, "value%ld", seq);
 
       kvs_option_store option = {KVS_STORE_POST, NULL};
 
@@ -1346,7 +1280,7 @@ int perform_delete(kvs_key_space_handle ks_hd, int count, int maxdepth, uint16_t
         ret = FAILED;
         goto exit;
       }
-      snprintf(key, klen, "%0*ld", klen - 1, seq++);
+      sprintf(key, "%0*ld", klen - 1, seq++);
       kvs_key *kvskey = _allocate_kvskey(key, klen);
       if(!kvskey) {
         _free_kv_buff(key, NULL, &keypool, NULL, &lock);
@@ -1410,7 +1344,7 @@ int perform_key_exist(kvs_key_space_handle ks_hd, int count, int maxdepth, uint1
         ret = FAILED;
         goto exit;
       }
-      snprintf(key, klen, "%0*ld", klen - 1, seq++);
+      sprintf(key, "%0*ld", klen - 1, seq++);
       kvs_key *kvskey = _allocate_kvskey(key, klen);
       if(!kvskey) {
         _free_kv_buff(key, NULL, &keypool, NULL, &lock);
@@ -1420,12 +1354,10 @@ int perform_key_exist(kvs_key_space_handle ks_hd, int count, int maxdepth, uint1
 
       uint8_t *status = (uint8_t*)malloc(sizeof(uint8_t));
       kvs_exist_list *list = (kvs_exist_list*)malloc(sizeof(kvs_exist_list));
-      if(list){
-        list->keys = kvskey;
-        list->length = 1;
-        list->num_keys = 1;
-        list->result_buffer = status;
-      }
+      list->keys = kvskey;
+      list->length = 1;
+      list->num_keys = 1;
+      list->result_buffer = status;
       ret = kvs_exist_kv_pairs_async(ks_hd, 1, kvskey, list, complete);
       if (ret != KVS_SUCCESS) {
         fprintf(stderr, "exit tuple failed with err 0x%x\n", ret);
@@ -1457,115 +1389,6 @@ exit:
   return SUCCESS;
 }
 
-int _env_exit(kvs_device_handle dev, char* keyspace_name,
-  kvs_key_space_handle ks_hd) {
-  uint32_t dev_util = 0;
-  kvs_get_device_utilization(dev, &dev_util);
-  fprintf(stdout, "After: Total used is %d\n", dev_util);  
-  kvs_close_key_space(ks_hd);
-  kvs_key_space_name ks_name;
-  ks_name.name_len = strlen(keyspace_name) + 1;
-  ks_name.name = keyspace_name;
-  kvs_delete_key_space(dev, &ks_name);
-  kvs_result ret = kvs_close_device(dev);
-  return ret;
-}
-
-int _env_init(char* dev_path, kvs_device_handle* dev, char *keyspace_name,
-  kvs_key_space_handle* ks_hd) {
-  kvs_result ret = kvs_open_device(dev_path, dev);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Device open failed 0x%x\n", ret);
-    return FAILED;
-  }
-
-  //keyspace list after create "test"
-  const uint32_t retrieve_cnt = 2;
-  kvs_key_space_name names[retrieve_cnt];
-  for(uint8_t idx = 0; idx < retrieve_cnt; idx++) {
-    names[idx].name_len = MAX_KEYSPACE_NAME_LEN;
-    names[idx].name = (char*)malloc(MAX_KEYSPACE_NAME_LEN);
-  }
-
-  uint32_t valid_cnt = 0;
-  ret = kvs_list_key_spaces(*dev, 1, retrieve_cnt*sizeof(kvs_key_space_name),
-    names, &valid_cnt);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "List current keyspace failed. error:0x%x.\n", ret);
-    kvs_close_device(*dev);
-    return FAILED;
-  }
-  for (uint8_t idx = 0; idx < valid_cnt; idx++) {
-    kvs_delete_key_space(*dev, &names[idx]);
-  }
-
-  //create key spaces
-  kvs_key_space_name ks_name;
-  kvs_option_key_space option = { KVS_KEY_ORDER_NONE };
-  ks_name.name = keyspace_name;
-  ks_name.name_len = strlen(keyspace_name);
-  //currently size of keyspace is not support specify
-  ret = kvs_create_key_space(*dev, &ks_name, 0, option);
-  if (ret != KVS_SUCCESS) {
-    kvs_close_device(*dev);
-    fprintf(stderr, "Create keyspace failed. error:0x%x.\n", ret);
-    return FAILED;
-  }
-
-  ret = kvs_open_key_space(*dev, keyspace_name, ks_hd);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Open keyspace %s failed. error:0x%x.\n", keyspace_name, ret);
-    kvs_delete_key_space(*dev, &ks_name);
-    kvs_close_device(*dev);
-    return FAILED;
-  }
-
-  kvs_key_space ks_info;
-  ks_info.name = (kvs_key_space_name *)malloc(sizeof(kvs_key_space_name));
-  if(!ks_info.name) {
-    fprintf(stderr, "Malloc resource failed.\n");
-    _env_exit(*dev, keyspace_name, *ks_hd);
-    return FAILED;
-  }
-  ks_info.name->name = (char*)malloc(MAX_CONT_PATH_LEN);
-  if(!ks_info.name->name) {
-    fprintf(stderr, "Malloc resource failed.\n");
-    free(ks_info.name);
-    _env_exit(*dev, keyspace_name, *ks_hd);
-    return FAILED;
-  }
-  ks_info.name->name_len = MAX_CONT_PATH_LEN;
-  ret = kvs_get_key_space_info(*ks_hd, &ks_info);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Get info of keyspace failed. error:0x%x.\n", ret);
-    free(ks_info.name->name);
-    free(ks_info.name);
-    return FAILED;
-  }
-  fprintf(stdout, "Keyspace information get name: %s\n", ks_info.name->name);
-  fprintf(stdout, "open:%d, count:%ld, capacity:%ld, free_size:%ld.\n", 
-    ks_info.opened, ks_info.count, ks_info.capacity, ks_info.free_size);
-  free(ks_info.name->name);
-  free(ks_info.name);
-
-  uint32_t dev_util = 0;
-  uint64_t dev_capa = 0;
-  kvs_get_device_utilization(*dev, &dev_util);
-  kvs_get_device_capacity(*dev, &dev_capa);
-  fprintf(stdout, "Before: Total size is %ld bytes, used is %d\n", dev_capa, dev_util);
-  kvs_device *dev_info = (kvs_device*)malloc(sizeof(kvs_device));
-  if(dev_info) {
-    kvs_get_device_info(*dev, dev_info);
-    fprintf(stdout, "Total size: %.2f GB\nMax value size: %d\nMax key size: %d\nOptimal value size: %d\n",
-    (float)dev_info->capacity/1000/1000/1000, dev_info->max_value_len,
-    dev_info->max_key_len, dev_info->optimal_value_len);
-    free(dev_info);
-  }
-
-  return SUCCESS;
-}
-
-
 int main(int argc, char *argv[]) {
   char* dev_path = NULL;
   int num_ios = 10;
@@ -1575,7 +1398,7 @@ int main(int argc, char *argv[]) {
   uint32_t vlen = 4096;
   //int is_polling = 0;
   int c;
-  int ret = SUCCESS;
+  int ret;
 
   while ((c = getopt(argc, argv, "d:n:q:o:k:v:h")) != -1) {
     switch(c) {
@@ -1611,14 +1434,40 @@ int main(int argc, char *argv[]) {
     usage(argv[0]);
     return SUCCESS;
   }
-  
-  char ks_name[MAX_KEYSPACE_NAME_LEN];
-  snprintf(ks_name, MAX_KEYSPACE_NAME_LEN, "%s", "keyspace_test");
+
   kvs_device_handle dev;
-  kvs_key_space_handle ks_hd;
-  if(_env_init(dev_path, &dev, ks_name, &ks_hd) != SUCCESS)
+  ret = kvs_open_device(dev_path, &dev);
+  if(ret != KVS_SUCCESS) {
+    fprintf(stderr, "Device open failed 0x%x\n", ret);
     return FAILED;
+  }
+  
+  const char* name = "test";
+  kvs_key_space_name ks_name;
+  kvs_option_key_space option = {KVS_KEY_ORDER_ASCEND};
+  ks_name.name = name;
+  ks_name.name_len = strlen(name);
+  
+  kvs_create_key_space(dev, &ks_name, 0, option);
+  
+  kvs_key_space_handle ks_hd;
+  kvs_open_key_space(dev, name, &ks_hd);
+
+  uint32_t dev_util = 0;
+  uint64_t dev_capa = 0;
+  kvs_device *dev_info = (kvs_device*)malloc(sizeof(kvs_device));
 
+  kvs_get_device_utilization(dev, &dev_util);
+
+  kvs_get_device_capacity(dev, &dev_capa);
+  fprintf(stdout, "Before: Total size is %ld bytes, used is %d\n", dev_capa, dev_util);
+
+  kvs_get_device_info(dev, dev_info);
+  fprintf(stdout, "Total size: %.2f GB\nMax value size: %d\nMax key size: %d\nOptimal value size: %d\n",
+    (float)dev_info->capacity/1000/1000/1000, dev_info->max_value_len,
+    dev_info->max_key_len, dev_info->optimal_value_len);
+  free(dev_info);
+  
   switch(op_type) {
   case WRITE_OP:
     perform_insertion(ks_hd, num_ios, qdepth, klen, vlen);
@@ -1641,10 +1490,16 @@ int main(int argc, char *argv[]) {
     break;
   default:
     fprintf(stderr, "Please specify a correct op_type for testing\n");
-    ret = FAILED;
+    return FAILED;
   }
 
-  _env_exit(dev, ks_name, ks_hd);
-  return ret;
+  kvs_get_device_utilization(dev, &dev_util);
+  fprintf(stdout, "After: Total size is %ld bytes, used is %d\n", dev_capa, dev_util);  
+  
+  kvs_close_key_space(ks_hd);
+  kvs_delete_key_space(dev, &ks_name);
+  kvs_close_device(dev);
+
+  return SUCCESS;
 }
 #endif
diff --git a/PDK/core/sample_code/test_sync.cpp b/PDK/core/sample_code/test_sync.cpp
index 6e680f9..d2a890a 100644
--- a/PDK/core/sample_code/test_sync.cpp
+++ b/PDK/core/sample_code/test_sync.cpp
@@ -39,6 +39,8 @@
 #include <queue>
 #include <kvs_api.h>
 
+#include "rdd_cl.h"
+
 #define SUCCESS 0
 #define FAILED 1
 #define WRITE_OP  1
@@ -46,6 +48,11 @@
 #define DELETE_OP 3
 #define ITERATOR_OP 4
 #define KEY_EXIST_OP 5
+#define KEY_LIST_OP 6
+#define KEY_READER_LOCK_UNLOCK_OP 7
+#define KEY_WRITER_LOCK_UNLOCK_OP 8
+#define RDD_READ_OP 9
+#define RDD_WRITE_OP 10
 
 void usage(char *program)
 {
@@ -53,11 +60,15 @@ void usage(char *program)
   printf("usage: %s -d device_path [-n num_ios] [-o op_type] [-k klen] [-v vlen] [-t threads]\n", program);
   printf("-d      device_path  :  kvssd device path. e.g. emul: /dev/kvemul; kdd: /dev/nvme0n1; udd: 0000:06:00.0\n");
   printf("-n      num_ios      :  total number of ios (ignore this for iterator)\n");
-  printf("-o      op_type      :  1: write; 2: read; 3: delete; 4: iterator;"
-                                " 5: key exist check;\n");
-  printf("-k      klen         :  key length (ignore this for iterator)\n");
+  printf("-o      op_type      :  1: write; 2: read; 3: delete; 4: iterator; 5: key exist check 6: list, 7: read lock/unlock test, 8: write lock/unlock, 9: RDMA direct read test, 10: RDMA direct write test\n");
+  printf("-k      klen         :  key length (ignore this for list and iterator)\n");
   printf("-v      vlen         :  value length (ignore this for iterator)\n");
+  printf("-p      pfx_key      :  target list: use this key as prefix\n");
+  printf("-l      pfx_keylen  :  target list: prefix key length\n");
+  printf("-m      max_keys     :  max keys to list from target\n");
+  printf("-a      start_key    :  target list: use this key as start key\n");
   printf("-t      threads      :  number of threads\n");
+  printf("-r      ip_addr      :  IP Address to connect for RDMA Direct connections\n");
   printf("==============\n");
 }
 
@@ -65,6 +76,11 @@ void usage(char *program)
 
 #ifdef SAMSUNG_API
 
+char *g_rdd_ip = NULL;
+char *g_rdd_port = (char *)"1234";
+struct rdd_client_ctx_s *g_rdd_cl_ctx = NULL;
+rdd_cl_conn_ctx_t *g_rdd_conn = NULL;
+
 static int use_udd = 0;
 
 struct iterator_info{
@@ -81,6 +97,10 @@ struct thread_args{
   int count;
   int op_type;
   kvs_container_handle cont_hd;
+  char *pfx_key;
+  kvs_key_t pfx_keylen;
+  uint16_t max;
+  char *start_key;
 };
 
 void print_iterator_keyvals(kvs_iterator_list *iter_list, kvs_iterator_option g_iter_mode){
@@ -154,7 +174,7 @@ int perform_iterator(kvs_container_handle cont_hd,
   char prefix_str[5] = "0000";
   unsigned int PREFIX_KV = 0;
   for (int i = 0; i < 4; i++){
-    PREFIX_KV |= (prefix_str[i] << (3-i)*8);
+    PREFIX_KV |= (prefix_str[i] << i*8);
   }
 
   iter_ctx_open.bit_pattern = PREFIX_KV;
@@ -273,6 +293,86 @@ int perform_read(int id, kvs_container_handle cont_hd, int count, kvs_key_t klen
   return SUCCESS;
 }
 
+int perform_rdd_read(int id, kvs_container_handle cont_hd, int count, kvs_key_t klen, uint32_t vlen) {
+  int ret;
+  char *key   = (char*)kvs_malloc(klen, 4096);
+  char *value = (char*)kvs_malloc(vlen, 4096);
+  if(key == NULL || value == NULL) {
+    fprintf(stderr, "failed to allocate\n");
+    return FAILED;
+  }
+
+  int start_key = id * count;
+  for (int i = start_key; i < start_key + count; i++) {
+    memset(value, 0, vlen);
+    sprintf(key, "%0*d", klen - 1, i);
+    kvs_retrieve_option option;
+    memset(&option, 0, sizeof(kvs_retrieve_option));
+    option.kvs_retrieve_decompress = false;
+    option.kvs_retrieve_delete = false;
+
+    const kvs_retrieve_context ret_ctx = {option, 0, 0};
+    const kvs_key  kvskey = {key, klen };
+    kvs_value kvsvalue = { value, vlen , 0, 0 /*offset */};
+
+    struct ibv_mr *mr = rdd_cl_conn_get_mr(g_rdd_conn, value, vlen);
+    ret = kvs_retrieve_tuple_direct(cont_hd, &kvskey, &kvsvalue, mr->rkey, g_rdd_conn->qhandle, &ret_ctx);
+    if(mr) rdd_cl_conn_put_mr(mr);
+
+    if(ret != KVS_SUCCESS) {
+      fprintf(stderr, "retrieve tuple direct %s failed with error 0x%x - %s\n", key, ret, kvs_errstr(ret));
+    } else {
+
+    }
+  }
+
+  if(key) kvs_free(key);
+  if(value) kvs_free(value);
+
+  return SUCCESS;
+}
+
+int perform_rdd_insert(int id, kvs_container_handle cont_hd, int count, kvs_key_t klen, uint32_t vlen) {
+  int ret;
+  char *key   = (char*)kvs_malloc(klen, 4096);
+  char *value = (char*)kvs_malloc(vlen, 4096);
+  if(key == NULL || value == NULL) {
+    fprintf(stderr, "failed to allocate\n");
+    return FAILED;
+  }
+
+  int start_key = id * count;
+  for (int i = start_key; i < start_key + count; i++) {
+    memset(value, 0, vlen);
+    sprintf(key, "%0*d", klen - 1, i);
+    sprintf(value, "%0*d", klen - 1, i + 10);
+
+    kvs_store_option option;
+    option.st_type = KVS_STORE_POST;
+    option.kvs_store_compress = false;
+
+    kvs_store_context put_ctx = {option, 0, 0};
+
+    const kvs_key  kvskey = {key, klen };
+    kvs_value kvsvalue = { value, vlen , 0, 0 /*offset */};
+
+    struct ibv_mr *mr = rdd_cl_conn_get_mr(g_rdd_conn, value, vlen);
+    ret = kvs_store_tuple_direct(cont_hd, &kvskey, &kvsvalue, mr->rkey, g_rdd_conn->qhandle, &put_ctx);
+    if(mr) rdd_cl_conn_put_mr(mr);
+
+    if(ret != KVS_SUCCESS) {
+      fprintf(stderr, "put tuple direct %s failed with error 0x%x - %s\n", key, ret, kvs_errstr(ret));
+    } else {
+
+    }
+  }
+
+  if(key) kvs_free(key);
+  if(value) kvs_free(value);
+
+  return SUCCESS;
+}
+
 
 int perform_insertion(int id, kvs_container_handle cont_hd, int count, kvs_key_t klen, uint32_t vlen) {
 
@@ -375,17 +475,149 @@ int perform_key_exist(int id, kvs_container_handle cont_hd, int count, kvs_key_t
   return SUCCESS;
 }
 
+int perform_lock_unlock(int id, kvs_container_handle cont_hd, int count, kvs_key_t klen, bool is_writer) {
 
-void do_io(int id, kvs_container_handle cont_hd, int count, kvs_key_t klen, uint32_t vlen, int op_type) {
+  char *key  = (char*)kvs_malloc(klen, 4096);
+  
+  if(key == NULL) {
+    fprintf(stderr, "failed to allocate\n");
+    return FAILED;
+  }
+
+  int start_key = id * count;  
+  for(int i = start_key; i < start_key + count; i++) {
+    sprintf(key, "%0*d", klen - 1, i);
+    const kvs_key  kvskey = { key, klen};
+    
+    const kvs_lock_context lock_ctx = { {!is_writer/*rw*/,false/*block*/,0/*prio*/,100/*dur*/ }, 0, 0};
+    int ret = kvs_lock_tuple(cont_hd, &kvskey, 1234/*uuid*/,&lock_ctx);
+    if(ret != KVS_SUCCESS) {
+      fprintf(stderr, "lock tuple failed with error 0x%x - %s\n", ret, kvs_errstr(ret));
+      kvs_free(key);
+      return FAILED;
+    } else {
+      fprintf(stderr, "lock key %s done \n", key);
+    }
+  }
+
+  for(int i = start_key; i < start_key + count; i++) {
+    sprintf(key, "%0*d", klen - 1, i);
+    const kvs_key  kvskey = { key, klen};
+    
+    const kvs_lock_context lock_ctx = { {!is_writer/*rw*/,false/*block*/,0/*prio*/,100/*dur*/ }, 0, 0};
+    int ret = kvs_unlock_tuple(cont_hd, &kvskey, 1234/*uuid*/,&lock_ctx);
+    if(ret != KVS_SUCCESS) {
+      fprintf(stderr, "unlock tuple failed with error 0x%x - %s\n", ret, kvs_errstr(ret));
+      kvs_free(key);
+      return FAILED;
+    } else {
+      fprintf(stderr, "unlock key %s done \n", key);
+    }
+  }
+
+  if(key) kvs_free(key);
+  return SUCCESS;
+}
+
+int perform_list(int id, kvs_container_handle ks_hd, int count, char *prefix_key, kvs_key_t plen, char *start_key, kvs_key_t slen, uint32_t vlen, uint16_t max) {
+    int ret = KVS_SUCCESS;
+    char *value = NULL;
+    kvs_key *kvskey_prefix = NULL, *kvskey_start = NULL;
+    kvs_key kvspkey = {0, 0};
+    kvs_key kvsskey = {0, 0};
+    kvs_list_context ctx = {0,0};
+
+    value = (char*)kvs_malloc(vlen, 4096);
+    if(value == NULL) {
+      fprintf(stderr, "failed to allocate\n");
+      return FAILED;
+    }
+
+    memset(value, 0, vlen);
+    kvs_value kvsvalue = {value, vlen, 0, 0};
+
+    if(plen){
+        kvspkey = {prefix_key, plen};
+        kvskey_prefix = &kvspkey;
+    }
+    if(slen){
+        kvsskey = {start_key, slen};
+        kvskey_start = &kvsskey;
+    }
+
+    int max_keys_listed = 0;
+    do {
+        uint32_t lkey_len;
+        uint32_t lnr_keys;
+        int count = 0;
+
+        ret = kvs_list_tuple(ks_hd, kvskey_prefix, kvskey_start, max, &kvsvalue, &ctx);
+        if(ret != KVS_SUCCESS) {
+            fprintf(stderr, "list tuple failed with error 0x%x\n", ret);
+            break;
+        }
+
+        if (kvsvalue.actual_value_size == 0) {
+            fprintf(stderr, "list tuple completed with UNEXPECTED zero length listing 0x%x\n", ret);
+            break;
+        }
+
+        uint32_t *value_buffer = (uint32_t *)((void *)kvsvalue.value + kvsvalue.offset);
+        lnr_keys = *value_buffer;
+        value_buffer += 1; /* number of keys field consumed already*/
+        count = 0;
+        max_keys_listed += lnr_keys;
+        while(lnr_keys){
+            lkey_len = *value_buffer;
+            value_buffer += 1; /* key length field consumed already*/
+            printf("Key%d: %*s len:%d\n", count++, lkey_len, (char*)value_buffer, lkey_len);
+            uint32_t k_dw = (lkey_len + 4 - 1) / 4; /* round to 4bytes */
+            if(--lnr_keys)
+                value_buffer += k_dw;
+        }
+
+        if(!lkey_len || lkey_len > 255){
+            fprintf(stderr, "list tuple failed with INVALID key length :%d status :0x%x\n", lkey_len);
+        }
+
+        kvskey_prefix = NULL;
+        if(plen) {
+            kvspkey = {prefix_key, plen};
+            kvskey_prefix = &kvspkey;
+        }
+
+        kvskey_start = NULL;
+        if(lkey_len) {
+            kvsskey = {(void*)value_buffer, lkey_len};
+            kvskey_start = &kvsskey;
+        }
+        kvsvalue = {value, vlen, 0, 0};
+    } while (ret == KVS_SUCCESS && max_keys_listed < max);
+
+  printf("Total number of key(s) listed :%d, Final status :0x%x\n", max_keys_listed, ret);
+
+  if(value) kvs_free(value);
+
+  return SUCCESS;
+}
+
+void do_io(int id, kvs_container_handle cont_hd, int count, uint16_t klen, uint32_t vlen, int op_type, char *prefix_key, uint16_t pfx_klen, char *start_key, uint16_t max) {
 
   switch(op_type) {
   case WRITE_OP:
     perform_insertion(id, cont_hd, count, klen, vlen);
     break;
+  case RDD_WRITE_OP:
+    perform_rdd_insert(id, cont_hd, count, klen, vlen);
+    break;
+
   case READ_OP:
     //perform_insertion(id, cont_hd, count, klen, vlen);
     perform_read(id, cont_hd, count, klen, vlen);
     break;
+  case RDD_READ_OP:
+    perform_rdd_read(id, cont_hd, count, klen, vlen);
+    break;
   case DELETE_OP:
     //perform_insertion(id, cont_hd, count, klen, vlen);
     perform_delete(id, cont_hd, count, klen, vlen);
@@ -400,6 +632,15 @@ void do_io(int id, kvs_container_handle cont_hd, int count, kvs_key_t klen, uint
     //perform_insertion(id, cont_hd, count, klen, vlen);
     perform_key_exist(id, cont_hd, count, klen, vlen);
     break;
+  case KEY_LIST_OP:
+    perform_list(id, cont_hd, count, prefix_key, pfx_klen, start_key, (start_key ? strlen(start_key) : 0), vlen, max);
+	break;
+  case KEY_READER_LOCK_UNLOCK_OP:
+	perform_lock_unlock(id, cont_hd, count, klen, false);
+    break;
+  case KEY_WRITER_LOCK_UNLOCK_OP:
+	perform_lock_unlock(id, cont_hd, count, klen, true);
+    break;
   default:
     fprintf(stderr, "Please specify a correct op_type for testing\n");
     return;
@@ -410,119 +651,23 @@ void do_io(int id, kvs_container_handle cont_hd, int count, kvs_key_t klen, uint
 void *iothread(void *args)
 {
   thread_args *targs = (thread_args *)args;
-  do_io(targs->id, targs->cont_hd, targs->count, targs->klen, targs->vlen, targs->op_type);
+  do_io(targs->id, targs->cont_hd, targs->count, targs->klen, targs->vlen, targs->op_type, targs->pfx_key, targs->pfx_keylen, targs->start_key, targs->max);
   return 0;
 }
 
-int _env_exit(kvs_device_handle dev, const char* cont_name,
-  kvs_container_handle cont_handle) {
-  int32_t dev_util = 0;
-  kvs_get_device_utilization(dev, &dev_util);
-  fprintf(stdout, "After: Total used is %d\n", dev_util);
-  kvs_close_container(cont_handle);
-  kvs_delete_container(dev, cont_name);
-  kvs_close_device(dev);
-  kvs_result ret = kvs_exit_env();
-  return ret;
-}
-
-int _env_init(kvs_init_options* options, char* dev_path, kvs_device_handle* dev,
-  const char *cont_name, kvs_container_handle* cont_handle) {
-  // initialize the environment
-  kvs_init_env(options);
-  kvs_result ret = kvs_open_device(dev_path, dev);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Device open failed\n");
-    return FAILED;
-  }
-
-  //container list before create "test"
-  uint32_t valid_cnt = 0;
-  const uint32_t retrieve_cnt = 2;
-  kvs_container_name names[retrieve_cnt];
-  for(uint8_t idx = 0; idx < retrieve_cnt; idx++) {
-    names[idx].name_len = MAX_CONT_PATH_LEN;
-    names[idx].name = (char*)malloc(MAX_CONT_PATH_LEN);
-  }
-  ret = kvs_list_containers(*dev, 1, retrieve_cnt*sizeof(kvs_container_name),
-    names, &valid_cnt);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "List current containers failed. error:0x%x.\n", ret);
-    kvs_close_device(*dev);
-    return FAILED;
-  }
-  for (uint8_t idx = 0; idx < valid_cnt; idx++) {
-    kvs_delete_container(*dev, names[idx].name);
-  }
-
-  kvs_container_context ctx;
-  // Initialize key order to KVS_KEY_ORDER_NONE
-  ctx.option.ordering = KVS_KEY_ORDER_NONE;
-  ret = kvs_create_container(*dev, cont_name, 0, &ctx);
-  if (ret != KVS_SUCCESS) {
-    fprintf(stderr, "Create containers failed. error:0x%x.\n", ret);
-    kvs_close_device(*dev);
-    return FAILED;
-  }
-
-
-  ret = kvs_open_container(*dev, cont_name, cont_handle);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Open containers %s failed. error:0x%x.\n", cont_name, ret);
-    kvs_delete_container(*dev, cont_name);
-    kvs_close_device(*dev);
-    return FAILED;
-  }
-
-  char *name_buff = (char*)malloc(MAX_CONT_PATH_LEN);
-  kvs_container_name name = {MAX_CONT_PATH_LEN, name_buff};
-  kvs_container cont = {false, 0, 0, 0, 0, &name};
-  ret = kvs_get_container_info(*cont_handle, &cont);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Get info of containers %s failed. error:0x%x.\n", name.name, ret);
-    _env_exit(*dev, cont_name, *cont_handle);
-    return FAILED;
-  }
-
-  fprintf(stdout, "Container information get name: %s\n", cont.name->name);
-  fprintf(stdout, "open:%d, scale:%d, capacity:%ld, free_size:%ld count:%ld.\n", 
-    cont.opened, cont.scale, cont.capacity, cont.free_size, cont.count);
-  free(name_buff);
-
-  int32_t dev_util = 0;
-  int64_t dev_capa = 0;
-  kvs_get_device_utilization(*dev, &dev_util);
-
-  float waf = 0.0;
-  kvs_get_device_waf(*dev, &waf);
-  kvs_get_device_capacity(*dev, &dev_capa);
-  fprintf(stdout, "Before: Total size is %ld bytes, used is %d, waf is %.2f\n", dev_capa, dev_util, waf);
-  
-  kvs_device *dev_info = (kvs_device*)malloc(sizeof(kvs_device));
-  if(dev_info){
-    kvs_get_device_info(*dev, dev_info);
-    fprintf(stdout, "Total size: %.2f GB\nMax value size: %d\nMax key size: %d\n"
-      "Optimal value size: %d\n", (float)dev_info->capacity/1000/1000/1000,
-    dev_info->max_value_len, dev_info->max_key_len, dev_info->optimal_value_len);
-    free(dev_info);
-  }else{
-    fprintf(stderr, "dev_info malloc failed\n");
-    _env_exit(*dev, cont_name, *cont_handle);
-    return FAILED;
-  }
-
-  return SUCCESS;
-}
 
 int main(int argc, char *argv[]) {
-  char* dev_path = NULL;
+  char *dev_path = NULL, *key = NULL, *prefix = NULL;
   int num_ios = 10;
   int op_type = 1;
   kvs_key_t klen = 16;
+  kvs_key_t pfx_klen = 0;
   uint32_t vlen = 4096;
   int ret, c, t = 1;
+  uint16_t max_keys = 0;
+  rdd_cl_conn_params_t rdd_params;
 
-  while ((c = getopt(argc, argv, "d:n:o:k:v:t:h")) != -1) {
+  while ((c = getopt(argc, argv, "d:n:o:k:v:t:p:l:m:a:r:h")) != -1) {
     switch(c) {
     case 'd':
       dev_path = optarg;
@@ -542,6 +687,21 @@ int main(int argc, char *argv[]) {
     case 't':
       t = atoi(optarg);
       break;
+    case 'p':
+      prefix = optarg;
+      break;
+    case 'l':
+      pfx_klen = atoi(optarg);
+      break;
+    case 'm':
+      max_keys = atoi(optarg);
+      break;
+    case 'a':
+      key = optarg;
+      break;
+	case 'r':
+		g_rdd_ip = optarg;
+		break;
     case 'h':
       usage(argv[0]);
       return SUCCESS;
@@ -557,11 +717,39 @@ int main(int argc, char *argv[]) {
     return SUCCESS;
   }
 
+  if ((op_type == RDD_READ_OP) || (op_type == RDD_WRITE_OP)) {
+    if(g_rdd_ip) {
+      rdd_cl_ctx_params_t param = {RDD_PD_GLOBAL};
+      g_rdd_cl_ctx = rdd_cl_init(param);
+      if(!g_rdd_cl_ctx) {
+        fprintf(stderr, "Could not initialize rdd library\n");
+        return SUCCESS;
+      }
+
+      rdd_params.ip = g_rdd_ip;
+      rdd_params.port = g_rdd_port;
+
+      g_rdd_conn = rdd_cl_create_conn(g_rdd_cl_ctx, rdd_params);
+      if(!g_rdd_conn) {
+        fprintf(stderr, "Could not establish direct data connection\n");
+        return SUCCESS;
+      }
+    } else {
+      fprintf(stderr, "RDMA direct needs IP to connect in -r option\n");
+      usage(argv[0]);
+      return SUCCESS;
+    }
+  }
+
   if(op_type == ITERATOR_OP && t > 1) {
     fprintf(stdout, "Iterator only supports single thread \n");
     return FAILED;
   }
   
+  if(prefix && !pfx_klen) {
+    fprintf(stdout, "Target LIST: prefix length not specified, using strlen\n");
+    pfx_klen = strlen(prefix);
+  }
   kvs_init_options options;
   kvs_init_env_opts(&options);
 
@@ -594,11 +782,21 @@ int main(int argc, char *argv[]) {
     options.memory.use_dpdk = 0;
   }
 
-  const char *cont_name = "test";
+  // initialize the environment
+  kvs_init_env(&options);
+
   kvs_device_handle dev;
-  kvs_container_handle cont_handle;
-  if(_env_init(&options, dev_path, &dev, cont_name, &cont_handle) != SUCCESS)
+  ret = kvs_open_device(dev_path, &dev);
+  if(ret != KVS_SUCCESS) {
+    fprintf(stderr, "Device open failed\n");
     return FAILED;
+  }
+
+  kvs_container_context ctx;
+  kvs_create_container(dev, "test", 4, &ctx);
+
+  kvs_container_handle cont_handle;
+  kvs_open_container(dev, "test", &cont_handle);
 
   thread_args args[t];
   pthread_t tid[t];
@@ -613,6 +811,10 @@ int main(int argc, char *argv[]) {
     args[i].count = num_ios;
     args[i].cont_hd = cont_handle;
     args[i].op_type = op_type;
+    args[i].pfx_key = prefix;
+    args[i].pfx_keylen = pfx_klen;
+    args[i].max = max_keys;
+    args[i].start_key = key;
     pthread_attr_t *attr = (pthread_attr_t *)malloc(sizeof(pthread_attr_t));
     cpu_set_t cpus;
     pthread_attr_init(attr);
@@ -642,8 +844,10 @@ int main(int argc, char *argv[]) {
     double sec = (double)(end - start) / 1000000000L;
     fprintf(stdout, "Total time %.2f sec; Throughput %.2f ops/sec\n", sec, (double) num_ios * t /sec );
   }
-
-  ret = _env_exit(dev, cont_name, cont_handle);
+  kvs_close_container(cont_handle);
+  kvs_delete_container(dev, "test");
+  kvs_exit_env();
+  
   return SUCCESS;
 }
 
@@ -766,6 +970,7 @@ int perform_key_exist(int id, kvs_key_space_handle ks_hd, int count, uint16_t kl
     fprintf(stderr, "failed to allocate\n");
     return FAILED;
   }
+  uint8_t status = 0;
   int start_key = id * count;
   uint8_t exist_status;
   kvs_exist_list list;
@@ -801,15 +1006,20 @@ int perform_iterator(kvs_key_space_handle ks_hd,
   /* Open iterator */
   kvs_key_group_filter iter_fltr;
 
-  iter_fltr.bitmask[0] = 0xff;
-  iter_fltr.bitmask[1] = 0xff;
-  iter_fltr.bitmask[2] = 0;
-  iter_fltr.bitmask[3] = 0;
+  iter_fltr.bitmask[0] = 0xffff0000 & 0xff;
+  iter_fltr.bitmask[1] = 0xffff0000 & 0xff00 >> 8;
+  iter_fltr.bitmask[2] = 0xffff0000 & 0xff0000 >> 16;
+  iter_fltr.bitmask[3] = 0xffff0000 & 0xff000000 >> 24;
+  char prefix_str[5] = "0000";
+  unsigned int PREFIX_KV = 0;
+  for (int i = 0; i < 4; i++){
+    PREFIX_KV |= (prefix_str[i] << i*8);
+  }
 
-  iter_fltr.bit_pattern[0] = '0';
-  iter_fltr.bit_pattern[1] = '0';
-  iter_fltr.bit_pattern[2] = '0';
-  iter_fltr.bit_pattern[3] = '0';
+  iter_fltr.bit_pattern[0] = PREFIX_KV & 0xff;
+  iter_fltr.bit_pattern[1] = PREFIX_KV & 0xff00 >> 8;
+  iter_fltr.bit_pattern[2] = PREFIX_KV & 0xff0000 >> 16;
+  iter_fltr.bit_pattern[3] = PREFIX_KV & 0xff000000 >> 24;
 
   ret = kvs_create_iterator(ks_hd, &(iter_info->g_iter_mode), &iter_fltr, &(iter_info->iter_handle));
   if(ret != KVS_SUCCESS) {
@@ -912,116 +1122,8 @@ void *iothread(void *args)
   return NULL;
 }
 
-int _env_exit(kvs_device_handle dev, char* keyspace_name,
-  kvs_key_space_handle ks_hd) {
-  uint32_t dev_util = 0;
-  kvs_get_device_utilization(dev, &dev_util);
-  fprintf(stdout, "After: Total used is %d\n", dev_util);  
-  kvs_close_key_space(ks_hd);
-  kvs_key_space_name ks_name;
-  ks_name.name_len = strlen(keyspace_name) + 1;
-  ks_name.name = keyspace_name;
-  kvs_delete_key_space(dev, &ks_name);
-  kvs_result ret = kvs_close_device(dev);
-  return ret;
-}
-
-int _env_init(char* dev_path, kvs_device_handle* dev, char *keyspace_name,
-  kvs_key_space_handle* ks_hd) {
-  kvs_result ret = kvs_open_device(dev_path, dev);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Device open failed 0x%x\n", ret);
-    return FAILED;
-  }
-
-  //keyspace list after create "test"
-  const uint32_t retrieve_cnt = 2;
-  kvs_key_space_name names[retrieve_cnt];
-  for(uint8_t idx = 0; idx < retrieve_cnt; idx++) {
-    names[idx].name_len = MAX_KEYSPACE_NAME_LEN;
-    names[idx].name = (char*)malloc(MAX_KEYSPACE_NAME_LEN);
-  }
-
-  uint32_t valid_cnt = 0;
-  ret = kvs_list_key_spaces(*dev, 1, retrieve_cnt*sizeof(kvs_key_space_name),
-    names, &valid_cnt);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "List current keyspace failed. error:0x%x.\n", ret);
-    kvs_close_device(*dev);
-    return FAILED;
-  }
-  for (uint8_t idx = 0; idx < valid_cnt; idx++) {
-    kvs_delete_key_space(*dev, &names[idx]);
-  }
-
-  //create key spaces
-  kvs_key_space_name ks_name;
-  kvs_option_key_space option = { KVS_KEY_ORDER_NONE };
-  ks_name.name = keyspace_name;
-  ks_name.name_len = strlen(keyspace_name);
-  //currently size of keyspace is not support specify
-  ret = kvs_create_key_space(*dev, &ks_name, 0, option);
-  if (ret != KVS_SUCCESS) {
-    kvs_close_device(*dev);
-    fprintf(stderr, "Create keyspace failed. error:0x%x.\n", ret);
-    return FAILED;
-  }
-
-  ret = kvs_open_key_space(*dev, keyspace_name, ks_hd);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Open keyspace %s failed. error:0x%x.\n", keyspace_name, ret);
-    kvs_delete_key_space(*dev, &ks_name);
-    kvs_close_device(*dev);
-    return FAILED;
-  }
-
-  kvs_key_space ks_info;
-  ks_info.name = (kvs_key_space_name *)malloc(sizeof(kvs_key_space_name));
-  if(!ks_info.name) {
-    fprintf(stderr, "Malloc resource failed.\n");
-    _env_exit(*dev, keyspace_name, *ks_hd);
-    return FAILED;
-  }
-  ks_info.name->name = (char*)malloc(MAX_CONT_PATH_LEN);
-  if(!ks_info.name->name) {
-    fprintf(stderr, "Malloc resource failed.\n");
-    free(ks_info.name);
-    _env_exit(*dev, keyspace_name, *ks_hd);
-    return FAILED;
-  }
-  ks_info.name->name_len = MAX_CONT_PATH_LEN;
-  ret = kvs_get_key_space_info(*ks_hd, &ks_info);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Get info of keyspace failed. error:0x%x.\n", ret);
-    free(ks_info.name->name);
-    free(ks_info.name);
-    return FAILED;
-  }
-  fprintf(stdout, "Keyspace information get name: %s\n", ks_info.name->name);
-  fprintf(stdout, "open:%d, count:%ld, capacity:%ld, free_size:%ld.\n", 
-    ks_info.opened, ks_info.count, ks_info.capacity, ks_info.free_size);
-  free(ks_info.name->name);
-  free(ks_info.name);
-
-  uint32_t dev_util = 0;
-  uint64_t dev_capa = 0;
-  kvs_get_device_utilization(*dev, &dev_util);
-  kvs_get_device_capacity(*dev, &dev_capa);
-  fprintf(stdout, "Before: Total size is %ld bytes, used is %d\n", dev_capa, dev_util);
-  kvs_device *dev_info = (kvs_device*)malloc(sizeof(kvs_device));
-  if(dev_info) {
-    kvs_get_device_info(*dev, dev_info);
-    fprintf(stdout, "Total size: %.2f GB\nMax value size: %d\nMax key size: %d\nOptimal value size: %d\n",
-    (float)dev_info->capacity/1000/1000/1000, dev_info->max_value_len,
-    dev_info->max_key_len, dev_info->optimal_value_len);
-    free(dev_info);
-  }
-
-  return SUCCESS;
-}
-
 int main(int argc, char *argv[]) {
-  char* dev_path = NULL;
+  char *dev_path = NULL;
   int num_ios = 10;
   int op_type = 1;
   uint16_t klen = 16;
@@ -1067,13 +1169,24 @@ int main(int argc, char *argv[]) {
     fprintf(stdout, "Iterator only supports single thread \n");
     return FAILED;
   }
-    
-  char ks_name[MAX_KEYSPACE_NAME_LEN];
-  snprintf(ks_name, MAX_KEYSPACE_NAME_LEN, "%s", "keyspace_test");
+  
   kvs_device_handle dev;
-  kvs_key_space_handle ks_hd;
-  if(_env_init(dev_path, &dev, ks_name, &ks_hd) != SUCCESS)
+  ret = kvs_open_device(dev_path, &dev);
+  if(ret != KVS_SUCCESS) {
+    fprintf(stderr, "Device open failed\n");
     return FAILED;
+  }
+
+  const char* name = "test";
+  kvs_key_space_name ks_name;
+  kvs_option_key_space option = {KVS_KEY_ORDER_ASCEND};
+  ks_name.name = name;
+  ks_name.name_len = strlen(name);
+  
+  kvs_create_key_space(dev, &ks_name, 0, option);
+
+  kvs_key_space_handle ks_hd;
+  kvs_open_key_space(dev, name, &ks_hd);
 
   thread_args args[t];
   pthread_t tid[t];
@@ -1117,9 +1230,10 @@ int main(int argc, char *argv[]) {
     double sec = (double)(end - start) / 1000000000L;
     fprintf(stdout, "Total time %.2f sec; Throughput %.2f ops/sec\n", sec, (double) num_ios * t /sec );
   }
-
-  ret = _env_exit(dev, ks_name, ks_hd);
-  return ret;
+  kvs_close_key_space(ks_hd);
+  kvs_delete_key_space(dev, &ks_name);
+  
+  return SUCCESS;
 }
 
 #endif
diff --git a/PDK/core/src/api/include/private/kvemul.hpp b/PDK/core/src/api/include/private/kvemul.hpp
index 255178a..7330156 100644
--- a/PDK/core/src/api/include/private/kvemul.hpp
+++ b/PDK/core/src/api/include/private/kvemul.hpp
@@ -78,25 +78,27 @@ public:
   virtual ~KvEmulator();
   virtual int32_t init(const char*devpath, const char* configfile, int queuedepth, int is_polling) override;
   virtual int32_t process_completions(int max) override;
-  virtual int32_t store_tuple(kvs_container_handle cont_hd, const kvs_key *key, const kvs_value *value, kvs_store_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
-  virtual int32_t retrieve_tuple(kvs_container_handle cont_hd, const kvs_key *key, kvs_value *value, kvs_retrieve_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
-  virtual int32_t delete_tuple(kvs_container_handle cont_hd, const kvs_key *key, kvs_delete_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
-  virtual int32_t exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
-  virtual int32_t open_iterator(kvs_container_handle cont_hd, kvs_iterator_option option, uint32_t bitmask, uint32_t bit_pattern, kvs_iterator_handle *iter_hd) override;
-  virtual int32_t iterator_next(kvs_container_handle cont_hd, kvs_iterator_handle hiter, kvs_iterator_list *iter_list, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
-  virtual int32_t close_iterator(kvs_container_handle cont_hd, kvs_iterator_handle hiter) override;
-  virtual int32_t close_iterator_all(kvs_container_handle cont_hd) override;
-  virtual int32_t list_iterators(kvs_container_handle cont_hd, kvs_iterator_info *kvs_iters, uint32_t count) override;
+  virtual int32_t store_tuple(int contid, const kvs_key *key, const kvs_value *value, kvs_store_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t retrieve_tuple(int contid, const kvs_key *key, kvs_value *value, kvs_retrieve_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t delete_tuple(int contid, const kvs_key *key, kvs_delete_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t exist_tuple(int contid, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t open_iterator(int contid, kvs_iterator_option option, uint32_t bitmask, uint32_t bit_pattern, kvs_iterator_handle *iter_hd) override;
+  virtual int32_t iterator_next(kvs_iterator_handle hiter, kvs_iterator_list *iter_list, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t close_iterator(int contid, kvs_iterator_handle hiter) override;
+  virtual int32_t close_iterator_all(int contid) override;
+  virtual int32_t list_iterators(int contid, kvs_iterator_info *kvs_iters, uint32_t count) override;
   virtual float get_waf() override;
   virtual int32_t get_used_size(int32_t *dev_util)override;
   virtual int32_t get_total_size(int64_t *dev_capa) override;
   virtual int32_t get_device_info(kvs_device *dev_info) override;
+  #ifdef KVS_REMOTE
+    virtual int32_t lock_tuple(int contid, const kvs_key *key, uint64_t instance_uuid, kvs_lock_option option, void *private1=NULL, void *private2=NULL, bool syncio = false, kvs_callback_function cbfn = NULL) { return -1; } //Not supported
+    virtual int32_t unlock_tuple(int contid, const kvs_key *key, uint64_t instance_uuid, kvs_lock_option option, void *private1=NULL, void *private2=NULL, bool syncio = false, kvs_callback_function cbfn=NULL) { return -1; } //Not supported
+  #endif
   
 private:
-  void wait_for_io(kv_emul_context *ctx);
-  int32_t trans_store_cmd_opt(kvs_store_option kvs_opt, kv_store_option *kv_opt);
   int create_queue(int qdepth, uint16_t qtype, kv_queue_handle *handle, int cqid, int is_polling);
-  kv_emul_context* prep_io_context(int opcode, kvs_container_handle cont_hd, const kvs_key *key, const kvs_value *value, void *private1, void *private2, bool syncio, kvs_callback_function cbfn);
+  kv_emul_context* prep_io_context(int opcode, int contid, const kvs_key *key, const kvs_value *value, void *private1, void *private2, bool syncio, kvs_callback_function cbfn);
   bool ispersist;
   std::string datapath;
 };
diff --git a/PDK/core/src/api/include/private/kvkdd.hpp b/PDK/core/src/api/include/private/kvkdd.hpp
index a8e07e6..a58bdca 100644
--- a/PDK/core/src/api/include/private/kvkdd.hpp
+++ b/PDK/core/src/api/include/private/kvkdd.hpp
@@ -80,32 +80,38 @@ public:
   virtual ~KDDriver();
   virtual int32_t init(const char*devpath, const char* configfile, int queuedepth, int is_polling) override;
   virtual int32_t process_completions(int max) override;
-  virtual int32_t store_tuple(kvs_container_handle cont_hd, const kvs_key *key, const kvs_value *value, kvs_store_option option /*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
-  virtual int32_t retrieve_tuple(kvs_container_handle cont_hd, const kvs_key *key, kvs_value *value, kvs_retrieve_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
-  virtual int32_t delete_tuple(kvs_container_handle cont_hd, const kvs_key *key, kvs_delete_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
-  virtual int32_t exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
-  virtual int32_t open_iterator(kvs_container_handle cont_hd, kvs_iterator_option option, uint32_t bitmask, uint32_t bit_pattern, kvs_iterator_handle *iter_hd) override;
-  virtual int32_t close_iterator(kvs_container_handle cont_hd, kvs_iterator_handle hiter);
-  virtual int32_t close_iterator_all(kvs_container_handle cont_hd);
-  virtual int32_t list_iterators(kvs_container_handle cont_hd, kvs_iterator_info *kvs_iters, uint32_t count);
-  virtual int32_t iterator_next(kvs_container_handle cont_hd, kvs_iterator_handle hiter, kvs_iterator_list *iter_list, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL);
+  virtual int32_t store_tuple(int contid, const kvs_key *key, const kvs_value *value, kvs_store_option option /*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t retrieve_tuple(int contid, const kvs_key *key, kvs_value *value, kvs_retrieve_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t delete_tuple(int contid, const kvs_key *key, kvs_delete_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t exist_tuple(int contid, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t open_iterator(int contid, kvs_iterator_option option, uint32_t bitmask, uint32_t bit_pattern, kvs_iterator_handle *iter_hd) override;
+  virtual int32_t close_iterator(int contid, kvs_iterator_handle hiter);
+  virtual int32_t close_iterator_all(int contid);
+  virtual int32_t list_iterators(int contid, kvs_iterator_info *kvs_iters, uint32_t count);
+  virtual int32_t iterator_next(kvs_iterator_handle hiter, kvs_iterator_list *iter_list, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL);
   virtual float get_waf() override;
   virtual int32_t get_used_size(int32_t *dev_util) override;
   virtual int32_t get_total_size(int64_t *dev_capa) override;
   virtual int32_t get_device_info(kvs_device *dev_info) override;
   void _kv_callback_thread();
+  #ifdef KVS_REMOTE
+    virtual int32_t lock_tuple(int contid, const kvs_key *key, uint64_t instance_uuid, kvs_lock_option option, void *private1=NULL, void *private2=NULL, bool syncio = false, kvs_callback_function cbfn = NULL);
+    virtual int32_t unlock_tuple(int contid, const kvs_key *key, uint64_t instance_uuid, kvs_lock_option option, void *private1=NULL, void *private2=NULL, bool syncio = false, kvs_callback_function cnfn = NULL);
+    virtual int32_t list_tuple(int contid, const kvs_key *prefix_key, const kvs_key *start_key, uint16_t max_keys_to_list, kvs_value *value, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+    virtual int32_t retrieve_tuple_direct(int contid, const kvs_key *key, const kvs_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, kvs_retrieve_option option/*uint8_t option*/,
+                                          void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+    virtual int32_t store_tuple_direct(int contid, const kvs_key *key, const kvs_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, kvs_store_option option/*uint8_t option*/,
+                                          void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  #endif
 
 private:
+  
   void wait_for_io(kv_kdd_context *ctx);
   int create_queue(int qdepth, uint16_t qtype, kv_queue_handle *handle, int cqid, int is_polling);
-  kv_kdd_context* prep_io_context(int opcode, kvs_container_handle cont_hd,
-    const kvs_key *key, const kvs_value *value, void *private1, void *private2,
-    bool syncio, kvs_callback_function cbfn);
-  int check_opened_iterators(uint32_t bitmask,uint32_t bit_pattern,
-    kvs_iterator_handle *iter_hd);
+  kv_kdd_context* prep_io_context(int opcode, int contid, const kvs_key *key, const kvs_value *value, void *private1, void *private2, bool syncio, kvs_callback_function cbfn,
+                                  const kvs_key *prefix_key = NULL, const kvs_key *start_key = NULL, uint16_t max_keys_to_list = 0);
+  int check_opened_iterators(uint32_t bitmask,uint32_t bit_pattern);
   int32_t trans_iter_type(uint8_t dev_it_type, uint8_t* kvs_it_type);
-  int32_t trans_store_cmd_opt(kvs_store_option kvs_opt,
-                                    kv_store_option *kv_opt);
 
   bool ispersist;
   std::string datapath;
diff --git a/PDK/core/src/api/include/private/kvs_utils.h b/PDK/core/src/api/include/private/kvs_utils.h
index 928f554..b6b7489 100644
--- a/PDK/core/src/api/include/private/kvs_utils.h
+++ b/PDK/core/src/api/include/private/kvs_utils.h
@@ -47,9 +47,9 @@
 #include <dirent.h>
 #include <unistd.h>
 #include <numa.h>
-#include <private_types.h>
 
-namespace api_private {
+namespace api_private
+{
 inline void yprintf(std::ostream &stream, const char* fmt, ...)
 {
     static char buffer[1024] = "";
@@ -120,12 +120,12 @@ inline void write_info(FILE * out, const char* format, ... ) {
 
 
 #ifdef ENABLE_LOGGING
-	#define WRITE_ERR(...) do { write_err(__VA_ARGS__); } while (0)
+	#define WRITE_ERR(...) do { write_err(__VA_ARGS__); exit(1); } while (0)
 	#define WRITE_WARNING(...) write_warn(stderr, __VA_ARGS__)
 	#define WRITE_INFO(...) write_info(stdout, __VA_ARGS__)
 	#define WRITE_LOG(...) logprintf(__VA_ARGS__)
 #else
-	#define WRITE_ERR(...) do { write_err(__VA_ARGS__); } while (0)
+	#define WRITE_ERR(...) do { write_err(__VA_ARGS__); exit(1); } while (0)
 	#define WRITE_WARNING(...) write_warn(stderr, __VA_ARGS__)
 	#define WRITE_INFO(...) write_info(stdout, __VA_ARGS__)
 	#define WRITE_LOG(...)
@@ -260,8 +260,7 @@ static inline void trim(std::string &s) {
     rtrim(s);
 }
 
-static inline int32_t validate_kv_pair_(
-  const kvs_key *key, const kvs_value *value, uint32_t max_valid_val_len){
+static inline int32_t validate_request(const kvs_key *key, const kvs_value *value) {
   if (key) {
     if(key->length < KVS_MIN_KEY_LENGTH || key->length > KVS_MAX_KEY_LENGTH) {
       WRITE_WARNING("key size is out of range, key size = %d\n", key->length);
@@ -273,22 +272,21 @@ static inline int32_t validate_kv_pair_(
     }
   }
   if(value) {
-    if(value->length < KVS_MIN_VALUE_LENGTH || value->length > max_valid_val_len) {
+    if(value->length < KVS_MIN_VALUE_LENGTH || value->length > KVS_MAX_VALUE_LENGTH) {
       WRITE_WARNING("value size is out of range, value size = %d\n", value->length);
       return KVS_ERR_VALUE_LENGTH_INVALID;
     }
-    if(value->offset & (KVS_ALIGNMENT_UNIT - 1)) {
-      return KVS_ERR_MISALIGNED_VALUE_OFFSET;
+    if(value->offset % KVS_ALIGNMENT_UNIT != 0) {
+      return KVS_ERR_VALUE_OFFSET_INVALID;
     }
     if(value->value == NULL && value->length > 0){
       WRITE_WARNING("value buffer inputted is NULL\n");
       return KVS_ERR_PARAM_INVALID;
     }
   }
-  return KVS_SUCCESS;
-}
-static inline int32_t validate_request(const kvs_key *key, const kvs_value *value) {
-  return validate_kv_pair_(key, value, KVS_MAX_VALUE_LENGTH);
+  return 0;
 }
 }
+
+
 #endif /* INCLUDE_KVS_UTILS_H_ */
diff --git a/PDK/core/src/api/include/private/private_api.h b/PDK/core/src/api/include/private/private_api.h
index f7cb1da..464aa66 100644
--- a/PDK/core/src/api/include/private/private_api.h
+++ b/PDK/core/src/api/include/private/private_api.h
@@ -50,12 +50,10 @@ kvs_result kvs_init_env_opts(kvs_init_options* options);
 kvs_result kvs_exit_env();
 kvs_result kvs_open_device(const char *dev_path, kvs_device_handle *dev_hd);
 kvs_result kvs_close_device(kvs_device_handle user_dev);
-kvs_result kvs_create_container(kvs_device_handle dev_hd, const char *name, uint64_t size, const kvs_container_context *ctx);
-kvs_result kvs_delete_container(kvs_device_handle dev_hd, const char *cont_name);
-kvs_result kvs_open_container(kvs_device_handle dev_hd, const char* name, kvs_container_handle *cont_hd);
-kvs_result kvs_close_container(kvs_container_handle cont_hd);
-kvs_result kvs_list_containers(kvs_device_handle dev_hd, uint32_t index,
-  uint32_t buffer_size, kvs_container_name *names, uint32_t *cont_cnt);
+kvs_result kvs_create_container (kvs_device_handle dev_hd, const char *name, uint64_t size, const kvs_container_context *ctx);
+kvs_result kvs_delete_container (kvs_device_handle dev_hd, const char *cont_name);
+kvs_result kvs_open_container (kvs_device_handle dev_hd, const char* name, kvs_container_handle *cont_hd);
+kvs_result kvs_close_container (kvs_container_handle cont_hd);
 kvs_result kvs_get_container_info (kvs_container_handle cont_hd, kvs_container *cont);
 int32_t kvs_get_ioevents(kvs_container_handle cont_hd, int maxevents);
 kvs_result kvs_get_tuple_info (kvs_container_handle cont_hd, const kvs_key *key, kvs_tuple_info *info);
@@ -66,8 +64,7 @@ kvs_result kvs_retrieve_tuple_async(kvs_container_handle cont_hd, const kvs_key
 kvs_result kvs_delete_tuple(kvs_container_handle cont_hd, const kvs_key *key, const kvs_delete_context *ctx);
 kvs_result kvs_delete_tuple_async(kvs_container_handle cont_hd, const kvs_key* key, const kvs_delete_context* ctx, kvs_callback_function cbfn);
 kvs_result kvs_exist_tuples(kvs_container_handle cont_hd, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, const kvs_exist_context *ctx);
-kvs_result kvs_exist_tuples_async(kvs_container_handle cont_hd, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, 
-                                  uint8_t *result_buffer, const kvs_exist_context *ctx, kvs_callback_function cbfn);
+kvs_result kvs_exist_tuples_async(kvs_container_handle cont_hd, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, const kvs_exist_context *ctx, kvs_callback_function cbfn);
 kvs_result kvs_open_iterator(kvs_container_handle cont_hd, const kvs_iterator_context *ctx, kvs_iterator_handle *iter_hd);
 kvs_result kvs_close_iterator(kvs_container_handle cont_hd, kvs_iterator_handle hiter, const kvs_iterator_context *ctx);
 kvs_result kvs_close_iterator_all(kvs_container_handle cont_hd);
@@ -83,13 +80,19 @@ kvs_result kvs_get_max_key_length (kvs_device_handle dev_hd, int32_t *max_key_le
 kvs_result kvs_get_min_value_length (kvs_device_handle dev_hd, int32_t *min_value_length);
 kvs_result kvs_get_max_value_length (kvs_device_handle dev_hd, int32_t *max_value_length);
 kvs_result kvs_get_optimal_value_length (kvs_device_handle dev_hd, int32_t *opt_value_length);
-
-
+#ifdef KVS_REMOTE
+  kvs_result kvs_lock_tuple(kvs_container_handle cont_hd, const kvs_key *key, uint64_t instance_uuid, const kvs_lock_context *ctx);
+  kvs_result kvs_unlock_tuple(kvs_container_handle cont_hd, const kvs_key *key, uint64_t instance_uuid, const kvs_lock_context *ctx);
+  kvs_result kvs_list_tuple(kvs_container_handle cont_hd, const kvs_key *prefix_key, const kvs_key *start_key, uint16_t max_keys_to_list, kvs_value *value, const kvs_list_context *ctx);
+  kvs_result kvs_list_tuple_async(kvs_container_handle cont_hd, const kvs_key *prefix_key, const kvs_key *start_key, uint16_t max_keys_to_list, kvs_value *value, const kvs_list_context *ctx, kvs_callback_function cbfn);
+  kvs_result kvs_retrieve_tuple_direct(kvs_container_handle cont_hd, const kvs_key *key, const kvs_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kvs_retrieve_context *ctx);
+  kvs_result kvs_store_tuple_direct(kvs_container_handle cont_hd, const kvs_key *key, const kvs_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kvs_store_context *ctx);
+#endif
 const char *kvs_errstr(int32_t errorno);
 // memory
-#define _FUNCTIONIZE(a, b)  a(b)
+#define _FUNCTIONIZE(a,b)  a(b)
 #define _STRINGIZE(a)      #a
-#define _INT2STRING(i)     _FUNCTIONIZE(_STRINGIZE, i)
+#define _INT2STRING(i)     _FUNCTIONIZE(_STRINGIZE,i)
 #define _LOCATION       __FILE__ ":" _INT2STRING(__LINE__)
 
 void *_kvs_zalloc(size_t size_bytes, size_t alignment, const char *file);
@@ -120,5 +123,5 @@ void  _kvs_free(void * buf, const char *file);
  */
 #define kvs_free(buf) _kvs_free(buf, _LOCATION)
 #define kvs_memstat() _kvs_report_memstat()
-}// namespace api_private
+}
 #endif
diff --git a/PDK/core/src/api/include/private/private_result.h b/PDK/core/src/api/include/private/private_result.h
index e2dd1fc..3f857fa 100644
--- a/PDK/core/src/api/include/private/private_result.h
+++ b/PDK/core/src/api/include/private/private_result.h
@@ -87,7 +87,15 @@ typedef enum {
   KVS_ERR_ITERATOR_NUM_OUT_RANGE=0x02B, //the number of iterator required out of range(equal 0, or bigger than 16)
   KVS_ERR_DD_UNSUPPORTED=0x02C,   //device driver does not support.
   KVS_ERR_ITERATOR_BUFFER_SIZE=0x02D, // buffer (iter_list->it_list) size in kvs_iterator_list error
-  KVS_ERR_MEMORY_MALLOC_FAIL=0x032,  // memory malloc function error.
+  KVS_ERR_NONEXIST_PREFIX         = 0x02E,    // prefix doesnt exist for target list op
+  KVS_ERR_NONEXIST_STARTKEY       = 0x02F,    // start key doesnt exist for target list op
+  KVS_ERR_UNSUPPORTED_OPTION      = 0x030,    // unsupported option for target list op
+  KVS_ERR_END_OF_LIST             = 0x031,    // target list op completed successfully, no more keys to list
+  KVS_ERR_KEY_IS_LOCKED           = 0x032,    // faild to lock. key is already locked
+  KVS_ERR_LOCK_UUID_MISMATCH       = 0x033,    // UUID mismatch for lock
+  KVS_ERR_NONEXIST_WRITER         = 0x034,    // No writer exist for unlock
+  KVS_ERR_NONEXIST_READER         = 0x035,    // No reader exist for unlock
+  KVS_ERR_LOCK_EXPIRED            = 0x036,    // Lock expired
 
   // From user driver	
   KVS_ERR_CACHE_INVALID_PARAM=0x200	, // (kv cache) invalid parameters
@@ -126,9 +134,6 @@ typedef enum {
   KVS_ERR_CONT_NAME=0x405	, // container name is invalid
   KVS_ERR_CONT_NOT_EXIST=0x406	, // container does not existi
   KVS_ERR_CONT_OPEN=0x407	, // container is already opened
-  KVS_ERR_CONT_PATH_TOO_LONG=0x408, // the length of container more than 255
-  KVS_ERR_CONT_MAX=0x409, // Exceeded max number of created container
-
 } kvs_result;	
-}// namespace api_private
+}
 #endif
diff --git a/PDK/core/src/api/include/private/private_struct.h b/PDK/core/src/api/include/private/private_struct.h
index ab2cdf7..44608b6 100644
--- a/PDK/core/src/api/include/private/private_struct.h
+++ b/PDK/core/src/api/include/private/private_struct.h
@@ -92,6 +92,9 @@ enum kvs_op {
   IOCB_ASYNC_ITER_OPEN_CMD=5,
   IOCB_ASYNC_ITER_CLOSE_CMD=6,
   IOCB_ASYNC_ITER_NEXT_CMD=7,
+  IOCB_ASYNC_LIST_CMD=8,
+  IOCB_ASYNC_LOCK_CMD=9,
+  IOCB_ASYNC_UNLOCK_CMD=10
 };
 
 /**
@@ -106,7 +109,7 @@ enum kvs_op {
 #endif
   
 
-typedef uint8_t kvs_key_t;
+typedef uint16_t kvs_key_t;
 typedef uint32_t kvs_value_t;
 typedef unsigned __int128 uint128_t;
 
@@ -229,6 +232,13 @@ typedef struct {
   void *private2;               /*!< the second pointer to a user's I/O context information */
 } kvs_retrieve_context;
 
+  /** options for \ref kvs_list_tuple()
+   */
+typedef struct {
+  void *private1;               /*!< a pointer to a user's I/O context information, which will be delivered to a callback function, unmodified */
+  void *private2;               /*!< the second pointer to a user's I/O context information */
+} kvs_list_context;
+
   /** options for \ref kvs_open_iterator()
    */
 typedef struct {
@@ -243,6 +253,24 @@ typedef struct {
   void *private1;
   void *private2;
 } kvs_exist_context;
+
+
+typedef struct {
+  bool kvs_reader_lock;      // [OPTION] reader or writer lock
+  bool kvs_blocking_lock;   // [OPTION] Blocking till get the lock or return immediately if didn't get
+  uint8_t lock_priority;        /*!< lock priority */
+  uint32_t lock_duration;       /*!< application instance wants to acquire lock */
+
+} kvs_lock_option;
+
+typedef struct {
+  kvs_lock_option option;       /*!< an option for a lock operation. */
+  void *private1;
+  void *private2;
+
+} kvs_lock_context;
+
+
   
 typedef struct {
   kvs_container_option option;
@@ -250,7 +278,7 @@ typedef struct {
 
 typedef struct {
   uint8_t  opcode;                /*!< operation opcode */
-  kvs_container_handle cont_hd;  /*!< container handle */
+  kvs_container_handle *cont_hd;  /*!< container handle */
   kvs_key *key;                   /*!< key data structure */
   kvs_value *value;               /*!< value data structure */
   uint32_t key_cnt;               /*!< kvs_exist_tuple_async */
@@ -259,9 +287,13 @@ typedef struct {
   void *private1;                 /*!< a pointer passed from a user */
   void *private2;                 /*!< a pointer passed from a user */
   kvs_result result;              /*!< IO result */
-  kvs_iterator_handle iter_hd;   /*!< iterator handle */
+  kvs_iterator_handle *iter_hd;   /*!< iterator handle */
+  kvs_key *prefix_key;            /*!< prefix_key data structure */
+  kvs_key *start_key;             /*!< start_key data structure */
+  uint16_t max_keys_to_list;
 } kvs_callback_context;
 
+
   /** A function prototype for I/O callback function
    *
    */
@@ -299,7 +331,7 @@ typedef struct {
     uint32_t mem_size_mb;
     int syncio;
   } udd;
-    char *emul_config_file;
+    const char *emul_config_file;
   } kvs_init_options;
   
   /** Device information
@@ -316,5 +348,5 @@ typedef struct {
   int  deviceid;                    /*!< device ID */
   char ven_dev_id[128];             /*!< vendor + device ID */
 } kv_device_info;
-}// namespace api_private
+}
 #endif
diff --git a/PDK/core/src/api/include/private/private_types.h b/PDK/core/src/api/include/private/private_types.h
index 6c30762..403ec16 100644
--- a/PDK/core/src/api/include/private/private_types.h
+++ b/PDK/core/src/api/include/private/private_types.h
@@ -53,25 +53,7 @@ extern "C" {
 #endif
 
 const int MAX_DEV_PATH_LEN = 256;
-
-
 namespace api_private {
-// max sub-command number in a batch command
-const int MAX_SUB_CMD_NUM = 8;
-// max value size of sub-command in a batch command */
-const int MAX_SUB_CMD_VALUE_LEN = 8192;
-
-/* the max number of container that supported currently, as currently kv ssd only support two 
-keyspace: 0 */ 
-const int NR_MAX_CONT = 1; 
-const int META_DATA_KEYSPACE_ID = 0; //use keyspace 0 as meta data key space
-const int USER_DATA_KEYSPACE_START_ID = 1; //start keyspace id that used for user containers 
-//extern const cf_digest cf_digest_zero;
-extern const char* KEY_SPACE_LIST_KEY_NAME; //the key of kv pair that store key
-                                            //spaces name list
-const int PAGE_ALIGN = 4096; //page align (4KB)
-const int DMA_ALIGN = 4; //DMA required size align(4B)
-
 class kv_device_priv {
 public:
   kv_device_priv() {
@@ -84,7 +66,6 @@ public:
     isattached = false;
     isspdkdev = false;
     isemul = false;
-    iskerneldev = false;
     num_opened_qpairs = 0;
   }
 
@@ -146,21 +127,28 @@ public:
   virtual int32_t init(const char* devpath, bool syncio, uint64_t sq_core, uint64_t cq_core, uint32_t mem_size_mb, int queue_depth) {return 0;}
   virtual int32_t init(const char* devpath, bool syncio) {return 0;}
   virtual int32_t process_completions(int max) =0;
-  virtual int32_t store_tuple(kvs_container_handle cont_hd, const kvs_key *key,
-    const kvs_value *value, kvs_store_option option, void *private1=NULL,
-    void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) = 0;
-  virtual int32_t retrieve_tuple(kvs_container_handle cont_hd, const kvs_key *key, kvs_value *value, kvs_retrieve_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) = 0;
-  virtual int32_t delete_tuple(kvs_container_handle cont_hd, const kvs_key *key, kvs_delete_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) = 0;
-  virtual int32_t exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) = 0;
-  virtual int32_t open_iterator(kvs_container_handle cont_hd, kvs_iterator_option option, uint32_t bitmask, uint32_t bit_pattern, kvs_iterator_handle *iter_hd) = 0;
-  virtual int32_t close_iterator(kvs_container_handle cont_hd, kvs_iterator_handle hiter) = 0;
-  virtual int32_t close_iterator_all(kvs_container_handle cont_hd) = 0;
-  virtual int32_t list_iterators(kvs_container_handle cont_hd, kvs_iterator_info *kvs_iters, uint32_t count) = 0;
-  virtual int32_t iterator_next(kvs_container_handle cont_hd, kvs_iterator_handle hiter, kvs_iterator_list *iter_list, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) = 0;
+  virtual int32_t store_tuple(int contid, const kvs_key *key, const kvs_value *value, kvs_store_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) = 0;
+  virtual int32_t retrieve_tuple(int contid, const kvs_key *key, kvs_value *value, kvs_retrieve_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) = 0;
+  virtual int32_t delete_tuple(int contid, const kvs_key *key, kvs_delete_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) = 0;
+  virtual int32_t exist_tuple(int contid, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) = 0;
+  virtual int32_t open_iterator(int contid, kvs_iterator_option option, uint32_t bitmask, uint32_t bit_pattern, kvs_iterator_handle *iter_hd) = 0;
+  virtual int32_t close_iterator(int contid, kvs_iterator_handle hiter) = 0;
+  virtual int32_t close_iterator_all(int contid) = 0;
+  virtual int32_t list_iterators(int contid, kvs_iterator_info *kvs_iters, uint32_t count) = 0;
+  virtual int32_t iterator_next(kvs_iterator_handle hiter, kvs_iterator_list *iter_list, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) = 0;
   virtual float get_waf() {return 0.0;}
   virtual int32_t get_used_size(int32_t *dev_util) {return 0;}
   virtual int32_t get_total_size(int64_t *dev_capa) {return 0;}
   virtual int32_t get_device_info(kvs_device *dev_info) {return 0;}
+  #ifdef KVS_REMOTE
+    virtual int32_t lock_tuple(int contid, const kvs_key *key, uint64_t instance_uuid, kvs_lock_option option, void *private1=NULL, void *private2=NULL, bool syncio = false, kvs_callback_function cbfn = NULL) = 0;
+    virtual int32_t unlock_tuple(int contid, const kvs_key *key, uint64_t instance_uuid, kvs_lock_option option, void *private1=NULL, void *private2=NULL, bool syncio = false, kvs_callback_function cbfn = NULL) = 0;
+    virtual int32_t list_tuple(int contid, const kvs_key *prefix_key, const kvs_key *start_key, uint16_t max_keys_to_list, kvs_value *value, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) = 0;
+    virtual int32_t retrieve_tuple_direct(int contid, const kvs_key *key, const kvs_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, kvs_retrieve_option option/*uint8_t option*/, 
+                                          void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) = 0;
+    virtual int32_t store_tuple_direct(int contid, const kvs_key *key, const kvs_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, kvs_store_option option/*uint8_t option*/,
+                                          void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) = 0;
+  #endif
   std::string path;
 };
 
@@ -168,15 +156,12 @@ struct _kvs_device_handle {
   kv_device_priv * dev;
   KvsDriver* driver;
   char* dev_path;
-  kvs_container_handle meta_cont_hd;
-  std::list<kvs_container_handle> open_cont_hds; //containers opened by user
 };
 
 struct _kvs_container_handle {
   uint8_t container_id;
-  uint8_t keyspace_id; //corresponding keyspace id in KVSSD
   kvs_device_handle dev;
-  char name[MAX_CONT_PATH_LEN];
+  char name[256];
 };
 
 struct _kvs_iterator_handle{
diff --git a/PDK/core/src/api/include/private/udd.hpp b/PDK/core/src/api/include/private/udd.hpp
index 7d9bcc6..b9c2a8b 100644
--- a/PDK/core/src/api/include/private/udd.hpp
+++ b/PDK/core/src/api/include/private/udd.hpp
@@ -53,11 +53,9 @@ class KUDDriver: public KvsDriver
   uint64_t handle;
   uint32_t queue_depth;
   uint64_t core_mask;
-  uint64_t sync_mask;
   uint64_t num_cq_threads;
   uint64_t cq_thread_mask;
   uint32_t mem_size_mb;
-  bool sync_io;
 
   char trid[1024];
 
@@ -78,30 +76,33 @@ public:
   KUDDriver(kv_device_priv *dev, kvs_callback_function user_io_complete_);
   virtual ~KUDDriver();
   virtual int32_t init(const char*devpath, bool syncio, uint64_t sq_core, uint64_t cq_core, uint32_t mem_size_mb, int queue_depth) override;
-  virtual int16_t _get_queue_id(kvs_container_handle cont_hd);
   virtual int32_t process_completions(int max) override;
-  virtual int32_t store_tuple(kvs_container_handle cont_hd, const kvs_key *key, const kvs_value *value, kvs_store_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
-  virtual int32_t retrieve_tuple(kvs_container_handle cont_hd, const kvs_key *key, kvs_value *value, kvs_retrieve_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
-  virtual int32_t delete_tuple(kvs_container_handle cont_hd, const kvs_key *key, kvs_delete_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
-  virtual int32_t exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
-  virtual int32_t open_iterator(kvs_container_handle cont_hd, kvs_iterator_option option, uint32_t bitmask, uint32_t bit_pattern, kvs_iterator_handle *iter_hd) override;
-  virtual int32_t close_iterator(kvs_container_handle cont_hd, kvs_iterator_handle hiter) override;
-  virtual int32_t close_iterator_all(kvs_container_handle cont_hd) override;
-  virtual int32_t list_iterators(kvs_container_handle cont_hd, kvs_iterator_info *kvs_iters, uint32_t count) override;
-  virtual int32_t iterator_next(kvs_container_handle cont_hd, kvs_iterator_handle hiter, kvs_iterator_list *iter_list, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t store_tuple(int contid, const kvs_key *key, const kvs_value *value, kvs_store_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t retrieve_tuple(int contid, const kvs_key *key, kvs_value *value, kvs_retrieve_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t delete_tuple(int contid, const kvs_key *key, kvs_delete_option option/*uint8_t option*/, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t exist_tuple(int contid, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
+  virtual int32_t open_iterator(int contid, kvs_iterator_option option, uint32_t bitmask, uint32_t bit_pattern, kvs_iterator_handle *iter_hd) override;
+  virtual int32_t close_iterator(int contid, kvs_iterator_handle hiter) override;
+  virtual int32_t close_iterator_all(int contid) override;
+  virtual int32_t list_iterators(int contid, kvs_iterator_info *kvs_iters, uint32_t count) override;
+  virtual int32_t iterator_next(kvs_iterator_handle hiter, kvs_iterator_list *iter_list, void *private1=NULL, void *private2=NULL, bool sync = false, kvs_callback_function cbfn = NULL) override;
   virtual float get_waf() override;
   virtual int32_t get_used_size(int32_t *dev_util) override;
   virtual int32_t get_total_size(int64_t *dev_capa) override;
   virtual int32_t get_device_info(kvs_device *dev_info) override;
+  #ifdef KVS_REMOTE
+    virtual int32_t lock_tuple(int contid, const kvs_key *key, uint64_t instance_uuid, kvs_lock_option option, void *private1=NULL, void *private2=NULL, bool syncio = false, kvs_callback_function cbfn = NULL) { return -1; } //Not supported
+    virtual int32_t unlock_tuple(int contid, const kvs_key *key, uint64_t instance_uuid, kvs_lock_option option, void *private1=NULL, void *private2=NULL, bool syncio = false, kvs_callback_function cbfn= NULL) { return -1; } //Not supported
+  #endif
+
   
 private:
 
   bool ispersist;
   std::string datapath;
 
-  kv_udd_context* prep_io_context(int opcode, kvs_container_handle cont_hd, const kvs_key *key, const kvs_value *value, void *private1, void *private2, bool syncio, kvs_callback_function cbfn);
+  kv_udd_context* prep_io_context(int opcode, int contid, const kvs_key *key, const kvs_value *value, void *private1, void *private2, bool syncio, kvs_callback_function cbfn);
   int32_t trans_iter_type(uint8_t dev_it_type, uint8_t* kvs_it_type);
-  int32_t trans_store_cmd_opt(kvs_store_option kvs_opt, int *kv_opt);
 };
 }
 
diff --git a/PDK/core/src/api/include/rdd_cl.h b/PDK/core/src/api/include/rdd_cl.h
new file mode 100644
index 0000000..d10cda4
--- /dev/null
+++ b/PDK/core/src/api/include/rdd_cl.h
@@ -0,0 +1,660 @@
+/**
+ *   BSD LICENSE
+ *
+ *   Copyright (c) 2021 Samsung Electronics Co., Ltd.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Samsung Electronics Co., Ltd. nor the names of
+ *       its contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef RDD_CL_H
+#define RDD_CL_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <netdb.h>
+#include <assert.h>
+#include <sys/queue.h>
+
+#include <rdma/rdma_cma.h>
+
+
+#define RDD_PROTOCOL_VERSION (0xCAB00001)
+
+#define RDD_RDMA_MAX_KEYED_SGL_LENGTH ((1u << 24u) - 1)
+
+struct rdd_queue_priv_s {
+    union {
+        struct {
+            uint32_t proto_ver;
+            uint32_t qid;
+	        uint32_t hsqsize;
+	        uint32_t hrqsize;
+        } client;//Private data from client
+        struct {
+            uint32_t proto_ver;
+            uint16_t qhandle; //Hash identifier provided to client for direct transfer
+        } server;//Private data from server
+    } data;
+};//RDD queue private
+
+/* Offset of member MEMBER in a struct of type TYPE. */
+#define offsetof(TYPE, MEMBER) __builtin_offsetof (TYPE, MEMBER)
+#define containerof(ptr, type, member) ((type *)((uintptr_t)ptr - offsetof(type, member)))
+
+
+#define RDD_CL_MAX_DEFAULT_QUEUEC (1)
+#define RDD_CL_DEFAULT_QDEPTH (128)
+#define RDD_CL_DEFAULT_SEND_WR_NUM RDD_CL_DEFAULT_QDEPTH
+#define RDD_CL_DEFAULT_RECV_WR_NUM RDD_CL_DEFAULT_SEND_WR_NUM
+#define RDD_CL_CQ_QDEPTH (RDD_CL_DEFAULT_SEND_WR_NUM + RDD_CL_DEFAULT_RECV_WR_NUM)
+
+#define RDD_CL_TIMEOUT_IN_MS (500)
+
+typedef struct rdd_cl_conn_ctx_s rdd_cl_conn_ctx_t;
+
+enum rdd_cl_queue_state_e {
+    RDD_CL_Q_INIT = 0,
+    RDD_CL_Q_LIVE,
+};
+
+
+typedef enum {
+	RDD_PD_GLOBAL = 0, // One Global Protection Domain per Client context
+	RDD_PD_CONN, // Each RDMA Direct queue gets it's own protection Domain
+} rdd_cl_pd_type_e;
+
+typedef struct {
+	rdd_cl_pd_type_e pd_type;//RDMA protection domain type for client context
+} rdd_cl_ctx_params_t;
+
+typedef struct rdd_cl_conn_params_s {
+    const char *ip;
+    const char *port;
+    uint32_t qd;
+} rdd_cl_conn_params_t;
+
+typedef struct rdd_cl_dev_s {
+    //TODO: ??Required??//struct ibv_pd *pd;
+	//TODO: ??Required??//struct ibv_mr *mr;
+	//TODO: ??Required??//struct ibv_mw *mw;
+	struct ibv_device *dev;
+    TAILQ_ENTRY(rdd_cl_dev_s) dev_link;
+} rdd_cl_dev_t;
+
+typedef struct rdd_cl_queue_s {
+    uint32_t qid;
+    rdd_cl_conn_ctx_t *conn;
+    pthread_mutex_t qlock;
+    enum rdd_cl_queue_state_e state;
+    struct rdma_cm_id *cm_id;
+    struct ibv_pd *pd;
+    struct ibv_context *ibv_ctx;
+    struct ibv_qp *qp;
+
+    struct ibv_mr *cmd_mr;
+    struct ibv_mr *rsp_mr;
+
+} rdd_cl_queue_t;
+
+struct rdd_cl_conn_ctx_s {
+    int conn_id;
+    struct addrinfo *ai;
+	uint16_t qhandle;
+    uint32_t qd;
+    uint32_t queuec;
+    rdd_cl_queue_t *queues;
+    rdd_cl_dev_t *device;
+    //IBV
+    struct ibv_comp_channel *ch;
+    union {
+        struct {
+            pthread_t cq_poll_thread;
+        }pt;//Pthread
+    }th;//Thread
+
+    pthread_mutex_t conn_lock;
+
+    struct rdd_client_ctx_s *cl_ctx;
+};
+
+struct rdd_client_ctx_s {
+    struct ibv_device **ibv_devs;
+    struct rdma_event_channel *cm_ch;
+    union {
+        struct {
+            pthread_t cm_thread;
+        }pt;//Pthread
+    }th;//Thread
+    struct ibv_pd *pd;//Valid when PD is global otherwise NULL
+    TAILQ_HEAD(, rdd_cl_dev_s) devices;
+};
+
+struct rdd_client_ctx_s *rdd_cl_init(rdd_cl_ctx_params_t params);
+void rdd_cl_destroy(struct rdd_client_ctx_s *ctx);
+
+rdd_cl_conn_ctx_t *rdd_cl_create_conn(struct rdd_client_ctx_s *cl_ctx, rdd_cl_conn_params_t params);
+void rdd_cl_destroy_connection(rdd_cl_conn_ctx_t *ctx);
+
+uint16_t rdd_cl_conn_get_qhandle(void * arg);
+struct ibv_mr *rdd_cl_conn_get_mr(void *ctx, void *addr, size_t len);
+void rdd_cl_conn_put_mr(struct ibv_mr *mr);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif // RDD_CL_H
+
+void rdd_cl_destory_queues(rdd_cl_conn_ctx_t *ctx);
+
+struct rdd_cl_dev_s *rdd_cl_find_dev(struct rdd_client_ctx_s *cl_ctx, const char *name)
+{
+    struct rdd_cl_dev_s *dev;
+    TAILQ_FOREACH(dev, &cl_ctx->devices, dev_link) {
+        if (!strcmp(dev->dev->name, name))
+			return dev;
+    }
+	return NULL;
+}
+
+static int rdd_cl_init_ib_queue(rdd_cl_queue_t *q)
+{
+ 	struct ibv_cq *cq;
+ 	struct ibv_qp_init_attr qp_attr;
+ 	struct rdma_cm_id *id = q->cm_id;
+    rdd_cl_conn_ctx_t * conn = q->conn;
+
+    int rc;
+    
+    //q->pd = ibv_alloc_pd(q->cm_id->verbs);
+    //assert(q->pd != NULL);//TODO: Handle error case
+
+  	cq = ibv_create_cq(id->verbs, RDD_CL_CQ_QDEPTH, NULL, conn->ch,
+ 			           q->qid % id->verbs->num_comp_vectors);
+ 	ibv_req_notify_cq(cq, 0);
+
+ 	memset(&qp_attr, 0, sizeof (struct ibv_qp_init_attr));
+
+ 	qp_attr.send_cq = cq;
+ 	qp_attr.recv_cq = cq;
+ 	qp_attr.qp_type = IBV_QPT_RC;
+
+ 	qp_attr.cap.max_send_wr = conn->qd;
+   	qp_attr.cap.max_recv_wr = conn->qd;
+ 	qp_attr.cap.max_send_sge = 16;
+ 	qp_attr.cap.max_recv_sge = 16;
+
+ 	rc = rdma_create_qp(id, NULL, &qp_attr);
+    assert(rc == 0);
+ 	q->qp = id->qp;
+    q->pd = id->qp->pd;
+
+	fprintf(stderr, "Setting pd for q %p with pd %p\n", q, q->pd); 
+
+ 	return 0;
+}
+
+int rdd_cl_process_wc(struct ibv_wc *wc)
+{
+    if (wc->status != IBV_WC_SUCCESS) {
+		fprintf(stderr, "wc error %d\n", wc->status);
+        abort();
+		return -1;
+	}
+
+	fprintf(stderr, "Recieved unexpected wc %d\n", wc->status);
+
+    return 0;
+}
+
+void *poll_cq(void *arg)
+{
+    struct ibv_comp_channel *ch = (struct ibv_comp_channel *)arg;
+    int rc;
+
+    while(1) {
+        struct ibv_cq *cq;
+		struct ibv_wc wc;
+		void *ctx;
+
+        ibv_get_cq_event(ch, &cq, &ctx);
+		ibv_ack_cq_events(cq, 1);
+		ibv_req_notify_cq(cq, 0);
+
+        while (ibv_poll_cq(cq, 1, &wc)) {
+			rc = rdd_cl_process_wc(&wc);
+            //TODO: Process error case
+            assert(rc == 0);
+        }
+    }
+    
+    return NULL;
+}
+
+int rdd_cl_cm_addr_resolved(struct rdma_cm_id *id)
+{
+	const char *name;
+    int rc = 0;
+	struct rdd_cl_dev_s *dev;
+	struct ibv_device_attr_ex device_attr;
+    rdd_cl_queue_t *queue = (rdd_cl_queue_t *)id->context;
+	
+    rdd_cl_conn_ctx_t *conn_ctx = queue->conn;
+
+	fprintf(stdout, "qid %u # of completion vector: %d\n",
+			queue->qid, id->verbs->num_comp_vectors);
+
+	ibv_query_device_ex(id->verbs, NULL, &device_attr);
+	if (device_attr.orig_attr.device_cap_flags & IBV_DEVICE_MEM_WINDOW ||
+		device_attr.orig_attr.device_cap_flags & IBV_DEVICE_MEM_WINDOW_TYPE_2B) {
+			printf("MW Supported\n");
+	} else {
+ 			printf("MW Unupported\n");
+	}
+
+    pthread_mutex_lock(&conn_ctx->conn_lock);
+
+	name = ibv_get_device_name(id->verbs->device);
+	dev = rdd_cl_find_dev(conn_ctx->cl_ctx, name);
+	if (dev == NULL) {
+		fprintf(stderr, "No matching device %s\n", name);
+		return -1;
+	}
+
+	//tgt->dev = ddev;//TODO: Find and assign target device
+    conn_ctx->device = dev;
+  	queue->ibv_ctx = id->verbs;
+
+    if(conn_ctx->ch == NULL) {
+        conn_ctx->ch = ibv_create_comp_channel(queue->cm_id->verbs);
+
+        pthread_create(&conn_ctx->th.pt.cq_poll_thread, NULL, poll_cq, conn_ctx->ch);
+    }
+
+	pthread_setschedprio(conn_ctx->th.pt.cq_poll_thread, sched_get_priority_max(SCHED_OTHER));
+
+    pthread_mutex_unlock(&conn_ctx->conn_lock);
+
+	rc = rdd_cl_init_ib_queue(queue);
+    assert(rc == 0);
+
+	rc = rdma_resolve_route(queue->cm_id, RDD_CL_TIMEOUT_IN_MS);
+    assert(rc == 0);
+
+	return 0;
+}
+
+int rdd_cl_cm_route_resolved(struct rdma_cm_id *id)
+{
+	int ret;
+    struct rdd_queue_priv_s priv;
+	struct rdma_conn_param conn_param;
+	struct ibv_device_attr device_attr;
+    rdd_cl_queue_t *queue = (rdd_cl_queue_t *)id->context;
+    rdd_cl_conn_ctx_t *conn = queue->conn;
+
+	fprintf(stdout, "qid %u route resolved.\n", queue->qid);
+
+    priv.data.client.proto_ver = RDD_PROTOCOL_VERSION;
+    priv.data.client.qid = queue->qid;
+    priv.data.client.hsqsize = conn->qd;
+    priv.data.client.hrqsize = conn->qd;
+
+	 memset(&conn_param, 0, sizeof(struct rdma_conn_param));
+
+	 ret = ibv_query_device(id->verbs, &device_attr);
+	 if (ret) {
+	 	fprintf(stderr, "Failed to query device %s", id->verbs->device->name);
+	 	return -1;
+	 }
+
+	conn_param.initiator_depth = device_attr.max_qp_init_rd_atom;
+	conn_param.responder_resources = device_attr.max_qp_rd_atom;
+	conn_param.retry_count = 7;
+	conn_param.rnr_retry_count = 7;
+	conn_param.flow_control = 1;
+	conn_param.private_data = &priv;
+	conn_param.private_data_len = sizeof (priv);
+
+	 ret = rdma_connect(id, &conn_param);
+	 if (ret) {
+	 	fprintf(stderr, "Failed to connect to qpair %d\n", queue->qid);
+	 	return -1;
+	 }
+
+	return 0;
+}
+
+uint16_t rdd_cl_conn_get_qhandle(void *arg)
+{
+	rdd_cl_conn_ctx_t *conn = (rdd_cl_conn_ctx_t *)arg;
+
+	return conn->qhandle;
+}
+
+struct ibv_mr *rdd_cl_conn_get_mr(void *ctx, void *addr, size_t len)
+{
+	rdd_cl_conn_ctx_t *conn = (rdd_cl_conn_ctx_t *)ctx;
+
+	//assume one IO queue per connection
+	return ibv_reg_mr(conn->queues[0].pd, addr, len,
+         					IBV_ACCESS_LOCAL_WRITE |
+						    IBV_ACCESS_REMOTE_WRITE |
+							IBV_ACCESS_REMOTE_READ);                   
+
+}
+
+
+void rdd_cl_conn_put_mr(struct ibv_mr *mr)
+{
+	ibv_dereg_mr(mr);
+	return;
+}
+
+
+int rdd_cl_cm_conn_established(struct rdma_cm_event *ev) {
+    struct rdma_cm_id *id = ev->id;
+	rdd_cl_queue_t *q = (rdd_cl_queue_t *)id->context;
+	rdd_cl_conn_ctx_t *conn = q->conn;
+    struct rdd_queue_priv_s *priv = (struct rdd_queue_priv_s *) ev->param.conn.private_data;
+
+	fprintf(stdout, "qid %u connection established\n", q->qid);
+    fprintf(stdout, "qid %u client handle %8x\n", q->qid, priv->data.server.qhandle);
+
+	conn->qhandle = priv->data.server.qhandle;
+
+	q->state = RDD_CL_Q_LIVE;
+
+    //TODO: Increment live queue counter
+
+	return 0;
+}
+
+int rdd_cl_on_disconnect(struct rdma_cm_id *id)
+{
+  printf("disconnected.\n");
+
+  //TODO: tear down connection?? wait for all queues
+  return 1; /* exit event loop */
+}
+
+static int rdd_cl_cm_event_handler(struct rdma_cm_event *ev)
+{
+	int r = 0;
+
+	switch (ev->event) {
+        case RDMA_CM_EVENT_ADDR_RESOLVED:
+            r = rdd_cl_cm_addr_resolved(ev->id);
+            break;
+        case RDMA_CM_EVENT_ROUTE_RESOLVED:
+            r = rdd_cl_cm_route_resolved(ev->id);
+            break;
+        case RDMA_CM_EVENT_ESTABLISHED:
+            r = rdd_cl_cm_conn_established(ev);
+            break;
+        case RDMA_CM_EVENT_DISCONNECTED:
+            r = rdd_cl_on_disconnect(ev->id);
+            break;
+        case RDMA_CM_EVENT_REJECTED:
+            fprintf(stderr, "event status %d\n", ev->status);
+            break;
+        default:
+            fprintf(stderr, "unsupported cm_event_handler: %d\n", ev->event);
+            //die("on_event: unknown event.");
+	}
+
+	return r;
+}
+
+void *_rdd_cl_cm_event_task(void *arg)
+{
+    struct rdd_client_ctx_s *ctx = (struct rdd_client_ctx_s *)arg;
+	struct rdma_event_channel *ch = ctx->cm_ch;
+	struct rdma_cm_event *event;
+
+	while (rdma_get_cm_event(ch, &event) == 0) {
+        printf("Processing event %s\n", rdma_event_str(event->event));
+		if (rdd_cl_cm_event_handler(event))
+    		break;
+
+		rdma_ack_cm_event(event);
+        //TODO: Respond to cancellation request
+  	}
+    printf("Exiting cm event task %p\n", ctx);
+
+	return NULL;
+}
+
+int rdd_cl_init_queues(rdd_cl_conn_ctx_t *ctx)
+{
+    int rc;
+	uint32_t i;
+    rdd_cl_queue_t *queue;
+
+    //TODO: validate upper limit for queuec
+    ctx->queues = (rdd_cl_queue_t *) calloc(ctx->queuec, sizeof(rdd_cl_queue_t));
+    if(!ctx->queues) {
+        return -1;
+    }
+
+    for(i=0; i < ctx->queuec; i++) {
+        queue = ctx->queues + i;
+        queue->conn = ctx;
+        rc = pthread_mutex_init(&queue->qlock, NULL);
+        if(rc) {
+            rdd_cl_destory_queues(ctx);
+            return rc;
+        }
+
+        rc = rdma_create_id(ctx->cl_ctx->cm_ch, &queue->cm_id, NULL, RDMA_PS_TCP);
+        if (rc) {
+            fprintf(stderr, "failed to create cm_id");
+            rdd_cl_destory_queues(ctx);
+            return rc;
+        }
+
+        queue->qid = i;
+        queue->state = RDD_CL_Q_INIT;
+        queue->cm_id->context = queue;
+
+        //TODO: Check if need to be moved
+        //queue->pd = ibv_alloc_pd(queue->cm_id->verbs);
+        //assert(queue->pd != NULL);//TODO: Handle error case
+
+        rc = rdma_resolve_addr(queue->cm_id, NULL, ctx->ai->ai_addr, RDD_CL_TIMEOUT_IN_MS);
+        if(rc != 0) {
+            assert(rc == -1);
+            fprintf(stderr, "rdma resolve failed %d\n", errno);
+            abort();
+        }
+        //TODO: Handle error
+    }
+
+    return 0;
+}
+
+void rdd_cl_destory_queues(rdd_cl_conn_ctx_t *ctx)
+{
+    uint32_t i;
+
+    if(ctx->queues) {
+        for (i=0; i<ctx->queuec; i++) {
+            if(ctx->queues[i].cm_id) {
+                rdma_destroy_id(ctx->queues[i].cm_id);
+            }
+
+            pthread_mutex_destroy(&ctx->queues[i].qlock);
+            //Error code?
+        }
+
+        free(ctx->queues);
+        ctx->queues = NULL;
+    }
+}
+
+rdd_cl_conn_ctx_t *rdd_cl_create_conn(struct rdd_client_ctx_s *cl_ctx, rdd_cl_conn_params_t params)
+{
+    rdd_cl_conn_ctx_t *conn_ctx = NULL;
+    int rc = 0;
+
+	int is_connect_done = 0;
+	uint32_t i;
+
+    conn_ctx = (rdd_cl_conn_ctx_t *)calloc(1, sizeof(rdd_cl_conn_ctx_t));
+    if(!conn_ctx) {
+        fprintf(stderr, "Failed to allocate context\n");
+        return NULL;
+    }
+
+    conn_ctx->cl_ctx = cl_ctx;
+    pthread_mutex_init(&conn_ctx->conn_lock, NULL);
+    //TODO: Handle failure
+
+    rc = getaddrinfo(params.ip, params.port, NULL, &conn_ctx->ai);
+    if (rc) {
+        fprintf(stderr, "failed to parse target addr: %s\n", gai_strerror(rc));
+        conn_ctx->ai = NULL; //Reset in case it is not defined
+        rdd_cl_destroy_connection(conn_ctx);
+        return NULL;
+    }
+
+    //TODO: Update from params
+    conn_ctx->queuec = RDD_CL_MAX_DEFAULT_QUEUEC;
+    conn_ctx->qd = RDD_CL_DEFAULT_SEND_WR_NUM;
+
+    rc = rdd_cl_init_queues(conn_ctx);
+    if(rc) {
+        rdd_cl_destroy_connection(conn_ctx);
+    }
+
+	do {
+		for(i=0; i< conn_ctx->queuec; i++) {
+			if(conn_ctx->queues[i].state != RDD_CL_Q_LIVE)
+				break;//This queue is not intialized.
+			if(i == conn_ctx->queuec - 1)
+				is_connect_done = 1;
+		}
+	} while(!is_connect_done);//Wait for all queues to connect
+
+    return conn_ctx;
+}
+
+void rdd_cl_destroy_connection(rdd_cl_conn_ctx_t *ctx)
+{
+    if(ctx) {
+        if(ctx->queues) {
+            rdd_cl_destory_queues(ctx);
+        }
+
+        if(ctx->ai) {
+            freeaddrinfo(ctx->ai);
+        }
+
+        ctx->ai = NULL;
+
+        pthread_mutex_destroy(&ctx->conn_lock);
+        //Handle failure??
+
+        free(ctx);
+
+    }
+}
+
+struct rdd_client_ctx_s *rdd_cl_init(rdd_cl_ctx_params_t params)
+{
+    struct rdd_client_ctx_s *ctx;
+    int rc = 0;
+    int device_count = 0;
+    int i;
+
+    ctx = (struct rdd_client_ctx_s *) calloc(1, sizeof(struct rdd_client_ctx_s));
+    if(!ctx) {
+        fprintf(stderr, "Failed to allocate context\n");
+        return NULL;
+    }
+
+    TAILQ_INIT(&ctx->devices);
+
+    //TODO: lock this devices data structure in case need to be updated
+    ctx->ibv_devs = ibv_get_device_list(&device_count);
+    if(ctx->ibv_devs == NULL) {
+        fprintf(stderr, "Failed to get rdma device list %d\n", errno);
+        rdd_cl_destroy(ctx);
+        return NULL;
+    }
+
+    for(i=0; i < device_count; i++) {
+        rdd_cl_dev_t *dev = (rdd_cl_dev_t *)calloc(1, sizeof(rdd_cl_dev_t));
+        assert(dev != NULL);//TODO: handle error
+        dev->dev = ctx->ibv_devs[i];
+        TAILQ_INSERT_TAIL(&ctx->devices, dev, dev_link);
+    }
+
+    ctx->cm_ch = rdma_create_event_channel();
+    if(!ctx->cm_ch) {
+        fprintf(stderr, "Failed to create rdma event channel %d\n", errno);
+        rdd_cl_destroy(ctx);
+        return NULL;
+    }
+
+    rc = pthread_create(&ctx->th.pt.cm_thread, NULL, _rdd_cl_cm_event_task, ctx);
+    if(rc) {
+        fprintf(stderr, "pthread create failed with error %d\n", rc);
+        rdd_cl_destroy(ctx);
+        return NULL;
+    }
+
+    return ctx;
+
+}
+
+void rdd_cl_destroy(struct rdd_client_ctx_s *ctx)
+{
+	int rc = 0;
+
+    if(ctx->th.pt.cm_thread) {
+        rc = pthread_cancel(ctx->th.pt.cm_thread);
+        //TODO: validate error code
+
+        rc = pthread_join(ctx->th.pt.cm_thread, NULL);
+        //TODO: validate error code
+        assert(rc ==0);
+    }
+
+    if(ctx->cm_ch) {
+        rdma_destroy_event_channel(ctx->cm_ch);
+    }
+
+    if(ctx) {
+        free(ctx);
+    }
+}
diff --git a/PDK/core/src/api/include/udd/kv_types.h b/PDK/core/src/api/include/udd/kv_types.h
index a6134d6..d4912fe 100644
--- a/PDK/core/src/api/include/udd/kv_types.h
+++ b/PDK/core/src/api/include/udd/kv_types.h
@@ -53,6 +53,7 @@
 #include <pthread.h>
 
 //Constants
+#define KV_ALIGNMENT_UNIT 64
 #define KV_MIN_VALUE_LEN 0
 #define KV_MAX_IO_VALUE_LEN (2048*1024) //28KB -> 2048KB
 #define LBA_MAX_IO_VALUE_LEN (2048*1024) //2024KB -> 2048KB
@@ -78,7 +79,7 @@
 */
 enum kv_sdk_init_types {
 	KV_SDK_INIT_FROM_JSON = 0x00,	/**< initialize sdk with json file */
-	KV_SDK_INIT_FROM_STR = 0x01,	/**< initialize sdk with data structure 'kv_sdk' */
+	KV_SDK_INIT_FROM_STR = 0x01,	/**< initialize sdk with data structure â€˜kv_sdkâ€™ */
 };
 
 /**
@@ -186,7 +187,7 @@ enum kv_keyspace_id {
 enum kv_iterate_read_option {		
 	KV_ITERATE_READ_DEFAULT = 0x00,			/**<  [DEFAULT] default operation for command */
 };
-
+		
 /**
  * @brief options used for store operation
  */
@@ -219,7 +220,6 @@ enum kv_result {
 	KV_ERR_ITERATE_TCG_LOCKED = 0x95,     			/**<  iterate TCG locked */
 	KV_ERR_ITERATE_ERROR = 0x96,     			/**<  an error while iterate, closing the iterate handle is recommended */
 
-
         //0x100 ~ 0x1FF for DD Error
 	KV_ERR_DD_NO_DEVICE = 0x100,
 	KV_ERR_DD_INVALID_PARAM = 0x101,
@@ -281,7 +281,7 @@ typedef struct {
 
 
         int nr_ssd;				/**< number of SSDs */
-        char dev_id[NR_MAX_SSD][DEV_ID_LEN];		/**< PCI devices' address */
+        char dev_id[NR_MAX_SSD][DEV_ID_LEN];		/**< PCI devicesâ€™ address */
         kv_nvme_io_options dd_options[NR_MAX_SSD];	/**< structure about description for devices */
         uint64_t dev_handle[NR_MAX_SSD];	/**< device handle */
 
@@ -323,7 +323,6 @@ typedef struct {
 		int iterate_request_option;
 		int iterate_read_option;
 		int exist_option;
-		int batch_option;
 	}io_option;			/**< options for operations */	
 } kv_param;	
 
diff --git a/PDK/core/src/api/include/udd/spdk/barrier.h b/PDK/core/src/api/include/udd/spdk/barrier.h
index 1b2a559..acae360 100644
--- a/PDK/core/src/api/include/udd/spdk/barrier.h
+++ b/PDK/core/src/api/include/udd/spdk/barrier.h
@@ -2,6 +2,7 @@
  *   BSD LICENSE
  *
  *   Copyright (c) Intel Corporation.
+ *   Copyright (c) 2017, IBM Corporation.
  *   All rights reserved.
  *
  *   Redistribution and use in source and binary forms, with or without
@@ -47,11 +48,66 @@ extern "C" {
 /** Compiler memory barrier */
 #define spdk_compiler_barrier() __asm volatile("" ::: "memory")
 
+/** Read memory barrier */
+#define spdk_rmb()	_spdk_rmb()
+
 /** Write memory barrier */
-#define spdk_wmb()	__asm volatile("sfence" ::: "memory")
+#define spdk_wmb()	_spdk_wmb()
 
 /** Full read/write memory barrier */
-#define spdk_mb()	__asm volatile("mfence" ::: "memory")
+#define spdk_mb()	_spdk_mb()
+
+/** SMP read memory barrier. */
+#define spdk_smp_rmb()	_spdk_smp_rmb()
+
+/** SMP write memory barrier. */
+#define spdk_smp_wmb()	_spdk_smp_wmb()
+
+/** SMP read/write memory barrier. */
+#define spdk_smp_mb()	_spdk_smp_mb()
+
+#ifdef __PPC64__
+
+#define _spdk_rmb()	__asm volatile("sync" ::: "memory")
+#define _spdk_wmb()	__asm volatile("sync" ::: "memory")
+#define _spdk_mb()	__asm volatile("sync" ::: "memory")
+#define _spdk_smp_rmb()	__asm volatile("lwsync" ::: "memory")
+#define _spdk_smp_wmb()	__asm volatile("lwsync" ::: "memory")
+#define _spdk_smp_mb()	spdk_mb()
+
+#elif defined(__aarch64__)
+
+#define _spdk_rmb()	__asm volatile("dsb ld" ::: "memory")
+#define _spdk_wmb()	__asm volatile("dsb st" ::: "memory")
+#define _spdk_mb()	__asm volatile("dsb sy" ::: "memory")
+#define _spdk_smp_rmb()	__asm volatile("dmb ishld" ::: "memory")
+#define _spdk_smp_wmb()	__asm volatile("dmb ishst" ::: "memory")
+#define _spdk_smp_mb()	__asm volatile("dmb ish" ::: "memory")
+
+#elif defined(__i386__) || defined(__x86_64__)
+
+#define _spdk_rmb()	__asm volatile("lfence" ::: "memory")
+#define _spdk_wmb()	__asm volatile("sfence" ::: "memory")
+#define _spdk_mb()	__asm volatile("mfence" ::: "memory")
+#define _spdk_smp_rmb()	spdk_compiler_barrier()
+#define _spdk_smp_wmb()	spdk_compiler_barrier()
+#if defined(__x86_64__)
+#define _spdk_smp_mb()	__asm volatile("lock addl $0, -128(%%rsp); " ::: "memory");
+#elif defined(__i386__)
+#define _spdk_smp_mb()	__asm volatile("lock addl $0, -128(%%esp); " ::: "memory");
+#endif
+
+#else
+
+#define _spdk_rmb()
+#define _spdk_wmb()
+#define _spdk_mb()
+#define _spdk_smp_rmb()
+#define _spdk_smp_wmb()
+#define _spdk_smp_mb()
+#error Unknown architecture
+
+#endif
 
 #ifdef __cplusplus
 }
diff --git a/PDK/core/src/api/include/udd/spdk/base64.h b/PDK/core/src/api/include/udd/spdk/base64.h
new file mode 100644
index 0000000..c158c23
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/base64.h
@@ -0,0 +1,138 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * \file
+ * Base64 utility functions
+ */
+
+#ifndef SPDK_BASE64_H
+#define SPDK_BASE64_H
+
+#include "spdk/stdinc.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * Following the Base64 part in RFC4648:
+ * https://tools.ietf.org/html/rfc4648.html
+ */
+
+/**
+ * Calculate strlen of encoded Base64 string based on raw buffer length.
+ *
+ * \param raw_len Length of raw buffer.
+ * \return Encoded Base64 string length, excluding the terminating null byte ('\0').
+ */
+static inline size_t spdk_base64_get_encoded_strlen(size_t raw_len)
+{
+	return (raw_len + 2) / 3 * 4;
+}
+
+/**
+ * Calculate length of raw buffer based on strlen of encoded Base64.
+ *
+ * \param encoded_strlen Length of enocded Base64 string, excluding terminating null byte ('\0').
+ * \return Length of raw buffer.
+ */
+static inline size_t spdk_base64_get_decoded_len(size_t encoded_strlen)
+{
+	/* text_strlen and raw_len should be (4n,3n), (4n+2, 3n+1) or (4n+3, 3n+2) */
+	return encoded_strlen / 4 * 3 + ((encoded_strlen % 4 + 1) / 2);
+}
+
+/**
+ * Base 64 Encoding with Standard Base64 Alphabet defined in RFC4684.
+ *
+ * \param dst Buffer address of encoded Base64 string. Its length should be enough
+ * to contain Base64 string and the terminating null byte ('\0'), so it needs to be at
+ * least as long as 1 + spdk_base64_get_encoded_strlen(src_len).
+ * \param src Raw data buffer to be encoded.
+ * \param src_len Length of raw data buffer.
+ *
+ * \return 0 on success.
+ * \return -EINVAL if dst or src is NULL, or binary_len <= 0.
+ */
+int spdk_base64_encode(char *dst, const void *src, size_t src_len);
+
+/**
+ * Base 64 Encoding with URL and Filename Safe Alphabet.
+ *
+ * \param dst Buffer address of encoded Base64 string. Its length should be enough
+ * to contain Base64 string and the terminating null byte ('\0'), so it needs to be at
+ * least as long as 1 + spdk_base64_get_encoded_strlen(src_len).
+ * \param src Raw data buffer to be encoded.
+ * \param src_len Length of raw data buffer.
+ *
+ * \return 0 on success.
+ * \return -EINVAL if dst or src is NULL, or binary_len <= 0.
+ */
+int spdk_base64_urlsafe_encode(char *dst, const void *src, size_t src_len);
+
+/**
+ * Base 64 Decoding with Standard Base64 Alphabet defined in RFC4684.
+ *
+ * \param dst Buffer address of decoded raw data. Its length should be enough
+ * to contain decoded raw data, so it needs to be at least as long as
+ * spdk_base64_get_decoded_len(encoded_strlen).
+ * \param dst_len Output parameter for the length of actual decoded raw data.
+ * If NULL, the actual decoded length won't be returned.
+ * \param src Data buffer for base64 string to be decoded.
+ *
+ * \return 0 on success.
+ * \return -EINVAL if dst or src is NULL, or content of src is illegal.
+ */
+int spdk_base64_decode(void *dst, size_t *dst_len, const char *src);
+
+/**
+ * Base 64 Decoding with URL and Filename Safe Alphabet.
+ *
+ * \param dst Buffer address of decoded raw data. Its length should be enough
+ * to contain decoded raw data, so it needs to be at least as long as
+ * spdk_base64_get_decoded_len(encoded_strlen).
+ * \param dst_len Output parameter for the length of actual decoded raw data.
+ * If NULL, the actual decoded length won't be returned.
+ * \param src Data buffer for base64 string to be decoded.
+ *
+ * \return 0 on success.
+ * \return -EINVAL if dst or src is NULL, or content of src is illegal.
+ */
+int spdk_base64_urlsafe_decode(void *dst, size_t *dst_len, const char *src);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* SPDK_BASE64_H */
diff --git a/PDK/core/src/api/include/udd/spdk/bdev.h b/PDK/core/src/api/include/udd/spdk/bdev.h
index b0cc916..0e41268 100644
--- a/PDK/core/src/api/include/udd/spdk/bdev.h
+++ b/PDK/core/src/api/include/udd/spdk/bdev.h
@@ -1,7 +1,6 @@
 /*-
  *   BSD LICENSE
  *
- *   Copyright (C) 2008-2012 Daisuke Aoyama <aoyama@peach.ne.jp>.
  *   Copyright (c) Intel Corporation.
  *   All rights reserved.
  *
@@ -41,12 +40,31 @@
 
 #include "spdk/stdinc.h"
 
-#include "spdk/event.h"
 #include "spdk/scsi_spec.h"
+#include "spdk/nvme_spec.h"
+#include "spdk/json.h"
+#include "spdk/queue.h"
+#include "spdk/histogram_data.h"
+#include "spdk/dif.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
 
 #define SPDK_BDEV_SMALL_BUF_MAX_SIZE 8192
 #define SPDK_BDEV_LARGE_BUF_MAX_SIZE (64 * 1024)
 
+/* Increase the buffer size to store interleaved metadata.  Increment is the
+ *  amount necessary to store metadata per data block.  16 byte metadata per
+ *  512 byte data block is the current maximum ratio of metadata per block.
+ */
+#define SPDK_BDEV_BUF_SIZE_WITH_MD(x)	(((x) / 512) * (512 + 16))
+
+/**
+ * Block device remove callback.
+ *
+ * \param remove_ctx Context for the removed block device.
+ */
 typedef void (*spdk_bdev_remove_cb_t)(void *remove_ctx);
 
 /**
@@ -59,12 +77,12 @@ struct spdk_bdev_io;
 struct spdk_bdev_fn_table;
 struct spdk_io_channel;
 struct spdk_json_write_ctx;
+struct spdk_uuid;
 
-/** Blockdev status */
+/** bdev status */
 enum spdk_bdev_status {
 	SPDK_BDEV_STATUS_INVALID,
-	SPDK_BDEV_STATUS_UNCLAIMED,
-	SPDK_BDEV_STATUS_CLAIMED,
+	SPDK_BDEV_STATUS_READY,
 	SPDK_BDEV_STATUS_REMOVING,
 };
 
@@ -75,78 +93,224 @@ enum spdk_bdev_status {
  */
 struct spdk_bdev;
 
-/** Blockdev I/O type */
+/**
+ * \brief Handle to an opened SPDK block device.
+ */
+struct spdk_bdev_desc;
+
+/** bdev I/O type */
 enum spdk_bdev_io_type {
-	SPDK_BDEV_IO_TYPE_READ = 1,
+	SPDK_BDEV_IO_TYPE_INVALID = 0,
+	SPDK_BDEV_IO_TYPE_READ,
 	SPDK_BDEV_IO_TYPE_WRITE,
 	SPDK_BDEV_IO_TYPE_UNMAP,
 	SPDK_BDEV_IO_TYPE_FLUSH,
 	SPDK_BDEV_IO_TYPE_RESET,
+	SPDK_BDEV_IO_TYPE_NVME_ADMIN,
+	SPDK_BDEV_IO_TYPE_NVME_IO,
+	SPDK_BDEV_IO_TYPE_NVME_IO_MD,
+	SPDK_BDEV_IO_TYPE_WRITE_ZEROES,
+	SPDK_BDEV_IO_TYPE_ZCOPY,
+	SPDK_BDEV_NUM_IO_TYPES /* Keep last */
 };
 
-/** Blockdev I/O completion status */
-enum spdk_bdev_io_status {
-	SPDK_BDEV_IO_STATUS_SCSI_ERROR = -3,
-	SPDK_BDEV_IO_STATUS_NVME_ERROR = -2,
-	SPDK_BDEV_IO_STATUS_FAILED = -1,
-	SPDK_BDEV_IO_STATUS_PENDING = 0,
-	SPDK_BDEV_IO_STATUS_SUCCESS = 1,
-};
-
-/** Blockdev reset operation type */
-enum spdk_bdev_reset_type {
-	/**
-	 * A hard reset indicates that the blockdev layer should not
-	 *  invoke the completion callback for I/Os issued before the
-	 *  reset is issued but completed after the reset is complete.
-	 */
-	SPDK_BDEV_RESET_HARD,
-
-	/**
-	 * A soft reset indicates that the blockdev layer should still
-	 *  invoke the completion callback for I/Os issued before the
-	 *  reset is issued but completed after the reset is complete.
-	 */
-	SPDK_BDEV_RESET_SOFT,
+/** bdev QoS rate limit type */
+enum spdk_bdev_qos_rate_limit_type {
+	/** IOPS rate limit for both read and write */
+	SPDK_BDEV_QOS_RW_IOPS_RATE_LIMIT = 0,
+	/** Byte per second rate limit for both read and write */
+	SPDK_BDEV_QOS_RW_BPS_RATE_LIMIT,
+	/** Byte per second rate limit for read only */
+	SPDK_BDEV_QOS_R_BPS_RATE_LIMIT,
+	/** Byte per second rate limit for write only */
+	SPDK_BDEV_QOS_W_BPS_RATE_LIMIT,
+	/** Keep last */
+	SPDK_BDEV_QOS_NUM_RATE_LIMIT_TYPES
 };
 
+/**
+ * Block device completion callback.
+ *
+ * \param bdev_io Block device I/O that has completed.
+ * \param success True if I/O completed successfully or false if it failed;
+ * additional error information may be retrieved from bdev_io by calling
+ * spdk_bdev_io_get_nvme_status() or spdk_bdev_io_get_scsi_status().
+ * \param cb_arg Callback argument specified when bdev_io was submitted.
+ */
 typedef void (*spdk_bdev_io_completion_cb)(struct spdk_bdev_io *bdev_io,
-		enum spdk_bdev_io_status status,
+		bool success,
 		void *cb_arg);
 
+struct spdk_bdev_io_stat {
+	uint64_t bytes_read;
+	uint64_t num_read_ops;
+	uint64_t bytes_written;
+	uint64_t num_write_ops;
+	uint64_t bytes_unmapped;
+	uint64_t num_unmap_ops;
+	uint64_t read_latency_ticks;
+	uint64_t write_latency_ticks;
+	uint64_t unmap_latency_ticks;
+	uint64_t ticks_rate;
+};
+
+struct spdk_bdev_opts {
+	uint32_t bdev_io_pool_size;
+	uint32_t bdev_io_cache_size;
+};
+
+void spdk_bdev_get_opts(struct spdk_bdev_opts *opts);
+
+int spdk_bdev_set_opts(struct spdk_bdev_opts *opts);
+
+/**
+ * Block device initialization callback.
+ *
+ * \param cb_arg Callback argument.
+ * \param rc 0 if block device initialized successfully or negative errno if it failed.
+ */
+typedef void (*spdk_bdev_init_cb)(void *cb_arg, int rc);
+
+/**
+ * Block device finish callback.
+ *
+ * \param cb_arg Callback argument.
+ */
+typedef void (*spdk_bdev_fini_cb)(void *cb_arg);
+typedef void (*spdk_bdev_get_device_stat_cb)(struct spdk_bdev *bdev,
+		struct spdk_bdev_io_stat *stat, void *cb_arg, int rc);
+
+/**
+ * Initialize block device modules.
+ *
+ * \param cb_fn Called when the initialization is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bdev_initialize(spdk_bdev_init_cb cb_fn, void *cb_arg);
+
+/**
+ * Perform cleanup work to remove the registered block device modules.
+ *
+ * \param cb_fn Called when the removal is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bdev_finish(spdk_bdev_fini_cb cb_fn, void *cb_arg);
+
+/**
+ * Get the configuration options for the registered block device modules.
+ *
+ * \param fp The pointer to a file that will be written to the configuration options.
+ */
+void spdk_bdev_config_text(FILE *fp);
+
+/**
+ * Get the full configuration options for the registered block device modules and created bdevs.
+ *
+ * \param w pointer to a JSON write context where the configuration will be written.
+ */
+void spdk_bdev_subsystem_config_json(struct spdk_json_write_ctx *w);
+
+/**
+ * Get block device by the block device name.
+ *
+ * \param bdev_name The name of the block device.
+ * \return Block device associated with the name or NULL if no block device with
+ * bdev_name is currently registered.
+ */
 struct spdk_bdev *spdk_bdev_get_by_name(const char *bdev_name);
-void spdk_bdev_unregister(struct spdk_bdev *bdev);
 
+/**
+ * Get the first registered block device.
+ *
+ * \return The first registered block device.
+ */
 struct spdk_bdev *spdk_bdev_first(void);
+
+/**
+ * Get the next registered block device.
+ *
+ * \param prev The current block device.
+ * \return The next registered block device.
+ */
 struct spdk_bdev *spdk_bdev_next(struct spdk_bdev *prev);
 
 /**
- * Claim ownership of a block device.
+ * Get the first block device without virtual block devices on top.
  *
- * User applications and virtual blockdevs may use this to mediate access to bdevs.
+ * This function only traverses over block devices which have no virtual block
+ * devices on top of them, then get the first one.
  *
- * When the ownership of the bdev is no longer needed, the user should call spdk_bdev_unclaim().
+ * \return The first block device without virtual block devices on top.
+ */
+struct spdk_bdev *spdk_bdev_first_leaf(void);
+
+/**
+ * Get the next block device without virtual block devices on top.
+ *
+ * This function only traverses over block devices which have no virtual block
+ * devices on top of them, then get the next one.
+ *
+ * \param prev The current block device.
+ * \return The next block device without virtual block devices on top.
+ */
+struct spdk_bdev *spdk_bdev_next_leaf(struct spdk_bdev *prev);
+
+/**
+ * Open a block device for I/O operations.
  *
- * \param bdev Block device to claim.
- * \param remove_cb callback function for hot remove the device.
- * \param remove_ctx param for hot removal callback function.
- * \return true if the caller claimed the bdev, or false if it was already claimed by another user.
+ * \param bdev Block device to open.
+ * \param write true is read/write access requested, false if read-only
+ * \param remove_cb notification callback to be called when the bdev gets
+ * hotremoved. This will always be called on the same thread that
+ * spdk_bdev_open() was called on. It can be NULL, in which case the upper
+ * layer won't be notified about the bdev hotremoval. The descriptor will
+ * have to be manually closed to make the bdev unregister proceed.
+ * \param remove_ctx param for remove_cb.
+ * \param desc output parameter for the descriptor when operation is successful
+ * \return 0 if operation is successful, suitable errno value otherwise
  */
-bool spdk_bdev_claim(struct spdk_bdev *bdev, spdk_bdev_remove_cb_t remove_cb, void *remove_ctx);
+int spdk_bdev_open(struct spdk_bdev *bdev, bool write, spdk_bdev_remove_cb_t remove_cb,
+		   void *remove_ctx, struct spdk_bdev_desc **desc);
 
 /**
- * Release claim of ownership of a block device.
+ * Close a previously opened block device.
  *
- * When a bdev reference acquired with spdk_bdev_claim() is no longer needed, the user should
- * release the claim using spdk_bdev_unclaim().
+ * Must be called on the same thread that the spdk_bdev_open()
+ * was performed on.
+ *
+ * \param desc Block device descriptor to close.
+ */
+void spdk_bdev_close(struct spdk_bdev_desc *desc);
+
+/**
+ * Get the bdev associated with a bdev descriptor.
  *
- * \param bdev Block device to release.
+ * \param desc Open block device desciptor
+ * \return bdev associated with the descriptor
  */
-void spdk_bdev_unclaim(struct spdk_bdev *bdev);
+struct spdk_bdev *spdk_bdev_desc_get_bdev(struct spdk_bdev_desc *desc);
 
+/**
+ * Check whether the block device supports the I/O type.
+ *
+ * \param bdev Block device to check.
+ * \param io_type The specific I/O type like read, write, flush, unmap.
+ * \return true if support, false otherwise.
+ */
 bool spdk_bdev_io_type_supported(struct spdk_bdev *bdev, enum spdk_bdev_io_type io_type);
 
-int spdk_bdev_dump_config_json(struct spdk_bdev *bdev, struct spdk_json_write_ctx *w);
+/**
+ * Output driver-specific information to a JSON stream.
+ *
+ * The JSON write context will be initialized with an open object, so the bdev
+ * driver should write a name(based on the driver name) followed by a JSON value
+ * (most likely another nested object).
+ *
+ * \param bdev Block device to query.
+ * \param w JSON write context. It will store the driver-specific configuration context.
+ * \return 0 on success, negated errno on failure.
+ */
+int spdk_bdev_dump_info_json(struct spdk_bdev *bdev, struct spdk_json_write_ctx *w);
 
 /**
  * Get block device name.
@@ -183,12 +347,35 @@ uint32_t spdk_bdev_get_block_size(const struct spdk_bdev *bdev);
 uint64_t spdk_bdev_get_num_blocks(const struct spdk_bdev *bdev);
 
 /**
- * Get maximum number of descriptors per unmap request.
+ * Get the string of quality of service rate limit.
+ *
+ * \param type Type of rate limit to query.
+ * \return String of QoS type.
+ */
+const char *spdk_bdev_get_qos_rpc_type(enum spdk_bdev_qos_rate_limit_type type);
+
+/**
+ * Get the quality of service rate limits on a bdev.
  *
  * \param bdev Block device to query.
- * \return Maximum number of unmap descriptors per request.
+ * \param limits Pointer to the QoS rate limits array which holding the limits.
+ *
+ * The limits are ordered based on the @ref spdk_bdev_qos_rate_limit_type enum.
+ */
+void spdk_bdev_get_qos_rate_limits(struct spdk_bdev *bdev, uint64_t *limits);
+
+/**
+ * Set the quality of service rate limits on a bdev.
+ *
+ * \param bdev Block device.
+ * \param limits Pointer to the QoS rate limits array which holding the limits.
+ * \param cb_fn Callback function to be called when the QoS limit has been updated.
+ * \param cb_arg Argument to pass to cb_fn.
+ *
+ * The limits are ordered based on the @ref spdk_bdev_qos_rate_limit_type enum.
  */
-uint32_t spdk_bdev_get_max_unmap_descriptors(const struct spdk_bdev *bdev);
+void spdk_bdev_set_qos_rate_limits(struct spdk_bdev *bdev, uint64_t *limits,
+				   void (*cb_fn)(void *cb_arg, int status), void *cb_arg);
 
 /**
  * Get minimum I/O buffer address alignment for a bdev.
@@ -199,6 +386,15 @@ uint32_t spdk_bdev_get_max_unmap_descriptors(const struct spdk_bdev *bdev);
 size_t spdk_bdev_get_buf_align(const struct spdk_bdev *bdev);
 
 /**
+ * Get optimal I/O boundary for a bdev.
+ *
+ * \param bdev Block device to query.
+ * \return Optimal I/O boundary in blocks that should not be crossed for best performance, or 0 if
+ *         no optimal boundary is reported.
+ */
+uint32_t spdk_bdev_get_optimal_io_boundary(const struct spdk_bdev *bdev);
+
+/**
  * Query whether block device has an enabled write cache.
  *
  * \param bdev Block device to query.
@@ -209,32 +405,767 @@ size_t spdk_bdev_get_buf_align(const struct spdk_bdev *bdev);
  */
 bool spdk_bdev_has_write_cache(const struct spdk_bdev *bdev);
 
-struct spdk_bdev_io *spdk_bdev_read(struct spdk_bdev *bdev, struct spdk_io_channel *ch,
-				    void *buf, uint64_t offset, uint64_t nbytes,
-				    spdk_bdev_io_completion_cb cb, void *cb_arg);
-struct spdk_bdev_io *
-spdk_bdev_readv(struct spdk_bdev *bdev, struct spdk_io_channel *ch,
-		struct iovec *iov, int iovcnt,
-		uint64_t offset, uint64_t nbytes,
-		spdk_bdev_io_completion_cb cb, void *cb_arg);
-struct spdk_bdev_io *spdk_bdev_write(struct spdk_bdev *bdev, struct spdk_io_channel *ch,
-				     void *buf, uint64_t offset, uint64_t nbytes,
-				     spdk_bdev_io_completion_cb cb, void *cb_arg);
-struct spdk_bdev_io *spdk_bdev_writev(struct spdk_bdev *bdev, struct spdk_io_channel *ch,
-				      struct iovec *iov, int iovcnt,
-				      uint64_t offset, uint64_t len,
-				      spdk_bdev_io_completion_cb cb, void *cb_arg);
-struct spdk_bdev_io *spdk_bdev_unmap(struct spdk_bdev *bdev, struct spdk_io_channel *ch,
-				     struct spdk_scsi_unmap_bdesc *unmap_d,
-				     uint16_t bdesc_count,
-				     spdk_bdev_io_completion_cb cb, void *cb_arg);
-struct spdk_bdev_io *spdk_bdev_flush(struct spdk_bdev *bdev, struct spdk_io_channel *ch,
-				     uint64_t offset, uint64_t length,
-				     spdk_bdev_io_completion_cb cb, void *cb_arg);
-int spdk_bdev_free_io(struct spdk_bdev_io *bdev_io);
-int spdk_bdev_reset(struct spdk_bdev *bdev, enum spdk_bdev_reset_type,
+/**
+ * Get a bdev's UUID.
+ *
+ * \param bdev Block device to query.
+ * \return Pointer to UUID.
+ *
+ * Not all bdevs will have a UUID; in this case, the returned UUID will be
+ * the nil UUID (all bytes zero).
+ */
+const struct spdk_uuid *spdk_bdev_get_uuid(const struct spdk_bdev *bdev);
+
+/**
+ * Get block device metadata size.
+ *
+ * \param bdev Block device to query.
+ * \return Size of metadata for this bdev in bytes.
+ */
+uint32_t spdk_bdev_get_md_size(const struct spdk_bdev *bdev);
+
+/**
+ * Query whether metadata is interleaved with block data or separated
+ * with block data.
+ *
+ * \param bdev Block device to query.
+ * \return true if metadata is interleaved with block data or false
+ * if metadata is separated with block data.
+ *
+ * Note this function is valid only if there is metadata.
+ */
+bool spdk_bdev_is_md_interleaved(const struct spdk_bdev *bdev);
+
+/**
+ * Get block device data block size.
+ *
+ * Data block size is equal to block size if there is no metadata or
+ * metadata is separated with block data, or equal to block size minus
+ * metadata size if there is metadata and it is interleaved with
+ * block data.
+ *
+ * \param bdev Block device to query.
+ * \return Size of data block for this bdev in bytes.
+ */
+uint32_t spdk_bdev_get_data_block_size(const struct spdk_bdev *bdev);
+
+/**
+ * Get DIF type of the block device.
+ *
+ * \param bdev Block device to query.
+ * \return DIF type of the block device.
+ */
+enum spdk_dif_type spdk_bdev_get_dif_type(const struct spdk_bdev *bdev);
+
+/**
+ * Check whether DIF is set in the first 8 bytes or the last 8 bytes of metadata.
+ *
+ * \param bdev Block device to query.
+ * \return true if DIF is set in the first 8 bytes of metadata, or false
+ * if DIF is set in the last 8 bytes of metadata.
+ *
+ * Note that this function is valid only if DIF type is not SPDK_DIF_DISABLE.
+ */
+bool spdk_bdev_is_dif_head_of_md(const struct spdk_bdev *bdev);
+
+/**
+ * Check whether the DIF check type is enabled.
+ *
+ * \param bdev Block device to query.
+ * \param check_type The specific DIF check type.
+ * \return true if enabled, false otherwise.
+ */
+bool spdk_bdev_is_dif_check_enabled(const struct spdk_bdev *bdev,
+				    enum spdk_dif_check_type check_type);
+
+/**
+ * Get the most recently measured queue depth from a bdev.
+ *
+ * The reported queue depth is the aggregate of outstanding I/O
+ * across all open channels associated with this bdev.
+ *
+ * \param bdev Block device to query.
+ *
+ * \return The most recent queue depth measurement for the bdev.
+ * If tracking is not enabled, the function will return UINT64_MAX
+ * It is also possible to receive UINT64_MAX after enabling tracking
+ * but before the first period has expired.
+ */
+uint64_t
+spdk_bdev_get_qd(const struct spdk_bdev *bdev);
+
+/**
+ * Get the queue depth polling period.
+ *
+ * The return value of this function is only valid if the bdev's
+ * queue depth tracking status is set to true.
+ *
+ * \param bdev Block device to query.
+ *
+ * \return The period at which this bdev's gueue depth is being refreshed.
+ */
+uint64_t
+spdk_bdev_get_qd_sampling_period(const struct spdk_bdev *bdev);
+
+/**
+ * Enable or disable queue depth sampling for this bdev.
+ *
+ * Enables queue depth sampling when period is greater than 0. Disables it when the period
+ * is equal to zero. The resulting queue depth is stored in the spdk_bdev object as
+ * measured_queue_depth.
+ *
+ * \param bdev Block device on which to enable queue depth tracking.
+ * \param period The period at which to poll this bdev's queue depth. If this is set
+ * to zero, polling will be disabled.
+ */
+void spdk_bdev_set_qd_sampling_period(struct spdk_bdev *bdev, uint64_t period);
+
+/**
+ * Get the time spent processing IO for this device.
+ *
+ * This value is dependent upon the queue depth sampling period and is
+ * incremented at sampling time by the sampling period only if the measured
+ * queue depth is greater than 0.
+ *
+ * The disk utilization can be calculated by the following formula:
+ * disk_util = (io_time_2 - io_time_1) / elapsed_time.
+ * The user is responsible for tracking the elapsed time between two measurements.
+ *
+ * \param bdev Block device to query.
+ *
+ * \return The io time for this device in microseconds.
+ */
+uint64_t spdk_bdev_get_io_time(const struct spdk_bdev *bdev);
+
+/**
+ * Get the weighted IO processing time for this bdev.
+ *
+ * This value is dependent upon the queue depth sampling period and is
+ * equal to the time spent reading from or writing to a device times
+ * the measured queue depth during each sampling period.
+ *
+ * The average queue depth can be calculated by the following formula:
+ * queue_depth = (weighted_io_time_2 - weighted_io_time_1) / elapsed_time.
+ * The user is responsible for tracking the elapsed time between two measurements.
+ *
+ * \param bdev Block device to query.
+ *
+ * \return The weighted io time for this device in microseconds.
+ */
+uint64_t spdk_bdev_get_weighted_io_time(const struct spdk_bdev *bdev);
+
+/**
+ * Obtain an I/O channel for the block device opened by the specified
+ * descriptor. I/O channels are bound to threads, so the resulting I/O
+ * channel may only be used from the thread it was originally obtained
+ * from.
+ *
+ * \param desc Block device descriptor.
+ *
+ * \return A handle to the I/O channel or NULL on failure.
+ */
+struct spdk_io_channel *spdk_bdev_get_io_channel(struct spdk_bdev_desc *desc);
+
+/**
+ * \defgroup bdev_io_submit_functions bdev I/O Submit Functions
+ *
+ * These functions submit a new I/O request to a bdev.  The I/O request will
+ *  be represented by an spdk_bdev_io structure allocated from a global pool.
+ *  These functions will return -ENOMEM if the spdk_bdev_io pool is empty.
+ */
+
+/**
+ * Submit a read request to the bdev on the given channel.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param buf Data buffer to read into.
+ * \param offset The offset, in bytes, from the start of the block device.
+ * \param nbytes The number of bytes to read.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset and/or nbytes are not aligned or out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ */
+int spdk_bdev_read(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+		   void *buf, uint64_t offset, uint64_t nbytes,
+		   spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit a read request to the bdev on the given channel.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param buf Data buffer to read into.
+ * \param offset_blocks The offset, in blocks, from the start of the block device.
+ * \param num_blocks The number of blocks to read.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset_blocks and/or num_blocks are out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ */
+int spdk_bdev_read_blocks(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+			  void *buf, uint64_t offset_blocks, uint64_t num_blocks,
+			  spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit a read request to the bdev on the given channel. This differs from
+ * spdk_bdev_read by allowing the data buffer to be described in a scatter
+ * gather list. Some physical devices place memory alignment requirements on
+ * data and may not be able to directly transfer into the buffers provided. In
+ * this case, the request may fail.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param iov A scatter gather list of buffers to be read into.
+ * \param iovcnt The number of elements in iov.
+ * \param offset The offset, in bytes, from the start of the block device.
+ * \param nbytes The number of bytes to read.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset and/or nbytes are not aligned or out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ */
+int spdk_bdev_readv(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+		    struct iovec *iov, int iovcnt,
+		    uint64_t offset, uint64_t nbytes,
 		    spdk_bdev_io_completion_cb cb, void *cb_arg);
-struct spdk_io_channel *spdk_bdev_get_io_channel(struct spdk_bdev *bdev, uint32_t priority);
+
+/**
+ * Submit a read request to the bdev on the given channel. This differs from
+ * spdk_bdev_read by allowing the data buffer to be described in a scatter
+ * gather list. Some physical devices place memory alignment requirements on
+ * data and may not be able to directly transfer into the buffers provided. In
+ * this case, the request may fail.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param iov A scatter gather list of buffers to be read into.
+ * \param iovcnt The number of elements in iov.
+ * \param offset_blocks The offset, in blocks, from the start of the block device.
+ * \param num_blocks The number of blocks to read.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset_blocks and/or num_blocks are out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ */
+int spdk_bdev_readv_blocks(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+			   struct iovec *iov, int iovcnt,
+			   uint64_t offset_blocks, uint64_t num_blocks,
+			   spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit a write request to the bdev on the given channel.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param buf Data buffer to written from.
+ * \param offset The offset, in bytes, from the start of the block device.
+ * \param nbytes The number of bytes to write. buf must be greater than or equal to this size.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset and/or nbytes are not aligned or out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ *   * -EBADF - desc not open for writing
+ */
+int spdk_bdev_write(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+		    void *buf, uint64_t offset, uint64_t nbytes,
+		    spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit a write request to the bdev on the given channel.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param buf Data buffer to written from.
+ * \param offset_blocks The offset, in blocks, from the start of the block device.
+ * \param num_blocks The number of blocks to write. buf must be greater than or equal to this size.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset_blocks and/or num_blocks are out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ *   * -EBADF - desc not open for writing
+ */
+int spdk_bdev_write_blocks(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+			   void *buf, uint64_t offset_blocks, uint64_t num_blocks,
+			   spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit a write request to the bdev on the given channel. This differs from
+ * spdk_bdev_write by allowing the data buffer to be described in a scatter
+ * gather list. Some physical devices place memory alignment requirements on
+ * data and may not be able to directly transfer out of the buffers provided. In
+ * this case, the request may fail.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param iov A scatter gather list of buffers to be written from.
+ * \param iovcnt The number of elements in iov.
+ * \param offset The offset, in bytes, from the start of the block device.
+ * \param len The size of data to write.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset and/or nbytes are not aligned or out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ *   * -EBADF - desc not open for writing
+ */
+int spdk_bdev_writev(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+		     struct iovec *iov, int iovcnt,
+		     uint64_t offset, uint64_t len,
+		     spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit a write request to the bdev on the given channel. This differs from
+ * spdk_bdev_write by allowing the data buffer to be described in a scatter
+ * gather list. Some physical devices place memory alignment requirements on
+ * data and may not be able to directly transfer out of the buffers provided. In
+ * this case, the request may fail.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param iov A scatter gather list of buffers to be written from.
+ * \param iovcnt The number of elements in iov.
+ * \param offset_blocks The offset, in blocks, from the start of the block device.
+ * \param num_blocks The number of blocks to write.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset_blocks and/or num_blocks are out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ *   * -EBADF - desc not open for writing
+ */
+int spdk_bdev_writev_blocks(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+			    struct iovec *iov, int iovcnt,
+			    uint64_t offset_blocks, uint64_t num_blocks,
+			    spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit a request to acquire a data buffer that represents the given
+ * range of blocks. The data buffer is placed in the spdk_bdev_io structure
+ * and can be obtained by calling spdk_bdev_io_get_iovec().
+ *
+ * \param desc Block device descriptor
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param offset_blocks The offset, in blocks, from the start of the block device.
+ * \param num_blocks The number of blocks.
+ * \param populate Whether the data buffer should be populated with the
+ *                 data at the given blocks. Populating the data buffer can
+ *                 be skipped if the user writes new data to the entire buffer.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ */
+int spdk_bdev_zcopy_start(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+			  uint64_t offset_blocks, uint64_t num_blocks,
+			  bool populate,
+			  spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+
+/**
+ * Submit a request to release a data buffer representing a range of blocks.
+ *
+ * \param bdev_io I/O request returned in the completion callback of spdk_bdev_zcopy_start().
+ * \param commit Whether to commit the data in the buffers to the blocks before releasing.
+ *               The data does not need to be committed if it was not modified.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ */
+int spdk_bdev_zcopy_end(struct spdk_bdev_io *bdev_io, bool commit,
+			spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit a write zeroes request to the bdev on the given channel. This command
+ *  ensures that all bytes in the specified range are set to 00h
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param offset The offset, in bytes, from the start of the block device.
+ * \param len The size of data to zero.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset and/or nbytes are not aligned or out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ *   * -EBADF - desc not open for writing
+ */
+int spdk_bdev_write_zeroes(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+			   uint64_t offset, uint64_t len,
+			   spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit a write zeroes request to the bdev on the given channel. This command
+ *  ensures that all bytes in the specified range are set to 00h
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param offset_blocks The offset, in blocks, from the start of the block device.
+ * \param num_blocks The number of blocks to zero.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset_blocks and/or num_blocks are out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ *   * -EBADF - desc not open for writing
+ */
+int spdk_bdev_write_zeroes_blocks(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+				  uint64_t offset_blocks, uint64_t num_blocks,
+				  spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit an unmap request to the block device. Unmap is sometimes also called trim or
+ * deallocate. This notifies the device that the data in the blocks described is no
+ * longer valid. Reading blocks that have been unmapped results in indeterminate data.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param offset The offset, in bytes, from the start of the block device.
+ * \param nbytes The number of bytes to unmap. Must be a multiple of the block size.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset and/or nbytes are not aligned or out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ *   * -EBADF - desc not open for writing
+ */
+int spdk_bdev_unmap(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+		    uint64_t offset, uint64_t nbytes,
+		    spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit an unmap request to the block device. Unmap is sometimes also called trim or
+ * deallocate. This notifies the device that the data in the blocks described is no
+ * longer valid. Reading blocks that have been unmapped results in indeterminate data.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param offset_blocks The offset, in blocks, from the start of the block device.
+ * \param num_blocks The number of blocks to unmap.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset_blocks and/or num_blocks are out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ *   * -EBADF - desc not open for writing
+ */
+int spdk_bdev_unmap_blocks(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+			   uint64_t offset_blocks, uint64_t num_blocks,
+			   spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit a flush request to the bdev on the given channel. For devices with volatile
+ * caches, data is not guaranteed to be persistent until the completion of a flush
+ * request. Call spdk_bdev_has_write_cache() to check if the bdev has a volatile cache.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param offset The offset, in bytes, from the start of the block device.
+ * \param length The number of bytes.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset and/or nbytes are not aligned or out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ *   * -EBADF - desc not open for writing
+ */
+int spdk_bdev_flush(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+		    uint64_t offset, uint64_t length,
+		    spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit a flush request to the bdev on the given channel. For devices with volatile
+ * caches, data is not guaranteed to be persistent until the completion of a flush
+ * request. Call spdk_bdev_has_write_cache() to check if the bdev has a volatile cache.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param offset_blocks The offset, in blocks, from the start of the block device.
+ * \param num_blocks The number of blocks.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -EINVAL - offset_blocks and/or num_blocks are out of range
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ *   * -EBADF - desc not open for writing
+ */
+int spdk_bdev_flush_blocks(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+			   uint64_t offset_blocks, uint64_t num_blocks,
+			   spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit a reset request to the bdev on the given channel.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ */
+int spdk_bdev_reset(struct spdk_bdev_desc *desc, struct spdk_io_channel *ch,
+		    spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit an NVMe Admin command to the bdev. This passes directly through
+ * the block layer to the device. Support for NVMe passthru is optional,
+ * indicated by calling spdk_bdev_io_type_supported().
+ *
+ * The SGL/PRP will be automated generated based on the given buffer,
+ * so that portion of the command may be left empty.
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * \param desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param cmd The raw NVMe command. Must be an admin command.
+ * \param buf Data buffer to written from.
+ * \param nbytes The number of bytes to transfer. buf must be greater than or equal to this size.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ *   * -EBADF - desc not open for writing
+ */
+int spdk_bdev_nvme_admin_passthru(struct spdk_bdev_desc *desc,
+				  struct spdk_io_channel *ch,
+				  const struct spdk_nvme_cmd *cmd,
+				  void *buf, size_t nbytes,
+				  spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit an NVMe I/O command to the bdev. This passes directly through
+ * the block layer to the device. Support for NVMe passthru is optional,
+ * indicated by calling spdk_bdev_io_type_supported().
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * The SGL/PRP will be automated generated based on the given buffer,
+ * so that portion of the command may be left empty. Also, the namespace
+ * id (nsid) will be populated automatically.
+ *
+ * \param bdev_desc Block device descriptor.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param cmd The raw NVMe command. Must be in the NVM command set.
+ * \param buf Data buffer to written from.
+ * \param nbytes The number of bytes to transfer. buf must be greater than or equal to this size.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ *   * -EBADF - desc not open for writing
+ */
+int spdk_bdev_nvme_io_passthru(struct spdk_bdev_desc *bdev_desc,
+			       struct spdk_io_channel *ch,
+			       const struct spdk_nvme_cmd *cmd,
+			       void *buf, size_t nbytes,
+			       spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Submit an NVMe I/O command to the bdev. This passes directly through
+ * the block layer to the device. Support for NVMe passthru is optional,
+ * indicated by calling spdk_bdev_io_type_supported().
+ *
+ * \ingroup bdev_io_submit_functions
+ *
+ * The SGL/PRP will be automated generated based on the given buffer,
+ * so that portion of the command may be left empty. Also, the namespace
+ * id (nsid) will be populated automatically.
+ *
+ * \param bdev_desc Block device descriptor
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param cmd The raw NVMe command. Must be in the NVM command set.
+ * \param buf Data buffer to written from.
+ * \param nbytes The number of bytes to transfer. buf must be greater than or equal to this size.
+ * \param md_buf Meta data buffer to written from.
+ * \param md_len md_buf size to transfer. md_buf must be greater than or equal to this size.
+ * \param cb Called when the request is complete.
+ * \param cb_arg Argument passed to cb.
+ *
+ * \return 0 on success. On success, the callback will always
+ * be called (even if the request ultimately failed). Return
+ * negated errno on failure, in which case the callback will not be called.
+ *   * -ENOMEM - spdk_bdev_io buffer cannot be allocated
+ *   * -EBADF - desc not open for writing
+ */
+int spdk_bdev_nvme_io_passthru_md(struct spdk_bdev_desc *bdev_desc,
+				  struct spdk_io_channel *ch,
+				  const struct spdk_nvme_cmd *cmd,
+				  void *buf, size_t nbytes, void *md_buf, size_t md_len,
+				  spdk_bdev_io_completion_cb cb, void *cb_arg);
+
+/**
+ * Free an I/O request. This should only be called after the completion callback
+ * for the I/O has been called and notifies the bdev layer that memory may now
+ * be released.
+ *
+ * \param bdev_io I/O request.
+ */
+void spdk_bdev_free_io(struct spdk_bdev_io *bdev_io);
+
+/**
+ * Block device I/O wait callback
+ *
+ * Callback function to notify when an spdk_bdev_io structure is available
+ * to satisfy a call to one of the @ref bdev_io_submit_functions.
+ */
+typedef void (*spdk_bdev_io_wait_cb)(void *cb_arg);
+
+/**
+ * Structure to register a callback when an spdk_bdev_io becomes available.
+ */
+struct spdk_bdev_io_wait_entry {
+	struct spdk_bdev			*bdev;
+	spdk_bdev_io_wait_cb			cb_fn;
+	void					*cb_arg;
+	TAILQ_ENTRY(spdk_bdev_io_wait_entry)	link;
+};
+
+/**
+ * Add an entry into the calling thread's queue to be notified when an
+ * spdk_bdev_io becomes available.
+ *
+ * When one of the @ref bdev_io_submit_functions returns -ENOMEM, it means
+ * the spdk_bdev_io buffer pool has no available buffers. This function may
+ * be called to register a callback to be notified when a buffer becomes
+ * available on the calling thread.
+ *
+ * The callback function will always be called on the same thread as this
+ * function was called.
+ *
+ * This function must only be called immediately after one of the
+ * @ref bdev_io_submit_functions returns -ENOMEM.
+ *
+ * \param bdev Block device.  The block device that the caller will submit
+ *             an I/O to when the callback is invoked.  Must match the bdev
+ *             member in the entry parameter.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param entry Data structure allocated by the caller specifying the callback
+ *              function and argument.
+ *
+ * \return 0 on success.
+ *         -EINVAL if bdev parameter does not match bdev member in entry
+ *         -EINVAL if an spdk_bdev_io structure was available on this thread.
+ */
+int spdk_bdev_queue_io_wait(struct spdk_bdev *bdev, struct spdk_io_channel *ch,
+			    struct spdk_bdev_io_wait_entry *entry);
+
+/**
+ * Return I/O statistics for this channel.
+ *
+ * \param bdev Block device.
+ * \param ch I/O channel. Obtained by calling spdk_bdev_get_io_channel().
+ * \param stat The per-channel statistics.
+ *
+ */
+void spdk_bdev_get_io_stat(struct spdk_bdev *bdev, struct spdk_io_channel *ch,
+			   struct spdk_bdev_io_stat *stat);
+
+
+/**
+ * Return I/O statistics for this bdev. All the required information will be passed
+ * via the callback function.
+ *
+ * \param bdev Block device to query.
+ * \param stat Structure for aggregating collected statistics.  Passed as argument to cb.
+ * \param cb Called when this operation completes.
+ * \param cb_arg Argument passed to callback function.
+ */
+void spdk_bdev_get_device_stat(struct spdk_bdev *bdev, struct spdk_bdev_io_stat *stat,
+			       spdk_bdev_get_device_stat_cb cb, void *cb_arg);
 
 /**
  * Get the status of bdev_io as an NVMe status code.
@@ -266,4 +1197,36 @@ void spdk_bdev_io_get_scsi_status(const struct spdk_bdev_io *bdev_io,
  */
 void spdk_bdev_io_get_iovec(struct spdk_bdev_io *bdev_io, struct iovec **iovp, int *iovcntp);
 
+typedef void (*spdk_bdev_histogram_status_cb)(void *cb_arg, int status);
+typedef void (*spdk_bdev_histogram_data_cb)(void *cb_arg, int status,
+		struct spdk_histogram_data *histogram);
+
+/**
+ * Enable or disable collecting histogram data on a bdev.
+ *
+ * \param bdev Block device.
+ * \param cb_fn Callback function to be called when histograms are enabled.
+ * \param cb_arg Argument to pass to cb_fn.
+ * \param enable Enable/disable flag
+ */
+void spdk_bdev_histogram_enable(struct spdk_bdev *bdev, spdk_bdev_histogram_status_cb cb_fn,
+				void *cb_arg, bool enable);
+
+/**
+ * Get aggregated histogram data from a bdev. Callback provides merged histogram
+ * for specified bdev.
+ *
+ * \param bdev Block device.
+ * \param histogram Histogram for aggregated data
+ * \param cb_fn Callback function to be called with data collected on bdev.
+ * \param cb_arg Argument to pass to cb_fn.
+ */
+void spdk_bdev_histogram_get(struct spdk_bdev *bdev, struct spdk_histogram_data *histogram,
+			     spdk_bdev_histogram_data_cb cb_fn,
+			     void *cb_arg);
+
+#ifdef __cplusplus
+}
+#endif
+
 #endif /* SPDK_BDEV_H_ */
diff --git a/PDK/core/src/api/include/udd/spdk/bdev_module.h b/PDK/core/src/api/include/udd/spdk/bdev_module.h
new file mode 100644
index 0000000..b8cfa5e
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/bdev_module.h
@@ -0,0 +1,1056 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/** \file
+ * Block Device Module Interface
+ *
+ * For information on how to write a bdev module, see @ref bdev_module.
+ */
+
+#ifndef SPDK_BDEV_MODULE_H
+#define SPDK_BDEV_MODULE_H
+
+#include "spdk/stdinc.h"
+
+#include "spdk/bdev.h"
+#include "spdk/queue.h"
+#include "spdk/scsi_spec.h"
+#include "spdk/thread.h"
+#include "spdk/util.h"
+#include "spdk/uuid.h"
+
+/** Block device module */
+struct spdk_bdev_module {
+	/**
+	 * Initialization function for the module.  Called by the spdk
+	 * application during startup.
+	 *
+	 * Modules are required to define this function.
+	 */
+	int (*module_init)(void);
+
+	/**
+	 * Optional callback for modules that require notification of when
+	 * the bdev subsystem has completed initialization.
+	 *
+	 * Modules are not required to define this function.
+	 */
+	void (*init_complete)(void);
+
+	/**
+	 * Optional callback for modules that require notification of when
+	 * the bdev subsystem is starting the fini process.
+	 *
+	 * Modules are not required to define this function.
+	 */
+	void (*fini_start)(void);
+
+	/**
+	 * Finish function for the module.  Called by the spdk application
+	 * after all bdevs for all modules have been unregistered.  This allows
+	 * the module to do any final cleanup before the SPDK application exits.
+	 *
+	 * Modules are not required to define this function.
+	 */
+	void (*module_fini)(void);
+
+	/**
+	 * Function called to return a text string representing the
+	 * module's configuration options for inclusion in a configuration file.
+	 */
+	void (*config_text)(FILE *fp);
+
+	/**
+	 * Function called to return a text string representing the module-level
+	 * JSON RPCs required to regenerate the current configuration.  This will
+	 * include module-level configuration options, or methods to construct
+	 * bdevs when one RPC may generate multiple bdevs (for example, an NVMe
+	 * controller with multiple namespaces).
+	 *
+	 * Per-bdev JSON RPCs (where one "construct" RPC always creates one bdev)
+	 * may be implemented here, or by the bdev's write_config_json function -
+	 * but not both.  Bdev module implementers may choose which mechanism to
+	 * use based on the module's design.
+	 *
+	 * \return 0 on success or Bdev specific negative error code.
+	 */
+	int (*config_json)(struct spdk_json_write_ctx *w);
+
+	/** Name for the modules being defined. */
+	const char *name;
+
+	/**
+	 * Returns the allocation size required for the backend for uses such as local
+	 * command structs, local SGL, iovecs, or other user context.
+	 */
+	int (*get_ctx_size)(void);
+
+	/**
+	 * First notification that a bdev should be examined by a virtual bdev module.
+	 * Virtual bdev modules may use this to examine newly-added bdevs and automatically
+	 * create their own vbdevs, but no I/O to device can be send to bdev at this point.
+	 * Only vbdevs based on config files can be created here. This callback must make
+	 * its decision to claim the module synchronously.
+	 * It must also call spdk_bdev_module_examine_done() before returning. If the module
+	 * needs to perform asynchronous operations such as I/O after claiming the bdev,
+	 * it may define an examine_disk callback.  The examine_disk callback will then
+	 * be called immediately after the examine_config callback returns.
+	 */
+	void (*examine_config)(struct spdk_bdev *bdev);
+
+	/**
+	 * Second notification that a bdev should be examined by a virtual bdev module.
+	 * Virtual bdev modules may use this to examine newly-added bdevs and automatically
+	 * create their own vbdevs. This callback may use I/O operations end finish asynchronously.
+	 */
+	void (*examine_disk)(struct spdk_bdev *bdev);
+
+	/**
+	 * Denotes if the module_init function may complete asynchronously. If set to true,
+	 * the module initialization has to be explicitly completed by calling
+	 * spdk_bdev_module_init_done().
+	 */
+	bool async_init;
+
+	/**
+	 * Denotes if the module_fini function may complete asynchronously.
+	 * If set to true finishing has to be explicitly completed by calling
+	 * spdk_bdev_module_fini_done().
+	 */
+	bool async_fini;
+
+	/**
+	 * Fields that are used by the internal bdev subsystem. Bdev modules
+	 *  must not read or write to these fields.
+	 */
+	struct __bdev_module_internal_fields {
+		/**
+		 * Count of bdev inits/examinations in progress. Used by generic bdev
+		 * layer and must not be modified by bdev modules.
+		 *
+		 * \note Used internally by bdev subsystem, don't change this value in bdev module.
+		 */
+		uint32_t action_in_progress;
+
+		TAILQ_ENTRY(spdk_bdev_module) tailq;
+	} internal;
+};
+
+typedef void (*spdk_bdev_unregister_cb)(void *cb_arg, int rc);
+
+/**
+ * Function table for a block device backend.
+ *
+ * The backend block device function table provides a set of APIs to allow
+ * communication with a backend. The main commands are read/write API
+ * calls for I/O via submit_request.
+ */
+struct spdk_bdev_fn_table {
+	/** Destroy the backend block device object */
+	int (*destruct)(void *ctx);
+
+	/** Process the IO. */
+	void (*submit_request)(struct spdk_io_channel *ch, struct spdk_bdev_io *);
+
+	/** Check if the block device supports a specific I/O type. */
+	bool (*io_type_supported)(void *ctx, enum spdk_bdev_io_type);
+
+	/** Get an I/O channel for the specific bdev for the calling thread. */
+	struct spdk_io_channel *(*get_io_channel)(void *ctx);
+
+	/**
+	 * Output driver-specific information to a JSON stream. Optional - may be NULL.
+	 *
+	 * The JSON write context will be initialized with an open object, so the bdev
+	 * driver should write a name (based on the driver name) followed by a JSON value
+	 * (most likely another nested object).
+	 */
+	int (*dump_info_json)(void *ctx, struct spdk_json_write_ctx *w);
+
+	/**
+	 * Output bdev-specific RPC configuration to a JSON stream. Optional - may be NULL.
+	 *
+	 * This function should only be implemented for bdevs which can be configured
+	 * independently of other bdevs.  For example, RPCs to create a bdev for an NVMe
+	 * namespace may not be generated by this function, since enumerating an NVMe
+	 * namespace requires attaching to an NVMe controller, and that controller may
+	 * contain multiple namespaces.  The spdk_bdev_module's config_json function should
+	 * be used instead for these cases.
+	 *
+	 * The JSON write context will be initialized with an open object, so the bdev
+	 * driver should write all data necessary to recreate this bdev by invoking
+	 * constructor method. No other data should be written.
+	 */
+	void (*write_config_json)(struct spdk_bdev *bdev, struct spdk_json_write_ctx *w);
+
+	/** Get spin-time per I/O channel in microseconds.
+	 *  Optional - may be NULL.
+	 */
+	uint64_t (*get_spin_time)(struct spdk_io_channel *ch);
+};
+
+/** bdev I/O completion status */
+enum spdk_bdev_io_status {
+	/*
+	 * NOMEM should be returned when a bdev module cannot start an I/O because of
+	 *  some lack of resources.  It may not be returned for RESET I/O.  I/O completed
+	 *  with NOMEM status will be retried after some I/O from the same channel have
+	 *  completed.
+	 */
+	SPDK_BDEV_IO_STATUS_NOMEM = -4,
+	SPDK_BDEV_IO_STATUS_SCSI_ERROR = -3,
+	SPDK_BDEV_IO_STATUS_NVME_ERROR = -2,
+	SPDK_BDEV_IO_STATUS_FAILED = -1,
+	SPDK_BDEV_IO_STATUS_PENDING = 0,
+	SPDK_BDEV_IO_STATUS_SUCCESS = 1,
+};
+
+struct spdk_bdev_alias {
+	char *alias;
+	TAILQ_ENTRY(spdk_bdev_alias) tailq;
+};
+
+typedef TAILQ_HEAD(, spdk_bdev_io) bdev_io_tailq_t;
+typedef STAILQ_HEAD(, spdk_bdev_io) bdev_io_stailq_t;
+
+struct spdk_bdev {
+	/** User context passed in by the backend */
+	void *ctxt;
+
+	/** Unique name for this block device. */
+	char *name;
+
+	/** Unique aliases for this block device. */
+	TAILQ_HEAD(spdk_bdev_aliases_list, spdk_bdev_alias) aliases;
+
+	/** Unique product name for this kind of block device. */
+	char *product_name;
+
+	/** write cache enabled, not used at the moment */
+	int write_cache;
+
+	/** Size in bytes of a logical block for the backend */
+	uint32_t blocklen;
+
+	/** Number of blocks */
+	uint64_t blockcnt;
+
+	/**
+	 * Specifies an alignment requirement for data buffers associated with an spdk_bdev_io.
+	 * 0 = no alignment requirement
+	 * >0 = alignment requirement is 2 ^ required_alignment.
+	 * bdev layer will automatically double buffer any spdk_bdev_io that violates this
+	 * alignment, before the spdk_bdev_io is submitted to the bdev module.
+	 */
+	uint8_t required_alignment;
+
+	/**
+	 * Specifies whether the optimal_io_boundary is mandatory or
+	 * only advisory.  If set to true, the bdev layer will split
+	 * READ and WRITE I/O that span the optimal_io_boundary before
+	 * submitting them to the bdev module.
+	 *
+	 * Note that this field cannot be used to force splitting of
+	 * UNMAP, WRITE_ZEROES or FLUSH I/O.
+	 */
+	bool split_on_optimal_io_boundary;
+
+	/**
+	 * Optimal I/O boundary in blocks, or 0 for no value reported.
+	 */
+	uint32_t optimal_io_boundary;
+
+	/**
+	 * UUID for this bdev.
+	 *
+	 * Fill with zeroes if no uuid is available.
+	 */
+	struct spdk_uuid uuid;
+
+	/** Size in bytes of a metadata for the backend */
+	uint32_t md_len;
+
+	/**
+	 * Specify metadata location and set to true if metadata is interleaved
+	 * with block data or false if metadata is separated with block data.
+	 *
+	 * Note that this field is valid only if there is metadata.
+	 */
+	bool md_interleave;
+
+	/**
+	 * DIF type for this bdev.
+	 *
+	 * Note that this field is valid only if there is metadata.
+	 */
+	enum spdk_dif_type dif_type;
+
+	/*
+	 * DIF location.
+	 *
+	 * Set to true if DIF is set in the first 8 bytes of metadata or false
+	 * if DIF is set in the last 8 bytes of metadata.
+	 *
+	 * Note that this field is valid only if DIF is enabled.
+	 */
+	bool dif_is_head_of_md;
+
+	/**
+	 * Specify whether each DIF check type is enabled.
+	 */
+	uint32_t dif_check_flags;
+
+	/**
+	 * Pointer to the bdev module that registered this bdev.
+	 */
+	struct spdk_bdev_module *module;
+
+	/** function table for all LUN ops */
+	const struct spdk_bdev_fn_table *fn_table;
+
+	/** Fields that are used internally by the bdev subsystem.  Bdev modules
+	 *  must not read or write to these fields.
+	 */
+	struct __bdev_internal_fields {
+		/** Quality of service parameters */
+		struct spdk_bdev_qos *qos;
+
+		/** True if the state of the QoS is being modified */
+		bool qos_mod_in_progress;
+
+		/** Mutex protecting claimed */
+		pthread_mutex_t mutex;
+
+		/** The bdev status */
+		enum spdk_bdev_status status;
+
+		/**
+		 * Pointer to the module that has claimed this bdev for purposes of creating virtual
+		 *  bdevs on top of it.  Set to NULL if the bdev has not been claimed.
+		 */
+		struct spdk_bdev_module *claim_module;
+
+		/** Callback function that will be called after bdev destruct is completed. */
+		spdk_bdev_unregister_cb	unregister_cb;
+
+		/** Unregister call context */
+		void *unregister_ctx;
+
+		/** List of open descriptors for this block device. */
+		TAILQ_HEAD(, spdk_bdev_desc) open_descs;
+
+		TAILQ_ENTRY(spdk_bdev) link;
+
+		/** points to a reset bdev_io if one is in progress. */
+		struct spdk_bdev_io *reset_in_progress;
+
+		/** poller for tracking the queue_depth of a device, NULL if not tracking */
+		struct spdk_poller *qd_poller;
+
+		/** period at which we poll for queue depth information */
+		uint64_t period;
+
+		/** used to aggregate queue depth while iterating across the bdev's open channels */
+		uint64_t temporary_queue_depth;
+
+		/** queue depth as calculated the last time the telemetry poller checked. */
+		uint64_t measured_queue_depth;
+
+		/** most recent value of ticks spent performing I/O. Used to calculate the weighted time doing I/O */
+		uint64_t io_time;
+
+		/** weighted time performing I/O. Equal to measured_queue_depth * period */
+		uint64_t weighted_io_time;
+
+		/** accumulated I/O statistics for previously deleted channels of this bdev */
+		struct spdk_bdev_io_stat stat;
+
+		/** histogram enabled on this bdev */
+		bool	histogram_enabled;
+		bool	histogram_in_progress;
+	} internal;
+};
+
+/**
+ * Callback when buffer is allocated for the bdev I/O.
+ *
+ * \param ch The I/O channel the bdev I/O was handled on.
+ * \param bdev_io The bdev I/O
+ * \param success True if buffer is allocated successfully or the bdev I/O has an SGL
+ * assigned already, or false if it failed. The possible reason of failure is the size
+ * of the buffer to allocate is greater than the permitted maximum.
+ */
+typedef void (*spdk_bdev_io_get_buf_cb)(struct spdk_io_channel *ch, struct spdk_bdev_io *bdev_io,
+					bool success);
+
+#define BDEV_IO_NUM_CHILD_IOV 32
+
+struct spdk_bdev_io {
+	/** The block device that this I/O belongs to. */
+	struct spdk_bdev *bdev;
+
+	/** Enumerated value representing the I/O type. */
+	uint8_t type;
+
+	/** A single iovec element for use by this bdev_io. */
+	struct iovec iov;
+
+	/** Array of iovecs used for I/O splitting. */
+	struct iovec child_iov[BDEV_IO_NUM_CHILD_IOV];
+
+	union {
+		struct {
+			/** For SG buffer cases, array of iovecs to transfer. */
+			struct iovec *iovs;
+
+			/** For SG buffer cases, number of iovecs in iovec array. */
+			int iovcnt;
+
+			/** Total size of data to be transferred. */
+			uint64_t num_blocks;
+
+			/** Starting offset (in blocks) of the bdev for this I/O. */
+			uint64_t offset_blocks;
+
+			/** stored user callback in case we split the I/O and use a temporary callback */
+			spdk_bdev_io_completion_cb stored_user_cb;
+
+			/** number of blocks remaining in a split i/o */
+			uint64_t split_remaining_num_blocks;
+
+			/** current offset of the split I/O in the bdev */
+			uint64_t split_current_offset_blocks;
+
+			/** count of outstanding batched split I/Os */
+			uint32_t split_outstanding;
+
+			struct {
+				/** Whether the buffer should be populated with the real data */
+				uint8_t populate : 1;
+
+				/** Whether the buffer should be committed back to disk */
+				uint8_t commit : 1;
+
+				/** True if this request is in the 'start' phase of zcopy. False if in 'end'. */
+				uint8_t start : 1;
+			} zcopy;
+		} bdev;
+		struct {
+			/** Channel reference held while messages for this reset are in progress. */
+			struct spdk_io_channel *ch_ref;
+		} reset;
+		struct {
+			/* The NVMe command to execute */
+			struct spdk_nvme_cmd cmd;
+
+			/* The data buffer to transfer */
+			void *buf;
+
+			/* The number of bytes to transfer */
+			size_t nbytes;
+
+			/* The meta data buffer to transfer */
+			void *md_buf;
+
+			/* meta data buffer size to transfer */
+			size_t md_len;
+		} nvme_passthru;
+	} u;
+
+	/** It may be used by modules to put the bdev_io into its own list. */
+	TAILQ_ENTRY(spdk_bdev_io) module_link;
+
+	/**
+	 *  Fields that are used internally by the bdev subsystem.  Bdev modules
+	 *  must not read or write to these fields.
+	 */
+	struct __bdev_io_internal_fields {
+		/** The bdev I/O channel that this was handled on. */
+		struct spdk_bdev_channel *ch;
+
+		/** The bdev I/O channel that this was submitted on. */
+		struct spdk_bdev_channel *io_submit_ch;
+
+		/** The bdev descriptor that was used when submitting this I/O. */
+		struct spdk_bdev_desc *desc;
+
+		/** User function that will be called when this completes */
+		spdk_bdev_io_completion_cb cb;
+
+		/** Context that will be passed to the completion callback */
+		void *caller_ctx;
+
+		/** Current tsc at submit time. Used to calculate latency at completion. */
+		uint64_t submit_tsc;
+
+		/** Error information from a device */
+		union {
+			/** Only valid when status is SPDK_BDEV_IO_STATUS_NVME_ERROR */
+			struct {
+				/** NVMe status code type */
+				uint8_t sct;
+				/** NVMe status code */
+				uint8_t sc;
+			} nvme;
+			/** Only valid when status is SPDK_BDEV_IO_STATUS_SCSI_ERROR */
+			struct {
+				/** SCSI status code */
+				uint8_t sc;
+				/** SCSI sense key */
+				uint8_t sk;
+				/** SCSI additional sense code */
+				uint8_t asc;
+				/** SCSI additional sense code qualifier */
+				uint8_t ascq;
+			} scsi;
+		} error;
+
+		/**
+		 * Set to true while the bdev module submit_request function is in progress.
+		 *
+		 * This is used to decide whether spdk_bdev_io_complete() can complete the I/O directly
+		 * or if completion must be deferred via an event.
+		 */
+		bool in_submit_request;
+
+		/** Status for the IO */
+		int8_t status;
+
+		/** bdev allocated memory associated with this request */
+		void *buf;
+
+		/** requested size of the buffer associated with this I/O */
+		uint64_t buf_len;
+
+		/** if the request is double buffered, store original request iovs here */
+		struct iovec  bounce_iov;
+		struct iovec *orig_iovs;
+		int           orig_iovcnt;
+
+		/** Callback for when buf is allocated */
+		spdk_bdev_io_get_buf_cb get_buf_cb;
+
+		/** Member used for linking child I/Os together. */
+		TAILQ_ENTRY(spdk_bdev_io) link;
+
+		/** Entry to the list need_buf of struct spdk_bdev. */
+		STAILQ_ENTRY(spdk_bdev_io) buf_link;
+
+		/** Enables queuing parent I/O when no bdev_ios available for split children. */
+		struct spdk_bdev_io_wait_entry waitq_entry;
+	} internal;
+
+	/**
+	 * Per I/O context for use by the bdev module.
+	 */
+	uint8_t driver_ctx[0];
+
+	/* No members may be added after driver_ctx! */
+};
+
+/**
+ * Register a new bdev.
+ *
+ * \param bdev Block device to register.
+ *
+ * \return 0 on success.
+ * \return -EINVAL if the bdev name is NULL.
+ * \return -EEXIST if a bdev or bdev alias with the same name already exists.
+ */
+int spdk_bdev_register(struct spdk_bdev *bdev);
+
+/**
+ * Start unregistering a bdev. This will notify each currently open descriptor
+ * on this bdev about the hotremoval in hopes that the upper layers will stop
+ * using this bdev and manually close all the descriptors with spdk_bdev_close().
+ * The actual bdev unregistration may be deferred until all descriptors are closed.
+ *
+ * \param bdev Block device to unregister.
+ * \param cb_fn Callback function to be called when the unregister is complete.
+ * \param cb_arg Argument to be supplied to cb_fn
+ */
+void spdk_bdev_unregister(struct spdk_bdev *bdev, spdk_bdev_unregister_cb cb_fn, void *cb_arg);
+
+/**
+ * Invokes the unregister callback of a bdev backing a virtual bdev.
+ *
+ * A Bdev with an asynchronous destruct path should return 1 from its
+ * destruct function and call this function at the conclusion of that path.
+ * Bdevs with synchronous destruct paths should return 0 from their destruct
+ * path.
+ *
+ * \param bdev Block device that was destroyed.
+ * \param bdeverrno Error code returned from bdev's destruct callback.
+ */
+void spdk_bdev_destruct_done(struct spdk_bdev *bdev, int bdeverrno);
+
+/**
+ * Register a virtual bdev.
+ *
+ * This function is deprecated.  Users should call spdk_bdev_register instead.
+ * The bdev layer currently makes no use of the base_bdevs array, so switching
+ * to spdk_bdev_register results in no loss of functionality.
+ *
+ * \param vbdev Virtual bdev to register.
+ * \param base_bdevs Array of bdevs upon which this vbdev is based.
+ * \param base_bdev_count Number of bdevs in base_bdevs.
+ *
+ * \return 0 on success
+ * \return -EINVAL if the bdev name is NULL.
+ * \return -EEXIST if the bdev already exists.
+ * \return -ENOMEM if allocation of the base_bdevs array or the base bdevs vbdevs array fails.
+ */
+int spdk_vbdev_register(struct spdk_bdev *vbdev, struct spdk_bdev **base_bdevs,
+			int base_bdev_count);
+
+/**
+ * Indicate to the bdev layer that the module is done examining a bdev.
+ *
+ * To be called synchronously or asynchronously in response to the
+ * module's examine function being called.
+ *
+ * \param module Pointer to the module completing the examination.
+ */
+void spdk_bdev_module_examine_done(struct spdk_bdev_module *module);
+
+/**
+ * Indicate to the bdev layer that the module is done initializing.
+ *
+ * To be called synchronously or asynchronously in response to the
+ * module_init function being called.
+ *
+ * \param module Pointer to the module completing the initialization.
+ */
+void spdk_bdev_module_init_done(struct spdk_bdev_module *module);
+
+/**
+ * Indicate to the bdev layer that the module is done cleaning up.
+ *
+ * To be called either synchronously or asynchronously
+ * in response to the module_fini function being called.
+ *
+ */
+void spdk_bdev_module_finish_done(void);
+
+/**
+ * Called by a bdev module to lay exclusive write claim to a bdev.
+ *
+ * Also upgrades that bdev's descriptor to have write access.
+ *
+ * \param bdev Block device to be claimed.
+ * \param desc Descriptor for the above block device.
+ * \param module Bdev module attempting to claim bdev.
+ *
+ * \return 0 on success
+ * \return -EPERM if the bdev is already claimed by another module.
+ */
+int spdk_bdev_module_claim_bdev(struct spdk_bdev *bdev, struct spdk_bdev_desc *desc,
+				struct spdk_bdev_module *module);
+
+/**
+ * Called to release a write claim on a block device.
+ *
+ * \param bdev Block device to be released.
+ */
+void spdk_bdev_module_release_bdev(struct spdk_bdev *bdev);
+
+/**
+ * Add alias to block device names list.
+ * Aliases can be add only to registered bdev.
+ *
+ * \param bdev Block device to query.
+ * \param alias Alias to be added to list.
+ *
+ * \return 0 on success
+ * \return -EEXIST if alias already exists as name or alias on any bdev
+ * \return -ENOMEM if memory cannot be allocated to store alias
+ * \return -EINVAL if passed alias is empty
+ */
+int spdk_bdev_alias_add(struct spdk_bdev *bdev, const char *alias);
+
+/**
+ * Removes name from block device names list.
+ *
+ * \param bdev Block device to query.
+ * \param alias Alias to be deleted from list.
+ * \return 0 on success
+ * \return -ENOENT if alias does not exists
+ */
+int spdk_bdev_alias_del(struct spdk_bdev *bdev, const char *alias);
+
+/**
+ * Removes all alias from block device alias list.
+ *
+ * \param bdev Block device to operate.
+ */
+void spdk_bdev_alias_del_all(struct spdk_bdev *bdev);
+
+/**
+ * Get pointer to block device aliases list.
+ *
+ * \param bdev Block device to query.
+ * \return Pointer to bdev aliases list.
+ */
+const struct spdk_bdev_aliases_list *spdk_bdev_get_aliases(const struct spdk_bdev *bdev);
+
+/**
+ * Allocate a buffer for given bdev_io.  Allocation will happen
+ * only if the bdev_io has no assigned SGL yet or SGL is not
+ * aligned to \c bdev->required_alignment.  If SGL is not aligned,
+ * this call will cause copy from SGL to bounce buffer on write
+ * path or copy from bounce buffer to SGL before completion
+ * callback on read path.  The buffer will be freed automatically
+ * on \c spdk_bdev_free_io() call. This call will never fail.
+ * In case of lack of memory given callback \c cb will be deferred
+ * until enough memory is freed.
+ *
+ * \param bdev_io I/O to allocate buffer for.
+ * \param cb callback to be called when the buffer is allocated
+ * or the bdev_io has an SGL assigned already.
+ * \param len size of the buffer to allocate. In case the bdev_io
+ * doesn't have an SGL assigned this field must be no bigger than
+ * \c SPDK_BDEV_LARGE_BUF_MAX_SIZE.
+ */
+void spdk_bdev_io_get_buf(struct spdk_bdev_io *bdev_io, spdk_bdev_io_get_buf_cb cb, uint64_t len);
+
+/**
+ * Set the given buffer as the data buffer described by this bdev_io.
+ *
+ * The portion of the buffer used may be adjusted for memory alignement
+ * purposes.
+ *
+ * \param bdev_io I/O to set the buffer on.
+ * \param buf The buffer to set as the active data buffer.
+ * \param len The length of the buffer.
+ *
+ */
+void spdk_bdev_io_set_buf(struct spdk_bdev_io *bdev_io, void *buf, size_t len);
+
+/**
+ * Complete a bdev_io
+ *
+ * \param bdev_io I/O to complete.
+ * \param status The I/O completion status.
+ */
+void spdk_bdev_io_complete(struct spdk_bdev_io *bdev_io,
+			   enum spdk_bdev_io_status status);
+
+/**
+ * Complete a bdev_io with an NVMe status code.
+ *
+ * \param bdev_io I/O to complete.
+ * \param sct NVMe Status Code Type.
+ * \param sc NVMe Status Code.
+ */
+void spdk_bdev_io_complete_nvme_status(struct spdk_bdev_io *bdev_io, int sct, int sc);
+
+/**
+ * Complete a bdev_io with a SCSI status code.
+ *
+ * \param bdev_io I/O to complete.
+ * \param sc SCSI Status Code.
+ * \param sk SCSI Sense Key.
+ * \param asc SCSI Additional Sense Code.
+ * \param ascq SCSI Additional Sense Code Qualifier.
+ */
+void spdk_bdev_io_complete_scsi_status(struct spdk_bdev_io *bdev_io, enum spdk_scsi_status sc,
+				       enum spdk_scsi_sense sk, uint8_t asc, uint8_t ascq);
+
+/**
+ * Get a thread that given bdev_io was submitted on.
+ *
+ * \param bdev_io I/O
+ * \return thread that submitted the I/O
+ */
+struct spdk_thread *spdk_bdev_io_get_thread(struct spdk_bdev_io *bdev_io);
+
+/**
+ * Get the bdev module's I/O channel that the given bdev_io was submitted on.
+ *
+ * \param bdev_io I/O
+ * \return the bdev module's I/O channel that the given bdev_io was submitted on.
+ */
+struct spdk_io_channel *spdk_bdev_io_get_io_channel(struct spdk_bdev_io *bdev_io);
+
+/**
+ * Resize for a bdev.
+ *
+ * Change number of blocks for provided block device.
+ * It can only be called on a registered bdev.
+ *
+ * \param bdev Block device to change.
+ * \param size New size of bdev.
+ * \return 0 on success, negated errno on failure.
+ */
+int spdk_bdev_notify_blockcnt_change(struct spdk_bdev *bdev, uint64_t size);
+
+/**
+ * Translates NVMe status codes to SCSI status information.
+ *
+ * The codes are stored in the user supplied integers.
+ *
+ * \param bdev_io I/O containing status codes to translate.
+ * \param sc SCSI Status Code will be stored here.
+ * \param sk SCSI Sense Key will be stored here.
+ * \param asc SCSI Additional Sense Code will be stored here.
+ * \param ascq SCSI Additional Sense Code Qualifier will be stored here.
+ */
+void spdk_scsi_nvme_translate(const struct spdk_bdev_io *bdev_io,
+			      int *sc, int *sk, int *asc, int *ascq);
+
+/**
+ * Add the given module to the list of registered modules.
+ * This function should be invoked by referencing the macro
+ * SPDK_BDEV_MODULE_REGISTER in the module c file.
+ *
+ * \param bdev_module Module to be added.
+ */
+void spdk_bdev_module_list_add(struct spdk_bdev_module *bdev_module);
+
+/**
+ * Find registered module with name pointed by \c name.
+ *
+ * \param name name of module to be searched for.
+ * \return pointer to module or NULL if no module with \c name exist
+ */
+struct spdk_bdev_module *spdk_bdev_module_list_find(const char *name);
+
+static inline struct spdk_bdev_io *
+spdk_bdev_io_from_ctx(void *ctx)
+{
+	return SPDK_CONTAINEROF(ctx, struct spdk_bdev_io, driver_ctx);
+}
+
+struct spdk_bdev_part_base;
+
+/**
+ * Returns a pointer to the spdk_bdev associated with an spdk_bdev_part_base
+ *
+ * \param part_base A pointer to an spdk_bdev_part_base object.
+ *
+ * \return A pointer to the base's spdk_bdev struct.
+ */
+struct spdk_bdev *spdk_bdev_part_base_get_bdev(struct spdk_bdev_part_base *part_base);
+
+/**
+ * Returns a pointer to the spdk_bdev_descriptor associated with an spdk_bdev_part_base
+ *
+ * \param part_base A pointer to an spdk_bdev_part_base object.
+ *
+ * \return A pointer to the base's spdk_bdev_desc struct.
+ */
+struct spdk_bdev_desc *spdk_bdev_part_base_get_desc(struct spdk_bdev_part_base *part_base);
+
+/**
+ * Returns a pointer to the tailq associated with an spdk_bdev_part_base
+ *
+ * \param part_base A pointer to an spdk_bdev_part_base object.
+ *
+ * \return The head of a tailq of spdk_bdev_part structs registered to the base's module.
+ */
+struct bdev_part_tailq *spdk_bdev_part_base_get_tailq(struct spdk_bdev_part_base *part_base);
+
+/**
+ * Returns a pointer to the module level context associated with an spdk_bdev_part_base
+ *
+ * \param part_base A pointer to an spdk_bdev_part_base object.
+ *
+ * \return A pointer to the module level context registered with the base in spdk_bdev_part_base_construct.
+ */
+void *spdk_bdev_part_base_get_ctx(struct spdk_bdev_part_base *part_base);
+
+typedef void (*spdk_bdev_part_base_free_fn)(void *ctx);
+
+struct spdk_bdev_part {
+	/* Entry into the module's global list of bdev parts */
+	TAILQ_ENTRY(spdk_bdev_part)	tailq;
+
+	/**
+	 * Fields that are used internally by part.c These fields should only
+	 * be accessed from a module using any pertinent get and set methods.
+	 */
+	struct bdev_part_internal_fields {
+
+		/* This part's corresponding bdev object. Not to be confused with the base bdev */
+		struct spdk_bdev		bdev;
+
+		/* The base to which this part belongs */
+		struct spdk_bdev_part_base	*base;
+
+		/* number of blocks from the start of the base bdev to the start of this part */
+		uint64_t			offset_blocks;
+	} internal;
+};
+
+struct spdk_bdev_part_channel {
+	struct spdk_bdev_part		*part;
+	struct spdk_io_channel		*base_ch;
+};
+
+typedef TAILQ_HEAD(bdev_part_tailq, spdk_bdev_part)	SPDK_BDEV_PART_TAILQ;
+
+/**
+ * Free the base corresponding to one or more spdk_bdev_part.
+ *
+ * \param base The base to free.
+ */
+void spdk_bdev_part_base_free(struct spdk_bdev_part_base *base);
+
+/**
+ * Free an spdk_bdev_part context.
+ *
+ * \param part The part to free.
+ *
+ * \return 1 always. To indicate that the operation is asynchronous.
+ */
+int spdk_bdev_part_free(struct spdk_bdev_part *part);
+
+/**
+ * Calls spdk_bdev_unregister on the bdev for each part associated with base_bdev.
+ *
+ * \param part_base The part base object built on top of an spdk_bdev
+ * \param tailq The list of spdk_bdev_part bdevs associated with this base bdev.
+ */
+void spdk_bdev_part_base_hotremove(struct spdk_bdev_part_base *part_base,
+				   struct bdev_part_tailq *tailq);
+
+/**
+ * Construct a new spdk_bdev_part_base on top of the provided bdev.
+ *
+ * \param bdev The spdk_bdev upon which this base will be built.
+ * \param remove_cb Function to be called upon hotremove of the bdev.
+ * \param module The module to which this bdev base belongs.
+ * \param fn_table Function table for communicating with the bdev backend.
+ * \param tailq The head of the list of all spdk_bdev_part structures registered to this base's module.
+ * \param free_fn User provided function to free base related context upon bdev removal or shutdown.
+ * \param ctx Module specific context for this bdev part base.
+ * \param channel_size Channel size in bytes.
+ * \param ch_create_cb Called after a new channel is allocated.
+ * \param ch_destroy_cb Called upon channel deletion.
+ *
+ * \return 0 on success
+ * \return -1 if the underlying bdev cannot be opened.
+ */
+struct spdk_bdev_part_base *spdk_bdev_part_base_construct(struct spdk_bdev *bdev,
+		spdk_bdev_remove_cb_t remove_cb,
+		struct spdk_bdev_module *module,
+		struct spdk_bdev_fn_table *fn_table,
+		struct bdev_part_tailq *tailq,
+		spdk_bdev_part_base_free_fn free_fn,
+		void *ctx,
+		uint32_t channel_size,
+		spdk_io_channel_create_cb ch_create_cb,
+		spdk_io_channel_destroy_cb ch_destroy_cb);
+
+/**
+ * Create a logical spdk_bdev_part on top of a base.
+ *
+ * \param part The part object allocated by the user.
+ * \param base The base from which to create the part.
+ * \param name The name of the new spdk_bdev_part.
+ * \param offset_blocks The offset into the base bdev at which this part begins.
+ * \param num_blocks The number of blocks that this part will span.
+ * \param product_name Unique name for this type of block device.
+ *
+ * \return 0 on success.
+ * \return -1 if the bases underlying bdev cannot be claimed by the current module.
+ */
+int spdk_bdev_part_construct(struct spdk_bdev_part *part, struct spdk_bdev_part_base *base,
+			     char *name, uint64_t offset_blocks, uint64_t num_blocks,
+			     char *product_name);
+
+/**
+ * Forwards I/O from an spdk_bdev_part to the underlying base bdev.
+ *
+ * This function will apply the offset_blocks the user provided to
+ * spdk_bdev_part_construct to the I/O. The user should not manually
+ * apply this offset before submitting any I/O through this function.
+ *
+ * \param ch The I/O channel associated with the spdk_bdev_part.
+ * \param bdev_io The I/O to be submitted to the underlying bdev.
+ * \return 0 on success or non-zero if submit request failed.
+ */
+int spdk_bdev_part_submit_request(struct spdk_bdev_part_channel *ch, struct spdk_bdev_io *bdev_io);
+
+/**
+ * Return a pointer to this part's spdk_bdev.
+ *
+ * \param part An spdk_bdev_part object.
+ *
+ * \return A pointer to this part's spdk_bdev object.
+ */
+struct spdk_bdev *spdk_bdev_part_get_bdev(struct spdk_bdev_part *part);
+
+/**
+ * Return a pointer to this part's base.
+ *
+ * \param part An spdk_bdev_part object.
+ *
+ * \return A pointer to this part's spdk_bdev_part_base object.
+ */
+struct spdk_bdev_part_base *spdk_bdev_part_get_base(struct spdk_bdev_part *part);
+
+/**
+ * Return a pointer to this part's base bdev.
+ *
+ * The return value of this function is equivalent to calling
+ * spdk_bdev_part_base_get_bdev on this part's base.
+ *
+ * \param part An spdk_bdev_part object.
+ *
+ * \return A pointer to the bdev belonging to this part's base.
+ */
+struct spdk_bdev *spdk_bdev_part_get_base_bdev(struct spdk_bdev_part *part);
+
+/**
+ * Return this part's offset from the beginning of the base bdev.
+ *
+ * This function should not be called in the I/O path. Any block
+ * translations to I/O will be handled in spdk_bdev_part_submit_request.
+ *
+ * \param part An spdk_bdev_part object.
+ *
+ * \return the block offset of this part from it's underlying bdev.
+ */
+uint64_t spdk_bdev_part_get_offset_blocks(struct spdk_bdev_part *part);
+
+/*
+ *  Macro used to register module for later initialization.
+ */
+#define SPDK_BDEV_MODULE_REGISTER(name, module) \
+static void __attribute__((constructor)) spdk_bdev_module_register_##name(void) \
+{ \
+	spdk_bdev_module_list_add(module); \
+} \
+
+#endif /* SPDK_BDEV_MODULE_H */
diff --git a/PDK/core/src/api/include/udd/spdk/bit_array.h b/PDK/core/src/api/include/udd/spdk/bit_array.h
index 3ccdabc..3019f9f 100644
--- a/PDK/core/src/api/include/udd/spdk/bit_array.h
+++ b/PDK/core/src/api/include/udd/spdk/bit_array.h
@@ -50,24 +50,37 @@ extern "C" {
 struct spdk_bit_array;
 
 /**
- * Return the number of bits a bit array is currently sized to hold.
+ * Return the number of bits that a bit array is currently sized to hold.
+ *
+ * \param ba Bit array to query.
+ *
+ * \return the number of bits.
  */
 uint32_t spdk_bit_array_capacity(const struct spdk_bit_array *ba);
 
 /**
  * Create a bit array.
+ *
+ * \param num_bits Number of bits that the bit array is sized to hold.
+ *
+ * All bits in the array will be cleared.
+ *
+ * \return a pointer to the new bit array.
  */
 struct spdk_bit_array *spdk_bit_array_create(uint32_t num_bits);
 
 /**
  * Free a bit array and set the pointer to NULL.
+ *
+ * \param bap Bit array to free.
  */
 void spdk_bit_array_free(struct spdk_bit_array **bap);
 
 /**
  * Create or resize a bit array.
  *
- * To create a new bit array, pass a pointer to a spdk_bit_array pointer that is NULL for bap.
+ * To create a new bit array, pass a pointer to a spdk_bit_array pointer that is
+ * NULL for bap.
  *
  * The bit array will be sized to hold at least num_bits.
  *
@@ -76,14 +89,24 @@ void spdk_bit_array_free(struct spdk_bit_array **bap);
  *
  * If num_bits is larger than the previous size of the bit array,
  * any data beyond the old num_bits size will be cleared.
+ *
+ * \param bap Bit array to create/resize.
+ * \param num_bits Number of bits that the bit array is sized to hold.
+ *
+ * \return 0 on success, negative errno on failure.
  */
 int spdk_bit_array_resize(struct spdk_bit_array **bap, uint32_t num_bits);
 
 /**
  * Get the value of a bit from the bit array.
  *
- * If bit_index is beyond the end of the current size of the bit array,
- * this function will return false (i.e. bits beyond the end of the array are implicitly 0).
+ * If bit_index is beyond the end of the current size of the bit array, this
+ * function will return false (i.e. bits beyond the end of the array are implicitly 0).
+ *
+ * \param ba Bit array to query.
+ * \param bit_index The index of a bit to query.
+ *
+ * \return the value of a bit from the bit array on success, or false on failure.
  */
 bool spdk_bit_array_get(const struct spdk_bit_array *ba, uint32_t bit_index);
 
@@ -91,15 +114,22 @@ bool spdk_bit_array_get(const struct spdk_bit_array *ba, uint32_t bit_index);
  * Set (to 1) a bit in the bit array.
  *
  * If bit_index is beyond the end of the bit array, this function will return -EINVAL.
- * On success, returns 0.
+ *
+ * \param ba Bit array to set a bit.
+ * \param bit_index The index of a bit to set.
+ *
+ * \return 0 on success, negative errno on failure.
  */
 int spdk_bit_array_set(struct spdk_bit_array *ba, uint32_t bit_index);
 
 /**
  * Clear (to 0) a bit in the bit array.
  *
- * If bit_index is beyond the end of the bit array, no action is taken. Bits beyond the end of the
- * bit array are implicitly 0.
+ * If bit_index is beyond the end of the bit array, no action is taken. Bits
+ * beyond the end of the bit array are implicitly 0.
+ *
+ * \param ba Bit array to clear a bit.
+ * \param bit_index The index of a bit to clear.
  */
 void spdk_bit_array_clear(struct spdk_bit_array *ba, uint32_t bit_index);
 
@@ -107,10 +137,10 @@ void spdk_bit_array_clear(struct spdk_bit_array *ba, uint32_t bit_index);
  * Find the index of the first set bit in the array.
  *
  * \param ba The bit array to search.
- * \param start_bit_index The bit index from which to start searching (0 to start from the beginning
- * of the array).
+ * \param start_bit_index The bit index from which to start searching (0 to start
+ * from the beginning of the array).
  *
- * \return The index of the first set bit. If no bits are set, returns UINT32_MAX.
+ * \return the index of the first set bit. If no bits are set, returns UINT32_MAX.
  */
 uint32_t spdk_bit_array_find_first_set(const struct spdk_bit_array *ba, uint32_t start_bit_index);
 
@@ -118,15 +148,54 @@ uint32_t spdk_bit_array_find_first_set(const struct spdk_bit_array *ba, uint32_t
  * Find the index of the first cleared bit in the array.
  *
  * \param ba The bit array to search.
- * \param start_bit_index The bit index from which to start searching (0 to start from the beginning
- * of the array)..
+ * \param start_bit_index The bit index from which to start searching (0 to start
+ * from the beginning of the array).
  *
- * \return The index of the first cleared bit. Bits beyond the current size of the array are
- * implicitly cleared, so if all bits within the current size are set, this function will return
- * the current number of bits + 1.
+ * \return the index of the first cleared bit. If no bits are cleared, returns UINT32_MAX.
  */
 uint32_t spdk_bit_array_find_first_clear(const struct spdk_bit_array *ba, uint32_t start_bit_index);
 
+/**
+ * Count the number of set bits in the array.
+ *
+ * \param ba The bit array to search.
+ *
+ * \return the number of bits set in the array.
+ */
+uint32_t spdk_bit_array_count_set(const struct spdk_bit_array *ba);
+
+/**
+ * Count the number of cleared bits in the array.
+ *
+ * \param ba The bit array to search.
+ *
+ * \return the number of bits cleared in the array.
+ */
+uint32_t spdk_bit_array_count_clear(const struct spdk_bit_array *ba);
+
+/**
+ * Store bitmask from bit array.
+ *
+ * \param ba Bit array.
+ * \param mask Destination mask. Mask and bit array capacity must be equal.
+ */
+void spdk_bit_array_store_mask(const struct spdk_bit_array *ba, void *mask);
+
+/**
+ * Load bitmask to bit array.
+ *
+ * \param ba Bit array.
+ * \param mask Source mask. Mask and bit array capacity must be equal.
+ */
+void spdk_bit_array_load_mask(struct spdk_bit_array *ba, const void *mask);
+
+/**
+ * Clear (to 0) bit array bitmask.
+ *
+ * \param ba Bit array.
+ */
+void spdk_bit_array_clear_mask(struct spdk_bit_array *ba);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/PDK/core/src/api/include/udd/spdk/blob.h b/PDK/core/src/api/include/udd/spdk/blob.h
index 5d4ef3d..065fcff 100644
--- a/PDK/core/src/api/include/udd/spdk/blob.h
+++ b/PDK/core/src/api/include/udd/spdk/blob.h
@@ -42,23 +42,17 @@
  *
  * The blobstore is designed to be very high performance, and thus has
  * a few general rules regarding thread safety to avoid taking locks
- * in the I/O path. Functions starting with the prefix "spdk_bs_md" must only
- * be called from the metadata thread, of which there is only one at a time.
- * The user application can declare which thread is the metadata thread by
- * calling \ref spdk_bs_register_md_thread, but by default it is the thread
- * that was used to create the blobstore initially. The metadata thread can
- * be changed at run time by first unregistering
- * (\ref spdk_bs_unregister_md_thread) and then re-registering. Registering
- * a thread as the metadata thread is expensive and should be avoided.
- *
- * Functions starting with the prefix "spdk_bs_io" are passed a channel
- * as an argument, and channels may only be used from the thread they were
- * created on. See \ref spdk_bs_alloc_io_channel.
+ * in the I/O path.  This is primarily done by only allowing most
+ * functions to be called on the metadata thread.  The metadata thread is
+ * the thread which called spdk_bs_init() or spdk_bs_load().
  *
- * Functions not starting with one of those two prefixes are thread safe
- * and may be called from any thread at any time.
+ * Functions starting with the prefix "spdk_blob_io" are passed a channel
+ * as an argument, and channels may only be used from the thread they were
+ * created on. See \ref spdk_bs_alloc_io_channel.  These are the only
+ * functions that may be called from a thread other than the metadata
+ * thread.
  *
- * The blob store returns errors using negated POSIX errno values, either
+ * The blobstore returns errors using negated POSIX errno values, either
  * returned in the callback or as a return value. An errno value of 0 means
  * success.
  */
@@ -68,25 +62,83 @@
 
 #include "spdk/stdinc.h"
 
+#ifdef __cplusplus
+extern "C" {
+#endif
+
 typedef uint64_t spdk_blob_id;
 #define SPDK_BLOBID_INVALID	(uint64_t)-1
+#define SPDK_BLOBSTORE_TYPE_LENGTH 16
+
+enum blob_clear_method {
+	BLOB_CLEAR_WITH_DEFAULT,
+	BLOB_CLEAR_WITH_NONE,
+	BLOB_CLEAR_WITH_UNMAP,
+	BLOB_CLEAR_WITH_WRITE_ZEROES,
+};
+
+enum bs_clear_method {
+	BS_CLEAR_WITH_UNMAP,
+	BS_CLEAR_WITH_WRITE_ZEROES,
+	BS_CLEAR_WITH_NONE,
+};
 
 struct spdk_blob_store;
 struct spdk_io_channel;
 struct spdk_blob;
 struct spdk_xattr_names;
 
+/**
+ * Blobstore operation completion callback.
+ *
+ * \param cb_arg Callback argument.
+ * \param bserrno 0 if it completed successfully, or negative errno if it failed.
+ */
 typedef void (*spdk_bs_op_complete)(void *cb_arg, int bserrno);
+
+/**
+ * Blobstore operation completion callback with handle.
+ *
+ * \param cb_arg Callback argument.
+ * \param bs Handle to a blobstore.
+ * \param bserrno 0 if it completed successfully, or negative errno if it failed.
+ */
 typedef void (*spdk_bs_op_with_handle_complete)(void *cb_arg, struct spdk_blob_store *bs,
 		int bserrno);
+
+/**
+ * Blob operation completion callback.
+ *
+ * \param cb_arg Callback argument.
+ * \param bserrno 0 if it completed successfully, or negative errno if it failed.
+ */
 typedef void (*spdk_blob_op_complete)(void *cb_arg, int bserrno);
+
+/**
+ * Blob operation completion callback with blob ID.
+ *
+ * \param cb_arg Callback argument.
+ * \param blobid Blob ID.
+ * \param bserrno 0 if it completed successfully, or negative errno if it failed.
+ */
 typedef void (*spdk_blob_op_with_id_complete)(void *cb_arg, spdk_blob_id blobid, int bserrno);
-typedef void (*spdk_blob_op_with_handle_complete)(void *cb_arg, struct spdk_blob *blb, int bserrno);
 
+/**
+ * Blob operation completion callback with handle.
+ *
+ * \param cb_arg Callback argument.
+ * \param bs Handle to a blob.
+ * \param bserrno 0 if it completed successfully, or negative errno if it failed.
+ */
+typedef void (*spdk_blob_op_with_handle_complete)(void *cb_arg, struct spdk_blob *blb, int bserrno);
 
-/* Calls to function pointers of this type must obey all of the normal
-   rules for channels. The channel passed to this completion must match
-   the channel the operation was initiated on. */
+/**
+ * Blobstore device completion callback.
+ *
+ * \param channel I/O channel the operation was initiated on.
+ * \param cb_arg Callback argument.
+ * \param bserrno 0 if it completed successfully, or negative errno if it failed.
+ */
 typedef void (*spdk_bs_dev_cpl)(struct spdk_io_channel *channel,
 				void *cb_arg, int bserrno);
 
@@ -94,12 +146,6 @@ struct spdk_bs_dev_cb_args {
 	spdk_bs_dev_cpl		cb_fn;
 	struct spdk_io_channel	*channel;
 	void			*cb_arg;
-	/*
-	 * Blobstore device implementations can use this for scratch space for any data
-	 *  structures needed to translate the function arguments to the required format
-	 *  for the backing store.
-	 */
-	uint8_t			scratch[32];
 };
 
 struct spdk_bs_dev {
@@ -124,9 +170,23 @@ struct spdk_bs_dev {
 		      uint64_t lba, uint32_t lba_count,
 		      struct spdk_bs_dev_cb_args *cb_args);
 
+	void (*readv)(struct spdk_bs_dev *dev, struct spdk_io_channel *channel,
+		      struct iovec *iov, int iovcnt,
+		      uint64_t lba, uint32_t lba_count,
+		      struct spdk_bs_dev_cb_args *cb_args);
+
+	void (*writev)(struct spdk_bs_dev *dev, struct spdk_io_channel *channel,
+		       struct iovec *iov, int iovcnt,
+		       uint64_t lba, uint32_t lba_count,
+		       struct spdk_bs_dev_cb_args *cb_args);
+
 	void (*flush)(struct spdk_bs_dev *dev, struct spdk_io_channel *channel,
 		      struct spdk_bs_dev_cb_args *cb_args);
 
+	void (*write_zeroes)(struct spdk_bs_dev *dev, struct spdk_io_channel *channel,
+			     uint64_t lba, uint32_t lba_count,
+			     struct spdk_bs_dev_cb_args *cb_args);
+
 	void (*unmap)(struct spdk_bs_dev *dev, struct spdk_io_channel *channel,
 		      uint64_t lba, uint32_t lba_count,
 		      struct spdk_bs_dev_cb_args *cb_args);
@@ -135,132 +195,699 @@ struct spdk_bs_dev {
 	uint32_t	blocklen; /* In bytes */
 };
 
+struct spdk_bs_type {
+	char bstype[SPDK_BLOBSTORE_TYPE_LENGTH];
+};
+
 struct spdk_bs_opts {
-	uint32_t cluster_sz; /* In bytes. Must be multiple of page size. */
-	uint32_t num_md_pages; /* Count of the number of pages reserved for metadata */
-	uint32_t max_md_ops; /* Maximum simultaneous metadata operations */
+	/** Size of cluster in bytes. Must be multiple of 4KiB page size. */
+	uint32_t cluster_sz;
+
+	/** Count of the number of pages reserved for metadata */
+	uint32_t num_md_pages;
+
+	/** Maximum simultaneous metadata operations */
+	uint32_t max_md_ops;
+
+	/** Maximum simultaneous operations per channel */
+	uint32_t max_channel_ops;
+
+	/** Clear method */
+	enum bs_clear_method  clear_method;
+
+	/** Blobstore type */
+	struct spdk_bs_type bstype;
+
+	/** Callback function to invoke for each blob. */
+	spdk_blob_op_with_handle_complete iter_cb_fn;
+
+	/** Argument passed to iter_cb_fn for each blob. */
+	void *iter_cb_arg;
 };
 
-/* Initialize an spdk_bs_opts structure to the default blobstore option values. */
+/**
+ * Initialize a spdk_bs_opts structure to the default blobstore option values.
+ *
+ * \param opts The spdk_bs_opts structure to be initialized.
+ */
 void spdk_bs_opts_init(struct spdk_bs_opts *opts);
 
-/* Load a blob store from the given device. This will fail (return NULL) if no blob store is present. */
-void spdk_bs_load(struct spdk_bs_dev *dev,
+/**
+ * Load a blobstore from the given device.
+ *
+ * \param dev Blobstore block device.
+ * \param opts The structure which contains the option values for the blobstore.
+ * \param cb_fn Called when the loading is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bs_load(struct spdk_bs_dev *dev, struct spdk_bs_opts *opts,
 		  spdk_bs_op_with_handle_complete cb_fn, void *cb_arg);
 
-/* Initialize a blob store on the given disk. Destroys all data present on the device. */
+/**
+ * Initialize a blobstore on the given device.
+ *
+ * \param dev Blobstore block device.
+ * \param opts The structure which contains the option values for the blobstore.
+ * \param cb_fn Called when the initialization is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
 void spdk_bs_init(struct spdk_bs_dev *dev, struct spdk_bs_opts *opts,
 		  spdk_bs_op_with_handle_complete cb_fn, void *cb_arg);
 
-/* Flush all volatile data to disk and destroy in-memory structures. */
+typedef void (*spdk_bs_dump_print_xattr)(FILE *fp, const char *bstype, const char *name,
+		const void *value, size_t value_length);
+
+/**
+ * Dump a blobstore's metadata to a given FILE in human-readable format.
+ *
+ * \param dev Blobstore block device.
+ * \param fp FILE pointer to dump the metadata contents.
+ * \param print_xattr_fn Callback function to interpret external xattrs.
+ * \param cb_fn Called when the dump is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bs_dump(struct spdk_bs_dev *dev, FILE *fp, spdk_bs_dump_print_xattr print_xattr_fn,
+		  spdk_bs_op_complete cb_fn, void *cb_arg);
+/**
+ * Destroy the blobstore.
+ *
+ * It will destroy the blobstore by zeroing the super block.
+ *
+ * \param bs blobstore to destroy.
+ * \param cb_fn Called when the destruction is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bs_destroy(struct spdk_blob_store *bs, spdk_bs_op_complete cb_fn,
+		     void *cb_arg);
+
+/**
+ * Unload the blobstore.
+ *
+ * It will flush all volatile data to disk.
+ *
+ * \param bs blobstore to unload.
+ * \param cb_fn Called when the unloading is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
 void spdk_bs_unload(struct spdk_blob_store *bs, spdk_bs_op_complete cb_fn, void *cb_arg);
 
-/* Set the given blob as the super blob. This will be retrievable immediately after an
- * spdk_bs_load on the next initialization.
+/**
+ * Set a super blob on the given blobstore.
+ *
+ * This will be retrievable immediately after spdk_bs_load() on the next initializaiton.
+ *
+ * \param bs blobstore.
+ * \param blobid The id of the blob which will be set as the super blob.
+ * \param cb_fn Called when the setting is complete.
+ * \param cb_arg Argument passed to function cb_fn.
  */
 void spdk_bs_set_super(struct spdk_blob_store *bs, spdk_blob_id blobid,
 		       spdk_bs_op_complete cb_fn, void *cb_arg);
 
-/* Open the super blob. */
+/**
+ * Get the super blob. The obtained blob id will be passed to the callback function.
+ *
+ * \param bs blobstore.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
 void spdk_bs_get_super(struct spdk_blob_store *bs,
 		       spdk_blob_op_with_id_complete cb_fn, void *cb_arg);
 
-/* Get the cluster size in bytes. Used in the extend operation. */
+/**
+ * Get the cluster size in bytes.
+ *
+ * \param bs blobstore to query.
+ *
+ * \return cluster size.
+ */
 uint64_t spdk_bs_get_cluster_size(struct spdk_blob_store *bs);
 
-/* Get the page size in bytes. This is the write and read granularity of blobs. */
+/**
+ * Get the page size in bytes. This is the write and read granularity of blobs.
+ *
+ * \param bs blobstore to query.
+ *
+ * \return page size.
+ */
 uint64_t spdk_bs_get_page_size(struct spdk_blob_store *bs);
 
-/* Get the number of free clusters. */
-uint64_t spdk_bs_free_cluster_count(struct spdk_blob_store *bs);
+/**
+ * Get the io unit size in bytes.
+ *
+ * \param bs blobstore to query.
+ *
+ * \return io unit size.
+ */
+uint64_t spdk_bs_get_io_unit_size(struct spdk_blob_store *bs);
 
-/* Register the current thread as the metadata thread. All functions beginning with
- * the prefix "spdk_bs_md" must be called only from this thread.
+/**
+ * Get the number of free clusters.
+ *
+ * \param bs blobstore to query.
+ *
+ * \return the number of free clusters.
  */
-int spdk_bs_register_md_thread(struct spdk_blob_store *bs);
+uint64_t spdk_bs_free_cluster_count(struct spdk_blob_store *bs);
 
-/* Unregister the current thread as the metadata thread. This allows a different
- * thread to be registered.
+/**
+ * Get the total number of clusters accessible by user.
+ *
+ * \param bs blobstore to query.
+ *
+ * \return the total number of clusters accessible by user.
  */
-int spdk_bs_unregister_md_thread(struct spdk_blob_store *bs);
+uint64_t spdk_bs_total_data_cluster_count(struct spdk_blob_store *bs);
 
-/* Return the blobid */
+/**
+ * Get the blob id.
+ *
+ * \param blob Blob struct to query.
+ *
+ * \return blob id.
+ */
 spdk_blob_id spdk_blob_get_id(struct spdk_blob *blob);
 
-/* Return the number of pages allocated to the blob */
+/**
+ * Get the number of pages allocated to the blob.
+ *
+ * \param blob Blob struct to query.
+ *
+ * \return the number of pages.
+ */
 uint64_t spdk_blob_get_num_pages(struct spdk_blob *blob);
 
-/* Return the number of clusters allocated to the blob */
+/**
+ * Get the number of io_units allocated to the blob.
+ *
+ * \param blob Blob struct to query.
+ *
+ * \return the number of io_units.
+ */
+uint64_t spdk_blob_get_num_io_units(struct spdk_blob *blob);
+
+/**
+ * Get the number of clusters allocated to the blob.
+ *
+ * \param blob Blob struct to query.
+ *
+ * \return the number of clusters.
+ */
 uint64_t spdk_blob_get_num_clusters(struct spdk_blob *blob);
 
-/* Create a new blob with initial size of 'sz' clusters. */
-void spdk_bs_md_create_blob(struct spdk_blob_store *bs,
-			    spdk_blob_op_with_id_complete cb_fn, void *cb_arg);
+struct spdk_blob_xattr_opts {
+	/* Number of attributes */
+	size_t	count;
+	/* Array of attribute names. Caller should free this array after use. */
+	char	**names;
+	/* User context passed to get_xattr_value function */
+	void	*ctx;
+	/* Callback that will return value for each attribute name. */
+	void	(*get_value)(void *xattr_ctx, const char *name,
+			     const void **value, size_t *value_len);
+};
 
-/* Delete an existing blob. */
-void spdk_bs_md_delete_blob(struct spdk_blob_store *bs, spdk_blob_id blobid,
-			    spdk_blob_op_complete cb_fn, void *cb_arg);
+struct spdk_blob_opts {
+	uint64_t  num_clusters;
+	bool	thin_provision;
+	struct spdk_blob_xattr_opts xattrs;
+};
 
-/* Open a blob */
-void spdk_bs_md_open_blob(struct spdk_blob_store *bs, spdk_blob_id blobid,
-			  spdk_blob_op_with_handle_complete cb_fn, void *cb_arg);
+/**
+ * Initialize a spdk_blob_opts structure to the default blob option values.
+ *
+ * \param opts spdk_blob_opts structure to initialize.
+ */
+void spdk_blob_opts_init(struct spdk_blob_opts *opts);
 
-/* Resize a blob to 'sz' clusters.
+/**
+ * Create a new blob with options on the given blobstore. The new blob id will
+ * be passed to the callback function.
  *
- * These changes are not persisted to disk until
- * spdk_bs_md_sync_blob() is called. */
-int spdk_bs_md_resize_blob(struct spdk_blob *blob, size_t sz);
+ * \param bs blobstore.
+ * \param opts The structure which contains the option values for the new blob.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to funcion cb_fn.
+ */
+void spdk_bs_create_blob_ext(struct spdk_blob_store *bs, const struct spdk_blob_opts *opts,
+			     spdk_blob_op_with_id_complete cb_fn, void *cb_arg);
 
-/* Sync a blob */
-/* Make a blob persistent. This applies to open, resize, set xattr,
- * and remove xattr. These operations will not be persistent until
- * the blob has been synced.
+/**
+ * Create a new blob with default option values on the given blobstore.
+ * The new blob id will be passed to the callback function.
  *
- * I/O operations (read/write) are synced independently. See
- * spdk_bs_io_flush_channel().
+ * \param bs blobstore.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
  */
-void spdk_bs_md_sync_blob(struct spdk_blob *blob,
-			  spdk_blob_op_complete cb_fn, void *cb_arg);
+void spdk_bs_create_blob(struct spdk_blob_store *bs,
+			 spdk_blob_op_with_id_complete cb_fn, void *cb_arg);
 
-/* Close a blob. This will automatically sync. */
-void spdk_bs_md_close_blob(struct spdk_blob **blob, spdk_blob_op_complete cb_fn, void *cb_arg);
+/**
+ * Create a read-only snapshot of specified blob with provided options.
+ * This will automatically sync specified blob.
+ *
+ * When operation is done, original blob is converted to the thin-provisioned
+ * blob with a newly created read-only snapshot set as a backing blob.
+ * Structure snapshot_xattrs as well as anything it references (like e.g. names
+ * array) must be valid until the completion is called.
+ *
+ * \param bs blobstore.
+ * \param blobid Id of the source blob used to create a snapshot.
+ * \param snapshot_xattrs xattrs specified for snapshot.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bs_create_snapshot(struct spdk_blob_store *bs, spdk_blob_id blobid,
+			     const struct spdk_blob_xattr_opts *snapshot_xattrs,
+			     spdk_blob_op_with_id_complete cb_fn, void *cb_arg);
 
-struct spdk_io_channel *spdk_bs_alloc_io_channel(struct spdk_blob_store *bs,
-		uint32_t priority, uint32_t max_ops);
+/**
+ * Create a clone of specified read-only blob.
+ *
+ * Structure clone_xattrs as well as anything it references (like e.g. names
+ * array) must be valid until the completion is called.
+ *
+ * \param bs blobstore.
+ * \param blobid Id of the read only blob used as a snapshot for new clone.
+ * \param clone_xattrs xattrs specified for clone.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bs_create_clone(struct spdk_blob_store *bs, spdk_blob_id blobid,
+			  const struct spdk_blob_xattr_opts *clone_xattrs,
+			  spdk_blob_op_with_id_complete cb_fn, void *cb_arg);
+
+/**
+ * Provide table with blob id's of clones are dependent on specified snapshot.
+ *
+ * Ids array should be allocated and the count parameter set to the number of
+ * id's it can store, before calling this function.
+ *
+ * If ids is NULL or count parameter is not sufficient to handle ids of all
+ * clones, -ENOMEM error is returned and count parameter is updated to the
+ * total number of clones.
+ *
+ * \param bs blobstore.
+ * \param blobid Snapshots blob id.
+ * \param ids Array of the clone ids or NULL to get required size in count.
+ * \param count Size of ids. After call it is updated to the number of clones.
+ *
+ * \return -ENOMEM if count is not sufficient to store all clones.
+ */
+int spdk_blob_get_clones(struct spdk_blob_store *bs, spdk_blob_id blobid, spdk_blob_id *ids,
+			 size_t *count);
 
+/**
+ * Get the blob id for the parent snapshot of this blob.
+ *
+ * \param bs blobstore.
+ * \param blobid Blob id.
+ *
+ * \return blob id of parent blob or SPDK_BLOBID_INVALID if have no parent
+ */
+spdk_blob_id spdk_blob_get_parent_snapshot(struct spdk_blob_store *bs, spdk_blob_id blobid);
+
+/**
+ * Check if blob is read only.
+ *
+ * \param blob Blob.
+ *
+ * \return true if blob is read only.
+ */
+bool spdk_blob_is_read_only(struct spdk_blob *blob);
+
+/**
+ * Check if blob is a snapshot.
+ *
+ * \param blob Blob.
+ *
+ * \return true if blob is a snapshot.
+ */
+bool spdk_blob_is_snapshot(struct spdk_blob *blob);
+
+/**
+ * Check if blob is a clone.
+ *
+ * \param blob Blob.
+ *
+ * \return true if blob is a clone.
+ */
+bool spdk_blob_is_clone(struct spdk_blob *blob);
+
+/**
+ * Check if blob is thin-provisioned.
+ *
+ * \param blob Blob.
+ *
+ * \return true if blob is thin-provisioned.
+ */
+bool spdk_blob_is_thin_provisioned(struct spdk_blob *blob);
+
+/**
+ * Delete an existing blob from the given blobstore.
+ *
+ * \param bs blobstore.
+ * \param blobid The id of the blob to delete.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bs_delete_blob(struct spdk_blob_store *bs, spdk_blob_id blobid,
+			 spdk_blob_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Allocate all clusters in this blob. Data for allocated clusters is copied
+ * from backing blob(s) if they exist.
+ *
+ * This call removes all dependencies on any backing blobs.
+ *
+ * \param bs blobstore.
+ * \param channel IO channel used to inflate blob.
+ * \param blobid The id of the blob to inflate.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bs_inflate_blob(struct spdk_blob_store *bs, struct spdk_io_channel *channel,
+			  spdk_blob_id blobid, spdk_blob_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Remove dependency on parent blob.
+ *
+ * This call allocates and copies data for any clusters that are allocated in
+ * the parent blob, and decouples parent updating dependencies of blob to
+ * its ancestor.
+ *
+ * If blob have no parent -EINVAL error is reported.
+ *
+ * \param bs blobstore.
+ * \param channel IO channel used to inflate blob.
+ * \param blobid The id of the blob.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bs_blob_decouple_parent(struct spdk_blob_store *bs, struct spdk_io_channel *channel,
+				  spdk_blob_id blobid, spdk_blob_op_complete cb_fn, void *cb_arg);
+
+struct spdk_blob_open_opts {
+	enum blob_clear_method  clear_method;
+};
+
+/**
+ * Initialize a spdk_blob_open_opts structure to the default blob option values.
+ *
+ * \param opts spdk_blob_open_opts structure to initialize.
+ */
+void spdk_blob_open_opts_init(struct spdk_blob_open_opts *opts);
+
+/**
+ * Open a blob from the given blobstore.
+ *
+ * \param bs blobstore.
+ * \param blobid The id of the blob to open.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bs_open_blob(struct spdk_blob_store *bs, spdk_blob_id blobid,
+		       spdk_blob_op_with_handle_complete cb_fn, void *cb_arg);
+
+/**
+ * Open a blob from the given blobstore with additional options.
+ *
+ * \param bs blobstore.
+ * \param blobid The id of the blob to open.
+ * \param opts The structure which contains the option values for the blob.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bs_open_blob_ext(struct spdk_blob_store *bs, spdk_blob_id blobid,
+			   struct spdk_blob_open_opts *opts, spdk_blob_op_with_handle_complete cb_fn, void *cb_arg);
+
+/**
+ * Resize a blob to 'sz' clusters. These changes are not persisted to disk until
+ * spdk_bs_md_sync_blob() is called.
+ * If called before previous resize finish, it will fail with errno -EBUSY
+ *
+ * \param blob Blob to resize.
+ * \param sz The new number of clusters.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ *
+ */
+void spdk_blob_resize(struct spdk_blob *blob, uint64_t sz, spdk_blob_op_complete cb_fn,
+		      void *cb_arg);
+
+/**
+ * Set blob as read only.
+ *
+ * These changes do not take effect until spdk_blob_sync_md() is called.
+ *
+ * \param blob Blob to set.
+ */
+int spdk_blob_set_read_only(struct spdk_blob *blob);
+
+/**
+ * Sync a blob.
+ *
+ * Make a blob persistent. This applies to open, resize, set xattr, and remove
+ * xattr. These operations will not be persistent until the blob has been synced.
+ *
+ * \param blob Blob to sync.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_blob_sync_md(struct spdk_blob *blob, spdk_blob_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Close a blob. This will automatically sync.
+ *
+ * \param blob Blob to close.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_blob_close(struct spdk_blob *blob, spdk_blob_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Allocate an I/O channel for the given blobstore.
+ *
+ * \param bs blobstore.
+ * \return a pointer to the allocated I/O channel.
+ */
+struct spdk_io_channel *spdk_bs_alloc_io_channel(struct spdk_blob_store *bs);
+
+/**
+ * Free the I/O channel.
+ *
+ * \param channel I/O channel to free.
+ */
 void spdk_bs_free_io_channel(struct spdk_io_channel *channel);
 
-/* Force all previously completed operations on this channel to be persistent. */
-void spdk_bs_io_flush_channel(struct spdk_io_channel *channel,
-			      spdk_blob_op_complete cb_fn, void *cb_arg);
+/**
+ * Write data to a blob.
+ *
+ * \param blob Blob to write.
+ * \param channel The I/O channel used to submit requests.
+ * \param payload The specified buffer which should contain the data to be written.
+ * \param offset Offset is in io units from the beginning of the blob.
+ * \param length Size of data in io units.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_blob_io_write(struct spdk_blob *blob, struct spdk_io_channel *channel,
+			void *payload, uint64_t offset, uint64_t length,
+			spdk_blob_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Read data from a blob.
+ *
+ * \param blob Blob to read.
+ * \param channel The I/O channel used to submit requests.
+ * \param payload The specified buffer which will store the obtained data.
+ * \param offset Offset is in io units from the beginning of the blob.
+ * \param length Size of data in io units.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_blob_io_read(struct spdk_blob *blob, struct spdk_io_channel *channel,
+		       void *payload, uint64_t offset, uint64_t length,
+		       spdk_blob_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Write the data described by 'iov' to 'length' pages beginning at 'offset' pages
+ * into the blob.
+ *
+ * \param blob Blob to write.
+ * \param channel I/O channel used to submit requests.
+ * \param iov The pointer points to an array of iovec structures.
+ * \param iovcnt The number of buffers.
+ * \param offset Offset is in io units from the beginning of the blob.
+ * \param length Size of data in io units.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_blob_io_writev(struct spdk_blob *blob, struct spdk_io_channel *channel,
+			 struct iovec *iov, int iovcnt, uint64_t offset, uint64_t length,
+			 spdk_blob_op_complete cb_fn, void *cb_arg);
 
-/* Write data to a blob. Offset is in pages from the beginning of the blob. */
-void spdk_bs_io_write_blob(struct spdk_blob *blob, struct spdk_io_channel *channel,
-			   void *payload, uint64_t offset, uint64_t length,
-			   spdk_blob_op_complete cb_fn, void *cb_arg);
+/**
+ * Read 'length' pages starting at 'offset' pages into the blob into the memory
+ * described by 'iov'.
+ *
+ * \param blob Blob to read.
+ * \param channel I/O channel used to submit requests.
+ * \param iov The pointer points to an array of iovec structures.
+ * \param iovcnt The number of buffers.
+ * \param offset Offset is in io units from the beginning of the blob.
+ * \param length Size of data in io units.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_blob_io_readv(struct spdk_blob *blob, struct spdk_io_channel *channel,
+			struct iovec *iov, int iovcnt, uint64_t offset, uint64_t length,
+			spdk_blob_op_complete cb_fn, void *cb_arg);
 
+/**
+ * Unmap 'length' pages beginning at 'offset' pages on the blob as unused. Unmapped
+ * pages may allow the underlying storage media to behave more effciently.
+ *
+ * \param blob Blob to unmap.
+ * \param channel I/O channel used to submit requests.
+ * \param offset Offset is in io units from the beginning of the blob.
+ * \param length Size of unmap area in pages.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_blob_io_unmap(struct spdk_blob *blob, struct spdk_io_channel *channel,
+			uint64_t offset, uint64_t length, spdk_blob_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Write zeros into area of a blob.
+ *
+ * \param blob Blob to write.
+ * \param channel I/O channel used to submit requests.
+ * \param offset Offset is in io units from the beginning of the blob.
+ * \param length Size of data in io units.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_blob_io_write_zeroes(struct spdk_blob *blob, struct spdk_io_channel *channel,
+			       uint64_t offset, uint64_t length, spdk_blob_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Get the first blob of the blobstore. The obtained blob will be passed to
+ * the callback function.
+ *
+ * \param bs blobstore to traverse.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bs_iter_first(struct spdk_blob_store *bs,
+			spdk_blob_op_with_handle_complete cb_fn, void *cb_arg);
+
+/**
+ * Get the next blob by using the current blob. The obtained blob will be passed
+ * to the callback function.
+ *
+ * \param bs blobstore to traverse.
+ * \param blob The current blob.
+ * \param cb_fn Called when the operation is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_bs_iter_next(struct spdk_blob_store *bs, struct spdk_blob *blob,
+		       spdk_blob_op_with_handle_complete cb_fn, void *cb_arg);
+
+/**
+ * Set an extended attribute for the given blob.
+ *
+ * \param blob Blob to set attribute.
+ * \param name Name of the extended attribute.
+ * \param value Value of the extended attribute.
+ * \param value_len Length of the value.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_blob_set_xattr(struct spdk_blob *blob, const char *name, const void *value,
+			uint16_t value_len);
 
-/* Read data from a blob. Offset is in pages from the beginning of the blob. */
-void spdk_bs_io_read_blob(struct spdk_blob *blob, struct spdk_io_channel *channel,
-			  void *payload, uint64_t offset, uint64_t length,
-			  spdk_blob_op_complete cb_fn, void *cb_arg);
+/**
+ * Remove the extended attribute from the given blob.
+ *
+ * \param blob Blob to remove attribute.
+ * \param name Name of the extended attribute.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_blob_remove_xattr(struct spdk_blob *blob, const char *name);
 
-/* Iterate through all blobs */
-void spdk_bs_md_iter_first(struct spdk_blob_store *bs,
-			   spdk_blob_op_with_handle_complete cb_fn, void *cb_arg);
-void spdk_bs_md_iter_next(struct spdk_blob_store *bs, struct spdk_blob **blob,
-			  spdk_blob_op_with_handle_complete cb_fn, void *cb_arg);
+/**
+ * Get the value of the specified extended attribute. The obtained value and its
+ * size will be stored in value and value_len.
+ *
+ * \param blob Blob to query.
+ * \param name Name of the extended attribute.
+ * \param value Parameter as output.
+ * \param value_len Parameter as output.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_blob_get_xattr_value(struct spdk_blob *blob, const char *name,
+			      const void **value, size_t *value_len);
 
-int spdk_blob_md_set_xattr(struct spdk_blob *blob, const char *name, const void *value,
-			   uint16_t value_len);
-int spdk_blob_md_remove_xattr(struct spdk_blob *blob, const char *name);
-int spdk_bs_md_get_xattr_value(struct spdk_blob *blob, const char *name,
-			       const void **value, size_t *value_len);
-int spdk_bs_md_get_xattr_names(struct spdk_blob *blob,
-			       struct spdk_xattr_names **names);
+/**
+ * Iterate through all extended attributes of the blob. Get the names of all extended
+ * attributes that will be stored in names.
+ *
+ * \param blob Blob to query.
+ * \param names Parameter as output.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_blob_get_xattr_names(struct spdk_blob *blob, struct spdk_xattr_names **names);
 
+/**
+ * Get the number of extended attributes.
+ *
+ * \param names Names of total extended attributes of the blob.
+ *
+ * \return the number of extended attributes.
+ */
 uint32_t spdk_xattr_names_get_count(struct spdk_xattr_names *names);
+
+/**
+ * Get the attribute name specified by the index.
+ *
+ * \param names Names of total extended attributes of the blob.
+ * \param index Index position of the specified attribute.
+ *
+ * \return attribute name.
+ */
 const char *spdk_xattr_names_get_name(struct spdk_xattr_names *names, uint32_t index);
+
+/**
+ * Free the attribute names.
+ *
+ * \param names Names of total extended attributes of the blob.
+ */
 void spdk_xattr_names_free(struct spdk_xattr_names *names);
 
+/**
+ * Get blobstore type of the given device.
+ *
+ * \param bs blobstore to query.
+ *
+ * \return blobstore type.
+ */
+struct spdk_bs_type spdk_bs_get_bstype(struct spdk_blob_store *bs);
+
+/**
+ * Set blobstore type to the given device.
+ *
+ * \param bs blobstore to set to.
+ * \param bstype Type label to set.
+ */
+void spdk_bs_set_bstype(struct spdk_blob_store *bs, struct spdk_bs_type bstype);
+
+#ifdef __cplusplus
+}
+#endif
+
 #endif /* SPDK_BLOB_H_ */
diff --git a/PDK/core/src/api/include/udd/spdk/blob_bdev.h b/PDK/core/src/api/include/udd/spdk/blob_bdev.h
index 674427b..dcaa5b1 100644
--- a/PDK/core/src/api/include/udd/spdk/blob_bdev.h
+++ b/PDK/core/src/api/include/udd/spdk/blob_bdev.h
@@ -39,6 +39,7 @@
 #define SPDK_BLOB_BDEV_H
 
 #include "spdk/stdinc.h"
+#include "spdk/bdev.h"
 
 #ifdef __cplusplus
 extern "C" {
@@ -46,8 +47,29 @@ extern "C" {
 
 struct spdk_bs_dev;
 struct spdk_bdev;
+struct spdk_bdev_module;
 
-struct spdk_bs_dev *spdk_bdev_create_bs_dev(struct spdk_bdev *);
+/**
+ * Create a blobstore block device from a bdev.
+ *
+ * \param bdev Bdev to use.
+ * \param remove_cb Called when the block device is removed.
+ * \param remove_ctx Argument passed to function remove_cb.
+ *
+ * \return a pointer to the blobstore block device on success or NULL otherwise.
+ */
+struct spdk_bs_dev *spdk_bdev_create_bs_dev(struct spdk_bdev *bdev, spdk_bdev_remove_cb_t remove_cb,
+		void *remove_ctx);
+
+/**
+ * Claim the bdev module for the given blobstore.
+ *
+ * \param bs_dev Blobstore block device.
+ * \param module Bdev module to claim.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_bs_bdev_claim(struct spdk_bs_dev *bs_dev, struct spdk_bdev_module *module);
 
 #ifdef __cplusplus
 }
diff --git a/PDK/core/src/api/include/udd/spdk/blobfs.h b/PDK/core/src/api/include/udd/spdk/blobfs.h
index 8a24908..7a166c8 100644
--- a/PDK/core/src/api/include/udd/spdk/blobfs.h
+++ b/PDK/core/src/api/include/udd/spdk/blobfs.h
@@ -42,6 +42,10 @@
 
 #include "spdk/blob.h"
 
+#ifdef __cplusplus
+extern "C" {
+#endif
+
 #define SPDK_FILE_NAME_MAX	255
 
 struct spdk_file;
@@ -49,85 +53,361 @@ struct spdk_filesystem;
 
 typedef struct spdk_file *spdk_fs_iter;
 
+struct spdk_blobfs_opts {
+	uint32_t	cluster_sz;
+};
+
 struct spdk_file_stat {
 	spdk_blob_id	blobid;
 	uint64_t	size;
 };
 
+/**
+ * Filesystem operation completion callback with handle.
+ *
+ * \param ctx Context for the operation.
+ * \param fs Handle to a blobfs.
+ * \param fserrno 0 if it completed successfully, or negative errno if it failed.
+ */
 typedef void (*spdk_fs_op_with_handle_complete)(void *ctx, struct spdk_filesystem *fs,
 		int fserrno);
+
+/**
+ * File operation completion callback with handle.
+ *
+ * \param ctx Context for the operation.
+ * \param f Handle to a file.
+ * \param fserrno 0 if it completed successfully, or negative errno if it failed.
+ */
 typedef void (*spdk_file_op_with_handle_complete)(void *ctx, struct spdk_file *f, int fserrno);
 typedef spdk_bs_op_complete spdk_fs_op_complete;
 
+/**
+ * File operation completion callback.
+ *
+ * \param ctx Context for the operation.
+ * \param fserrno 0 if it completed successfully, or negative errno if it failed.
+ */
 typedef void (*spdk_file_op_complete)(void *ctx, int fserrno);
+
+/**
+ * File stat operation completion callback.
+ *
+ * \param ctx Context for the operation.
+ * \param stat Handle to the stat about the file.
+ * \param fserrno 0 if it completed successfully, or negative errno if it failed.
+ */
 typedef void (*spdk_file_stat_op_complete)(void *ctx, struct spdk_file_stat *stat, int fserrno);
 
-typedef void (*fs_request_fn)(void *);
-typedef void (*fs_send_request_fn)(fs_request_fn, void *);
+/**
+ * Function for a request of file system.
+ *
+ * \param arg Argument to the request function.
+ */
+typedef void (*fs_request_fn)(void *arg);
 
-void spdk_fs_init(struct spdk_bs_dev *dev, fs_send_request_fn send_request_fn,
+/**
+ * Function for sending request.
+ *
+ * This function will be invoked any time when the filesystem wants to pass a
+ * message to the main dispatch thread.
+ *
+ * \param fs_request_fn A pointer to the request function.
+ * \param arg Argument to the request function.
+ */
+typedef void (*fs_send_request_fn)(fs_request_fn, void *arg);
+
+/**
+ * Initialize a spdk_blobfs_opts structure to the default option values.
+ *
+ * \param opts spdk_blobf_opts struture to intialize.
+ */
+void spdk_fs_opts_init(struct spdk_blobfs_opts *opts);
+
+/**
+ * Initialize blobstore filesystem.
+ *
+ * Initialize the blobstore filesystem on the blobstore block device which has
+ * been created by the function spdk_bdev_create_bs_dev() in the blob_bdev.h.
+ * The obtained blobstore filesystem will be passed to the callback function.
+ *
+ * \param dev Blobstore block device used by this blobstore filesystem.
+ * \param opt Initialization options used for this blobstore filesystem.
+ * \param send_request_fn The function for sending request. This function will
+ * be invoked any time when the blobstore filesystem wants to pass a message to
+ * the main dispatch thread.
+ * \param cb_fn Called when the initialization is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_fs_init(struct spdk_bs_dev *dev, struct spdk_blobfs_opts *opt,
+		  fs_send_request_fn send_request_fn,
 		  spdk_fs_op_with_handle_complete cb_fn, void *cb_arg);
+
+/**
+ * Load blobstore filesystem from the given blobstore block device.
+ *
+ * The obtained blobstore filesystem will be passed to the callback function.
+ *
+ * \param dev Blobstore block device used by this blobstore filesystem.
+ * \param send_request_fn The function for sending request. This function will
+ * be invoked any time when the blobstore filesystem wants to pass a message to
+ * the main dispatch thread.
+ * \param cb_fn Called when the loading is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
 void spdk_fs_load(struct spdk_bs_dev *dev, fs_send_request_fn send_request_fn,
 		  spdk_fs_op_with_handle_complete cb_fn, void *cb_arg);
-void spdk_fs_unload(struct spdk_filesystem *fs, spdk_fs_op_complete cb_fn, void *cb_arg);
 
-struct spdk_io_channel *spdk_fs_alloc_io_channel(struct spdk_filesystem *fs, uint32_t priority);
+/**
+ * Unload blobstore filesystem.
+ *
+ * \param fs Blobstore filesystem to unload.
+ * \param cb_fn Called when the unloading is complete.
+ * \param cb_arg Argument passed to function cb_fn.
+ */
+void spdk_fs_unload(struct spdk_filesystem *fs, spdk_fs_op_complete cb_fn, void *cb_arg);
 
-/*
- * Allocates an I/O channel suitable for using the synchronous blobfs API.  These channels do
- *  not allocate an I/O channel for the underlying blobstore, but rather allocate synchronizaiton
- *  primitives used to block until any necessary I/O operations are completed on a separate
- *  polling thread.
+/**
+ * Allocate an I/O channel for asynchronous operations.
+ *
+ * \param fs Blobstore filesystem to allocate I/O channel.
+ *
+ * \return a pointer to the I/O channel on success or NULL otherwise.
  */
-struct spdk_io_channel *spdk_fs_alloc_io_channel_sync(struct spdk_filesystem *fs,
-		uint32_t priority);
+struct spdk_io_channel *spdk_fs_alloc_io_channel(struct spdk_filesystem *fs);
 
+/**
+ * Free I/O channel.
+ *
+ * This function will decrease the references of this I/O channel. If the reference
+ * is reduced to 0, the I/O channel will be freed.
+ *
+ * \param channel I/O channel to free.
+ */
 void spdk_fs_free_io_channel(struct spdk_io_channel *channel);
 
-int spdk_fs_file_stat(struct spdk_filesystem *fs, struct spdk_io_channel *channel,
+/**
+ * Allocate a context for synchronous operations.
+ *
+ * \param fs Blobstore filesystem for this context.
+ *
+ * \return a pointer to the context on success or NULL otherwise.
+ */
+struct spdk_fs_thread_ctx *spdk_fs_alloc_thread_ctx(struct spdk_filesystem *fs);
+
+/**
+ * Free thread context.
+ *
+ * \param ctx Thread context to free.
+ */
+void spdk_fs_free_thread_ctx(struct spdk_fs_thread_ctx *ctx);
+
+/**
+ * Get statistics about the file including the underlying blob id and the file size.
+ *
+ * \param fs Blobstore filesystem.
+ * \param ctx The thread context for this operation
+ * \param name The file name used to look up the matched file in the blobstore filesystem.
+ * \param stat Caller allocated structure to store the obtained information about
+ * this file.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_fs_file_stat(struct spdk_filesystem *fs, struct spdk_fs_thread_ctx *ctx,
 		      const char *name, struct spdk_file_stat *stat);
 
 #define SPDK_BLOBFS_OPEN_CREATE	(1ULL << 0)
 
-int spdk_fs_create_file(struct spdk_filesystem *fs, struct spdk_io_channel *channel,
+/**
+ * Create a new file on the given blobstore filesystem.
+ *
+ * \param fs Blobstore filesystem.
+ * \param ctx The thread context for this operation
+ * \param name The file name for this new file.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_fs_create_file(struct spdk_filesystem *fs, struct spdk_fs_thread_ctx *ctx,
 			const char *name);
 
-int spdk_fs_open_file(struct spdk_filesystem *fs, struct spdk_io_channel *channel,
+/**
+ * Open the file.
+ *
+ * \param fs Blobstore filesystem.
+ * \param ctx The thread context for this operation
+ * \param name The file name used to look up the matched file in the blobstore filesystem.
+ * \param flags This flags will be used to control the open mode.
+ * \param file It will point to the open file if sccessful or NULL otherwirse.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_fs_open_file(struct spdk_filesystem *fs, struct spdk_fs_thread_ctx *ctx,
 		      const char *name, uint32_t flags, struct spdk_file **file);
 
-int spdk_file_close(struct spdk_file *file, struct spdk_io_channel *channel);
+/**
+ * Close the file.
+ *
+ * \param file File to close.
+ * \param ctx The thread context for this operation
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_file_close(struct spdk_file *file, struct spdk_fs_thread_ctx *ctx);
 
-int spdk_fs_rename_file(struct spdk_filesystem *fs, struct spdk_io_channel *channel,
+/**
+ * Change the file name.
+ *
+ * This operation will overwrite an existing file if there is a file with the
+ * same name.
+ *
+ * \param fs Blobstore filesystem.
+ * \param ctx The thread context for this operation
+ * \param old_name Old name of the file.
+ * \param new_name New name of the file.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_fs_rename_file(struct spdk_filesystem *fs, struct spdk_fs_thread_ctx *ctx,
 			const char *old_name, const char *new_name);
 
-int spdk_fs_delete_file(struct spdk_filesystem *fs, struct spdk_io_channel *channel,
+/**
+ * Delete the file.
+ *
+ * \param fs Blobstore filesystem.
+ * \param ctx The thread context for this operation
+ * \param name The name of the file to be deleted.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_fs_delete_file(struct spdk_filesystem *fs, struct spdk_fs_thread_ctx *ctx,
 			const char *name);
 
+/**
+ * Get the first file in the blobstore filesystem.
+ *
+ * \param fs Blobstore filesystem to traverse.
+ *
+ * \return an iterator which points to the first file in the blobstore filesystem.
+ */
 spdk_fs_iter spdk_fs_iter_first(struct spdk_filesystem *fs);
+
+/**
+ * Get the next file in the blobstore filesystem by using the input iterator.
+ *
+ * \param iter The iterator which points to the current file struct.
+ *
+ * \return an iterator which points to the next file in the blobstore filesystem.
+ */
 spdk_fs_iter spdk_fs_iter_next(spdk_fs_iter iter);
+
 #define spdk_fs_iter_get_file(iter)	((struct spdk_file *)(iter))
 
-void spdk_file_truncate(struct spdk_file *file, struct spdk_io_channel *channel,
-			uint64_t length);
+/**
+ * Truncate the file.
+ *
+ * \param file File to truncate.
+ * \param ctx The thread context for this operation
+ * \param length New size in bytes of the file.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_file_truncate(struct spdk_file *file, struct spdk_fs_thread_ctx *ctx,
+		       uint64_t length);
 
+/**
+ * Get file name.
+ *
+ * \param file File to query.
+ *
+ * \return the name of the file.
+ */
 const char *spdk_file_get_name(struct spdk_file *file);
 
+/**
+ * Obtain the size of the file.
+ *
+ * \param file File to query.
+ *
+ * \return the size in bytes of the file.
+ */
 uint64_t spdk_file_get_length(struct spdk_file *file);
 
-int spdk_file_write(struct spdk_file *file, struct spdk_io_channel *channel,
+/**
+ * Write data to the given file.
+ *
+ * \param file File to write.
+ * \param ctx The thread context for this operation
+ * \param payload The specified buffer which should contain the data to be transmitted.
+ * \param offset The beginning position to write data.
+ * \param length The size in bytes of data to write.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_file_write(struct spdk_file *file, struct spdk_fs_thread_ctx *ctx,
 		    void *payload, uint64_t offset, uint64_t length);
 
-int64_t spdk_file_read(struct spdk_file *file, struct spdk_io_channel *channel,
+/**
+ * Read data to user buffer from the given file.
+ *
+ * \param file File to read.
+ * \param ctx The thread context for this operation
+ * \param payload The specified buffer which will store the obtained data.
+ * \param offset The beginning position to read.
+ * \param length The size in bytes of data to read.
+ *
+ * \return the end position of this read operation on success, negated errno on failure.
+ */
+int64_t spdk_file_read(struct spdk_file *file, struct spdk_fs_thread_ctx *ctx,
 		       void *payload, uint64_t offset, uint64_t length);
 
+/**
+ * Set cache size for the blobstore filesystem.
+ *
+ * \param size_in_mb Cache size in megabytes.
+ */
 void spdk_fs_set_cache_size(uint64_t size_in_mb);
+
+/**
+ * Obtain the cache size.
+ *
+ * \return cache size in megabytes.
+ */
 uint64_t spdk_fs_get_cache_size(void);
 
 #define SPDK_FILE_PRIORITY_LOW	0 /* default */
 #define SPDK_FILE_PRIORITY_HIGH	1
 
+/**
+ * Set priority for the file.
+ *
+ * \param file File to set priority.
+ * \param priority Priority level (SPDK_FILE_PRIORITY_LOW or SPDK_FILE_PRIORITY_HIGH).
+ */
 void spdk_file_set_priority(struct spdk_file *file, uint32_t priority);
 
-int spdk_file_sync(struct spdk_file *file, struct spdk_io_channel *channel);
+/**
+ * Synchronize the data from the cache to the disk.
+ *
+ * \param file File to sync.
+ * \param ctx The thread context for this operation
+ *
+ * \return 0 on success.
+ */
+int spdk_file_sync(struct spdk_file *file, struct spdk_fs_thread_ctx *ctx);
+
+/**
+ * Get the unique ID for the file.
+ *
+ * \param file File to get the ID.
+ * \param id ID buffer.
+ * \param size Size of the ID buffer.
+ *
+ * \return the length of ID on success.
+ */
+int spdk_file_get_id(struct spdk_file *file, void *id, size_t size);
+
+#ifdef __cplusplus
+}
+#endif
 
 #endif /* SPDK_FS_H_ */
diff --git a/PDK/core/src/api/include/udd/spdk/conf.h b/PDK/core/src/api/include/udd/spdk/conf.h
index 586b466..51cbd8d 100644
--- a/PDK/core/src/api/include/udd/spdk/conf.h
+++ b/PDK/core/src/api/include/udd/spdk/conf.h
@@ -41,30 +41,168 @@
 
 #include "spdk/stdinc.h"
 
+#ifdef __cplusplus
+extern "C" {
+#endif
+
 struct spdk_conf_value;
 struct spdk_conf_item;
 struct spdk_conf_section;
 struct spdk_conf;
 
+/**
+ * Allocate a configuration struct used for the initialization of SPDK app.
+ *
+ * \return a pointer to the allocated configuration struct.
+ */
 struct spdk_conf *spdk_conf_allocate(void);
+
+/**
+ * Free the configuration struct.
+ *
+ * \param cp Configuration struct to free.
+ */
 void spdk_conf_free(struct spdk_conf *cp);
+
+/**
+ * Read configuration file for spdk_conf struct.
+ *
+ * \param cp Configuration struct used for the initialization of SPDK app.
+ * \param file File to read that is created by user to configure SPDK app.
+ *
+ * \return 0 on success, -1 on failure.
+ */
 int spdk_conf_read(struct spdk_conf *cp, const char *file);
+
+/**
+ * Find the specified section of the configuration.
+ *
+ * \param cp Configuration struct used for the initialization of SPDK app.
+ * \param name Name of section to find.
+ *
+ * \return a pointer to the requested section on success or NULL otherwise.
+ */
 struct spdk_conf_section *spdk_conf_find_section(struct spdk_conf *cp, const char *name);
 
-/* Configuration file iteration */
+/**
+ * Get the first section of the configuration.
+ *
+ * \param cp Configuration struct used for the initialization of SPDK app.
+ *
+ * \return a pointer to the requested section on success or NULL otherwise.
+ */
 struct spdk_conf_section *spdk_conf_first_section(struct spdk_conf *cp);
+
+/**
+ * Get the next section of the configuration.
+ *
+ * \param sp The current section of the configuration.
+ *
+ * \return a pointer to the requested section on success or NULL otherwise.
+ */
 struct spdk_conf_section *spdk_conf_next_section(struct spdk_conf_section *sp);
 
+/**
+ * Match prefix of the name of section.
+ *
+ * \param sp The section of the configuration.
+ * \param name_prefix Prefix name to match.
+ *
+ * \return ture on success, false on failure.
+ */
 bool spdk_conf_section_match_prefix(const struct spdk_conf_section *sp, const char *name_prefix);
+
+/**
+ * Get the name of the section.
+ *
+ * \param sp The section of the configuration.
+ *
+ * \return the name of the section.
+ */
 const char *spdk_conf_section_get_name(const struct spdk_conf_section *sp);
+
+/**
+ * Get the number of the section.
+ *
+ * \param sp The section of the configuration.
+ *
+ * \return the number of the section.
+ */
 int spdk_conf_section_get_num(const struct spdk_conf_section *sp);
+
+/**
+ * Get the value of the item with name 'key' in the section.
+ *
+ * If key appears multiple times, idx1 will control which version to retrieve.
+ * Indices will start from the top of the configuration file at 0 and increment
+ * by one for each new apperarance. If the configuration key contains multiple
+ * whitespace delimited values, idx2 controls which value is returned. The index
+ * begins at 0.
+ *
+ *
+ * \param sp The section of the configuration.
+ * \param key Name of item.
+ * \param idx1 The index into the item list for the key.
+ * \param idx2 The index into the value list for the item.
+ *
+ * \return the requested value on success or NULL otherwise.
+ */
 char *spdk_conf_section_get_nmval(struct spdk_conf_section *sp, const char *key,
 				  int idx1, int idx2);
+
+/**
+ * Get the first value of the item with name 'key' in the section.
+ *
+ * \param sp The section of the configuration.
+ * \param key Name of item.
+ * \param idx The index into the value list for the item.
+ *
+ * \return the requested value on success or NULL otherwise.
+ */
 char *spdk_conf_section_get_nval(struct spdk_conf_section *sp, const char *key, int idx);
+
+/**
+ * Get the first value of the first item with name 'key' in the section.
+ *
+ * \param sp The section of the configuration.
+ * \param key Name of item.
+ *
+ * \return the requested value on success or NULL otherwise.
+ */
 char *spdk_conf_section_get_val(struct spdk_conf_section *sp, const char *key);
+
+/**
+ * Get the first value of the first item with name 'key' in the section.
+ *
+ * \param sp The section of the configuration.
+ * \param key Name of item.
+ *
+ * \return the requested value on success or NULL otherwise.
+ */
 int spdk_conf_section_get_intval(struct spdk_conf_section *sp, const char *key);
+
+/**
+ * Get the bool value of the item with name 'key' in the section.
+ *
+ * This is used to check whether the service is enabled.
+ *
+ * \param sp The section of the configuration.
+ * \param key Name of item.
+ * \param default_val Default value.
+ *
+ * \return true if matching 'Yes/Y/True', false if matching 'No/N/False', default value otherwise.
+ */
 bool spdk_conf_section_get_boolval(struct spdk_conf_section *sp, const char *key, bool default_val);
 
+/**
+ * Set the configuration as the default.
+ *
+ * \param cp Configuration to set.
+ */
 void spdk_conf_set_as_default(struct spdk_conf *cp);
 
+#ifdef __cplusplus
+}
+#endif
+
 #endif
diff --git a/PDK/core/src/api/include/udd/spdk/config.h b/PDK/core/src/api/include/udd/spdk/config.h
new file mode 100644
index 0000000..4486af3
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/config.h
@@ -0,0 +1,44 @@
+#ifndef SPDK_CONFIG_H
+#define SPDK_CONFIG_H
+#undef SPDK_CONFIG_ASAN
+#undef SPDK_CONFIG_COVERAGE
+#undef SPDK_CONFIG_CRYPTO
+#undef SPDK_CONFIG_CUSTOMOCF
+#undef SPDK_CONFIG_DEBUG
+#define SPDK_CONFIG_DPDK_DIR /benixona/2H2019/nkv-unvme/driver/external/dpdk-19.05
+#define SPDK_CONFIG_ENV /benixona/2H2019/nkv-unvme/driver/external/spdk-19.04.1/lib/env_dpdk
+#undef SPDK_CONFIG_FIO_PLUGIN
+#define SPDK_CONFIG_FIO_SOURCE_DIR 
+#undef SPDK_CONFIG_FTL
+#undef SPDK_CONFIG_IGB_UIO_DRIVER
+#undef SPDK_CONFIG_IPSEC_MB
+#undef SPDK_CONFIG_ISAL
+#undef SPDK_CONFIG_ISCSI_INITIATOR
+#undef SPDK_CONFIG_LOG_BACKTRACE
+#undef SPDK_CONFIG_LTO
+#undef SPDK_CONFIG_OCF
+#define SPDK_CONFIG_OCF_PATH 
+#undef SPDK_CONFIG_PGO_CAPTURE
+#undef SPDK_CONFIG_PGO_USE
+#undef SPDK_CONFIG_PMDK
+#define SPDK_CONFIG_PMDK_DIR 
+#define SPDK_CONFIG_PREFIX /usr/local
+#undef SPDK_CONFIG_RBD
+#undef SPDK_CONFIG_RDMA
+#undef SPDK_CONFIG_RDMA_SEND_WITH_INVAL
+#undef SPDK_CONFIG_REDUCE
+#undef SPDK_CONFIG_SHARED
+#define SPDK_CONFIG_TESTS 1
+#undef SPDK_CONFIG_TSAN
+#undef SPDK_CONFIG_UBSAN
+#undef SPDK_CONFIG_URING
+#define SPDK_CONFIG_URING_PATH 
+#define SPDK_CONFIG_VHOST 1
+#define SPDK_CONFIG_VHOST_INTERNAL_LIB 1
+#define SPDK_CONFIG_VIRTIO 1
+#undef SPDK_CONFIG_VPP
+#define SPDK_CONFIG_VPP_DIR 
+#undef SPDK_CONFIG_VTUNE
+#define SPDK_CONFIG_VTUNE_DIR 
+#undef SPDK_CONFIG_WERROR
+#endif /* SPDK_CONFIG_H */
diff --git a/PDK/core/src/api/include/udd/spdk/copy_engine.h b/PDK/core/src/api/include/udd/spdk/copy_engine.h
index e450387..06dc156 100644
--- a/PDK/core/src/api/include/udd/spdk/copy_engine.h
+++ b/PDK/core/src/api/include/udd/spdk/copy_engine.h
@@ -40,17 +40,108 @@
 
 #include "spdk/stdinc.h"
 
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * Copy operation callback.
+ *
+ * \param ref 'copy_req' passed to the corresponding spdk_copy_submit() call.
+ * \param status 0 if it completed successfully, or negative errno if it failed.
+ */
 typedef void (*spdk_copy_completion_cb)(void *ref, int status);
 
+/**
+ * Copy engine finish callback.
+ *
+ * \param cb_arg Callback argument.
+ */
+typedef void (*spdk_copy_fini_cb)(void *cb_arg);
+
 struct spdk_io_channel;
 
 struct spdk_copy_task;
 
-struct spdk_io_channel *spdk_copy_engine_get_io_channel(uint32_t priority);
-int64_t spdk_copy_submit(struct spdk_copy_task *copy_req, struct spdk_io_channel *ch, void *dst,
-			 void *src, uint64_t nbytes, spdk_copy_completion_cb cb);
-int64_t spdk_copy_submit_fill(struct spdk_copy_task *copy_req, struct spdk_io_channel *ch,
-			      void *dst, uint8_t fill, uint64_t nbytes, spdk_copy_completion_cb cb);
+/**
+ * Initialize the copy engine.
+ *
+ * \return 0 on success.
+ */
+int spdk_copy_engine_initialize(void);
+
+/**
+ * Close the copy engine.
+ *
+ * \param cb_fn Called when the close operation completes.
+ * \param cb_arg Argument passed to the callback function.
+ */
+void spdk_copy_engine_finish(spdk_copy_fini_cb cb_fn, void *cb_arg);
+
+/**
+ * Get the configuration for the copy engine.
+ *
+ * \param fp The pointer to a file that will be written to the configuration.
+ */
+void spdk_copy_engine_config_text(FILE *fp);
+
+/**
+ * Close the copy engine module and perform any necessary cleanup.
+ */
+void spdk_copy_engine_module_finish(void);
+
+/**
+ * Get the I/O channel registered on the copy engine.
+ *
+ * This I/O channel is used to submit copy request.
+ *
+ * \return a pointer to the I/O channel on success, or NULL on failure.
+ */
+struct spdk_io_channel *spdk_copy_engine_get_io_channel(void);
+
+/**
+ * Submit a copy request.
+ *
+ * \param copy_req Copy request task.
+ * \param ch I/O channel to submit request to the copy engine. This channel can
+ * be obtained by the function spdk_copy_engine_get_io_channel().
+ * \param dst Destination to copy to.
+ * \param src Source to copy from.
+ * \param nbytes Length in bytes to copy.
+ * \param cb Called when this copy operation completes.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_copy_submit(struct spdk_copy_task *copy_req, struct spdk_io_channel *ch, void *dst,
+		     void *src, uint64_t nbytes, spdk_copy_completion_cb cb);
+
+/**
+ * Submit a fill request.
+ *
+ * This operation will fill the destination buffer with the specified value.
+ *
+ * \param copy_req Copy request task.
+ * \param ch I/O channel to submit request to the copy engine. This channel can
+ * be obtained by the function spdk_copy_engine_get_io_channel().
+ * \param dst Destination to fill.
+ * \param fill Constant byte to fill to the destination.
+ * \param nbytes Length in bytes to fill.
+ * \param cb Called when this copy operation completes.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_copy_submit_fill(struct spdk_copy_task *copy_req, struct spdk_io_channel *ch,
+			  void *dst, uint8_t fill, uint64_t nbytes, spdk_copy_completion_cb cb);
+
+/**
+ * Get the size of copy task.
+ *
+ * \return the size of copy task.
+ */
 size_t spdk_copy_task_size(void);
 
+#ifdef __cplusplus
+}
+#endif
+
 #endif
diff --git a/PDK/core/src/api/include/udd/spdk/cpuset.h b/PDK/core/src/api/include/udd/spdk/cpuset.h
new file mode 100644
index 0000000..153fe0f
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/cpuset.h
@@ -0,0 +1,179 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * \file
+ * CPU set management functions
+ */
+
+#ifndef SPDK_CPUSET_H
+#define SPDK_CPUSET_H
+
+#include "spdk/stdinc.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#define SPDK_CPUSET_SIZE 1024
+
+/**
+ * List of CPUs.
+ */
+struct spdk_cpuset;
+
+/**
+ * Allocate CPU set object.
+ *
+ * \return a pointer to the allocated zeroed cpuset on success, or NULL on failure.
+ */
+struct spdk_cpuset *spdk_cpuset_alloc(void);
+
+/**
+ * Free allocated CPU set.
+ *
+ * \param set CPU set to be freed.
+ */
+void spdk_cpuset_free(struct spdk_cpuset *set);
+
+/**
+ * Compare two CPU sets.
+ *
+ * \param set1 CPU set1.
+ * \param set2 CPU set2.
+ *
+ * \return true if both CPU sets are equal.
+ */
+bool spdk_cpuset_equal(const struct spdk_cpuset *set1, const struct spdk_cpuset *set2);
+
+/**
+ * Copy the content of CPU set to another.
+ *
+ * \param dst Destination CPU set
+ * \param src Source CPU set
+ */
+void spdk_cpuset_copy(struct spdk_cpuset *dst, const struct spdk_cpuset *src);
+
+/**
+ * Perform AND operation on two CPU sets. The result is stored in dst.
+ *
+ * \param dst First argument of operation. This value also stores the result of operation.
+ * \param src Second argument of operation.
+ */
+void spdk_cpuset_and(struct spdk_cpuset *dst, const struct spdk_cpuset *src);
+
+/**
+ * Perform OR operation on two CPU sets. The result is stored in dst.
+ *
+ * \param dst First argument of operation. This value also stores the result of operation.
+ * \param src Second argument of operation.
+ */
+void spdk_cpuset_or(struct spdk_cpuset *dst, const struct spdk_cpuset *src);
+
+/**
+ * Perform XOR operation on two CPU sets. The result is stored in dst.
+ *
+ * \param dst First argument of operation. This value also stores the result of operation.
+ * \param src Second argument of operation.
+ */
+void spdk_cpuset_xor(struct spdk_cpuset *dst, const struct spdk_cpuset *src);
+
+/**
+ * Negate all CPUs in CPU set.
+ *
+ * \param set CPU set to be negated. This value also stores the result of operation.
+ */
+void spdk_cpuset_negate(struct spdk_cpuset *set);
+
+/**
+ * Clear all CPUs in CPU set.
+ *
+ * \param set CPU set to be cleared.
+ */
+void spdk_cpuset_zero(struct spdk_cpuset *set);
+
+/**
+ * Set or clear CPU state in CPU set.
+ *
+ * \param set CPU set object.
+ * \param cpu CPU index to be set or cleared.
+ * \param state *true* to set cpu, *false* to clear.
+ */
+void spdk_cpuset_set_cpu(struct spdk_cpuset *set, uint32_t cpu, bool state);
+
+/**
+ * Get the state of CPU in CPU set.
+ *
+ * \param set CPU set object.
+ * \param cpu CPU index.
+ *
+ * \return the state of selected CPU.
+ */
+bool spdk_cpuset_get_cpu(const struct spdk_cpuset *set, uint32_t cpu);
+
+/**
+ * Get the number of CPUs that are set in CPU set.
+ *
+ * \param set CPU set object.
+ *
+ * \return the number of CPUs.
+ */
+uint32_t spdk_cpuset_count(const struct spdk_cpuset *set);
+
+/**
+ * Convert a CPU set to hex string.
+ *
+ * \param set CPU set.
+ *
+ * \return a pointer to hexadecimal representation of CPU set. Buffer to store a
+ * string is dynamically allocated internally and freed with CPU set object.
+ * Memory returned by this function might be changed after subsequent calls to
+ * this function so string should be copied by user.
+ */
+const char *spdk_cpuset_fmt(struct spdk_cpuset *set);
+
+/**
+ * Convert a string containing a CPU core mask into a CPU set.
+ *
+ * \param set CPU set.
+ * \param mask String defining CPU set. By default hexadecimal value is used or
+ * as CPU list enclosed in square brackets defined as: 'c1[-c2][,c3[-c4],...]'.
+ *
+ * \return zero if success, non zero if fails.
+ */
+int spdk_cpuset_parse(struct spdk_cpuset *set, const char *mask);
+
+#ifdef __cplusplus
+}
+#endif
+#endif /* SPDK_CPUSET_H */
diff --git a/PDK/core/src/api/include/udd/spdk/crc16.h b/PDK/core/src/api/include/udd/spdk/crc16.h
new file mode 100644
index 0000000..053fbd5
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/crc16.h
@@ -0,0 +1,78 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * \file
+ * CRC-16 utility functions
+ */
+
+#ifndef SPDK_CRC16_H
+#define SPDK_CRC16_H
+
+#include "spdk/stdinc.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * T10-DIF CRC-16 polynomial
+ */
+#define SPDK_T10DIF_CRC16_POLYNOMIAL 0x8bb7u
+
+/**
+ * Calculate T10-DIF CRC-16 checksum.
+ *
+ * \param init_crc Initial CRC-16 value.
+ * \param buf Data buffer to checksum.
+ * \param len Length of buf in bytes.
+ * \return CRC-16 value.
+ */
+uint16_t spdk_crc16_t10dif(uint16_t init_crc, const void *buf, size_t len);
+
+/**
+ * Calculate T10-DIF CRC-16 checksum and copy data.
+ *
+ * \param init_crc Initial CRC-16 value.
+ * \param dst Destination data buffer for copy.
+ * \param src Source data buffer for CRC calculation and copy.
+ * \param len Length of buffer in bytes.
+ * \return CRC-16 value.
+ */
+uint16_t spdk_crc16_t10dif_copy(uint16_t init_crc, uint8_t *dst, uint8_t *src,
+				size_t len);
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* SPDK_CRC16_H */
diff --git a/PDK/core/src/api/include/udd/spdk/crc32.h b/PDK/core/src/api/include/udd/spdk/crc32.h
new file mode 100644
index 0000000..83df6fd
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/crc32.h
@@ -0,0 +1,109 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * \file
+ * CRC-32 utility functions
+ */
+
+#ifndef SPDK_CRC32_H
+#define SPDK_CRC32_H
+
+#include "spdk/stdinc.h"
+#include "spdk/config.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * IEEE CRC-32 polynomial (bit reflected)
+ */
+#define SPDK_CRC32_POLYNOMIAL_REFLECT 0xedb88320UL
+
+/**
+ * CRC-32C (Castagnoli) polynomial (bit reflected)
+ */
+#define SPDK_CRC32C_POLYNOMIAL_REFLECT 0x82f63b78UL
+
+struct spdk_crc32_table {
+	uint32_t table[256];
+};
+
+/**
+ * Initialize a CRC32 lookup table for a given polynomial.
+ *
+ * \param table Table to fill with precalculated CRC-32 data.
+ * \param polynomial_reflect Bit-reflected CRC-32 polynomial.
+ */
+void spdk_crc32_table_init(struct spdk_crc32_table *table,
+			   uint32_t polynomial_reflect);
+
+/**
+ * Calculate a partial CRC-32 checksum.
+ *
+ * \param table CRC-32 table initialized with spdk_crc32_table_init().
+ * \param buf Data buffer to checksum.
+ * \param len Length of buf in bytes.
+ * \param crc Previous CRC-32 value.
+ * \return Updated CRC-32 value.
+ */
+uint32_t spdk_crc32_update(const struct spdk_crc32_table *table,
+			   const void *buf, size_t len,
+			   uint32_t crc);
+
+/**
+ * Calculate a partial CRC-32 IEEE checksum.
+ *
+ * \param buf Data buffer to checksum.
+ * \param len Length of buf in bytes.
+ * \param crc Previous CRC-32 value.
+ * \return Updated CRC-32 value.
+ */
+uint32_t spdk_crc32_ieee_update(const void *buf, size_t len, uint32_t crc);
+
+/**
+ * Calculate a partial CRC-32C checksum.
+ *
+ * \param buf Data buffer to checksum.
+ * \param len Length of buf in bytes.
+ * \param crc Previous CRC-32C value.
+ * \return Updated CRC-32C value.
+ */
+uint32_t spdk_crc32c_update(const void *buf, size_t len, uint32_t crc);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* SPDK_CRC32_H */
diff --git a/PDK/core/src/api/include/udd/spdk/dif.h b/PDK/core/src/api/include/udd/spdk/dif.h
new file mode 100644
index 0000000..45605f4
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/dif.h
@@ -0,0 +1,303 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef SPDK_DIF_H
+#define SPDK_DIF_H
+
+#include "spdk/stdinc.h"
+#include "spdk/assert.h"
+
+#define SPDK_DIF_FLAGS_REFTAG_CHECK	(1U << 26)
+#define SPDK_DIF_FLAGS_APPTAG_CHECK	(1U << 27)
+#define SPDK_DIF_FLAGS_GUARD_CHECK	(1U << 28)
+
+#define SPDK_DIF_REFTAG_ERROR	0x1
+#define SPDK_DIF_APPTAG_ERROR	0x2
+#define SPDK_DIF_GUARD_ERROR	0x4
+#define SPDK_DIF_DATA_ERROR	0x8
+
+enum spdk_dif_type {
+	SPDK_DIF_DISABLE = 0,
+	SPDK_DIF_TYPE1 = 1,
+	SPDK_DIF_TYPE2 = 2,
+	SPDK_DIF_TYPE3 = 3,
+};
+
+enum spdk_dif_check_type {
+	SPDK_DIF_CHECK_TYPE_REFTAG = 1,
+	SPDK_DIF_CHECK_TYPE_APPTAG = 2,
+	SPDK_DIF_CHECK_TYPE_GUARD = 3,
+};
+
+struct spdk_dif {
+	uint16_t guard;
+	uint16_t app_tag;
+	uint32_t ref_tag;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_dif) == 8, "Incorrect size");
+
+/** DIF context information */
+struct spdk_dif_ctx {
+	/** Block size */
+	uint32_t		block_size;
+
+	/** Metadata size */
+	uint32_t		md_size;
+
+	/** Interval for guard computation for DIF */
+	uint32_t		guard_interval;
+
+	/** DIF type */
+	enum spdk_dif_type	dif_type;
+
+	/* Flags to specify the DIF action */
+	uint32_t		dif_flags;
+
+	/* Initial reference tag */
+	uint32_t		init_ref_tag;
+
+	/** Application tag */
+	uint16_t		app_tag;
+
+	/* Application tag mask */
+	uint16_t		apptag_mask;
+
+	/* Seed value for guard computation */
+	uint16_t		guard_seed;
+};
+
+/** DIF error information */
+struct spdk_dif_error {
+	/** Error type */
+	uint8_t		err_type;
+
+	/** Expected value */
+	uint32_t	expected;
+
+	/** Actual value */
+	uint32_t	actual;
+
+	/** Offset the error occurred at, block based */
+	uint32_t	err_offset;
+};
+
+/**
+ * Initialize DIF context.
+ *
+ * \param ctx DIF context.
+ * \param block_size Block size in a block.
+ * \param md_size Metadata size in a block.
+ * \param md_interleave If true, metadata is interleaved with block data.
+ * If false, metadata is separated with block data.
+ * \param dif_loc DIF location. If true, DIF is set in the first 8 bytes of metadata.
+ * If false, DIF is in the last 8 bytes of metadata.
+ * \param dif_type Type of DIF.
+ * \param dif_flags Flag to specify the DIF action.
+ * \param init_ref_tag Initial reference tag. For type 1, this is the
+ * starting block address.
+ * \param apptag_mask Application tag mask.
+ * \param app_tag Application tag.
+ * \param guard_seed Seed value for guard computation.
+ *
+ * \return 0 on success and negated errno otherwise.
+ */
+int spdk_dif_ctx_init(struct spdk_dif_ctx *ctx, uint32_t block_size, uint32_t md_size,
+		      bool md_interleave, bool dif_loc, enum spdk_dif_type dif_type, uint32_t dif_flags,
+		      uint32_t init_ref_tag, uint16_t apptag_mask, uint16_t app_tag,
+		      uint16_t guard_seed);
+
+/**
+ * Generate DIF for extended LBA payload.
+ *
+ * \param iovs iovec array describing the extended LBA payload.
+ * \param iovcnt Number of elements in the iovec array.
+ * \param num_blocks Number of blocks of the payload.
+ * \param ctx DIF context.
+ *
+ * \return 0 on success and negated errno otherwise.
+ */
+int spdk_dif_generate(struct iovec *iovs, int iovcnt, uint32_t num_blocks,
+		      const struct spdk_dif_ctx *ctx);
+
+/**
+ * Verify DIF for extended LBA payload.
+ *
+ * \param iovs iovec array describing the extended LBA payload.
+ * \param iovcnt Number of elements in the iovec array.
+ * \param num_blocks Number of blocks of the payload.
+ * \param ctx DIF context.
+ * \param err_blk Error information of the block in which DIF error is found.
+ *
+ * \return 0 on success and negated errno otherwise.
+ */
+int spdk_dif_verify(struct iovec *iovs, int iovcnt, uint32_t num_blocks,
+		    const struct spdk_dif_ctx *ctx, struct spdk_dif_error *err_blk);
+
+/**
+ * Copy data and generate DIF for extended LBA payload.
+ *
+ * \param iovs iovec array describing the LBA payload.
+ * \param iovcnt Number of elements in the iovec array.
+ * \param bounce_iov A contiguous buffer forming extended LBA payload.
+ * \param num_blocks Number of blocks of the LBA payload.
+ * \param ctx DIF context.
+ *
+ * \return 0 on success and negated errno otherwise.
+ */
+int spdk_dif_generate_copy(struct iovec *iovs, int iovcnt, struct iovec *bounce_iov,
+			   uint32_t num_blocks, const struct spdk_dif_ctx *ctx);
+
+/**
+ * Verify DIF and copy data for extended LBA payload.
+ *
+ * \param iovs iovec array describing the LBA payload.
+ * \param iovcnt Number of elements in the iovec array.
+ * \param bounce_iov A contiguous buffer forming extended LBA payload.
+ * \param num_blocks Number of blocks of the LBA payload.
+ * \param ctx DIF context.
+ * \param err_blk Error information of the block in which DIF error is found.
+ *
+ * \return 0 on success and negated errno otherwise.
+ */
+int spdk_dif_verify_copy(struct iovec *iovs, int iovcnt, struct iovec *bounce_iov,
+			 uint32_t num_blocks, const struct spdk_dif_ctx *ctx,
+			 struct spdk_dif_error *err_blk);
+
+/**
+ * Inject bit flip error to extended LBA payload.
+ *
+ * \param iovs iovec array describing the extended LBA payload.
+ * \param iovcnt Number of elements in the iovec array.
+ * \param num_blocks Number of blocks of the payload.
+ * \param ctx DIF context.
+ * \param inject_flags Flags to specify the action of error injection.
+ * \param inject_offset Offset, in blocks, to which error is injected.
+ * If multiple error is injected, only the last injection is stored.
+ *
+ * \return 0 on success and negated errno otherwise including no metadata.
+ */
+int spdk_dif_inject_error(struct iovec *iovs, int iovcnt, uint32_t num_blocks,
+			  const struct spdk_dif_ctx *ctx, uint32_t inject_flags,
+			  uint32_t *inject_offset);
+
+/**
+ * Generate DIF for separate metadata payload.
+ *
+ * \param iovs iovec array describing the LBA payload.
+ * \params iovcnt Number of elements in iovs.
+ * \param md_iov A contiguous buffer for metadata.
+ * \param num_blocks Number of blocks of the separate metadata payload.
+ * \param ctx DIF context.
+ *
+ * \return 0 on success and negated errno otherwise.
+ */
+int spdk_dix_generate(struct iovec *iovs, int iovcnt, struct iovec *md_iov,
+		      uint32_t num_blocks, const struct spdk_dif_ctx *ctx);
+
+/**
+ * Verify DIF for separate metadata payload.
+ *
+ * \param iovs iovec array describing the LBA payload.
+ * \params iovcnt Number of elements in iovs.
+ * \param md_iov A contiguous buffer for metadata.
+ * \param num_blocks Number of blocks of the separate metadata payload.
+ * \param ctx DIF context.
+ * \param err_blk Error information of the block in which DIF error is found.
+ *
+ * \return 0 on success and negated errno otherwise.
+ */
+int spdk_dix_verify(struct iovec *iovs, int iovcnt, struct iovec *md_iov,
+		    uint32_t num_blocks, const struct spdk_dif_ctx *ctx,
+		    struct spdk_dif_error *err_blk);
+
+/**
+ * Inject bit flip error to separate metadata payload.
+ *
+ * \param iovs iovec array describing the extended LBA payload.
+ * \param iovcnt Number of elements in the iovec array.
+ * \param md_iov A contiguous buffer for metadata.
+ * \param num_blocks Number of blocks of the payload.
+ * \param ctx DIF context.
+ * \param inject_flags Flag to specify the action of error injection.
+ * \param inject_offset Offset, in blocks, to which error is injected.
+ * If multiple error is injected, only the last injection is stored.
+ *
+ * \return 0 on success and negated errno otherwise including no metadata.
+ */
+int spdk_dix_inject_error(struct iovec *iovs, int iovcnt, struct iovec *md_iov,
+			  uint32_t num_blocks, const struct spdk_dif_ctx *ctx,
+			  uint32_t inject_flags, uint32_t *inject_offset);
+
+/**
+ * Setup iovec array to leave a space for metadata for each block.
+ *
+ * This function is used to leave a space for metadata for each block when
+ * the network socket reads data, or to make the network socket ignore a
+ * space for metadata for each block when the network socket writes data.
+ * This function removes the necessity of data copy in the SPDK application
+ * during DIF insertion and strip.
+ *
+ * \param iovs iovec array set by this function.
+ * \param num_iovs Number of elements in the iovec array.
+ * \param buf Buffer to create extended LBA payload.
+ * \param buf_len Length of the buffer to create extended LBA payload.
+ * \param data_offset Offset to store the next incoming data.
+ * \param data_len Expected data length of the payload.
+ * \param mapped_len Output parameter that will contain data length mapped by
+ * the iovec array.
+ * \param ctx DIF context.
+ *
+ * \return Number of used elements in the iovec array on success or negated
+ * errno otherwise.
+ */
+int spdk_dif_set_md_interleave_iovs(struct iovec *iovs, int num_iovs,
+				    uint8_t *buf, uint32_t buf_len,
+				    uint32_t data_offset, uint32_t data_len,
+				    uint32_t *mapped_len,
+				    const struct spdk_dif_ctx *ctx);
+
+/**
+ * Generate and insert DIF into metadata space for newly read data block.
+ *
+ * \param buf Buffer to create extended LBA payload.
+ * \param buf_len Length of the buffer to create extended LBA payload.
+ * \param offset Offset to the newly read data.
+ * \param read_len Length of the newly read data.
+ * \param ctx DIF context.
+ *
+ * \return 0 on success and negated errno otherwise.
+ */
+int spdk_dif_generate_stream(uint8_t *buf, uint32_t buf_len,
+			     uint32_t offset, uint32_t read_len,
+			     const struct spdk_dif_ctx *ctx);
+#endif /* SPDK_DIF_H */
diff --git a/PDK/core/src/api/include/udd/spdk/env.h b/PDK/core/src/api/include/udd/spdk/env.h
index 83a1968..8273f1e 100644
--- a/PDK/core/src/api/include/udd/spdk/env.h
+++ b/PDK/core/src/api/include/udd/spdk/env.h
@@ -40,160 +40,436 @@
 #define SPDK_ENV_H
 
 #include "spdk/stdinc.h"
+#include "spdk/queue.h"
 
 #ifdef __cplusplus
 extern "C" {
 #endif
 
 #define SPDK_ENV_SOCKET_ID_ANY	(-1)
+#define SPDK_ENV_LCORE_ID_ANY	(UINT32_MAX)
 
-struct spdk_pci_device;
+/**
+ * Memory is dma-safe.
+ */
+#define SPDK_MALLOC_DMA    0x01
+
+/**
+ * Memory is sharable across process boundries.
+ */
+#define SPDK_MALLOC_SHARE  0x02
+
+#define SPDK_MAX_MEMZONE_NAME_LEN 32
+#define SPDK_MAX_MEMPOOL_NAME_LEN 29
+
+/**
+ * Memzone flags
+ */
+#define SPDK_MEMZONE_NO_IOVA_CONTIG 0x00100000 /**< no iova contiguity */
 
 /**
  * \brief Environment initialization options
  */
 struct spdk_env_opts {
-	const char 		*name;
-	const char 		*core_mask;
-	int 			shm_id;
-	int	 		dpdk_mem_channel;
-	int	 		dpdk_master_core;
-	int			dpdk_mem_size;
+	const char		*name;
+	const char		*core_mask;
+	int			shm_id;
+	int			mem_channel;
+	int			master_core;
+	int			mem_size;
+	bool			no_pci;
+	bool			hugepage_single_segments;
+	bool			unlink_hugepage;
+	size_t			num_pci_addr;
+	const char		*hugedir;
+	struct spdk_pci_addr	*pci_blacklist;
+	struct spdk_pci_addr	*pci_whitelist;
+
+	/** Opaque context for use of the env implementation. */
+	void			*env_context;
 };
 
 /**
- * \brief Initialize the default value of opts
+ * Allocate dma/sharable memory based on a given dma_flg. It is a memory buffer
+ * with the given size, alignment and socket id.
+ *
+ * \param size Size in bytes.
+ * \param align Alignment value for the allocated memory. If '0', the allocated
+ * buffer is suitably aligned (in the same manner as malloc()). Otherwise, the
+ * allocated buffer is aligned to the multiple of align. In this case, it must
+ * be a power of two.
+ * \param phys_addr **Deprecated**. Please use spdk_vtophys() for retrieving physical
+ * addresses. A pointer to the variable to hold the physical address of
+ * the allocated buffer is passed. If NULL, the physical address is not returned.
+ * \param socket_id Socket ID to allocate memory on, or SPDK_ENV_SOCKET_ID_ANY
+ * for any socket.
+ * \param flags Combination of SPDK_MALLOC flags (\ref SPDK_MALLOC_DMA, \ref SPDK_MALLOC_SHARE).
+ * At least one flag must be specified.
+ *
+ * \return a pointer to the allocated memory buffer.
+ */
+void *spdk_malloc(size_t size, size_t align, uint64_t *phys_addr, int socket_id, uint32_t flags);
+
+/**
+ * Allocate dma/sharable memory based on a given dma_flg. It is a memory buffer
+ * with the given size, alignment and socket id. Also, the buffer will be zeroed.
+ *
+ * \param size Size in bytes.
+ * \param align Alignment value for the allocated memory. If '0', the allocated
+ * buffer is suitably aligned (in the same manner as malloc()). Otherwise, the
+ * allocated buffer is aligned to the multiple of align. In this case, it must
+ * be a power of two.
+ * \param phys_addr **Deprecated**. Please use spdk_vtophys() for retrieving physical
+ * addresses. A pointer to the variable to hold the physical address of
+ * the allocated buffer is passed. If NULL, the physical address is not returned.
+ * \param socket_id Socket ID to allocate memory on, or SPDK_ENV_SOCKET_ID_ANY
+ * for any socket.
+ * \param flags Combination of SPDK_MALLOC flags (\ref SPDK_MALLOC_DMA, \ref SPDK_MALLOC_SHARE).
+ *
+ * \return a pointer to the allocated memory buffer.
+ */
+void *spdk_zmalloc(size_t size, size_t align, uint64_t *phys_addr, int socket_id, uint32_t flags);
+
+/**
+ * Resize a dma/sharable memory buffer with the given new size and alignment.
+ * Existing contents are preserved.
+ *
+ * \param buf Buffer to resize.
+ * \param size Size in bytes.
+ * \param align Alignment value for the allocated memory. If '0', the allocated
+ * buffer is suitably aligned (in the same manner as malloc()). Otherwise, the
+ * allocated buffer is aligned to the multiple of align. In this case, it must
+ * be a power of two.
+ *
+ * \return a pointer to the resized memory buffer.
+ */
+void *spdk_realloc(void *buf, size_t size, size_t align);
+
+/**
+ * Free buffer memory that was previously allocated with spdk_malloc() or spdk_zmalloc().
+ *
+ * \param buf Buffer to free.
+ */
+void spdk_free(void *buf);
+
+/**
+ * Initialize the default value of opts.
+ *
+ * \param opts Data structure where SPDK will initialize the default options.
  */
 void spdk_env_opts_init(struct spdk_env_opts *opts);
 
 /**
- * \brief Initialize the environment library. This must be called prior to using
+ * Initialize the environment library. This must be called prior to using
  * any other functions in this library.
+ *
+ * \param opts Environment initialization options.
+ * \return 0 on success, or negative errno on failure.
  */
-void spdk_env_init(const struct spdk_env_opts *opts);
+int spdk_env_init(const struct spdk_env_opts *opts);
 
 /**
- * Allocate a pinned, physically contiguous memory buffer with the
- *   given size and alignment.
+ * Release any resources of the environment library that were alllocated with
+ * spdk_env_init(). After this call, no SPDK env function calls may be made.
+ * It is expected that common usage of this function is to call it just before
+ * terminating the process.
  */
-void *spdk_malloc(size_t size, size_t align, uint64_t *phys_addr);
+void spdk_env_fini(void);
 
 /**
- * Allocate a pinned, physically contiguous memory buffer with the
- *   given size, alignment and socket id.
+ * Allocate a pinned memory buffer with the given size and alignment.
+ *
+ * \param size Size in bytes.
+ * \param align Alignment value for the allocated memory. If '0', the allocated
+ * buffer is suitably aligned (in the same manner as malloc()). Otherwise, the
+ * allocated buffer is aligned to the multiple of align. In this case, it must
+ * be a power of two.
+ * \param phys_addr A pointer to the variable to hold the physical address of
+ * the allocated buffer is passed. If NULL, the physical address is not returned.
+ *
+ * \return a pointer to the allocated memory buffer.
+ */
+void *spdk_dma_malloc(size_t size, size_t align, uint64_t *phys_addr);
+
+/**
+ * Allocate a pinned, memory buffer with the given size, alignment and socket id.
+ *
+ * \param size Size in bytes.
+ * \param align Alignment value for the allocated memory. If '0', the allocated
+ * buffer is suitably aligned (in the same manner as malloc()). Otherwise, the
+ * allocated buffer is aligned to the multiple of align. In this case, it must
+ * be a power of two.
+ * \param phys_addr A pointer to the variable to hold the physical address of
+ * the allocated buffer is passed. If NULL, the physical address is not returned.
+ * \param socket_id Socket ID to allocate memory on, or SPDK_ENV_SOCKET_ID_ANY
+ * for any socket.
+ *
+ * \return a pointer to the allocated memory buffer.
  */
-void *spdk_malloc_socket(size_t size, size_t align, uint64_t *phys_addr, int socket_id);
+void *spdk_dma_malloc_socket(size_t size, size_t align, uint64_t *phys_addr, int socket_id);
 
 /**
- * Allocate a pinned, physically contiguous memory buffer with the
- *   given size and alignment. The buffer will be zeroed.
+ * Allocate a pinned memory buffer with the given size and alignment. The buffer
+ * will be zeroed.
+ *
+ * \param size Size in bytes.
+ * \param align Alignment value for the allocated memory. If '0', the allocated
+ * buffer is suitably aligned (in the same manner as malloc()). Otherwise, the
+ * allocated buffer is aligned to the multiple of align. In this case, it must
+ * be a power of two.
+ * \param phys_addr A pointer to the variable to hold the physical address of
+ * the allocated buffer is passed. If NULL, the physical address is not returned.
+ *
+ * \return a pointer to the allocated memory buffer.
  */
-void *spdk_zmalloc(size_t size, size_t align, uint64_t *phys_addr);
+void *spdk_dma_zmalloc(size_t size, size_t align, uint64_t *phys_addr);
 
 /**
- * Allocate a pinned, physically contiguous memory buffer with the
- *   given size, alignment and socket id. The buffer will be zeroed.
+ * Allocate a pinned memory buffer with the given size, alignment and socket id.
+ * The buffer will be zeroed.
+ *
+ * \param size Size in bytes.
+ * \param align Alignment value for the allocated memory. If '0', the allocated
+ * buffer is suitably aligned (in the same manner as malloc()). Otherwise, the
+ * allocated buffer is aligned to the multiple of align. In this case, it must
+ * be a power of two.
+ * \param phys_addr A pointer to the variable to hold the physical address of
+ * the allocated buffer is passed. If NULL, the physical address is not returned.
+ * \param socket_id Socket ID to allocate memory on, or SPDK_ENV_SOCKET_ID_ANY
+ * for any socket.
+ *
+ * \return a pointer to the allocated memory buffer.
  */
-void *spdk_zmalloc_socket(size_t size, size_t align, uint64_t *phys_addr, int socket_id);
+void *spdk_dma_zmalloc_socket(size_t size, size_t align, uint64_t *phys_addr, int socket_id);
 
 /**
- * Resize the allocated and pinned memory buffer with the given
- *   new size and alignment. Existing contents are preserved.
+ * Resize the allocated and pinned memory buffer with the given new size and
+ * alignment. Existing contents are preserved.
+ *
+ * \param buf Buffer to resize.
+ * \param size Size in bytes.
+ * \param align Alignment value for the allocated memory. If '0', the allocated
+ * buffer is suitably aligned (in the same manner as malloc()). Otherwise, the
+ * allocated buffer is aligned to the multiple of align. In this case, it must
+ * be a power of two.
+ * \param phys_addr A pointer to the variable to hold the physical address of
+ * the allocated buffer is passed. If NULL, the physical address is not returned.
+ *
+ * \return a pointer to the resized memory buffer.
  */
-void *spdk_realloc(void *buf, size_t size, size_t align, uint64_t *phys_addr);
+void *spdk_dma_realloc(void *buf, size_t size, size_t align, uint64_t *phys_addr);
 
 /**
- * Free a memory buffer previously allocated with spdk_zmalloc.
- *   This call is never made from the performance path.
+ * Free a memory buffer previously allocated, for example from spdk_dma_zmalloc().
+ * This call is never made from the performance path.
+ *
+ * \param buf Buffer to free.
  */
-void spdk_free(void *buf);
+void spdk_dma_free(void *buf);
 
 /**
- * Reserve a named, process shared memory zone with the given size,
- *   socket_id and flags.
- * Return a pointer to the allocated memory address. If the allocation
- *   cannot be done, return NULL.
- * Note: to pick any socket id, just set socket_id to SPDK_ENV_SOCKET_ID_ANY.
+ * Reserve a named, process shared memory zone with the given size, socket_id
+ * and flags. Unless `SPDK_MEMZONE_NO_IOVA_CONTIG` flag is provided, the returned
+ * memory will be IOVA contiguous.
+ *
+ * \param name Name to set for this memory zone.
+ * \param len Length in bytes.
+ * \param socket_id Socket ID to allocate memory on, or SPDK_ENV_SOCKET_ID_ANY
+ * for any socket.
+ * \param flags Flags to set for this memory zone.
+ *
+ * \return a pointer to the allocated memory address on success, or NULL on failure.
  */
 void *spdk_memzone_reserve(const char *name, size_t len, int socket_id, unsigned flags);
 
 /**
+ * Reserve a named, process shared memory zone with the given size, socket_id,
+ * flags and alignment. Unless `SPDK_MEMZONE_NO_IOVA_CONTIG` flag is provided,
+ * the returned memory will be IOVA contiguous.
+ *
+ * \param name Name to set for this memory zone.
+ * \param len Length in bytes.
+ * \param socket_id Socket ID to allocate memory on, or SPDK_ENV_SOCKET_ID_ANY
+ * for any socket.
+ * \param flags Flags to set for this memory zone.
+ * \param align Alignment for resulting memzone. Must be a power of 2.
+ *
+ * \return a pointer to the allocated memory address on success, or NULL on failure.
+ */
+void *spdk_memzone_reserve_aligned(const char *name, size_t len, int socket_id,
+				   unsigned flags, unsigned align);
+
+/**
  * Lookup the memory zone identified by the given name.
- * Return a pointer to the reserved memory address. If the reservation
- *   cannot be found, return NULL.
+ *
+ * \param name Name of the memory zone.
+ *
+ * \return a pointer to the reserved memory address on success, or NULL on failure.
  */
 void *spdk_memzone_lookup(const char *name);
 
 /**
  * Free the memory zone identified by the given name.
+ *
+ * \return 0 on success, -1 on failure.
  */
 int spdk_memzone_free(const char *name);
 
 /**
  * Dump debug information about all memzones.
+ *
+ * \param f File to write debug information to.
  */
 void spdk_memzone_dump(FILE *f);
 
 struct spdk_mempool;
 
+#define SPDK_MEMPOOL_DEFAULT_CACHE_SIZE	SIZE_MAX
+
 /**
- * Create a thread-safe memory pool. Cache size is the number of
- * elements in a thread-local cache. Can be 0 for no caching, or -1
- * for unspecified.
+ * Create a thread-safe memory pool.
+ *
+ * \param name Name for the memory pool.
+ * \param count Count of elements.
+ * \param ele_size Element size in bytes.
+ * \param cache_size How many elements may be cached in per-core caches. Use
+ * SPDK_MEMPOOL_DEFAULT_CACHE_SIZE for a reasonable default, or 0 for no per-core cache.
+ * \param socket_id Socket ID to allocate memory on, or SPDK_ENV_SOCKET_ID_ANY
+ * for any socket.
  *
- * \param socket_id Socket ID to allocate memory on, or SPDK_ENV_SOCKET_ID_ANY for any socket.
+ * \return a pointer to the created memory pool.
  */
 struct spdk_mempool *spdk_mempool_create(const char *name, size_t count,
 		size_t ele_size, size_t cache_size, int socket_id);
 
 /**
+ * An object callback function for memory pool.
+ *
+ * Used by spdk_mempool_create_ctor().
+ */
+typedef void (spdk_mempool_obj_cb_t)(struct spdk_mempool *mp,
+				     void *opaque, void *obj, unsigned obj_idx);
+
+/**
+ * Create a thread-safe memory pool with user provided initialization function
+ * and argument.
+ *
+ * \param name Name for the memory pool.
+ * \param count Count of elements.
+ * \param ele_size Element size in bytes.
+ * \param cache_size How many elements may be cached in per-core caches. Use
+ * SPDK_MEMPOOL_DEFAULT_CACHE_SIZE for a reasonable default, or 0 for no per-core cache.
+ * \param socket_id Socket ID to allocate memory on, or SPDK_ENV_SOCKET_ID_ANY
+ * for any socket.
+ * \param obj_init User provided object calllback initialization function.
+ * \param obj_init_arg User provided callback initialization function argument.
+ *
+ * \return a pointer to the created memory pool.
+ */
+struct spdk_mempool *spdk_mempool_create_ctor(const char *name, size_t count,
+		size_t ele_size, size_t cache_size, int socket_id,
+		spdk_mempool_obj_cb_t *obj_init, void *obj_init_arg);
+
+/**
+ * Get the name of a memory pool.
+ *
+ * \param mp Memory pool to query.
+ *
+ * \return the name of the memory pool.
+ */
+char *spdk_mempool_get_name(struct spdk_mempool *mp);
+
+/**
  * Free a memory pool.
  */
 void spdk_mempool_free(struct spdk_mempool *mp);
 
 /**
  * Get an element from a memory pool. If no elements remain, return NULL.
+ *
+ * \param mp Memory pool to query.
+ *
+ * \return a pointer to the element.
  */
 void *spdk_mempool_get(struct spdk_mempool *mp);
 
 /**
+ * Get multiple elements from a memory pool.
+ *
+ * \param mp Memory pool to get multiple elements from.
+ * \param ele_arr Array of the elements to fill.
+ * \param count Count of elements to get.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_mempool_get_bulk(struct spdk_mempool *mp, void **ele_arr, size_t count);
+
+/**
  * Put an element back into the memory pool.
+ *
+ * \param mp Memory pool to put element back into.
+ * \param ele Element to put.
  */
 void spdk_mempool_put(struct spdk_mempool *mp, void *ele);
 
 /**
  * Put multiple elements back into the memory pool.
+ *
+ * \param mp Memory pool to put multiple elements back into.
+ * \param ele_arr Array of the elements to put.
+ * \param count Count of elements to put.
  */
-void spdk_mempool_put_bulk(struct spdk_mempool *mp, void *const *ele_arr, size_t count);
+void spdk_mempool_put_bulk(struct spdk_mempool *mp, void **ele_arr, size_t count);
 
 /**
- * Return the number of entries in the mempool.
+ * Get the number of entries in the memory pool.
+ *
+ * \param pool Memory pool to query.
+ *
+ * \return the number of entries in the memory pool.
  */
 size_t spdk_mempool_count(const struct spdk_mempool *pool);
 
 /**
- * \brief Return the number of dedicated CPU cores utilized by
- * 	  this env abstraction
+ * Get the number of dedicated CPU cores utilized by this env abstraction.
+ *
+ * \return the number of dedicated CPU cores.
  */
 uint32_t spdk_env_get_core_count(void);
 
 /**
- * \brief Return the CPU core index of the current thread. This
- *	  will only function when called from threads set up by
- *	  this environment abstraction.
+ * Get the CPU core index of the current thread.
+ *
+ * This will only function when called from threads set up by
+ * this environment abstraction. For any other threads \c SPDK_ENV_LCORE_ID_ANY
+ * will be returned.
+ *
+ * \return the CPU core index of the current thread.
  */
 uint32_t spdk_env_get_current_core(void);
 
 /**
- * \brief Return the index of the first dedicated CPU core for
- *	  this application.
+ * Get the index of the first dedicated CPU core for this application.
+ *
+ * \return the index of the first dedicated CPU core.
  */
 uint32_t spdk_env_get_first_core(void);
 
 /**
- * \brief Return the index of the next dedicated CPU core for
- *	  this application.
- *        If there is no next core, return UINT32_MAX.
+ * Get the index of the last dedicated CPU core for this application.
+ *
+ * \return the index of the last dedicated CPU core.
+ */
+uint32_t spdk_env_get_last_core(void);
+
+/**
+ * Get the index of the next dedicated CPU core for this application.
+ *
+ * If there is no next core, return UINT32_MAX.
+ *
+ * \param prev_core Index of previous core.
+ *
+ * \return the index of the next dedicated CPU core.
  */
 uint32_t spdk_env_get_next_core(uint32_t prev_core);
 
@@ -203,67 +479,149 @@ uint32_t spdk_env_get_next_core(uint32_t prev_core);
 	     i = spdk_env_get_next_core(i))
 
 /**
- * \brief Return the socket ID for the given core.
+ * Get the socket ID for the given core.
+ *
+ * \param core CPU core to query.
+ *
+ * \return the socket ID for the given core.
  */
 uint32_t spdk_env_get_socket_id(uint32_t core);
 
+typedef int (*thread_start_fn)(void *);
+
+/**
+ * Launch a thread pinned to the given core. Only a single pinned thread may be
+ * launched per core. Subsequent attempts to launch pinned threads on that core
+ * will fail.
+ *
+ * \param core The core to pin the thread to.
+ * \param fn Entry point on the new thread.
+ * \param arg Argument apssed to thread_start_fn
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_env_thread_launch_pinned(uint32_t core, thread_start_fn fn, void *arg);
+
 /**
- * Return true if the calling process is primary process
+ * Wait for all threads to exit before returning.
+ */
+void spdk_env_thread_wait_all(void);
+
+/**
+ * Check whether the calling process is primary process.
+ *
+ * \return true if the calling process is primary process, or false otherwise.
  */
 bool spdk_process_is_primary(void);
 
 /**
  * Get a monotonic timestamp counter.
+ *
+ * \return the monotonic timestamp counter.
  */
 uint64_t spdk_get_ticks(void);
 
 /**
  * Get the tick rate of spdk_get_ticks() per second.
+ *
+ * \return the tick rate of spdk_get_ticks() per second.
  */
 uint64_t spdk_get_ticks_hz(void);
 
 /**
- * Delay the given number of microseconds
+ * Delay the given number of microseconds.
+ *
+ * \param us Number of microseconds.
  */
 void spdk_delay_us(unsigned int us);
 
+/**
+ * Pause CPU execution for a short while
+ */
+void spdk_pause(void);
+
 struct spdk_ring;
 
 enum spdk_ring_type {
+	SPDK_RING_TYPE_SP_SC,		/* Single-producer, single-consumer */
 	SPDK_RING_TYPE_MP_SC,		/* Multi-producer, single-consumer */
+	SPDK_RING_TYPE_MP_MC,		/* Multi-producer, multi-consumer */
 };
 
 /**
- * Create a ring
+ * Create a ring.
+ *
+ * \param type Type for the ring. (SPDK_RING_TYPE_SP_SC or SPDK_RING_TYPE_MP_SC).
+ * \param count Size of the ring in elements.
+ * \param socket_id Socket ID to allocate memory on, or SPDK_ENV_SOCKET_ID_ANY
+ * for any socket.
+ *
+ * \return a pointer to the created ring.
  */
-struct spdk_ring *spdk_ring_create(enum spdk_ring_type type, size_t count,
-				   size_t ele_size, int socket_id);
+struct spdk_ring *spdk_ring_create(enum spdk_ring_type type, size_t count, int socket_id);
 
 /**
- * Free the ring
+ * Free the ring.
+ *
+ * \param ring Ring to free.
  */
 void spdk_ring_free(struct spdk_ring *ring);
 
 /**
+ * Get the number of objects in the ring.
+ *
+ * \param ring the ring.
+ *
+ * \return the number of objects in the ring.
+ */
+size_t spdk_ring_count(struct spdk_ring *ring);
+
+/**
  * Queue the array of objects (with length count) on the ring.
  *
- * Return the number of objects enqueued.
+ * \param ring A pointer to the ring.
+ * \param objs A pointer to the array to be queued.
+ * \param count Length count of the array of objects.
+ *
+ * \return the number of objects enqueued.
  */
 size_t spdk_ring_enqueue(struct spdk_ring *ring, void **objs, size_t count);
 
 /**
  * Dequeue count objects from the ring into the array objs.
  *
- * Return the number of objects dequeued.
+ * \param ring A pointer to the ring.
+ * \param objs A pointer to the array to be dequeued.
+ * \param count Maximum number of elements to be dequeued.
+ *
+ * \return the number of objects dequeued which is less than 'count'.
  */
 size_t spdk_ring_dequeue(struct spdk_ring *ring, void **objs, size_t count);
 
+/**
+ * Reports whether the SPDK application is using the IOMMU for DMA
+ *
+ * \return True if we are using the IOMMU, false otherwise.
+ */
+bool spdk_iommu_is_enabled(void);
+
 #define SPDK_VTOPHYS_ERROR	(0xFFFFFFFFFFFFFFFFULL)
 
-uint64_t spdk_vtophys(void *buf);
+/**
+ * Get the physical address of a buffer.
+ *
+ * \param buf A pointer to a buffer.
+ * \param size Contains the size of the memory region pointed to by vaddr.
+ * If vaddr is successfully translated, then this is updated with the size of
+ * the memory region for which the translation is valid.
+ *
+ * \return the physical address of this buffer on success, or SPDK_VTOPHYS_ERROR
+ * on failure.
+ */
+uint64_t spdk_vtophys(void *buf, uint64_t *size);
 
 struct spdk_pci_addr {
-	uint16_t			domain;
+	uint32_t			domain;
 	uint8_t				bus;
 	uint8_t				dev;
 	uint8_t				func;
@@ -276,59 +634,369 @@ struct spdk_pci_id {
 	uint16_t	subdevice_id;
 };
 
+struct spdk_pci_device {
+	void				*dev_handle;
+	struct spdk_pci_addr		addr;
+	struct spdk_pci_id		id;
+	int				socket_id;
+
+	int (*map_bar)(struct spdk_pci_device *dev, uint32_t bar,
+		       void **mapped_addr, uint64_t *phys_addr, uint64_t *size);
+	int (*unmap_bar)(struct spdk_pci_device *dev, uint32_t bar,
+			 void *addr);
+	int (*cfg_read)(struct spdk_pci_device *dev, void *value,
+			uint32_t len, uint32_t offset);
+	int (*cfg_write)(struct spdk_pci_device *dev, void *value,
+			 uint32_t len, uint32_t offset);
+	void (*detach)(struct spdk_pci_device *dev);
+
+	struct _spdk_pci_device_internal {
+		struct spdk_pci_driver		*driver;
+		bool				attached;
+		bool				pending_removal;
+		TAILQ_ENTRY(spdk_pci_device)	tailq;
+	} internal;
+};
+
 typedef int (*spdk_pci_enum_cb)(void *enum_ctx, struct spdk_pci_device *pci_dev);
 
-int spdk_pci_nvme_enumerate(spdk_pci_enum_cb enum_cb, void *enum_ctx);
-int spdk_pci_ioat_enumerate(spdk_pci_enum_cb enum_cb, void *enum_ctx);
+/**
+ * Get the NVMe PCI driver object.
+ *
+ * \return PCI driver.
+ */
+struct spdk_pci_driver *spdk_pci_nvme_get_driver(void);
+
+/**
+ * Get the I/OAT PCI driver object.
+ *
+ * \return PCI driver.
+ */
+struct spdk_pci_driver *spdk_pci_ioat_get_driver(void);
+
+/**
+ * Get the Virtio PCI driver object.
+ *
+ * \return PCI driver.
+ */
+struct spdk_pci_driver *spdk_pci_virtio_get_driver(void);
 
-struct spdk_pci_device *spdk_pci_get_device(struct spdk_pci_addr *pci_addr);
+/**
+ * Enumerate all PCI devices supported by the provided driver and try to
+ * attach those that weren't attached yet. The provided callback will be
+ * called for each such device and its return code will decide whether that
+ * device is attached or not. Attached devices have to be manually detached
+ * with spdk_pci_device_detach() to be attach-able again.
+ *
+ * \param driver Driver for a specific device type.
+ * \param enum_cb Callback to be called for each non-attached PCI device.
+ * The return code can be as follows:
+ *  -1 - device was not attached, the enumeration is stopped
+ *   0 - device attached successfully, enumeration continues
+ *   1 - device was not attached, enumeration continues
+ * \param enum_ctx Additional context passed to the callback function.
+ *
+ * \return -1 if an internal error occured or the provided callback returned -1,
+ *         0 otherwise
+ */
+int spdk_pci_enumerate(struct spdk_pci_driver *driver, spdk_pci_enum_cb enum_cb, void *enum_ctx);
 
+/**
+ * Map a PCI BAR in the current process.
+ *
+ * \param dev PCI device.
+ * \param bar BAR number.
+ * \param mapped_addr A variable to store the virtual address of the mapping.
+ * \param phys_addr A variable to store the physical address of the mapping.
+ * \param size A variable to store the size of the bar (in bytes).
+ *
+ * \return 0 on success.
+ */
 int spdk_pci_device_map_bar(struct spdk_pci_device *dev, uint32_t bar,
 			    void **mapped_addr, uint64_t *phys_addr, uint64_t *size);
-int spdk_pci_device_unmap_bar(struct spdk_pci_device *dev, uint32_t bar, void *addr);
 
-uint16_t spdk_pci_device_get_domain(struct spdk_pci_device *dev);
+/**
+ * Unmap a PCI BAR from the current process. This happens automatically when
+ * the PCI device is detached.
+ *
+ * \param dev PCI device.
+ * \param bar BAR number.
+ * \param mapped_addr Virtual address of the bar.
+ *
+ * \return 0 on success.
+ */
+int spdk_pci_device_unmap_bar(struct spdk_pci_device *dev, uint32_t bar,
+			      void *mapped_addr);
+
+/**
+ * Get the domain of a PCI device.
+ *
+ * \param dev PCI device.
+ *
+ * \return PCI device domain.
+ */
+uint32_t spdk_pci_device_get_domain(struct spdk_pci_device *dev);
+
+/**
+ * Get the bus number of a PCI device.
+ *
+ * \param dev PCI device.
+ *
+ * \return PCI bus number.
+ */
 uint8_t spdk_pci_device_get_bus(struct spdk_pci_device *dev);
+
+/**
+ * Get the device number within the PCI bus the device is on.
+ *
+ * \param dev PCI device.
+ *
+ * \return PCI device number.
+ */
 uint8_t spdk_pci_device_get_dev(struct spdk_pci_device *dev);
+
+/**
+ * Get the particular function number represented by struct spdk_pci_device.
+ *
+ * \param dev PCI device.
+ *
+ * \return PCI function number.
+ */
 uint8_t spdk_pci_device_get_func(struct spdk_pci_device *dev);
 
+/**
+ * Get the full DomainBDF address of a PCI device.
+ *
+ * \param dev PCI device.
+ *
+ * \return PCI address.
+ */
 struct spdk_pci_addr spdk_pci_device_get_addr(struct spdk_pci_device *dev);
 
+/**
+ * Get the vendor ID of a PCI device.
+ *
+ * \param dev PCI device.
+ *
+ * \return vendor ID.
+ */
 uint16_t spdk_pci_device_get_vendor_id(struct spdk_pci_device *dev);
+
+/**
+ * Get the device ID of a PCI device.
+ *
+ * \param dev PCI device.
+ *
+ * \return device ID.
+ */
 uint16_t spdk_pci_device_get_device_id(struct spdk_pci_device *dev);
+
+/**
+ * Get the subvendor ID of a PCI device.
+ *
+ * \param dev PCI device.
+ *
+ * \return subvendor ID.
+ */
 uint16_t spdk_pci_device_get_subvendor_id(struct spdk_pci_device *dev);
+
+/**
+ * Get the subdevice ID of a PCI device.
+ *
+ * \param dev PCI device.
+ *
+ * \return subdevice ID.
+ */
 uint16_t spdk_pci_device_get_subdevice_id(struct spdk_pci_device *dev);
 
+/**
+ * Get the PCI ID of a PCI device.
+ *
+ * \param dev PCI device.
+ *
+ * \return PCI ID.
+ */
 struct spdk_pci_id spdk_pci_device_get_id(struct spdk_pci_device *dev);
 
 /**
- * Get the NUMA socket ID of a PCI device.
+ * Get the NUMA node the PCI device is on.
  *
- * \param dev PCI device to get the socket ID of.
+ * \param dev PCI device.
  *
- * \return Socket ID (>= 0), or negative if unknown.
+ * \return NUMA node index (>= 0).
  */
 int spdk_pci_device_get_socket_id(struct spdk_pci_device *dev);
 
+/**
+ * Serialize the PCIe Device Serial Number into the provided buffer.
+ * The buffer will contain a 16-character-long serial number followed by
+ * a NULL terminator.
+ *
+ * \param dev PCI device.
+ * \param sn Buffer to store the serial number in.
+ * \param len Length of buffer. Must be at least 17.
+ *
+ * \return 0 on success, -1 on failure.
+ */
 int spdk_pci_device_get_serial_number(struct spdk_pci_device *dev, char *sn, size_t len);
+
+/**
+ * Claim a PCI device for exclusive SPDK userspace access.
+ *
+ * Uses F_SETLK on a shared memory file with the PCI address embedded in its name.
+ * As long as this file remains open with the lock acquired, other processes will
+ * not be able to successfully call this function on the same PCI device.
+ *
+ * \param pci_addr PCI address of the device to claim
+ *
+ * \return -1 if the device has already been claimed, an fd otherwise. This fd
+ * should be closed when the application no longer needs access to the PCI device
+ * (including when it is hot removed).
+ */
 int spdk_pci_device_claim(const struct spdk_pci_addr *pci_addr);
+
+/**
+ * Release all resources associated with the given device and detach it. As long
+ * as the PCI device is physically available, it will attachable again.
+ *
+ * \param device PCI device.
+ */
 void spdk_pci_device_detach(struct spdk_pci_device *device);
 
-int spdk_pci_nvme_device_attach(spdk_pci_enum_cb enum_cb, void *enum_ctx,
-				struct spdk_pci_addr *pci_address);
-int spdk_pci_ioat_device_attach(spdk_pci_enum_cb enum_cb, void *enum_ctx,
-				struct spdk_pci_addr *pci_address);
+/**
+ * Attach a PCI device. This will bypass all blacklist rules and explicitly
+ * attach a device at the provided address. The return code of the provided
+ * callback will decide whether that device is attached or not. Attached
+ * devices have to be manually detached with spdk_pci_device_detach() to be
+ * attach-able again.
+ *
+ * \param driver Driver for a specific device type. The device will only be
+ * attached if it's supported by this driver.
+ * \param enum_cb Callback to be called for the PCI device once it's found.
+ * The return code can be as follows:
+ *  -1, 1 - an error occured, fail the attach request entirely
+ *   0 - device attached successfully
+ * \param enum_ctx Additional context passed to the callback function.
+ * \param pci_address Address of the device to attach.
+ *
+ * \return -1 if a device at the provided PCI address couldn't be found,
+ *         -1 if an internal error happened or the provided callback returned non-zero,
+ *         0 otherwise
+ */
+int spdk_pci_device_attach(struct spdk_pci_driver *driver, spdk_pci_enum_cb enum_cb,
+			   void *enum_ctx, struct spdk_pci_addr *pci_address);
+
+/**
+ * Read \c len bytes from the PCI configuration space.
+ *
+ * \param dev PCI device.
+ * \param buf A buffer to copy the data into.
+ * \param len Number of bytes to read.
+ * \param offset Offset (in bytes) in the PCI config space to start reading from.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_pci_device_cfg_read(struct spdk_pci_device *dev, void *buf, uint32_t len,
+			     uint32_t offset);
+
+/**
+ * Write \c len bytes into the PCI configuration space.
+ *
+ * \param dev PCI device.
+ * \param buf A buffer to copy the data from.
+ * \param len Number of bytes to write.
+ * \param offset Offset (in bytes) in the PCI config space to start writing to.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_pci_device_cfg_write(struct spdk_pci_device *dev, void *buf, uint32_t len,
+			      uint32_t offset);
 
+/**
+ * Read 1 byte from the PCI configuration space.
+ *
+ * \param dev PCI device.
+ * \param value A buffer to copy the data into.
+ * \param offset Offset (in bytes) in the PCI config space to start reading from.
+ *
+ * \return 0 on success, -1 on failure.
+ */
 int spdk_pci_device_cfg_read8(struct spdk_pci_device *dev, uint8_t *value, uint32_t offset);
+
+/**
+ * Write 1 byte into the PCI configuration space.
+ *
+ * \param dev PCI device.
+ * \param value A value to write.
+ * \param offset Offset (in bytes) in the PCI config space to start writing to.
+ *
+ * \return 0 on success, -1 on failure.
+ */
 int spdk_pci_device_cfg_write8(struct spdk_pci_device *dev, uint8_t value, uint32_t offset);
+
+/**
+ * Read 2 bytes from the PCI configuration space.
+ *
+ * \param dev PCI device.
+ * \param value A buffer to copy the data into.
+ * \param offset Offset (in bytes) in the PCI config space to start reading from.
+ *
+ * \return 0 on success, -1 on failure.
+ */
 int spdk_pci_device_cfg_read16(struct spdk_pci_device *dev, uint16_t *value, uint32_t offset);
+
+/**
+ * Write 2 bytes into the PCI configuration space.
+ *
+ * \param dev PCI device.
+ * \param value A value to write.
+ * \param offset Offset (in bytes) in the PCI config space to start writing to.
+ *
+ * \return 0 on success, -1 on failure.
+ */
 int spdk_pci_device_cfg_write16(struct spdk_pci_device *dev, uint16_t value, uint32_t offset);
+
+/**
+ * Read 4 bytes from the PCI configuration space.
+ *
+ * \param dev PCI device.
+ * \param value A buffer to copy the data into.
+ * \param offset Offset (in bytes) in the PCI config space to start reading from.
+ *
+ * \return 0 on success, -1 on failure.
+ */
 int spdk_pci_device_cfg_read32(struct spdk_pci_device *dev, uint32_t *value, uint32_t offset);
+
+/**
+ * Write 4 bytes into the PCI configuration space.
+ *
+ * \param dev PCI device.
+ * \param value A value to write.
+ * \param offset Offset (in bytes) in the PCI config space to start writing to.
+ *
+ * \return 0 on success, -1 on failure.
+ */
 int spdk_pci_device_cfg_write32(struct spdk_pci_device *dev, uint32_t value, uint32_t offset);
 
 /**
+ * Check if device was requested to be removed from the process. This can be
+ * caused either by physical device hotremoval or OS-triggered removal. In the
+ * latter case, the device may continue to function properly even if this
+ * function returns \c true . The upper-layer driver may check this function
+ * periodically and eventually detach the device.
+ *
+ * \param dev PCI device.
+ *
+ * \return if device was requested to be removed
+ */
+bool spdk_pci_device_is_removed(struct spdk_pci_device *dev);
+
+/**
  * Compare two PCI addresses.
  *
+ * \param a1 PCI address 1.
+ * \param a2 PCI address 2.
+ *
  * \return 0 if a1 == a2, less than 0 if a1 < a2, greater than 0 if a1 > a2
  */
 int spdk_pci_addr_compare(const struct spdk_pci_addr *a1, const struct spdk_pci_addr *a2);
@@ -336,11 +1004,11 @@ int spdk_pci_addr_compare(const struct spdk_pci_addr *a1, const struct spdk_pci_
 /**
  * Convert a string representation of a PCI address into a struct spdk_pci_addr.
  *
- * \param addr PCI adddress output on success
+ * \param addr PCI adddress output on success.
  * \param bdf PCI address in domain:bus:device.function format or
- *	domain.bus.device.function format
+ *	domain.bus.device.function format.
  *
- * \return 0 on success, or a negated errno value on failure.
+ * \return 0 on success, negative errno on failure.
  */
 int spdk_pci_addr_parse(struct spdk_pci_addr *addr, const char *bdf);
 
@@ -348,30 +1016,51 @@ int spdk_pci_addr_parse(struct spdk_pci_addr *addr, const char *bdf);
  * Convert a struct spdk_pci_addr to a string.
  *
  * \param bdf String into which a string will be output in the format
- *            domain:bus:device.function. The string must be at least
- *            14 characters in size.
- * \param sz Size of bdf. Must be at least 14.
- * \param addr PCI address input
+ *  domain:bus:device.function. The string must be at least 14 characters in size.
+ * \param sz Size of bdf in bytes. Must be at least 14.
+ * \param addr PCI address.
  *
- * \return 0 on success, or a negated errno value on failure.
+ * \return 0 on success, or a negated errno on failure.
  */
 int spdk_pci_addr_fmt(char *bdf, size_t sz, const struct spdk_pci_addr *addr);
 
 /**
+ * Hook a custom PCI device into the PCI layer. The device will be attachable,
+ * enumerable, and will call provided callbacks on each PCI resource access
+ * request.
+ *
+ * \param drv driver that will be able to attach the device
+ * \param dev fully initialized PCI device struct
+ */
+void spdk_pci_hook_device(struct spdk_pci_driver *drv, struct spdk_pci_device *dev);
+
+/**
+ * Un-hook a custom PCI device from the PCI layer. The device must not be attached.
+ *
+ * \param dev fully initialized PCI device struct
+ */
+void spdk_pci_unhook_device(struct spdk_pci_device *dev);
+
+/**
+ * Remove any CPU affinity from the current thread.
+ */
+void spdk_unaffinitize_thread(void);
+
+/**
  * Call a function with CPU affinity unset.
  *
  * This can be used to run a function that creates other threads without inheriting the calling
  * thread's CPU affinity.
  *
- * \param cb function to call
- * \param arg parameter to cb function
+ * \param cb Function to call
+ * \param arg Parameter to the function cb().
  *
- * \return the return value of cb()
+ * \return the return value of cb().
  */
 void *spdk_call_unaffinitized(void *cb(void *arg), void *arg);
 
 /**
- * Page-granularity memory address translation table
+ * Page-granularity memory address translation table.
  */
 struct spdk_mem_map;
 
@@ -380,58 +1069,107 @@ enum spdk_mem_map_notify_action {
 	SPDK_MEM_MAP_NOTIFY_UNREGISTER,
 };
 
-typedef void (*spdk_mem_map_notify_cb)(void *cb_ctx, struct spdk_mem_map *map,
-				       enum spdk_mem_map_notify_action action,
-				       void *vaddr, size_t size);
+typedef int (*spdk_mem_map_notify_cb)(void *cb_ctx, struct spdk_mem_map *map,
+				      enum spdk_mem_map_notify_action action,
+				      void *vaddr, size_t size);
+
+typedef int (*spdk_mem_map_contiguous_translations)(uint64_t addr_1, uint64_t addr_2);
 
 /**
- * Allocate a virtual memory address translation map
+ * A function table to be implemented by each memory map.
+ */
+struct spdk_mem_map_ops {
+	spdk_mem_map_notify_cb notify_cb;
+	spdk_mem_map_contiguous_translations are_contiguous;
+};
+
+/**
+ * Allocate a virtual memory address translation map.
+ *
+ * \param default_translation Default translation for the map.
+ * \param ops Table of callback functions for map operations.
+ * \param cb_ctx Argument passed to the callback function.
+ *
+ * \return a pointer to the allocated virtual memory address translation map.
  */
 struct spdk_mem_map *spdk_mem_map_alloc(uint64_t default_translation,
-					spdk_mem_map_notify_cb notify_cb, void *cb_ctx);
+					const struct spdk_mem_map_ops *ops, void *cb_ctx);
 
 /**
- * Free a memory map previously allocated by spdk_mem_map_alloc()
+ * Free a memory map previously allocated by spdk_mem_map_alloc().
+ *
+ * \param pmap Memory map to free.
  */
 void spdk_mem_map_free(struct spdk_mem_map **pmap);
 
 /**
  * Register an address translation for a range of virtual memory.
  *
- * \param map Memory map
+ * \param map Memory map.
  * \param vaddr Virtual address of the region to register - must be 2 MB aligned.
- * \param size Size of the region - must be 2 MB in the current implementation.
+ * \param size Size of the region in bytes - must be multiple of 2 MB in the
+ *  current implementation.
  * \param translation Translation to store in the map for this address range.
  *
- * \sa spdk_mem_map_clear_translation()
+ * \sa spdk_mem_map_clear_translation().
+ *
+ * \return 0 on success, negative errno on failure.
  */
-void spdk_mem_map_set_translation(struct spdk_mem_map *map, uint64_t vaddr, uint64_t size,
-				  uint64_t translation);
+int spdk_mem_map_set_translation(struct spdk_mem_map *map, uint64_t vaddr, uint64_t size,
+				 uint64_t translation);
 
 /**
  * Unregister an address translation.
  *
- * \sa spdk_mem_map_set_translation()
+ * \param map Memory map.
+ * \param vaddr Virtual address of the region to unregister - must be 2 MB aligned.
+ * \param size Size of the region in bytes - must be multiple of 2 MB in the
+ *  current implementation.
+ *
+ * \sa spdk_mem_map_set_translation().
+ *
+ * \return 0 on success, negative errno on failure.
  */
-void spdk_mem_map_clear_translation(struct spdk_mem_map *map, uint64_t vaddr, uint64_t size);
+int spdk_mem_map_clear_translation(struct spdk_mem_map *map, uint64_t vaddr, uint64_t size);
 
 /**
  * Look up the translation of a virtual address in a memory map.
+ *
+ * \param map Memory map.
+ * \param vaddr Virtual address.
+ * \param size Contains the size of the memory region pointed to by vaddr.
+ * If vaddr is successfully translated, then this is updated with the size of
+ * the memory region for which the translation is valid.
+ *
+ * \return the translation of vaddr stored in the map, or default_translation
+ * as specified in spdk_mem_map_alloc() if vaddr is not present in the map.
  */
-uint64_t spdk_mem_map_translate(const struct spdk_mem_map *map, uint64_t vaddr);
+uint64_t spdk_mem_map_translate(const struct spdk_mem_map *map, uint64_t vaddr, uint64_t *size);
 
 /**
  * Register the specified memory region for address translation.
+ *
  * The memory region must map to pinned huge pages (2MB or greater).
+ *
+ * \param vaddr Virtual address to register.
+ * \param len Length in bytes of the vaddr.
+ *
+ * \return 0 on success, negative errno on failure.
  */
-void spdk_mem_register(void *vaddr, size_t len);
+int spdk_mem_register(void *vaddr, size_t len);
 
 /**
  * Unregister the specified memory region from vtophys address translation.
+ *
  * The caller must ensure all in-flight DMA operations to this memory region
- *  are completed or cancelled before calling this function.
+ * are completed or cancelled before calling this function.
+ *
+ * \param vaddr Virtual address to unregister.
+ * \param len Length in bytes of the vaddr.
+ *
+ * \return 0 on success, negative errno on failure.
  */
-void spdk_mem_unregister(void *vaddr, size_t len);
+int spdk_mem_unregister(void *vaddr, size_t len);
 
 #ifdef __cplusplus
 }
diff --git a/PDK/core/src/api/include/udd/spdk/env_dpdk.h b/PDK/core/src/api/include/udd/spdk/env_dpdk.h
new file mode 100644
index 0000000..9729ad5
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/env_dpdk.h
@@ -0,0 +1,75 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/** \file
+ * Encapsulated DPDK specific dependencies
+ */
+
+#ifndef SPDK_ENV_DPDK_H
+#define SPDK_ENV_DPDK_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * Initialize the environment library after DPDK env is already initialized.
+ * If DPDK's rte_eal_init is already called, this function must be called
+ * instead of spdk_env_init, prior to using any other functions in SPDK
+ * env library.
+ *
+ * \return 0 on success, or negative errno on failure.
+ */
+int spdk_env_dpdk_post_init(void);
+
+/**
+ * Release any resources of the environment library that were alllocated with
+ * spdk_env_dpdk_post_init(). After this call, no DPDK function calls may
+ * be made. It is expected that common usage of this function is to call it
+ * just before terminating the process.
+ */
+void spdk_env_dpdk_post_fini(void);
+
+/**
+ * Check if DPDK was initialized external to the SPDK env_dpdk library.
+ *
+ * \return true if DPDK was initialized external to the SPDK env_dpdk library.
+ * \return false otherwise
+ */
+bool spdk_env_dpdk_external_init(void);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/PDK/core/src/api/include/udd/spdk/event.h b/PDK/core/src/api/include/udd/spdk/event.h
index 9d2767b..88532af 100644
--- a/PDK/core/src/api/include/udd/spdk/event.h
+++ b/PDK/core/src/api/include/udd/spdk/event.h
@@ -43,8 +43,21 @@
 
 #include "spdk/stdinc.h"
 
+#include "spdk/cpuset.h"
 #include "spdk/queue.h"
+#include "spdk/log.h"
+#include "spdk/thread.h"
 
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * Event handler function.
+ *
+ * \param arg1 Argument 1.
+ * \param arg2 Argument 2.
+ */
 typedef void (*spdk_event_fn)(void *arg1, void *arg2);
 
 /**
@@ -52,15 +65,24 @@ typedef void (*spdk_event_fn)(void *arg1, void *arg2);
  */
 struct spdk_event;
 
-typedef void (*spdk_poller_fn)(void *arg);
-
 /**
  * \brief A poller is a function that is repeatedly called on an lcore.
  */
 struct spdk_poller;
 
+/**
+ * Callback function for customized shutdown handling of application.
+ */
 typedef void (*spdk_app_shutdown_cb)(void);
-typedef void (*spdk_sighandler_t)(int);
+
+/**
+ * Signal handler fucntion.
+ *
+ * \param signal Signal number.
+ */
+typedef void (*spdk_sighandler_t)(int signal);
+
+#define SPDK_DEFAULT_RPC_ADDR "/var/tmp/spdk.sock"
 
 /**
  * \brief Event framework initialization options
@@ -68,8 +90,9 @@ typedef void (*spdk_sighandler_t)(int);
 struct spdk_app_opts {
 	const char *name;
 	const char *config_file;
+	const char *json_config_file;
+	const char *rpc_addr; /* Can be UNIX domain socket path or IP address + TCP port */
 	const char *reactor_mask;
-	const char *log_facility;
 	const char *tpoint_group_mask;
 
 	int shm_id;
@@ -78,109 +101,212 @@ struct spdk_app_opts {
 	spdk_sighandler_t	usr1_handler;
 
 	bool			enable_coredump;
-	int			dpdk_mem_channel;
-	int	 		dpdk_master_core;
-	int			dpdk_mem_size;
+	int			mem_channel;
+	int			master_core;
+	int			mem_size;
+	bool			no_pci;
+	bool			hugepage_single_segments;
+	bool			unlink_hugepage;
+	const char		*hugedir;
+	enum spdk_log_level	print_level;
+	size_t			num_pci_addr;
+	struct spdk_pci_addr	*pci_blacklist;
+	struct spdk_pci_addr	*pci_whitelist;
 
-	/* The maximum latency allowed when passing an event
+	/* DEPRECATED. No longer has any effect.
+	 *
+	 * The maximum latency allowed when passing an event
 	 * from one core to another. A value of 0
 	 * means all cores continually poll. This is
 	 * specified in microseconds.
 	 */
 	uint64_t		max_delay_us;
-	int                     init_from;
-	char                    *options;
+
+	/* Wait for the associated RPC before initializing subsystems
+	 * when this flag is enabled.
+	 */
+	bool			delay_subsystem_init;
+
+	/* Number of trace entries allocated for each core */
+	uint64_t		num_entries;
+
+	/** Opaque context for use of the env implementation. */
+	void			*env_context;
+
+	/**
+	 * for passing user-provided log call
+	 *
+	 * \param level Log level threshold.
+	 * \param file Name of the current source file.
+	 * \param line Current source file line.
+	 * \param func Current source function name.
+	 * \param format Format string to the message.
+	 */
+	void (* log)(int level, const char *file, const int line,
+		     const char *func, const char *format);
+
 };
 
 /**
- * \brief Initialize the default value of opts
+ * Initialize the default value of opts
+ *
+ * \param opts Data structure where SPDK will initialize the default options.
  */
 void spdk_app_opts_init(struct spdk_app_opts *opts);
 
 /**
- * \brief Initialize an application to use the event framework. This must be called prior to using
- * any other functions in this library.
- */
-void spdk_app_init(struct spdk_app_opts *opts);
-
-/**
- * \brief Perform final shutdown operations on an application using the event framework.
+ * Start the framework.
+ *
+ * Before calling this function, opts must be initialized by
+ * spdk_app_opts_init(). Once started, the framework will call start_fn on
+ * an spdk_thread running on the current system thread with the
+ * argument provided. This call will block until spdk_app_stop()
+ * is called. If an error condition occurs during the intialization
+ * code within spdk_app_start(), this function will immediately return
+ * before invoking start_fn.
+ *
+ * \param opts Initialization options used for this application.
+ * \param start_fn Entry point that will execute on an internally created thread
+ *                 once the framework has been started.
+ * \param ctx Argument passed to function start_fn.
+ *
+ * \return 0 on success or non-zero on failure.
  */
-int spdk_app_fini(void);
+int spdk_app_start(struct spdk_app_opts *opts, spdk_msg_fn start_fn,
+		   void *ctx);
 
 /**
- * \brief Start the framework. Once started, the framework will call start_fn on the master
- * core with the arguments provided. This call will block until \ref spdk_app_stop is called.
+ * Perform final shutdown operations on an application using the event framework.
  */
-int spdk_app_start(spdk_event_fn start_fn, void *arg1, void *arg2);
+void spdk_app_fini(void);
 
 /**
- * \brief Start shutting down the framework.  Typically this function is not called directly, and
- * the shutdown process is started implicitly by a process signal.  But in applications that are
- * using SPDK for a subset of its process threads, this function can be called in lieu of a signal.
+ * Start shutting down the framework.
+ *
+ * Typically this function is not called directly, and the shutdown process is
+ * started implicitly by a process signal. But in applications that are using
+ * SPDK for a subset of its process threads, this function can be called in lieu
+ * of a signal.
  */
 void spdk_app_start_shutdown(void);
 
 /**
- * \brief Stop the framework. This does not wait for all threads to exit. Instead, it kicks off
- * the shutdown process and returns. Once the shutdown process is complete, \ref spdk_app_start will return.
+ * Stop the framework.
+ *
+ * This does not wait for all threads to exit. Instead, it kicks off the shutdown
+ * process and returns. Once the shutdown process is complete, spdk_app_start()
+ * will return.
+ *
+ * \param rc The rc value specified here will be returned to caller of spdk_app_start().
  */
 void spdk_app_stop(int rc);
 
 /**
- * \brief Generate a configuration file that corresponds to the current running state.
+ * Generate a configuration file that corresponds to the current running state.
+ *
+ * \param config_str Values obtained from the generated configuration file.
+ * \param name Prefix for name of temporary configuration file to save the current config.
+ *
+ * \return 0 on success, -1 on failure.
  */
 int spdk_app_get_running_config(char **config_str, char *name);
 
 /**
- * \brief Return the shared memory id for this application.
+ * Return the shared memory id for this application.
+ *
+ * \return shared memory id.
  */
 int spdk_app_get_shm_id(void);
 
 /**
- * \brief Convert a string containing a CPU core mask into a bitmask
+ * Convert a string containing a CPU core mask into a bitmask
+ *
+ * \param mask String containing a CPU core mask.
+ * \param cpumask Bitmask of CPU cores.
+ *
+ * \return 0 on success, -1 on failure.
  */
-int spdk_app_parse_core_mask(const char *mask, uint64_t *cpumask);
+int spdk_app_parse_core_mask(const char *mask, struct spdk_cpuset *cpumask);
 
 /**
- * \brief Return a mask of the CPU cores active for this application
+ * Get the mask of the CPU cores active for this application
+ *
+ * \return the bitmask of the active CPU cores.
  */
-uint64_t spdk_app_get_core_mask(void);
+struct spdk_cpuset *spdk_app_get_core_mask(void);
+
+#define SPDK_APP_GETOPT_STRING "c:de:ghi:m:n:p:r:s:uB:L:RW:"
+
+enum spdk_app_parse_args_rvals {
+	SPDK_APP_PARSE_ARGS_HELP = 0,
+	SPDK_APP_PARSE_ARGS_SUCCESS = 1,
+	SPDK_APP_PARSE_ARGS_FAIL = 2
+};
+typedef enum spdk_app_parse_args_rvals spdk_app_parse_args_rvals_t;
 
 /**
- * \brief Return the number of CPU cores utilized by this application
+ * Helper function for parsing arguments and printing usage messages.
+ *
+ * \param argc Count of arguments in argv parameter array.
+ * \param argv Array of command line arguments.
+ * \param opts Default options for the application.
+ * \param getopt_str String representing the app-specific command line parameters.
+ * Characters in this string must not conflict with characters in SPDK_APP_GETOPT_STRING.
+ * \param app_long_opts Array of full-name parameters. Can be NULL.
+ * \param parse Function pointer to call if an argument in getopt_str is found.
+ * \param usage Function pointer to print usage messages for app-specific command
+ *		line parameters.
+ *\return SPDK_APP_PARSE_ARGS_FAIL on failure, SPDK_APP_PARSE_ARGS_SUCCESS on
+ *        success, SPDK_APP_PARSE_ARGS_HELP if '-h' passed as an option.
  */
-int spdk_app_get_core_count(void) __attribute__((deprecated));
+spdk_app_parse_args_rvals_t spdk_app_parse_args(int argc, char **argv,
+		struct spdk_app_opts *opts, const char *getopt_str,
+		struct option *app_long_opts, int (*parse)(int ch, char *arg),
+		void (*usage)(void));
 
 /**
- * \brief Return the lcore of the current thread.
+ * Print usage strings for common SPDK command line options.
+ *
+ * May only be called after spdk_app_parse_args().
  */
-uint32_t spdk_app_get_current_core(void) __attribute__((deprecated));
+void spdk_app_usage(void);
 
 /**
- * \brief Allocate an event to be passed to \ref spdk_event_call
+ * Allocate an event to be passed to spdk_event_call().
+ *
+ * \param lcore Lcore to run this event.
+ * \param fn Function used to execute event.
+ * \param arg1 Argument passed to function fn.
+ * \param arg2 Argument passed to function fn.
+ *
+ * \return a pointer to the allocated event.
  */
 struct spdk_event *spdk_event_allocate(uint32_t lcore, spdk_event_fn fn,
 				       void *arg1, void *arg2);
 
 /**
- * \brief Pass the given event to the associated lcore and call the function.
+ * Pass the given event to the associated lcore and call the function.
+ *
+ * \param event Event to execute.
  */
 void spdk_event_call(struct spdk_event *event);
 
 /**
- * \brief Register a poller on the given lcore.
+ * Enable or disable monitoring of context switches.
+ *
+ * \param enabled True to enable, false to disable.
  */
-void spdk_poller_register(struct spdk_poller **ppoller,
-			  spdk_poller_fn fn,
-			  void *arg,
-			  uint32_t lcore,
-			  uint64_t period_microseconds);
+void spdk_reactor_enable_context_switch_monitor(bool enabled);
 
 /**
- * \brief Unregister a poller on the given lcore.
+ * Return whether context switch monitoring is enabled.
+ *
+ * \return true if enabled or false otherwise.
  */
-void spdk_poller_unregister(struct spdk_poller **ppoller,
-			    struct spdk_event *complete);
+bool spdk_reactor_context_switch_monitor_enabled(void);
+
+#ifdef __cplusplus
+}
+#endif
 
 #endif
diff --git a/PDK/core/src/api/include/udd/spdk/fd.h b/PDK/core/src/api/include/udd/spdk/fd.h
index 8fec287..8da7f2c 100644
--- a/PDK/core/src/api/include/udd/spdk/fd.h
+++ b/PDK/core/src/api/include/udd/spdk/fd.h
@@ -44,7 +44,22 @@
 extern "C" {
 #endif
 
+/**
+ * Get the file size.
+ *
+ * \param fd  File descriptor.
+ *
+ * \return    File size.
+ */
 uint64_t spdk_fd_get_size(int fd);
+
+/**
+ * Get the block size of the file.
+ *
+ * \param fd  File descriptor.
+ *
+ * \return    Block size.
+ */
 uint32_t spdk_fd_get_blocklen(int fd);
 
 #ifdef __cplusplus
diff --git a/PDK/core/src/api/include/udd/spdk/ftl.h b/PDK/core/src/api/include/udd/spdk/ftl.h
new file mode 100644
index 0000000..5b90a30
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/ftl.h
@@ -0,0 +1,264 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef SPDK_FTL_H
+#define SPDK_FTL_H
+
+#include "spdk/stdinc.h"
+#include "spdk/nvme.h"
+#include "spdk/nvme_ocssd.h"
+#include "spdk/uuid.h"
+#include "spdk/thread.h"
+#include "spdk/bdev.h"
+
+struct spdk_ftl_dev;
+
+/* Limit thresholds */
+enum {
+	SPDK_FTL_LIMIT_CRIT,
+	SPDK_FTL_LIMIT_HIGH,
+	SPDK_FTL_LIMIT_LOW,
+	SPDK_FTL_LIMIT_START,
+	SPDK_FTL_LIMIT_MAX
+};
+
+struct spdk_ftl_limit {
+	/* Threshold from which the limiting starts */
+	size_t					thld;
+
+	/* Limit percentage */
+	size_t					limit;
+};
+
+struct spdk_ftl_conf {
+	/* Number of reserved addresses not exposed to the user */
+	size_t					lba_rsvd;
+
+	/* Write buffer size */
+	size_t					rwb_size;
+
+	/* Threshold for opening new band */
+	size_t					band_thld;
+
+	/* Maximum IO depth per band relocate */
+	size_t					max_reloc_qdepth;
+
+	/* Maximum active band relocates */
+	size_t					max_active_relocs;
+
+	/* IO pool size per user thread */
+	size_t					user_io_pool_size;
+
+	struct {
+		/* Lowest percentage of invalid lbks for a band to be defragged */
+		size_t				invalid_thld;
+
+		/* User writes limits */
+		struct spdk_ftl_limit		limits[SPDK_FTL_LIMIT_MAX];
+	} defrag;
+};
+
+/* Range of parallel units (inclusive) */
+struct spdk_ftl_punit_range {
+	unsigned int				begin;
+	unsigned int				end;
+};
+
+enum spdk_ftl_mode {
+	/* Create new device */
+	SPDK_FTL_MODE_CREATE = (1 << 0),
+};
+
+struct spdk_ftl_dev_init_opts {
+	/* NVMe controller */
+	struct spdk_nvme_ctrlr			*ctrlr;
+	/* Controller's transport ID */
+	struct spdk_nvme_transport_id		trid;
+	/* Write buffer cache */
+	struct spdk_bdev_desc			*cache_bdev_desc;
+
+	/* Thread responsible for core tasks execution */
+	struct spdk_thread			*core_thread;
+	/* Thread responsible for read requests */
+	struct spdk_thread			*read_thread;
+
+	/* Device's config */
+	struct spdk_ftl_conf			*conf;
+	/* Device's name */
+	const char				*name;
+	/* Parallel unit range */
+	struct spdk_ftl_punit_range		range;
+	/* Mode flags */
+	unsigned int				mode;
+	/* Device UUID (valid when restoring device from disk) */
+	struct spdk_uuid			uuid;
+};
+
+struct spdk_ftl_attrs {
+	/* Device's UUID */
+	struct spdk_uuid			uuid;
+	/* Parallel unit range */
+	struct spdk_ftl_punit_range		range;
+	/* Number of logical blocks */
+	uint64_t				lbk_cnt;
+	/* Logical block size */
+	size_t					lbk_size;
+	/* Write buffer cache */
+	struct spdk_bdev_desc			*cache_bdev_desc;
+};
+
+struct ftl_module_init_opts {
+	/* Thread on which to poll for ANM events */
+	struct spdk_thread			*anm_thread;
+};
+
+typedef void (*spdk_ftl_fn)(void *, int);
+typedef void (*spdk_ftl_init_fn)(struct spdk_ftl_dev *, void *, int);
+
+/**
+ * Initialize the FTL module.
+ *
+ * \param opts module configuration
+ * \param cb callback function to call when the module is initialized
+ * \param cb_arg callback's argument
+ *
+ * \return 0 if successfully started initialization, negative values if
+ * resources could not be allocated.
+ */
+int spdk_ftl_module_init(const struct ftl_module_init_opts *opts, spdk_ftl_fn cb, void *cb_arg);
+
+/**
+ * Deinitialize the FTL module. All FTL devices have to be unregistered prior to
+ * calling this function.
+ *
+ * \param cb callback function to call when the deinitialization is completed
+ * \param cb_arg callback's argument
+ *
+ * \return 0 if successfully scheduled deinitialization, negative errno
+ * otherwise.
+ */
+int spdk_ftl_module_fini(spdk_ftl_fn cb, void *cb_arg);
+
+/**
+ * Initialize the FTL on given NVMe device and parallel unit range.
+ *
+ * Covers the following:
+ * - initialize and register NVMe ctrlr,
+ * - retrieve geometry and check if the device has proper configuration,
+ * - allocate buffers and resources,
+ * - initialize internal structures,
+ * - initialize internal thread(s),
+ * - restore or create L2P table.
+ *
+ * \param opts configuration for new device
+ * \param cb callback function to call when the device is created
+ * \param cb_arg callback's argument
+ *
+ * \return 0 if initialization was started successfully, negative errno otherwise.
+ */
+int spdk_ftl_dev_init(const struct spdk_ftl_dev_init_opts *opts, spdk_ftl_init_fn cb, void *cb_arg);
+
+/**
+ * Deinitialize and free given device.
+ *
+ * \param dev device
+ * \param cb callback function to call when the device is freed
+ * \param cb_arg callback's argument
+ *
+ * \return 0 if successfully scheduled free, negative errno otherwise.
+ */
+int spdk_ftl_dev_free(struct spdk_ftl_dev *dev, spdk_ftl_fn cb, void *cb_arg);
+
+/**
+ * Initialize FTL configuration structure with default values.
+ *
+ * \param conf FTL configuration to initialize
+ */
+void spdk_ftl_conf_init_defaults(struct spdk_ftl_conf *conf);
+
+/**
+ * Retrieve deviceâ€™s attributes.
+ *
+ * \param dev device
+ * \param attr Attribute structure to fill
+ */
+void  spdk_ftl_dev_get_attrs(const struct spdk_ftl_dev *dev, struct spdk_ftl_attrs *attr);
+
+/**
+ * Submits a read to the specified device.
+ *
+ * \param dev Device
+ * \param ch I/O channel
+ * \param lba Starting LBA to read the data
+ * \param lba_cnt Number of sectors to read
+ * \param iov Single IO vector or pointer to IO vector table
+ * \param iov_cnt Number of IO vectors
+ * \param cb_fn Callback function to invoke when the I/O is completed
+ * \param cb_arg Argument to pass to the callback function
+ *
+ * \return 0 if successfully submitted, negative errno otherwise.
+ */
+int spdk_ftl_read(struct spdk_ftl_dev *dev, struct spdk_io_channel *ch, uint64_t lba,
+		  size_t lba_cnt,
+		  struct iovec *iov, size_t iov_cnt, spdk_ftl_fn cb_fn, void *cb_arg);
+
+/**
+ * Submits a write to the specified device.
+ *
+ * \param dev Device
+ * \param ch I/O channel
+ * \param lba Starting LBA to write the data
+ * \param lba_cnt Number of sectors to write
+ * \param iov Single IO vector or pointer to IO vector table
+ * \param iov_cnt Number of IO vectors
+ * \param cb_fn Callback function to invoke when the I/O is completed
+ * \param cb_arg Argument to pass to the callback function
+ *
+ * \return 0 if successfully submitted, negative errno otherwise.
+ */
+int spdk_ftl_write(struct spdk_ftl_dev *dev, struct spdk_io_channel *ch, uint64_t lba,
+		   size_t lba_cnt,
+		   struct iovec *iov, size_t iov_cnt, spdk_ftl_fn cb_fn, void *cb_arg);
+
+/**
+ * Submits a flush request to the specified device.
+ *
+ * \param dev device
+ * \param cb_fn Callback function to invoke when all prior IOs have been completed
+ * \param cb_arg Argument to pass to the callback function
+ *
+ * \return 0 if successfully submitted, negative errno otherwise.
+ */
+int spdk_ftl_flush(struct spdk_ftl_dev *dev, spdk_ftl_fn cb_fn, void *cb_arg);
+
+#endif /* SPDK_FTL_H */
diff --git a/PDK/core/src/api/include/udd/spdk/gpt_spec.h b/PDK/core/src/api/include/udd/spdk/gpt_spec.h
index 90f6aad..c67eb57 100644
--- a/PDK/core/src/api/include/udd/spdk/gpt_spec.h
+++ b/PDK/core/src/api/include/udd/spdk/gpt_spec.h
@@ -135,7 +135,7 @@ struct spdk_gpt_partition_entry {
 		uint64_t reserved_uefi : 45;
 		uint64_t guid_specific : 16;
 	} attr;
-	uint8_t partition_name[72];
+	uint16_t partition_name[36];
 };
 SPDK_STATIC_ASSERT(sizeof(struct spdk_gpt_partition_entry) == 128, "size incorrect");
 
diff --git a/PDK/core/src/api/include/udd/spdk/histogram_data.h b/PDK/core/src/api/include/udd/spdk/histogram_data.h
new file mode 100644
index 0000000..5f114fe
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/histogram_data.h
@@ -0,0 +1,264 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * \file
+ * Generic histogram library
+ */
+
+#ifndef _SPDK_HISTOGRAM_DATA_H_
+#define _SPDK_HISTOGRAM_DATA_H_
+
+#include "spdk/stdinc.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#define SPDK_HISTOGRAM_BUCKET_SHIFT_DEFAULT	7
+#define SPDK_HISTOGRAM_BUCKET_SHIFT(h)		h->bucket_shift
+#define SPDK_HISTOGRAM_BUCKET_LSB(h)		(64 - SPDK_HISTOGRAM_BUCKET_SHIFT(h))
+#define SPDK_HISTOGRAM_NUM_BUCKETS_PER_RANGE(h)	(1ULL << SPDK_HISTOGRAM_BUCKET_SHIFT(h))
+#define SPDK_HISTOGRAM_BUCKET_MASK(h)		(SPDK_HISTOGRAM_NUM_BUCKETS_PER_RANGE(h) - 1)
+#define SPDK_HISTOGRAM_NUM_BUCKET_RANGES(h)	(SPDK_HISTOGRAM_BUCKET_LSB(h) + 1)
+#define SPDK_HISTOGRAM_NUM_BUCKETS(h)		(SPDK_HISTOGRAM_NUM_BUCKETS_PER_RANGE(h) * \
+						 SPDK_HISTOGRAM_NUM_BUCKET_RANGES(h))
+
+/*
+ * SPDK histograms are implemented using ranges of bucket arrays.  The most common usage
+ * model is using TSC datapoints to capture an I/O latency histogram.  For this usage model,
+ * the histogram tracks only TSC deltas - any translation to microseconds is done by the
+ * histogram user calling spdk_histogram_data_iterate() to iterate over the buckets to perform
+ * the translations.
+ *
+ * Each range has a number of buckets determined by SPDK_HISTOGRAM_NUM_BUCKETS_PER_RANGE
+ * which is 128.  The buckets in ranges 0 and 1 each map to one specific datapoint value.
+ * The buckets in subsequent ranges each map to twice as many datapoint values as buckets
+ * in the range before it:
+ *
+ * Range 0:  1 value each  - 128 buckets cover 0 to 127 (2^7-1)
+ * Range 1:  1 value each  - 128 buckets cover 128 to 255 (2^8-1)
+ * Range 2:  2 values each - 128 buckets cover 256 to 511 (2^9-1)
+ * Range 3:  4 values each - 128 buckets cover 512 to 1023 (2^10-1)
+ * Range 4:  8 values each - 128 buckets cover 1024 to 2047 (2^11-1)
+ * Range 5: 16 values each - 128 buckets cover 2048 to 4095 (2^12-1)
+ * ...
+ * Range 55: 2^54 values each - 128 buckets cover 2^61 to 2^62-1
+ * Range 56: 2^55 values each - 128 buckets cover 2^62 to 2^63-1
+ * Range 57: 2^56 values each - 128 buckets cover 2^63 to 2^64-1
+ *
+ * On a 2.3GHz processor, this strategy results in 50ns buckets in the 7-14us range (sweet
+ * spot for Intel Optane SSD latency testing).
+ *
+ * Buckets can be made more granular by increasing SPDK_HISTOGRAM_BUCKET_SHIFT.  This
+ * comes at the cost of additional storage per namespace context to store the bucket data.
+ */
+
+struct spdk_histogram_data {
+
+	uint32_t	bucket_shift;
+	uint64_t	*bucket;
+
+};
+
+static inline void
+__spdk_histogram_increment(struct spdk_histogram_data *h, uint32_t range, uint32_t index)
+{
+	uint64_t *count;
+
+	count = &h->bucket[(range << SPDK_HISTOGRAM_BUCKET_SHIFT(h)) + index];
+	(*count)++;
+}
+
+static inline uint64_t
+__spdk_histogram_get_count(const struct spdk_histogram_data *h, uint32_t range, uint32_t index)
+{
+	return h->bucket[(range << SPDK_HISTOGRAM_BUCKET_SHIFT(h)) + index];
+}
+
+static inline uint64_t *
+__spdk_histogram_get_bucket(const struct spdk_histogram_data *h, uint32_t range, uint32_t index)
+{
+	return &h->bucket[(range << SPDK_HISTOGRAM_BUCKET_SHIFT(h)) + index];
+}
+
+static inline void
+spdk_histogram_data_reset(struct spdk_histogram_data *histogram)
+{
+	memset(histogram->bucket, 0, SPDK_HISTOGRAM_NUM_BUCKETS(histogram) * sizeof(uint64_t));
+}
+
+static inline uint32_t
+__spdk_histogram_data_get_bucket_range(struct spdk_histogram_data *h, uint64_t datapoint)
+{
+	uint32_t clz, range;
+
+	assert(datapoint != 0);
+
+	clz = __builtin_clzll(datapoint);
+
+	if (clz <= SPDK_HISTOGRAM_BUCKET_LSB(h)) {
+		range = SPDK_HISTOGRAM_BUCKET_LSB(h) - clz;
+	} else {
+		range = 0;
+	}
+
+	return range;
+}
+
+static inline uint32_t
+__spdk_histogram_data_get_bucket_index(struct spdk_histogram_data *h, uint64_t datapoint,
+				       uint32_t range)
+{
+	uint32_t shift;
+
+	if (range == 0) {
+		shift = 0;
+	} else {
+		shift = range - 1;
+	}
+
+	return (datapoint >> shift) & SPDK_HISTOGRAM_BUCKET_MASK(h);
+}
+
+static inline void
+spdk_histogram_data_tally(struct spdk_histogram_data *histogram, uint64_t datapoint)
+{
+	uint32_t range = __spdk_histogram_data_get_bucket_range(histogram, datapoint);
+	uint32_t index = __spdk_histogram_data_get_bucket_index(histogram, datapoint, range);
+
+	__spdk_histogram_increment(histogram, range, index);
+}
+
+static inline uint64_t
+__spdk_histogram_data_get_bucket_start(const struct spdk_histogram_data *h, uint32_t range,
+				       uint32_t index)
+{
+	uint64_t bucket;
+
+	index += 1;
+	if (range > 0) {
+		bucket = 1ULL << (range + SPDK_HISTOGRAM_BUCKET_SHIFT(h) - 1);
+		bucket += (uint64_t)index << (range - 1);
+	} else {
+		bucket = index;
+	}
+
+	return bucket;
+}
+
+typedef void (*spdk_histogram_data_fn)(void *ctx, uint64_t start, uint64_t end, uint64_t count,
+				       uint64_t total, uint64_t so_far);
+
+static inline void
+spdk_histogram_data_iterate(const struct spdk_histogram_data *histogram,
+			    spdk_histogram_data_fn fn, void *ctx)
+{
+	uint64_t i, j, count, so_far, total;
+	uint64_t bucket, last_bucket;
+
+	total = 0;
+
+	for (i = 0; i < SPDK_HISTOGRAM_NUM_BUCKET_RANGES(histogram); i++) {
+		for (j = 0; j < SPDK_HISTOGRAM_NUM_BUCKETS_PER_RANGE(histogram); j++) {
+			total += __spdk_histogram_get_count(histogram, i, j);
+		}
+	}
+
+	so_far = 0;
+	bucket = 0;
+
+	for (i = 0; i < SPDK_HISTOGRAM_NUM_BUCKET_RANGES(histogram); i++) {
+		for (j = 0; j < SPDK_HISTOGRAM_NUM_BUCKETS_PER_RANGE(histogram); j++) {
+			count = __spdk_histogram_get_count(histogram, i, j);
+			so_far += count;
+			last_bucket = bucket;
+			bucket = __spdk_histogram_data_get_bucket_start(histogram, i, j);
+			fn(ctx, last_bucket, bucket, count, total, so_far);
+		}
+	}
+}
+
+static inline void
+spdk_histogram_data_merge(const struct spdk_histogram_data *dst,
+			  const struct spdk_histogram_data *src)
+{
+	uint64_t i;
+
+	for (i = 0; i < SPDK_HISTOGRAM_NUM_BUCKETS(dst); i++) {
+		dst->bucket[i] += src->bucket[i];
+	}
+}
+
+static inline struct spdk_histogram_data *
+spdk_histogram_data_alloc_sized(uint32_t bucket_shift)
+{
+	struct spdk_histogram_data *h;
+
+	h = (struct spdk_histogram_data *)calloc(1, sizeof(*h));
+	if (h == NULL) {
+		return NULL;
+	}
+
+	h->bucket_shift = bucket_shift;
+	h->bucket = (uint64_t *)calloc(SPDK_HISTOGRAM_NUM_BUCKETS(h), sizeof(uint64_t));
+	if (h->bucket == NULL) {
+		free(h);
+		return NULL;
+	}
+
+	return h;
+}
+
+static inline struct spdk_histogram_data *
+spdk_histogram_data_alloc(void)
+{
+	return spdk_histogram_data_alloc_sized(SPDK_HISTOGRAM_BUCKET_SHIFT_DEFAULT);
+}
+
+static inline void
+spdk_histogram_data_free(struct spdk_histogram_data *h)
+{
+	if (h == NULL) {
+		return;
+	}
+
+	free(h->bucket);
+	free(h);
+}
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/PDK/core/src/api/include/udd/spdk/io_channel.h b/PDK/core/src/api/include/udd/spdk/io_channel.h
index 8f5f77c..71ddb90 100644
--- a/PDK/core/src/api/include/udd/spdk/io_channel.h
+++ b/PDK/core/src/api/include/udd/spdk/io_channel.h
@@ -38,88 +38,11 @@
 #ifndef SPDK_IO_CHANNEL_H_
 #define SPDK_IO_CHANNEL_H_
 
-#include "spdk/stdinc.h"
-
-#include "spdk/queue.h"
-
-#define SPDK_IO_PRIORITY_DEFAULT	100
-
-struct spdk_io_channel;
-
-typedef int (*io_channel_create_cb_t)(void *io_device, uint32_t priority, void *ctx_buf,
-				      void *unique_ctx);
-typedef void (*io_channel_destroy_cb_t)(void *io_device, void *ctx_buf);
-
-/**
- * \brief Initializes the calling thread for I/O channel allocation.
- */
-void spdk_allocate_thread(void);
-
-/**
- * \brief Releases any resources related to the calling thread for I/O channel allocation.
- *
- * All I/O channel references related to the calling thread must be released using
- *  spdk_put_io_channel() prior to calling this function.
- */
-void spdk_free_thread(void);
-
-/**
- * \brief Register the opaque io_device context as an I/O device.
- *
- * After an I/O device is registered, it can return I/O channels using the
- *  spdk_get_io_channel() function.  create_cb is the callback function invoked
- *  to allocate any resources required for a new I/O channel.  destroy_cb is the
- *  callback function invoked to release the resources for an I/O channel.  ctx_size
- *  is the size of the context buffer allocated to store references to allocated I/O
- *  channel resources.
- */
-void spdk_io_device_register(void *io_device, io_channel_create_cb_t create_cb,
-			     io_channel_destroy_cb_t destroy_cb, uint32_t ctx_size);
-
-/**
- * \brief Unregister the opaque io_device context as an I/O device.
- *
- * Callers must ensure they release references to any I/O channel related to this
- *  device before calling this function.
- */
-void spdk_io_device_unregister(void *io_device);
-
-/**
- * \brief Gets an I/O channel for the specified io_device to be used by the calling thread.
- *
- * The io_device context pointer specified must have previously been registered using
- *  spdk_io_device_register().  If an existing I/O channel does not exist yet for the given
- *  io_device on the calling thread, it will allocate an I/O channel and invoke the create_cb
- *  function pointer specified in spdk_io_device_register().  If an I/O channel already
- *  exists for the given io_device on the calling thread, its reference is returned rather
- *  than creating a new I/O channel.
- *
- * The priority parameter allows callers to create different I/O channels to the same
- *  I/O device with varying priorities.  Currently this value must be set to
- *  SPDK_IO_PRIORITY_DEFAULT.
- *
- * The unique parameter allows callers to specify that an existing channel should not
- *  be used to satisfy this request, even if the io_device and priority fields match.
- *
- * The unique_ctx parameter allows callers to pass channel-specific context to the create_cb
- *  handler for unique channels.  This value must be NULL for shared channels.
- */
-struct spdk_io_channel *spdk_get_io_channel(void *io_device, uint32_t priority, bool unique,
-		void *unique_ctx);
-
-/**
- * \brief Releases a reference to an I/O channel.
- *
- * Must be called from the same thread that called spdk_get_io_channel() for the specified
- *  I/O channel.  If this releases the last reference to the I/O channel, The destroy_cb
- *  function specified in spdk_io_device_register() will be invoked to release any
- *  associated resources.
+/*
+ * This file has been renamed to thread.h. Please update
+ * include paths.
  */
-void spdk_put_io_channel(struct spdk_io_channel *ch);
 
-/**
- * \brief Returns the context buffer associated with an I/O channel.
- */
-void *spdk_io_channel_get_ctx(struct spdk_io_channel *ch);
+#include "spdk/thread.h"
 
 #endif /* SPDK_IO_CHANNEL_H_ */
diff --git a/PDK/core/src/api/include/udd/spdk/ioat.h b/PDK/core/src/api/include/udd/spdk/ioat.h
index d366c9e..fc17509 100644
--- a/PDK/core/src/api/include/udd/spdk/ioat.h
+++ b/PDK/core/src/api/include/udd/spdk/ioat.h
@@ -54,7 +54,8 @@ struct spdk_ioat_chan;
 /**
  * Signature for callback function invoked when a request is completed.
  *
- * \param arg User-specified opaque value corresponding to cb_arg from the request submission.
+ * \param arg User-specified opaque value corresponding to cb_arg from the
+ * request submission.
  */
 typedef void (*spdk_ioat_req_cb)(void *arg);
 
@@ -69,7 +70,8 @@ typedef void (*spdk_ioat_req_cb)(void *arg);
 typedef bool (*spdk_ioat_probe_cb)(void *cb_ctx, struct spdk_pci_device *pci_dev);
 
 /**
- * Callback for spdk_ioat_probe() to report a device that has been attached to the userspace I/OAT driver.
+ * Callback for spdk_ioat_probe() to report a device that has been attached to
+ * the userspace I/OAT driver.
  *
  * \param cb_ctx User-specified opaque value corresponding to cb_ctx from spdk_ioat_probe().
  * \param pci_dev PCI device that was attached to the driver.
@@ -79,63 +81,133 @@ typedef void (*spdk_ioat_attach_cb)(void *cb_ctx, struct spdk_pci_device *pci_de
 				    struct spdk_ioat_chan *ioat);
 
 /**
- * \brief Enumerate the I/OAT devices attached to the system and attach the userspace I/OAT driver
- * to them if desired.
+ * Enumerate the I/OAT devices attached to the system and attach the userspace
+ * I/OAT driver to them if desired.
  *
- * \param cb_ctx Opaque value which will be passed back in cb_ctx parameter of the callbacks.
- * \param probe_cb will be called once per I/OAT device found in the system.
- * \param attach_cb will be called for devices for which probe_cb returned true once the I/OAT
- * controller has been attached to the userspace driver.
+ * If called more than once, only devices that are not already attached to the
+ * SPDK I/OAT driver will be reported.
+ *
+ * To stop using the controller and release its associated resources, call
+ * spdk_ioat_detach() with the ioat_channel instance returned by this function.
  *
- * If called more than once, only devices that are not already attached to the SPDK I/OAT driver
- * will be reported.
+ * \param cb_ctx Opaque value which will be passed back in cb_ctx parameter of
+ * the callbacks.
+ * \param probe_cb will be called once per I/OAT device found in the system.
+ * \param attach_cb will be called for devices for which probe_cb returned true
+ * once the I/OAT controller has been attached to the userspace driver.
  *
- * To stop using the the controller and release its associated resources,
- * call \ref spdk_ioat_detach with the ioat_channel instance returned by this function.
+ * \return 0 on success, -1 on failure.
  */
 int spdk_ioat_probe(void *cb_ctx, spdk_ioat_probe_cb probe_cb, spdk_ioat_attach_cb attach_cb);
 
 /**
- * Detaches specified device returned by \ref spdk_ioat_probe() from the I/OAT driver.
+ * Detach specified device returned by spdk_ioat_probe() from the I/OAT driver.
  *
  * \param ioat I/OAT channel to detach from the driver.
+  */
+void spdk_ioat_detach(struct spdk_ioat_chan *ioat);
+
+/**
+ * Build a DMA engine memory copy request.
+ *
+ * This function will build the descriptor in the channel's ring.  The
+ * caller must also explicitly call spdk_ioat_flush to submit the
+ * descriptor, possibly after building additional descriptors.
+ *
+ * \param chan I/OAT channel to build request.
+ * \param cb_arg Opaque value which will be passed back as the arg parameter in
+ * the completion callback.
+ * \param cb_fn Callback function which will be called when the request is complete.
+ * \param dst Destination virtual address.
+ * \param src Source virtual address.
+ * \param nbytes Number of bytes to copy.
+ *
+ * \return 0 on success, negative errno on failure.
  */
-int spdk_ioat_detach(struct spdk_ioat_chan *ioat);
+int spdk_ioat_build_copy(struct spdk_ioat_chan *chan,
+			 void *cb_arg, spdk_ioat_req_cb cb_fn,
+			 void *dst, const void *src, uint64_t nbytes);
 
 /**
- * Submit a DMA engine memory copy request.
+ * Build and submit a DMA engine memory copy request.
+ *
+ * This function will build the descriptor in the channel's ring and then
+ * immediately submit it by writing the channel's doorbell.  Calling this
+ * function does not require a subsequent call to spdk_ioat_flush.
  *
  * \param chan I/OAT channel to submit request.
- * \param cb_arg Opaque value which will be passed back as the arg parameter in the completion callback.
+ * \param cb_arg Opaque value which will be passed back as the arg parameter in
+ * the completion callback.
  * \param cb_fn Callback function which will be called when the request is complete.
  * \param dst Destination virtual address.
  * \param src Source virtual address.
  * \param nbytes Number of bytes to copy.
+ *
+ * \return 0 on success, negative errno on failure.
  */
-int64_t spdk_ioat_submit_copy(struct spdk_ioat_chan *chan,
-			      void *cb_arg, spdk_ioat_req_cb cb_fn,
-			      void *dst, const void *src, uint64_t nbytes);
+int spdk_ioat_submit_copy(struct spdk_ioat_chan *chan,
+			  void *cb_arg, spdk_ioat_req_cb cb_fn,
+			  void *dst, const void *src, uint64_t nbytes);
 
 /**
- * Submit a DMA engine memory fill request.
+ * Build a DMA engine memory fill request.
+ *
+ * This function will build the descriptor in the channel's ring.  The
+ * caller must also explicitly call spdk_ioat_flush to submit the
+ * descriptor, possibly after building additional descriptors.
+ *
+ * \param chan I/OAT channel to build request.
+ * \param cb_arg Opaque value which will be passed back as the cb_arg parameter
+ * in the completion callback.
+ * \param cb_fn Callback function which will be called when the request is complete.
+ * \param dst Destination virtual address.
+ * \param fill_pattern Repeating eight-byte pattern to use for memory fill.
+ * \param nbytes Number of bytes to fill.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_ioat_build_fill(struct spdk_ioat_chan *chan,
+			 void *cb_arg, spdk_ioat_req_cb cb_fn,
+			 void *dst, uint64_t fill_pattern, uint64_t nbytes);
+
+/**
+ * Build and submit a DMA engine memory fill request.
+ *
+ * This function will build the descriptor in the channel's ring and then
+ * immediately submit it by writing the channel's doorbell.  Calling this
+ * function does not require a subsequent call to spdk_ioat_flush.
  *
  * \param chan I/OAT channel to submit request.
- * \param cb_arg Opaque value which will be passed back as the cb_arg parameter in the completion callback.
+ * \param cb_arg Opaque value which will be passed back as the cb_arg parameter
+ * in the completion callback.
  * \param cb_fn Callback function which will be called when the request is complete.
  * \param dst Destination virtual address.
  * \param fill_pattern Repeating eight-byte pattern to use for memory fill.
  * \param nbytes Number of bytes to fill.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_ioat_submit_fill(struct spdk_ioat_chan *chan,
+			  void *cb_arg, spdk_ioat_req_cb cb_fn,
+			  void *dst, uint64_t fill_pattern, uint64_t nbytes);
+
+/**
+ * Flush previously built descriptors.
+ *
+ * Descriptors are flushed by writing the channel's dmacount doorbell
+ * register.  This function enables batching multiple descriptors followed by
+ * a single doorbell write.
+ *
+ * \param chan I/OAT channel to flush.
  */
-int64_t spdk_ioat_submit_fill(struct spdk_ioat_chan *chan,
-			      void *cb_arg, spdk_ioat_req_cb cb_fn,
-			      void *dst, uint64_t fill_pattern, uint64_t nbytes);
+void spdk_ioat_flush(struct spdk_ioat_chan *chan);
 
 /**
  * Check for completed requests on an I/OAT channel.
  *
  * \param chan I/OAT channel to check for completions.
  *
- * \returns 0 on success or negative if something went wrong.
+ * \return 0 on success, negative errno on failure.
  */
 int spdk_ioat_process_events(struct spdk_ioat_chan *chan);
 
@@ -152,7 +224,7 @@ enum spdk_ioat_dma_capability_flags {
  *
  * \param chan I/OAT channel to query.
  *
- * \return A combination of flags from \ref spdk_ioat_dma_capability_flags.
+ * \return a combination of flags from spdk_ioat_dma_capability_flags().
  */
 uint32_t spdk_ioat_get_dma_capabilities(struct spdk_ioat_chan *chan);
 
diff --git a/PDK/core/src/api/include/udd/spdk/ioat_spec.h b/PDK/core/src/api/include/udd/spdk/ioat_spec.h
index 7a9baf7..0c49405 100644
--- a/PDK/core/src/api/include/udd/spdk/ioat_spec.h
+++ b/PDK/core/src/api/include/udd/spdk/ioat_spec.h
@@ -47,6 +47,8 @@ extern "C" {
 
 #include "spdk/assert.h"
 
+#define SPDK_IOAT_PCI_CHANERR_INT_OFFSET	0x180
+
 #define SPDK_IOAT_INTRCTRL_MASTER_INT_EN	0x01
 
 #define SPDK_IOAT_VER_3_0                0x30
diff --git a/PDK/core/src/api/include/udd/spdk/iscsi_spec.h b/PDK/core/src/api/include/udd/spdk/iscsi_spec.h
index 8714d33..06e5678 100644
--- a/PDK/core/src/api/include/udd/spdk/iscsi_spec.h
+++ b/PDK/core/src/api/include/udd/spdk/iscsi_spec.h
@@ -203,7 +203,8 @@ struct iscsi_bhs_logout_req {
 	uint8_t opcode		: 6;	/* opcode = 0x06 */
 	uint8_t immediate	: 1;
 	uint8_t reserved	: 1;
-	uint8_t reason;
+	uint8_t reason		: 7;
+	uint8_t reason_1	: 1;
 	uint8_t res[2];
 	uint8_t total_ahs_len;
 	uint8_t data_segment_len[3];
@@ -310,9 +311,9 @@ struct iscsi_bhs_scsi_req {
 	uint8_t reserved	: 1;
 	uint8_t attribute	: 3;
 	uint8_t reserved2	: 2;
-	uint8_t write		: 1;
-	uint8_t read		: 1;
-	uint8_t final		: 1;
+	uint8_t write_bit	: 1;
+	uint8_t read_bit	: 1;
+	uint8_t final_bit	: 1;
 	uint8_t res[2];
 	uint8_t total_ahs_len;
 	uint8_t data_segment_len[3];
@@ -482,9 +483,6 @@ struct iscsi_bhs_text_resp {
 /* text flags */
 #define ISCSI_TEXT_CONTINUE			0x40
 
-/* logout flags */
-#define ISCSI_LOGOUT_REASON_MASK		0x7f
-
 /* datain flags */
 #define ISCSI_DATAIN_ACKNOLWEDGE		0x40
 #define ISCSI_DATAIN_OVERFLOW			0x04
diff --git a/PDK/core/src/api/include/udd/spdk/json.h b/PDK/core/src/api/include/udd/spdk/json.h
index 915939d..8109e51 100644
--- a/PDK/core/src/api/include/udd/spdk/json.h
+++ b/PDK/core/src/api/include/udd/spdk/json.h
@@ -41,18 +41,23 @@
 
 #include "spdk/stdinc.h"
 
+#ifdef __cplusplus
+extern "C" {
+#endif
+
 enum spdk_json_val_type {
-	SPDK_JSON_VAL_INVALID,
-	SPDK_JSON_VAL_NULL,
-	SPDK_JSON_VAL_TRUE,
-	SPDK_JSON_VAL_FALSE,
-	SPDK_JSON_VAL_NUMBER,
-	SPDK_JSON_VAL_STRING,
-	SPDK_JSON_VAL_ARRAY_BEGIN,
-	SPDK_JSON_VAL_ARRAY_END,
-	SPDK_JSON_VAL_OBJECT_BEGIN,
-	SPDK_JSON_VAL_OBJECT_END,
-	SPDK_JSON_VAL_NAME,
+	SPDK_JSON_VAL_INVALID = 0,
+#define SPDK_JSON_VAL_ANY SPDK_JSON_VAL_INVALID
+	SPDK_JSON_VAL_NULL = 1U << 1,
+	SPDK_JSON_VAL_TRUE = 1U << 2,
+	SPDK_JSON_VAL_FALSE = 1U << 3,
+	SPDK_JSON_VAL_NUMBER = 1U << 4,
+	SPDK_JSON_VAL_STRING = 1U << 5,
+	SPDK_JSON_VAL_ARRAY_BEGIN = 1U << 6,
+	SPDK_JSON_VAL_ARRAY_END = 1U << 7,
+	SPDK_JSON_VAL_OBJECT_BEGIN = 1U << 8,
+	SPDK_JSON_VAL_OBJECT_END = 1U << 9,
+	SPDK_JSON_VAL_NAME = 1U << 10,
 };
 
 struct spdk_json_val {
@@ -143,8 +148,10 @@ int spdk_json_decode_array(const struct spdk_json_val *values, spdk_json_decode_
 			   void *out, size_t max_size, size_t *out_size, size_t stride);
 
 int spdk_json_decode_bool(const struct spdk_json_val *val, void *out);
+int spdk_json_decode_uint16(const struct spdk_json_val *val, void *out);
 int spdk_json_decode_int32(const struct spdk_json_val *val, void *out);
 int spdk_json_decode_uint32(const struct spdk_json_val *val, void *out);
+int spdk_json_decode_uint64(const struct spdk_json_val *val, void *out);
 int spdk_json_decode_string(const struct spdk_json_val *val, void *out);
 
 /**
@@ -176,9 +183,10 @@ bool spdk_json_strequal(const struct spdk_json_val *val, const char *str);
  */
 char *spdk_json_strdup(const struct spdk_json_val *val);
 
-int spdk_json_number_to_double(const struct spdk_json_val *val, double *num);
+int spdk_json_number_to_uint16(const struct spdk_json_val *val, uint16_t *num);
 int spdk_json_number_to_int32(const struct spdk_json_val *val, int32_t *num);
 int spdk_json_number_to_uint32(const struct spdk_json_val *val, uint32_t *num);
+int spdk_json_number_to_uint64(const struct spdk_json_val *val, uint64_t *num);
 
 struct spdk_json_write_ctx;
 
@@ -197,8 +205,31 @@ int spdk_json_write_int64(struct spdk_json_write_ctx *w, int64_t val);
 int spdk_json_write_uint64(struct spdk_json_write_ctx *w, uint64_t val);
 int spdk_json_write_string(struct spdk_json_write_ctx *w, const char *val);
 int spdk_json_write_string_raw(struct spdk_json_write_ctx *w, const char *val, size_t len);
+
+/**
+ * Write null-terminated UTF-16LE string.
+ *
+ * \param w JSON write context.
+ * \param val UTF-16LE string; must be null terminated.
+ * \return 0 on success or negative on failure.
+ */
+int spdk_json_write_string_utf16le(struct spdk_json_write_ctx *w, const uint16_t *val);
+
+/**
+ * Write UTF-16LE string.
+ *
+ * \param w JSON write context.
+ * \param val UTF-16LE string; may contain embedded null characters.
+ * \param len Length of val in 16-bit code units (i.e. size of string in bytes divided by 2).
+ * \return 0 on success or negative on failure.
+ */
+int spdk_json_write_string_utf16le_raw(struct spdk_json_write_ctx *w, const uint16_t *val,
+				       size_t len);
+
 int spdk_json_write_string_fmt(struct spdk_json_write_ctx *w, const char *fmt,
 			       ...) __attribute__((__format__(__printf__, 2, 3)));
+int spdk_json_write_string_fmt_v(struct spdk_json_write_ctx *w, const char *fmt, va_list args);
+
 int spdk_json_write_array_begin(struct spdk_json_write_ctx *w);
 int spdk_json_write_array_end(struct spdk_json_write_ctx *w);
 int spdk_json_write_object_begin(struct spdk_json_write_ctx *w);
@@ -215,4 +246,92 @@ int spdk_json_write_val(struct spdk_json_write_ctx *w, const struct spdk_json_va
  */
 int spdk_json_write_val_raw(struct spdk_json_write_ctx *w, const void *data, size_t len);
 
+/* Utility functions */
+int spdk_json_write_named_null(struct spdk_json_write_ctx *w, const char *name);
+int spdk_json_write_named_bool(struct spdk_json_write_ctx *w, const char *name, bool val);
+int spdk_json_write_named_int32(struct spdk_json_write_ctx *w, const char *name, int32_t val);
+int spdk_json_write_named_uint32(struct spdk_json_write_ctx *w, const char *name, uint32_t val);
+int spdk_json_write_named_uint64(struct spdk_json_write_ctx *w, const char *name, uint64_t val);
+int spdk_json_write_named_int64(struct spdk_json_write_ctx *w, const char *name, int64_t val);
+int spdk_json_write_named_string(struct spdk_json_write_ctx *w, const char *name, const char *val);
+int spdk_json_write_named_string_fmt(struct spdk_json_write_ctx *w, const char *name,
+				     const char *fmt, ...) __attribute__((__format__(__printf__, 3, 4)));
+int spdk_json_write_named_string_fmt_v(struct spdk_json_write_ctx *w, const char *name,
+				       const char *fmt, va_list args);
+
+int spdk_json_write_named_array_begin(struct spdk_json_write_ctx *w, const char *name);
+int spdk_json_write_named_object_begin(struct spdk_json_write_ctx *w, const char *name);
+
+/**
+ * Return JSON value asociated with key \c key_name. Subobjects won't be searched.
+ *
+ * \param object JSON object to be examined
+ * \param key_name name of the key
+ * \param key optional, will be set with found key
+ * \param val optional, will be set with value of the key
+ * \param type search for specific value type. Pass SPDK_JSON_VAL_ANY to match any type.
+ * \return 0 if found or negative error code:
+ * -EINVAL - json object is invalid
+ * -ENOENT - key not found
+ * -EDOM - key exists but value type mismatch.
+ */
+int spdk_json_find(struct spdk_json_val *object, const char *key_name, struct spdk_json_val **key,
+		   struct spdk_json_val **val, enum spdk_json_val_type type);
+
+/**
+ * The same as calling \c spdk_json_find() function with \c type set to \c SPDK_JSON_VAL_STRING
+ *
+ * \param object JSON object to be examined
+ * \param key_name name of the key
+ * \param key optional, will be set with found key
+ * \param val optional, will be set with value of the key
+ * \return See \c spdk_json_find
+ */
+
+int spdk_json_find_string(struct spdk_json_val *object, const char *key_name,
+			  struct spdk_json_val **key, struct spdk_json_val **val);
+
+/**
+ * The same as calling \c spdk_json_key() function with \c type set to \c SPDK_JSON_VAL_ARRAY_BEGIN
+ *
+ * \param object JSON object to be examined
+ * \param key_name name of the key
+ * \param key optional, will be set with found key
+ * \param value optional, will be set with key value
+ * \return See \c spdk_json_find
+ */
+int spdk_json_find_array(struct spdk_json_val *object, const char *key_name,
+			 struct spdk_json_val **key, struct spdk_json_val **value);
+
+/**
+ * Return first JSON value in given JSON object.
+ *
+ * \param object pointer to JSON object begin
+ * \return Pointer to first object or NULL if object is empty or is not an JSON object
+ */
+struct spdk_json_val *spdk_json_object_first(struct spdk_json_val *object);
+
+/**
+ * Return first JSON value in array.
+ *
+ * \param array_begin pointer to JSON array begin
+ * \return Pointer to first JSON value or NULL if array is empty or is not an JSON array.
+ */
+
+struct spdk_json_val *spdk_json_array_first(struct spdk_json_val *array_begin);
+
+/**
+ * Advance to the next JSON value in JSON object or array.
+ *
+ * \warning if \c pos is not JSON key or JSON array element behaviour is undefined.
+ *
+ * \param pos pointer to JSON key if iterating over JSON object or array element
+ * \return next JSON value or NULL if there is no more objects or array elements
+ */
+struct spdk_json_val *spdk_json_next(struct spdk_json_val *pos);
+
+#ifdef __cplusplus
+}
+#endif
+
 #endif
diff --git a/PDK/core/src/api/include/udd/spdk/jsonrpc.h b/PDK/core/src/api/include/udd/spdk/jsonrpc.h
index 2486382..127e30d 100644
--- a/PDK/core/src/api/include/udd/spdk/jsonrpc.h
+++ b/PDK/core/src/api/include/udd/spdk/jsonrpc.h
@@ -43,39 +43,311 @@
 
 #include "spdk/json.h"
 
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/* Defined error codes in JSON-RPC specification 2.0 */
 #define SPDK_JSONRPC_ERROR_PARSE_ERROR		-32700
 #define SPDK_JSONRPC_ERROR_INVALID_REQUEST	-32600
 #define SPDK_JSONRPC_ERROR_METHOD_NOT_FOUND	-32601
 #define SPDK_JSONRPC_ERROR_INVALID_PARAMS	-32602
 #define SPDK_JSONRPC_ERROR_INTERNAL_ERROR	-32603
 
+/* Custom error codes in SPDK
+
+ * Error codes from and including -32768 to -32000 are reserved for
+ * predefined errors, hence custom error codes must be outside of the range.
+ */
+#define SPDK_JSONRPC_ERROR_INVALID_STATE	-1
+
 struct spdk_jsonrpc_server;
-struct spdk_jsonrpc_server_conn;
+struct spdk_jsonrpc_request;
+
+struct spdk_jsonrpc_client;
+struct spdk_jsonrpc_client_request;
+
+struct spdk_jsonrpc_client_response {
+	struct spdk_json_val *version;
+	struct spdk_json_val *id;
+	struct spdk_json_val *result;
+	struct spdk_json_val *error;
+};
 
 /**
  * User callback to handle a single JSON-RPC request.
  *
  * The user should respond by calling one of spdk_jsonrpc_begin_result() or
- *  spdk_jsonrpc_send_error_response().
+ * spdk_jsonrpc_send_error_response().
+ *
+ * \param request JSON-RPC request to handle.
+ * \param method Function to handle the request.
+ * \param param Parameters passed to the function 'method'.
  */
 typedef void (*spdk_jsonrpc_handle_request_fn)(
-	struct spdk_jsonrpc_server_conn *conn,
+	struct spdk_jsonrpc_request *request,
 	const struct spdk_json_val *method,
-	const struct spdk_json_val *params,
-	const struct spdk_json_val *id);
+	const struct spdk_json_val *params);
+
+struct spdk_jsonrpc_server_conn;
+
+typedef void (*spdk_jsonrpc_conn_closed_fn)(struct spdk_jsonrpc_server_conn *conn, void *arg);
 
+/**
+ * Function for specific RPC method response parsing handlers.
+ *
+ * \param parser_ctx context where analysis are put.
+ * \param result json values responsed to this method.
+ *
+ * \return 0 on success.
+ *         SPDK_JSON_PARSE_INVALID on failure.
+ */
+typedef int (*spdk_jsonrpc_client_response_parser)(
+	void *parser_ctx,
+	const struct spdk_json_val *result);
+
+/**
+ * Create a JSON-RPC server listening on the required address.
+ *
+ * \param domain Socket family.
+ * \param protocol Protocol.
+ * \param listen_addr Listening address.
+ * \param addrlen Length of address.
+ * \param handle_request User callback to handle a JSON-RPC request.
+ *
+ * \return a pointer to the JSON-RPC server.
+ */
 struct spdk_jsonrpc_server *spdk_jsonrpc_server_listen(int domain, int protocol,
 		struct sockaddr *listen_addr, socklen_t addrlen, spdk_jsonrpc_handle_request_fn handle_request);
 
+/**
+ * Poll the requests to the JSON-RPC server.
+ *
+ * This function does accept, receive, handle the requests and reply to them.
+ *
+ * \param server JSON-RPC server.
+ *
+ * \return 0 on success.
+ */
 int spdk_jsonrpc_server_poll(struct spdk_jsonrpc_server *server);
 
+/**
+ * Shutdown the JSON-RPC server.
+ *
+ * \param server JSON-RPC server.
+ */
 void spdk_jsonrpc_server_shutdown(struct spdk_jsonrpc_server *server);
 
-struct spdk_json_write_ctx *spdk_jsonrpc_begin_result(struct spdk_jsonrpc_server_conn *conn,
-		const struct spdk_json_val *id);
-void spdk_jsonrpc_end_result(struct spdk_jsonrpc_server_conn *conn, struct spdk_json_write_ctx *w);
+/**
+ * Return connection associated to \c request
+ *
+ * \param request JSON-RPC request
+ * \return JSON RPC server connection
+ */
+struct spdk_jsonrpc_server_conn *spdk_jsonrpc_get_conn(struct spdk_jsonrpc_request *request);
+
+/**
+ * Add callback called when connection is closed. Pair of  \c cb and \c ctx must be unique or error is returned.
+ * Registered callback is called only once and there is no need to call  \c spdk_jsonrpc_conn_del_close_cb
+ * inside from \c cb.
+ *
+ * \note Current implementation allow only one close callback per connection.
+ *
+ * \param conn JSON RPC server connection
+ * \param cb calback function
+ * \param ctx argument for \c cb
+ *
+ * \return 0 on success, or negated errno code:
+ *  -EEXIST \c cb and \c ctx is already registered
+ *  -ENOTCONN Callback can't be added because connection is closed.
+ *  -ENOSPC no more space to register callback.
+ */
+int spdk_jsonrpc_conn_add_close_cb(struct spdk_jsonrpc_server_conn *conn,
+				   spdk_jsonrpc_conn_closed_fn cb, void *ctx);
+
+/**
+ * Remove registered close callback.
+ *
+ * \param conn JSON RPC server connection
+ * \param cb calback function
+ * \param ctx argument for \c cb
+ *
+ * \return 0 on success, or negated errno code:
+ *  -ENOENT \c cb and \c ctx pair is not registered
+ */
+int spdk_jsonrpc_conn_del_close_cb(struct spdk_jsonrpc_server_conn *conn,
+				   spdk_jsonrpc_conn_closed_fn cb, void *ctx);
+
+/**
+ * Begin building a response to a JSON-RPC request.
+ *
+ * If this function returns non-NULL, the user must call spdk_jsonrpc_end_result()
+ * on the request after writing the desired response object to the spdk_json_write_ctx.
+ *
+ * \param request JSON-RPC request to respond to.
+
+ * \return JSON write context to write the response object to, or NULL if no
+ * response is necessary.
+ */
+struct spdk_json_write_ctx *spdk_jsonrpc_begin_result(struct spdk_jsonrpc_request *request);
+
+/**
+ * Complete and send a JSON-RPC response.
+ *
+ * \param request Request to complete the response for.
+ * \param w JSON write context returned from spdk_jsonrpc_begin_result().
+ */
+void spdk_jsonrpc_end_result(struct spdk_jsonrpc_request *request, struct spdk_json_write_ctx *w);
+
+/**
+ * Send an error response to a JSON-RPC request.
+ *
+ * This is shorthand for spdk_jsonrpc_begin_result() + spdk_jsonrpc_end_result()
+ * with an error object.
+ *
+ * \param request JSON-RPC request to respond to.
+ * \param error_code Integer error code to return (may be one of the
+ * SPDK_JSONRPC_ERROR_ errors, or a custom error code).
+ * \param msg String error message to return.
+ */
+void spdk_jsonrpc_send_error_response(struct spdk_jsonrpc_request *request,
+				      int error_code, const char *msg);
+
+/**
+ * Send an error response to a JSON-RPC request.
+ *
+ * This is shorthand for printf() + spdk_jsonrpc_send_error_response().
+ *
+ * \param request JSON-RPC request to respond to.
+ * \param error_code Integer error code to return (may be one of the
+ * SPDK_JSONRPC_ERROR_ errors, or a custom error code).
+ * \param fmt Printf-like format string.
+ */
+void spdk_jsonrpc_send_error_response_fmt(struct spdk_jsonrpc_request *request,
+		int error_code, const char *fmt, ...) __attribute__((format(printf, 3, 4)));
 
-void spdk_jsonrpc_send_error_response(struct spdk_jsonrpc_server_conn *conn,
-				      const struct spdk_json_val *id, int error_code, const char *msg);
+/**
+ * Begin building a JSON-RPC request.
+ *
+ * If this function returns non-NULL, the user must call spdk_jsonrpc_end_request()
+ * on the request after writing the desired request object to the spdk_json_write_ctx.
+ *
+ * \param request JSON-RPC request.
+ * \param id ID index for the request. If < 0 skip ID.
+ * \param method Name of the RPC method. If NULL caller will have to create "method" key.
+ *
+ * \return JSON write context or NULL in case of error.
+ */
+struct spdk_json_write_ctx *
+spdk_jsonrpc_begin_request(struct spdk_jsonrpc_client_request *request, int32_t id,
+			   const char *method);
+
+/**
+ * Complete a JSON-RPC request.
+ *
+ * \param request JSON-RPC request.
+ * \param w JSON write context returned from spdk_jsonrpc_begin_request().
+ */
+void spdk_jsonrpc_end_request(struct spdk_jsonrpc_client_request *request,
+			      struct spdk_json_write_ctx *w);
+
+/**
+ * Connect to the specified RPC server.
+ *
+ * \param addr RPC socket address.
+ * \param addr_family Protocol families of address.
+ *
+ * \return JSON-RPC client on success, NULL on failure and errno set to indicate
+ * the cause of the error.
+ */
+struct spdk_jsonrpc_client *spdk_jsonrpc_client_connect(const char *addr, int addr_family);
+
+/**
+ * Close JSON-RPC connection and free \c client object.
+ *
+ * This function is not thread safe and should only be called from one thread at
+ * a time while no other threads are actively \c client object.
+ *
+ * \param client JSON-RPC client.
+ */
+void spdk_jsonrpc_client_close(struct spdk_jsonrpc_client *client);
+
+/**
+ * Create one JSON-RPC request. Returned request must be passed to
+ * \c spdk_jsonrpc_client_send_request when done or to \c spdk_jsonrpc_client_free_request
+ * if discaded.
+ *
+ * \return pointer to JSON-RPC request object.
+ */
+struct spdk_jsonrpc_client_request *spdk_jsonrpc_client_create_request(void);
+
+/**
+ * Free one JSON-RPC request.
+ *
+ * \param req pointer to JSON-RPC request object.
+ */
+void spdk_jsonrpc_client_free_request(struct spdk_jsonrpc_client_request *req);
+
+/**
+ * Send the JSON-RPC request in JSON-RPC client. Library takes ownership of the
+ * request object and will free it when done.
+ *
+ * This function is not thread safe and should only be called from one thread at
+ * a time while no other threads are actively \c client object.
+ *
+ * \param client JSON-RPC client.
+ * \param req JSON-RPC request.
+ *
+ * \return 0 on success or negative error code.
+ * -ENOSPC - no space left to queue another request. Try again later.
+ */
+int spdk_jsonrpc_client_send_request(struct spdk_jsonrpc_client *client,
+				     struct spdk_jsonrpc_client_request *req);
+
+/**
+ * Poll the JSON-RPC client. When any response is available use
+ * \c spdk_jsonrpc_client_get_response to retrieve it.
+ *
+ * This function is not thread safe and should only be called from one thread at
+ * a time while no other threads are actively \c client object.
+ *
+ * \param client JSON-RPC client.
+ * \param timeout Time in miliseconds this function will block. -1 block forever, 0 don't block.
+ *
+ * \return If no error occurred, this function returns a non-negative number indicating how
+ * many ready responses can be retrieved. If an error occurred, this function returns one of
+ * the following negated errno values:
+ *  -ENOTCONN - not connected yet. Try again later.
+ *  -EINVAL - response is detected to be invalid. Client connection should be terminated.
+ *  -ENOSPC - no space to receive another response. User need to retrieve waiting responses.
+ *  -EIO - connection terminated (or other critical error). Client connection should be terminated.
+ *  -ENOMEM - out of memory
+ */
+int spdk_jsonrpc_client_poll(struct spdk_jsonrpc_client *client, int timeout);
+
+/**
+ * Return JSON RPC response object representing next available response from client connection.
+ * Returned pointer must be freed using \c spdk_jsonrpc_client_free_response
+ *
+ * This function is not thread safe and should only be called from one thread at
+ * a time while no other threads are actively \c client object.
+ *
+ * \param client
+ * \return pointer to JSON RPC response object or NULL if no response available.
+ */
+struct spdk_jsonrpc_client_response *spdk_jsonrpc_client_get_response(struct spdk_jsonrpc_client
+		*client);
+
+/**
+ * Free response object obtained from \c spdk_jsonrpc_client_get_response
+ *
+ * \param resp pointer to JSON RPC response object. If NULL no operation is performed.
+ */
+void spdk_jsonrpc_client_free_response(struct spdk_jsonrpc_client_response *resp);
+
+
+#ifdef __cplusplus
+}
+#endif
 
 #endif
diff --git a/PDK/core/src/api/include/udd/spdk/kvnvme_spdk.h b/PDK/core/src/api/include/udd/spdk/kvnvme_spdk.h
index a0b1669..3c32201 100644
--- a/PDK/core/src/api/include/udd/spdk/kvnvme_spdk.h
+++ b/PDK/core/src/api/include/udd/spdk/kvnvme_spdk.h
@@ -42,13 +42,12 @@ extern "C" {
 
 #define KV_MAX_KEY_SIZE (255)
 #define KV_MIN_KEY_SIZE (4)
+#define KV_MAX_VALUE_SIZE (1<<21)
 #define KV_MAX_EMBED_KEY_SIZE (16)
-#define KV_MAX_VALUE_SIZE (4<<20)
 
 enum spdk_nvme_samsung_nvm_opcode {
 
   SPDK_NVME_OPC_KV_STORE      = 0x81,
-  SPDK_NVME_OPC_KV_STORE_BATCH  = 0x85,
   SPDK_NVME_OPC_KV_RETRIEVE   = 0x90,
   SPDK_NVME_OPC_KV_DELETE     = 0xA1,
   SPDK_NVME_OPC_KV_ITERATE_REQUEST	= 0xB1,
@@ -268,36 +267,7 @@ spdk_nvme_kv_cmd_iterate_close (struct spdk_nvme_ns *ns, struct spdk_nvme_qpair
 			      uint8_t iterator,
 			      spdk_nvme_cmd_cb cb_fn, void *cb_arg,
 		              uint32_t io_flags, uint8_t  option);
-
-/**
- * \brief Submits a KV Batch Request command with batch option
- *
- * \param ns NVMe namespace to submit the KV Iterate I/O
- * \param qpair I/O queue pair to submit the request
- * \param buffer Batch command payload buffer
- * \param buffer_size Batch command payload buffer size
- * \param cmd_cnt Sub-command number in batch command
- * \param cb_fn callback function to invoke when the I/O is completed
- * \param cb_arg argument to pass to the callback function
- * \param io_flags set flags, defined by the SPDK_NVME_IO_FLAGS_* entries
- *                      in spdk/nvme_spec.h, for this I/O.
- * \param option option to pass to NVMe command
- *       0 - no atomicity; 1 - atomicity
- *
- * \return 0 if successfully submitted, KV_ERR_DD_NO_AVAILABLE_RESOURCE if an nvme_request
- *           structure cannot be allocated for the I/O request, KV_ERR_DD_INVALID_PARAM if
- *           param error.
- *
- * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
- */
-
-int
-spdk_nvme_kv_batch_cmd_store(struct spdk_nvme_ns *ns,
-    struct spdk_nvme_qpair *qpair, void *buffer, uint32_t buffer_size,
-    uint32_t cmd_cnt, spdk_nvme_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
-    uint8_t option);
-
+ 
 /**
  * \brief Submits a KV Iterate I/O to the specified NVMe namespace.
  * \param ns NVMe namespace to submit the KV Iterate I/O
diff --git a/PDK/core/src/api/include/udd/spdk/log.h b/PDK/core/src/api/include/udd/spdk/log.h
index 1348415..bc71861 100644
--- a/PDK/core/src/api/include/udd/spdk/log.h
+++ b/PDK/core/src/api/include/udd/spdk/log.h
@@ -41,39 +41,152 @@
 
 #include "spdk/stdinc.h"
 
-/*
- * Default: 1 - noticelog messages will print to stderr and syslog.
- * Can be set to 0 to print noticelog messages to syslog only.
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+typedef void logfunc(int level, const char *file, const int line,
+		     const char *func, const char *format);
+
+/**
+ * Initialize the logging module. Messages prior
+ * to this call will be dropped.
+ */
+void spdk_log_open(logfunc *logf);
+
+/**
+ * Close the currently active log. Messages after this call
+ * will be dropped.
+ */
+void spdk_log_close(void);
+
+enum spdk_log_level {
+	/** All messages will be suppressed. */
+	SPDK_LOG_DISABLED = -1,
+	SPDK_LOG_ERROR,
+	SPDK_LOG_WARN,
+	SPDK_LOG_NOTICE,
+	SPDK_LOG_INFO,
+	SPDK_LOG_DEBUG,
+};
+
+/**
+ * Set the log level threshold to log messages. Messages with a higher
+ * level than this are ignored.
+ *
+ * \param level Log level threshold to set to log messages.
+ */
+void spdk_log_set_level(enum spdk_log_level level);
+
+/**
+ * Get the current log level threshold.
+ *
+ * \return the current log level threshold.
  */
-extern unsigned int spdk_g_notice_stderr_flag;
+enum spdk_log_level spdk_log_get_level(void);
+
+/**
+ * Set the log level threshold to include stack trace in log messages.
+ * Messages with a higher level than this will not contain stack trace. You
+ * can use \c SPDK_LOG_DISABLED to completely disable stack trace printing
+ * even if it is supported.
+ *
+ * \note This function has no effect if SPDK is built without stack trace
+ *  printing support.
+ *
+ * \param level Log level threshold for stacktrace.
+ */
+void spdk_log_set_backtrace_level(enum spdk_log_level level);
+
+/**
+ * Get the current log level threshold for showing stack trace in log message.
+ *
+ * \return the current log level threshold for stack trace.
+ */
+enum spdk_log_level spdk_log_get_backtrace_level(void);
+
+/**
+ * Set the current log level threshold for printing to stderr.
+ * Messages with a level less than or equal to this level
+ * are also printed to stderr. You can use \c SPDK_LOG_DISABLED to completely
+ * suppress log printing.
+ *
+ * \param level Log level threshold for printing to stderr.
+ */
+void spdk_log_set_print_level(enum spdk_log_level level);
+
+/**
+ * Get the current log level print threshold.
+ *
+ * \return the current log level print threshold.
+ */
+enum spdk_log_level spdk_log_get_print_level(void);
 
 #define SPDK_NOTICELOG(...) \
-	spdk_noticelog(NULL, 0, NULL, __VA_ARGS__)
+	spdk_log(SPDK_LOG_NOTICE, __FILE__, __LINE__, __func__, __VA_ARGS__)
 #define SPDK_WARNLOG(...) \
-	spdk_warnlog(NULL, 0, NULL, __VA_ARGS__)
+	spdk_log(SPDK_LOG_WARN, __FILE__, __LINE__, __func__, __VA_ARGS__)
 #define SPDK_ERRLOG(...) \
-	spdk_errlog(__FILE__, __LINE__, __func__, __VA_ARGS__)
-
-int spdk_set_log_facility(const char *facility);
-const char *spdk_get_log_facility(void);
-int spdk_set_log_priority(const char *priority);
-void spdk_noticelog(const char *file, const int line, const char *func,
-		    const char *format, ...) __attribute__((__format__(__printf__, 4, 5)));
-void spdk_warnlog(const char *file, const int line, const char *func,
-		  const char *format, ...) __attribute__((__format__(__printf__, 4, 5)));
-void spdk_tracelog(const char *flag, const char *file, const int line,
-		   const char *func, const char *format, ...) __attribute__((__format__(__printf__, 5, 6)));
-void spdk_errlog(const char *file, const int line, const char *func,
-		 const char *format, ...) __attribute__((__format__(__printf__, 4, 5)));
-void spdk_trace_dump(const char *label, const uint8_t *buf, size_t len);
-
-bool spdk_log_get_trace_flag(const char *flag);
-int spdk_log_set_trace_flag(const char *flag);
-int spdk_log_clear_trace_flag(const char *flag);
-
-void spdk_open_log(void);
-void spdk_close_log(void);
-
-void spdk_tracelog_usage(FILE *f, const char *trace_arg);
+	spdk_log(SPDK_LOG_ERROR, __FILE__, __LINE__, __func__, __VA_ARGS__)
+
+/**
+ * Write messages to the log file. If \c level is set to \c SPDK_LOG_DISABLED,
+ * this log message won't be written.
+ *
+ * \param level Log level threshold.
+ * \param file Name of the current source file.
+ * \param line Current source line number.
+ * \param func Current source function name.
+ * \param format Format string to the message.
+ */
+void spdk_log(enum spdk_log_level level, const char *file, const int line, const char *func,
+	      const char *format, ...) __attribute__((__format__(__printf__, 5, 6)));
+
+/**
+ * Log the contents of a raw buffer to a file.
+ *
+ * \param fp File to hold the log.
+ * \param label Label to print to the file.
+ * \param buf Buffer that holds the log information.
+ * \param len Length of buffer to dump.
+ */
+void spdk_log_dump(FILE *fp, const char *label, const void *buf, size_t len);
+
+/**
+ * Check whether the log flag exists and is enabled.
+ *
+ * \return true if enabled, or false otherwise.
+ */
+bool spdk_log_get_flag(const char *flag);
+
+/**
+ * Enable the log flag.
+ *
+ * \param flag Log flag to be enabled.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_log_set_flag(const char *flag);
+
+/**
+ * Clear a log flag.
+ *
+ * \param flag Log flag to clear.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_log_clear_flag(const char *flag);
+
+/**
+ * Show all the log flags and their usage.
+ *
+ * \param f File to hold all the flags' information.
+ * \param log_arg Command line option to set/enable the log flag.
+ */
+void spdk_log_usage(FILE *f, const char *log_arg);
+
+#ifdef __cplusplus
+}
+#endif
 
 #endif /* SPDK_LOG_H */
diff --git a/PDK/core/src/api/include/udd/spdk/lvol.h b/PDK/core/src/api/include/udd/spdk/lvol.h
new file mode 100644
index 0000000..ca271a6
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/lvol.h
@@ -0,0 +1,299 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/** \file
+ * Logical Volume Interface
+ */
+
+#ifndef SPDK_LVOL_H
+#define SPDK_LVOL_H
+
+#include "spdk/stdinc.h"
+#include "spdk/blob.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+struct spdk_bs_dev;
+struct spdk_lvol_store;
+struct spdk_lvol;
+
+enum lvol_clear_method {
+	LVOL_CLEAR_WITH_DEFAULT = BLOB_CLEAR_WITH_DEFAULT,
+	LVOL_CLEAR_WITH_NONE = BLOB_CLEAR_WITH_NONE,
+	LVOL_CLEAR_WITH_UNMAP = BLOB_CLEAR_WITH_UNMAP,
+	LVOL_CLEAR_WITH_WRITE_ZEROES = BLOB_CLEAR_WITH_WRITE_ZEROES,
+};
+
+enum lvs_clear_method {
+	LVS_CLEAR_WITH_UNMAP = BS_CLEAR_WITH_UNMAP,
+	LVS_CLEAR_WITH_WRITE_ZEROES = BS_CLEAR_WITH_WRITE_ZEROES,
+	LVS_CLEAR_WITH_NONE = BS_CLEAR_WITH_NONE,
+};
+
+/* Must include null terminator. */
+#define SPDK_LVS_NAME_MAX	64
+#define SPDK_LVOL_NAME_MAX	64
+
+/**
+ * Parameters for lvolstore initialization.
+ */
+struct spdk_lvs_opts {
+	uint32_t		cluster_sz;
+	enum lvs_clear_method	clear_method;
+	char			name[SPDK_LVS_NAME_MAX];
+};
+
+/**
+ * Initialize an spdk_lvs_opts structure to the defaults.
+ *
+ * \param opts Pointer to the spdk_lvs_opts structure to initialize.
+ */
+void spdk_lvs_opts_init(struct spdk_lvs_opts *opts);
+
+/**
+ * Callback definition for lvolstore operations, including handle to lvs.
+ *
+ * \param cb_arg Custom arguments
+ * \param lvol_store Handle to lvol_store or NULL when lvserrno is set
+ * \param lvserrno Error
+ */
+typedef void (*spdk_lvs_op_with_handle_complete)(void *cb_arg, struct spdk_lvol_store *lvol_store,
+		int lvserrno);
+
+/**
+ * Callback definition for lvolstore operations without handle.
+ *
+ * \param cb_arg Custom arguments
+ * \param lvserrno Error
+ */
+typedef void (*spdk_lvs_op_complete)(void *cb_arg, int lvserrno);
+
+
+/**
+ * Callback definition for lvol operations with handle to lvol.
+ *
+ * \param cb_arg Custom arguments
+ * \param lvol Handle to lvol or NULL when lvserrno is set
+ * \param lvolerrno Error
+ */
+typedef void (*spdk_lvol_op_with_handle_complete)(void *cb_arg, struct spdk_lvol *lvol,
+		int lvolerrno);
+
+/**
+ * Callback definition for lvol operations without handle to lvol.
+ *
+ * \param cb_arg Custom arguments
+ * \param lvolerrno Error
+ */
+typedef void (*spdk_lvol_op_complete)(void *cb_arg, int lvolerrno);
+
+/**
+ * Initialize lvolstore on given bs_bdev.
+ *
+ * \param bs_dev This is created on the given bdev by using spdk_bdev_create_bs_dev()
+ * beforehand.
+ * \param o Options for lvolstore.
+ * \param cb_fn Completion callback.
+ * \param cb_arg Completion callback custom arguments.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_lvs_init(struct spdk_bs_dev *bs_dev, struct spdk_lvs_opts *o,
+		  spdk_lvs_op_with_handle_complete cb_fn, void *cb_arg);
+
+/**
+ * Rename the given lvolstore.
+ *
+ * \param lvs Pointer to lvolstore.
+ * \param new_name New name of lvs.
+ * \param cb_fn Completion callback.
+ * \param cb_arg Completion callback custom arguments.
+ */
+void spdk_lvs_rename(struct spdk_lvol_store *lvs, const char *new_name,
+		     spdk_lvs_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Unload lvolstore.
+ *
+ * All lvols have to be closed beforehand, when doing unload.
+ *
+ * \param lvol_store Handle to lvolstore.
+ * \param cb_fn Completion callback.
+ * \param cb_arg Completion callback custom arguments.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_lvs_unload(struct spdk_lvol_store *lvol_store,
+		    spdk_lvs_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Destroy lvolstore.
+ *
+ * All lvols have to be closed beforehand, when doing destroy.
+ *
+ * \param lvol_store Handle to lvolstore.
+ * \param cb_fn Completion callback.
+ * \param cb_arg Completion callback custom arguments.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_lvs_destroy(struct spdk_lvol_store *lvol_store,
+		     spdk_lvs_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Create lvol on given lvolstore with specified size.
+ *
+ * \param lvs Handle to lvolstore.
+ * \param name Name of lvol.
+ * \param sz size of lvol in bytes.
+ * \param thin_provisioned Enables thin provisioning.
+ * \param clear_method Changes default data clusters clear method
+ * \param cb_fn Completion callback.
+ * \param cb_arg Completion callback custom arguments.
+ *
+ * \return 0 on success, negative errno on failure.
+ */
+int spdk_lvol_create(struct spdk_lvol_store *lvs, const char *name, uint64_t sz,
+		     bool thin_provisioned, enum lvol_clear_method clear_method,
+		     spdk_lvol_op_with_handle_complete cb_fn, void *cb_arg);
+/**
+ * Create snapshot of given lvol.
+ *
+ * \param lvol Handle to lvol.
+ * \param snapshot_name Name of created snapshot.
+ * \param cb_fn Completion callback.
+ * \param cb_arg Completion callback custom arguments.
+ */
+void spdk_lvol_create_snapshot(struct spdk_lvol *lvol, const char *snapshot_name,
+			       spdk_lvol_op_with_handle_complete cb_fn, void *cb_arg);
+
+/**
+ * Create clone of given snapshot.
+ *
+ * \param lvol Handle to lvol snapshot.
+ * \param clone_name Name of created clone.
+ * \param cb_fn Completion callback.
+ * \param cb_arg Completion callback custom arguments.
+ */
+void spdk_lvol_create_clone(struct spdk_lvol *lvol, const char *clone_name,
+			    spdk_lvol_op_with_handle_complete cb_fn, void *cb_arg);
+
+/**
+ * Rename lvol with new_name.
+ *
+ * \param lvol Handle to lvol.
+ * \param new_name new name for lvol.
+ * \param cb_fn Completion callback.
+ * \param cb_arg Completion callback custom arguments.
+ */
+void
+spdk_lvol_rename(struct spdk_lvol *lvol, const char *new_name,
+		 spdk_lvol_op_complete cb_fn, void *cb_arg);
+
+/**
+ * \brief Returns if it is possible to delete an lvol (i.e. lvol is not a snapshot that have at least one clone).
+ * \param lvol Handle to lvol
+ */
+bool spdk_lvol_deletable(struct spdk_lvol *lvol);
+
+/**
+ * Close lvol and remove information about lvol from its lvolstore.
+ *
+ * \param lvol Handle to lvol.
+ * \param cb_fn Completion callback.
+ * \param cb_arg Completion callback custom arguments.
+ */
+void spdk_lvol_destroy(struct spdk_lvol *lvol, spdk_lvol_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Close lvol, but information is kept on lvolstore.
+ *
+ * \param lvol Handle to lvol.
+ * \param cb_fn Completion callback.
+ * \param cb_arg Completion callback custom arguments.
+ */
+void spdk_lvol_close(struct spdk_lvol *lvol, spdk_lvol_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Get I/O channel of bdev associated with specified lvol.
+ *
+ * \param lvol Handle to lvol.
+ *
+ * \return a pointer to the I/O channel.
+ */
+struct spdk_io_channel *spdk_lvol_get_io_channel(struct spdk_lvol *lvol);
+
+/**
+ * Load lvolstore from the given blobstore device.
+ *
+ * \param bs_dev Pointer to the blobstore device.
+ * \param cb_fn Completion callback.
+ * \param cb_arg Completion callback custom arguments.
+ */
+void spdk_lvs_load(struct spdk_bs_dev *bs_dev, spdk_lvs_op_with_handle_complete cb_fn,
+		   void *cb_arg);
+
+/**
+ * Open a lvol.
+ *
+ * \param lvol Handle to lvol.
+ * \param cb_fn Completion callback.
+ * \param cb_arg Completion callback custom arguments.
+ */
+void spdk_lvol_open(struct spdk_lvol *lvol, spdk_lvol_op_with_handle_complete cb_fn, void *cb_arg);
+
+/**
+ * Inflate lvol
+ *
+ * \param lvol Handle to lvol
+ * \param cb_fn Completion callback
+ * \param cb_arg Completion callback custom arguments
+ */
+void spdk_lvol_inflate(struct spdk_lvol *lvol, spdk_lvol_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Decouple parent of lvol
+ *
+ * \param lvol Handle to lvol
+ * \param cb_fn Completion callback
+ * \param cb_arg Completion callback custom arguments
+ */
+void spdk_lvol_decouple_parent(struct spdk_lvol *lvol, spdk_lvol_op_complete cb_fn, void *cb_arg);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif  /* SPDK_LVOL_H */
diff --git a/PDK/core/src/api/include/udd/spdk/mmio.h b/PDK/core/src/api/include/udd/spdk/mmio.h
index 1dbfa58..68b1660 100644
--- a/PDK/core/src/api/include/udd/spdk/mmio.h
+++ b/PDK/core/src/api/include/udd/spdk/mmio.h
@@ -52,6 +52,34 @@ extern "C" {
 #define SPDK_MMIO_64BIT	0
 #endif
 
+static inline uint8_t
+spdk_mmio_read_1(const volatile uint8_t *addr)
+{
+	spdk_compiler_barrier();
+	return *addr;
+}
+
+static inline void
+spdk_mmio_write_1(volatile uint8_t *addr, uint8_t val)
+{
+	spdk_compiler_barrier();
+	*addr = val;
+}
+
+static inline uint16_t
+spdk_mmio_read_2(const volatile uint16_t *addr)
+{
+	spdk_compiler_barrier();
+	return *addr;
+}
+
+static inline void
+spdk_mmio_write_2(volatile uint16_t *addr, uint16_t val)
+{
+	spdk_compiler_barrier();
+	*addr = val;
+}
+
 static inline uint32_t
 spdk_mmio_read_4(const volatile uint32_t *addr)
 {
diff --git a/PDK/core/src/api/include/udd/spdk/nbd.h b/PDK/core/src/api/include/udd/spdk/nbd.h
new file mode 100644
index 0000000..be57c09
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/nbd.h
@@ -0,0 +1,102 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/** \file
+ * Network block device layer
+ */
+
+#ifndef SPDK_NBD_H_
+#define SPDK_NBD_H_
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+struct spdk_bdev;
+struct spdk_nbd_disk;
+struct spdk_json_write_ctx;
+
+/**
+ * Initialize the network block device layer.
+ *
+ * \return 0 on success.
+ */
+int spdk_nbd_init(void);
+
+/**
+ * Stop and close all the running network block devices.
+ */
+void spdk_nbd_fini(void);
+
+/**
+ * Called when an NBD device has been started.
+ * On success, rc is assigned 0; On failure, rc is assigned negated errno.
+ */
+typedef void (*spdk_nbd_start_cb)(void *cb_arg, struct spdk_nbd_disk *nbd,
+				  int rc);
+
+/**
+ * Start a network block device backed by the bdev.
+ *
+ * \param bdev_name Name of bdev exposed as a network block device.
+ * \param nbd_path Path to the registered network block device.
+ * \param cb_fn Callback to be always called.
+ * \param cb_arg Passed to cb_fn.
+ */
+void spdk_nbd_start(const char *bdev_name, const char *nbd_path,
+		    spdk_nbd_start_cb cb_fn, void *cb_arg);
+
+/**
+ * Stop the running network block device safely.
+ *
+ * \param nbd A pointer to the network block device to stop.
+ */
+void spdk_nbd_stop(struct spdk_nbd_disk *nbd);
+
+/**
+ * Get the local filesystem path used for the network block device.
+ */
+const char *spdk_nbd_get_path(struct spdk_nbd_disk *nbd);
+
+/**
+ * Write NBD subsystem configuration into provided JSON context.
+ *
+ * \param w JSON write context
+ */
+void spdk_nbd_write_config_json(struct spdk_json_write_ctx *w);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/PDK/core/src/api/include/udd/spdk/net.h b/PDK/core/src/api/include/udd/spdk/net.h
index 327a0cc..e493223 100644
--- a/PDK/core/src/api/include/udd/spdk/net.h
+++ b/PDK/core/src/api/include/udd/spdk/net.h
@@ -42,42 +42,79 @@
 
 #include "spdk/queue.h"
 
-#define IDLE_INTERVAL_TIME_IN_US 5000
-
-const char *spdk_net_framework_get_name(void);
-int spdk_net_framework_start(void);
-void spdk_net_framework_clear_socket_association(int sock);
-int spdk_net_framework_fini(void);
-int spdk_net_framework_idle_time(void);
-
-#define SPDK_IFNAMSIZE		32
-#define SPDK_MAX_IP_PER_IFC	32
-
-struct spdk_interface {
-	char name[SPDK_IFNAMSIZE];
-	uint32_t index;
-	uint32_t num_ip_addresses; /* number of IP addresses defined */
-	uint32_t ip_address[SPDK_MAX_IP_PER_IFC];
-	TAILQ_ENTRY(spdk_interface)	tailq;
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+struct spdk_sock;
+
+struct spdk_net_framework {
+	const char *name;
+
+	void (*init)(void);
+	void (*fini)(void);
+
+	STAILQ_ENTRY(spdk_net_framework) link;
 };
 
-int spdk_interface_add_ip_address(int ifc_index, char *ip_addr);
-int spdk_interface_delete_ip_address(int ifc_index, char *ip_addr);
-void *spdk_interface_get_list(void);
+/**
+ * Register a net framework.
+ *
+ * \param frame Net framework to register.
+ */
+void spdk_net_framework_register(struct spdk_net_framework *frame);
+
+#define SPDK_NET_FRAMEWORK_REGISTER(name, frame) \
+static void __attribute__((constructor)) net_framework_register_##name(void) \
+{ \
+	spdk_net_framework_register(frame); \
+}
+
+/**
+ * Initialize the network interfaces by getting information through netlink socket.
+ *
+ * \return 0 on success, 1 on failure.
+ */
+int spdk_interface_init(void);
 
-int spdk_sock_getaddr(int sock, char *saddr, int slen, char *caddr, int clen);
-int spdk_sock_connect(const char *ip, int port);
-int spdk_sock_listen(const char *ip, int port);
-int spdk_sock_accept(int sock);
-int spdk_sock_close(int sock);
-ssize_t spdk_sock_recv(int sock, void *buf, size_t len);
-ssize_t spdk_sock_writev(int sock, struct iovec *iov, int iovcnt);
+/**
+ * Destroy the network interfaces.
+ */
+void spdk_interface_destroy(void);
 
-int spdk_sock_set_recvlowat(int sock, int nbytes);
-int spdk_sock_set_recvbuf(int sock, int sz);
-int spdk_sock_set_sendbuf(int sock, int sz);
+/**
+ * Net framework initialization callback.
+ *
+ * \param cb_arg Callback argument.
+ * \param rc 0 if net framework initialized successfully or negative errno if it failed.
+ */
+typedef void (*spdk_net_init_cb)(void *cb_arg, int rc);
+
+/**
+ * Net framework finish callback.
+ *
+ * \param cb_arg Callback argument.
+ */
+typedef void (*spdk_net_fini_cb)(void *cb_arg);
+
+void spdk_net_framework_init_next(int rc);
+
+/**
+ * Start all registered frameworks.
+ *
+ * \return 0 on success.
+ */
+void spdk_net_framework_start(spdk_net_init_cb cb_fn, void *cb_arg);
+
+void spdk_net_framework_fini_next(void);
+
+/**
+ * Stop all registered frameworks.
+ */
+void spdk_net_framework_fini(spdk_net_fini_cb cb_fn, void *cb_arg);
 
-bool spdk_sock_is_ipv6(int sock);
-bool spdk_sock_is_ipv4(int sock);
+#ifdef __cplusplus
+}
+#endif
 
-#endif /* SPDK_NET_FRAMEWORK_H */
+#endif /* SPDK_NET_H */
diff --git a/PDK/core/src/api/include/udd/spdk/notify.h b/PDK/core/src/api/include/udd/spdk/notify.h
new file mode 100644
index 0000000..fbf5021
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/notify.h
@@ -0,0 +1,128 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef SPDK_NOTIFY_H
+#define SPDK_NOTIFY_H
+
+#include "spdk/stdinc.h"
+#include "spdk/json.h"
+#include "spdk/queue.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * Opaque event type.
+ */
+struct spdk_notify_type;
+
+typedef int (*spdk_notify_get_type_cb)(const struct spdk_notify_type *type, void *ctx);
+
+#define SPDK_NOTIFY_MAX_NAME_SIZE 128
+#define SPDK_NOTIFY_MAX_CTX_SIZE 128
+
+struct spdk_notify_event {
+	char type[SPDK_NOTIFY_MAX_NAME_SIZE];
+	char ctx[SPDK_NOTIFY_MAX_CTX_SIZE];
+};
+
+/**
+ * Callback type for event enumeration.
+ *
+ * \param idx Event index
+ * \param event Event data
+ * \param ctx User context
+ * \return Non zero to break iteration.
+ */
+typedef int (*spdk_notify_get_event_cb)(uint64_t idx, const struct spdk_notify_event *event,
+					void *ctx);
+
+/**
+ * Register \c type as new notification type.
+ *
+ * The \c type must be valid through whole program lifetime (chance being a global variable).
+ *
+ * \note This function is thread safe.
+ *
+ * \param type New notification type to register.
+ * \return registered notification type or NULL on failure.
+ */
+struct spdk_notify_type *spdk_notify_type_register(const char *type);
+
+/**
+ * Return name of the notification type.
+ *
+ * \param type Notification type we are talking about.
+ * \return Name of notification type.
+ */
+const char *spdk_notify_type_get_name(const struct spdk_notify_type *type);
+
+/**
+ * Call cb_fn for all event types.
+ *
+ * \note Whole function call is under lock so user callback should not sleep.
+ * \param cb_fn
+ * \param ctx
+ */
+void spdk_notify_get_types(spdk_notify_get_type_cb cb_fn, void *ctx);
+
+/**
+ * Send given notification.
+ *
+ * \param type Notification type
+ * \param ctx Notification context
+ *
+ * \return Event index.
+ */
+uint64_t spdk_notify_send(const char *type, const char *ctx);
+
+/**
+ * Call cb_fn with events from given range.
+ *
+ * \note Whole function call is under lock so user callback should not sleep.
+ *
+ * \param start_idx First event index
+ * \param cb_fn User callback function. Return non-zero to break iteration.
+ * \param max Maximum number of invocations of user calback function.
+ * \param ctx User context
+ * \return Number of user callback invocations
+ */
+uint64_t spdk_notify_get_events(uint64_t start_idx, uint64_t max, spdk_notify_get_event_cb cb_fn,
+				void *ctx);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* SPDK_NOTIFY_H */
diff --git a/PDK/core/src/api/include/udd/spdk/nvme.h b/PDK/core/src/api/include/udd/spdk/nvme.h
index 9e755d4..4660027 100644
--- a/PDK/core/src/api/include/udd/spdk/nvme.h
+++ b/PDK/core/src/api/include/udd/spdk/nvme.h
@@ -44,6 +44,7 @@
 extern "C" {
 #endif
 
+#include "spdk/config.h"
 #include "spdk/env.h"
 #include "spdk/nvme_spec.h"
 #include "spdk/nvmf_spec.h"
@@ -53,11 +54,13 @@ extern int32_t		spdk_nvme_retry_count;
 
 
 
-/** \brief Opaque handle to a controller. Returned by \ref spdk_nvme_probe()'s attach_cb. */
+/**
+ * Opaque handle to a controller. Returned by spdk_nvme_probe()'s attach_cb.
+ */
 struct spdk_nvme_ctrlr;
 
 /**
- * \brief NVMe controller initialization options.
+ * NVMe controller initialization options.
  *
  * A pointer to this structure will be provided for each probe callback from spdk_nvme_probe() to
  * allow the user to request non-default options, and the actual options enabled on the controller
@@ -116,9 +119,89 @@ struct spdk_nvme_ctrlr_opts {
 	 * or driver-assisted striping.
 	 */
 	uint32_t io_queue_requests;
+
+	/**
+	 * Source address for NVMe-oF connections.
+	 * Set src_addr and src_svcid to empty strings if no source address should be
+	 * specified.
+	 */
+	char src_addr[SPDK_NVMF_TRADDR_MAX_LEN + 1];
+
+	/**
+	 * Source service ID (port) for NVMe-oF connections.
+	 * Set src_addr and src_svcid to empty strings if no source address should be
+	 * specified.
+	 */
+	char src_svcid[SPDK_NVMF_TRSVCID_MAX_LEN + 1];
+
+	/**
+	 * The host identifier to use when connecting to controllers with 64-bit host ID support.
+	 *
+	 * Set to all zeroes to specify that no host ID should be provided to the controller.
+	 */
+	uint8_t host_id[8];
+
+	/**
+	 * The host identifier to use when connecting to controllers with extended (128-bit) host ID support.
+	 *
+	 * Set to all zeroes to specify that no host ID should be provided to the controller.
+	 */
+	uint8_t extended_host_id[16];
+
+	/**
+	 * The I/O command set to select.
+	 *
+	 * If the requested command set is not supported, the controller
+	 * initialization process will not proceed. By default, the NVM
+	 * command set is used.
+	 */
+	enum spdk_nvme_cc_css command_set;
+
+	/**
+	 * Admin commands timeout in milliseconds (0 = no timeout).
+	 *
+	 * The timeout value is used for admin commands submitted internally
+	 * by the nvme driver during initialization, before the user is able
+	 * to call spdk_nvme_ctrlr_register_timeout_callback(). By default,
+	 * this is set to 120 seconds, users can change it in the probing
+	 * callback.
+	 */
+	uint32_t admin_timeout_ms;
+
+	/**
+	 * It is used for TCP transport.
+	 *
+	 * Set to true, means having header digest for the header in the NVMe/TCP PDU
+	 */
+	bool header_digest;
+
+	/**
+	 * It is used for TCP transport.
+	 *
+	 * Set to true, means having data digest for the data in the NVMe/TCP PDU
+	 */
+	bool data_digest;
 };
 
 /**
+ * Indicate whether a ctrlr handle is associated with a Discovery controller.
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ *
+ * \return true if a discovery controller, else false.
+ */
+bool spdk_nvme_ctrlr_is_discovery(struct spdk_nvme_ctrlr *ctrlr);
+
+/**
+ * Get the default options for the creation of a specific NVMe controller.
+ *
+ * \param[out] opts Will be filled with the default option.
+ * \param opts_size Must be set to sizeof(struct spdk_nvme_ctrlr_opts).
+ */
+void spdk_nvme_ctrlr_get_default_ctrlr_opts(struct spdk_nvme_ctrlr_opts *opts,
+		size_t opts_size);
+
+/**
  * NVMe library transports
  *
  * NOTE: These are mapped directly to the NVMe over Fabrics TRTYPE values, except for PCIe,
@@ -137,8 +220,21 @@ enum spdk_nvme_transport_type {
 	 * RDMA Transport (RoCE, iWARP, etc.)
 	 */
 	SPDK_NVME_TRANSPORT_RDMA = SPDK_NVMF_TRTYPE_RDMA,
+
+	/**
+	 * Fibre Channel (FC) Transport
+	 */
+	SPDK_NVME_TRANSPORT_FC = SPDK_NVMF_TRTYPE_FC,
+
+	/**
+	 * TCP Transport
+	 */
+	SPDK_NVME_TRANSPORT_TCP = SPDK_NVMF_TRTYPE_TCP,
 };
 
+/* typedef added for coding style reasons */
+typedef enum spdk_nvme_transport_type spdk_nvme_transport_type_t;
+
 /**
  * NVMe transport identifier.
  *
@@ -164,14 +260,16 @@ struct spdk_nvme_transport_id {
 	 * Transport address of the NVMe-oF endpoint. For transports which use IP
 	 * addressing (e.g. RDMA), this should be an IP address. For PCIe, this
 	 * can either be a zero length string (the whole bus) or a PCI address
-	 * in the format DDDD:BB:DD.FF or DDDD.BB.DD.FF
+	 * in the format DDDD:BB:DD.FF or DDDD.BB.DD.FF. For FC the string is
+	 * formatted as: nn-0xWWNN:pn-0xWWPNâ€ where WWNN is the Node_Name of the
+	 * target NVMe_Port and WWPN is the N_Port_Name of the target NVMe_Port.
 	 */
 	char traddr[SPDK_NVMF_TRADDR_MAX_LEN + 1];
 
 	/**
 	 * Transport service id of the NVMe-oF endpoint.  For transports which use
 	 * IP addressing (e.g. RDMA), this field shoud be the port number. For PCIe,
-	 * this is always a zero length string.
+	 * and FC this is always a zero length string.
 	 */
 	char trsvcid[SPDK_NVMF_TRSVCID_MAX_LEN + 1];
 
@@ -182,66 +280,178 @@ struct spdk_nvme_transport_id {
 };
 
 /**
+ * NVMe host identifier
+ *
+ * Used for defining the host identity for an NVMe-oF connection.
+ *
+ * In terms of configuration, this object can be considered a subtype of TransportID
+ * Please see etc/spdk/nvmf.conf.in for more details.
+ *
+ * A string representation of this type may be converted to this type using
+ * spdk_nvme_host_id_parse().
+ */
+struct spdk_nvme_host_id {
+	/**
+	 * Transport address to be used by the host when connecting to the NVMe-oF endpoint.
+	 * May be an IP address or a zero length string for transports which
+	 * use IP addressing (e.g. RDMA).
+	 * For PCIe and FC this is always a zero length string.
+	 */
+	char hostaddr[SPDK_NVMF_TRADDR_MAX_LEN + 1];
+
+	/**
+	 * Transport service ID used by the host when connecting to the NVMe.
+	 * May be a port number or a zero length string for transports which
+	 * use IP addressing (e.g. RDMA).
+	 * For PCIe and FC this is always a zero length string.
+	 */
+	char hostsvcid[SPDK_NVMF_TRSVCID_MAX_LEN + 1];
+};
+
+/*
+ * Controller support flags
+ *
+ * Used for identifying if the controller supports these flags.
+ */
+enum spdk_nvme_ctrlr_flags {
+	SPDK_NVME_CTRLR_SGL_SUPPORTED			= 0x1, /**< The SGL is supported */
+	SPDK_NVME_CTRLR_SECURITY_SEND_RECV_SUPPORTED	= 0x2, /**< security send/receive is supported */
+};
+
+/**
  * Parse the string representation of a transport ID.
  *
  * \param trid Output transport ID structure (must be allocated and initialized by caller).
  * \param str Input string representation of a transport ID to parse.
- * \return 0 if parsing was successful and trid is filled out, or negated errno values on failure.
  *
- * str must be a zero-terminated C string containing one or more key:value pairs separated by
- * whitespace.
+ * str must be a zero-terminated C string containing one or more key:value pairs
+ * separated by whitespace.
  *
  * Key          | Value
  * ------------ | -----
  * trtype       | Transport type (e.g. PCIe, RDMA)
  * adrfam       | Address family (e.g. IPv4, IPv6)
- * traddr       | Transport address (e.g. 0000:04:00.0 for PCIe or 192.168.100.8 for RDMA)
+ * traddr       | Transport address (e.g. 0000:04:00.0 for PCIe, 192.168.100.8 for RDMA, or WWN for FC)
  * trsvcid      | Transport service identifier (e.g. 4420)
  * subnqn       | Subsystem NQN
  *
- * Unspecified fields of trid are left unmodified, so the caller must initialize trid (for example,
- * memset() to 0) before calling this function.
+ * Unspecified fields of trid are left unmodified, so the caller must initialize
+ * trid (for example, memset() to 0) before calling this function.
+ *
+ * \return 0 if parsing was successful and trid is filled out, or negated errno
+ * values on failure.
  */
 int spdk_nvme_transport_id_parse(struct spdk_nvme_transport_id *trid, const char *str);
 
 /**
+ * Parse the string representation of a host ID.
+ *
+ * \param hostid Output host ID structure (must be allocated and initialized by caller).
+ * \param str Input string representation of a transport ID to parse (hostid is a sub-configuration).
+ *
+ * str must be a zero-terminated C string containing one or more key:value pairs
+ * separated by whitespace.
+ *
+ * Key            | Value
+ * -------------- | -----
+ * hostaddr       | Transport address (e.g. 192.168.100.8 for RDMA)
+ * hostsvcid      | Transport service identifier (e.g. 4420)
+ *
+ * Unspecified fields of trid are left unmodified, so the caller must initialize
+ * hostid (for example, memset() to 0) before calling this function.
+ *
+ * This function should not be used with Fiber Channel or PCIe as these transports
+ * do not require host information for connections.
+ *
+ * \return 0 if parsing was successful and hostid is filled out, or negated errno
+ * values on failure.
+ */
+int spdk_nvme_host_id_parse(struct spdk_nvme_host_id *hostid, const char *str);
+
+/**
  * Parse the string representation of a transport ID tranport type.
  *
  * \param trtype Output transport type (allocated by caller).
- * \param str Input string representation of transport type (e.g. "PCIe", "RDMA")
- * \return 0 if parsing was successful and trtype is filled out, or negated errno values on failure.
+ * \param str Input string representation of transport type (e.g. "PCIe", "RDMA").
+ *
+ * \return 0 if parsing was successful and trtype is filled out, or negated errno
+ * values on failure.
  */
 int spdk_nvme_transport_id_parse_trtype(enum spdk_nvme_transport_type *trtype, const char *str);
 
 /**
+ * Look up the string representation of a transport ID transport type.
+ *
+ * \param trtype Transport type to convert.
+ *
+ * \return static string constant describing trtype, or NULL if trtype not found.
+ */
+const char *spdk_nvme_transport_id_trtype_str(enum spdk_nvme_transport_type trtype);
+
+/**
+ * Look up the string representation of a transport ID address family.
+ *
+ * \param adrfam Address family to convert.
+ *
+ * \return static string constant describing adrfam, or NULL if adrmfam not found.
+ */
+const char *spdk_nvme_transport_id_adrfam_str(enum spdk_nvmf_adrfam adrfam);
+
+/**
  * Parse the string representation of a tranport ID address family.
  *
  * \param adrfam Output address family (allocated by caller).
- * \param str Input string representation of address family (e.g. "IPv4", "IPv6")
- * \return 0 if parsing was successful and adrfam is filled out, or negated errno values on failure.
+ * \param str Input string representation of address family (e.g. "IPv4", "IPv6").
+ *
+ * \return 0 if parsing was successful and adrfam is filled out, or negated errno
+ * values on failure.
  */
 int spdk_nvme_transport_id_parse_adrfam(enum spdk_nvmf_adrfam *adrfam, const char *str);
 
 /**
  * Compare two transport IDs.
  *
- * \param trid1 First transport ID to compare.
- * \param trid2 Second transport ID to compare.
+ * The result of this function may be used to sort transport IDs in a consistent
+ * order; however, the comparison result is not guaranteed to be consistent across
+ * library versions.
  *
- * \return 0 if trid1 == trid2, less than 0 if trid1 < trid2, greater than 0 if trid1 > trid2.
+ * This function uses a case-insensitive comparison for string fields, but it does
+ * not otherwise normalize the transport ID. It is the caller's responsibility to
+ * provide the transport IDs in a consistent format.
  *
- * The result of this function may be used to sort transport IDs in a consistent order; however,
- * the comparison result is not guaranteed to be consistent across library versions.
+ * \param trid1 First transport ID to compare.
+ * \param trid2 Second transport ID to compare.
  *
- * This function uses a case-insensitive comparison for string fields, but it does not otherwise
- * normalize the transport ID. It is the caller's responsibility to provide the transport IDs in
- * a consistent format.
+ * \return 0 if trid1 == trid2, less than 0 if trid1 < trid2, greater than 0 if
+ * trid1 > trid2.
  */
 int spdk_nvme_transport_id_compare(const struct spdk_nvme_transport_id *trid1,
 				   const struct spdk_nvme_transport_id *trid2);
 
 /**
- * Determine whether the NVMe library can handle a specific NVMe over Fabrics transport type.
+ * Parse the string representation of PI check settings (prchk:guard|reftag)
+ *
+ * \param prchk_flags Output PI check flags.
+ * \param str Input string representation of PI check settings.
+ *
+ * \return 0 if parsing was successful and prchk_flags is set, or negated errno
+ * values on failure.
+ */
+int spdk_nvme_prchk_flags_parse(uint32_t *prchk_flags, const char *str);
+
+/**
+ * Look up the string representation of PI check settings  (prchk:guard|reftag)
+ *
+ * \param prchk_flags PI check flags to convert.
+ *
+ * \return static string constant describing PI check settings. If prchk_flags is 0,
+ * NULL is returned.
+ */
+const char *spdk_nvme_prchk_flags_str(uint32_t prchk_flags);
+
+/**
+ * Determine whether the NVMe library can handle a specific NVMe over Fabrics
+ * transport type.
  *
  * \param trtype NVMe over Fabrics transport type to check.
  *
@@ -252,65 +462,80 @@ bool spdk_nvme_transport_available(enum spdk_nvme_transport_type trtype);
 /**
  * Callback for spdk_nvme_probe() enumeration.
  *
- * \param opts NVMe controller initialization options.  This structure will be populated with the
- * default values on entry, and the user callback may update any options to request a different
- * value.  The controller may not support all requested parameters, so the final values will be
- * provided during the attach callback.
+ * \param cb_ctx Opaque value passed to spdk_nvme_probe().
+ * \param trid NVMe transport identifier.
+ * \param opts NVMe controller initialization options. This structure will be
+ * populated with the default values on entry, and the user callback may update
+ * any options to request a different value. The controller may not support all
+ * requested parameters, so the final values will be provided during the attach
+ * callback.
+ *
  * \return true to attach to this device.
  */
 typedef bool (*spdk_nvme_probe_cb)(void *cb_ctx, const struct spdk_nvme_transport_id *trid,
 				   struct spdk_nvme_ctrlr_opts *opts);
 
 /**
- * Callback for spdk_nvme_probe() to report a device that has been attached to the userspace NVMe driver.
+ * Callback for spdk_nvme_attach() to report a device that has been attached to
+ * the userspace NVMe driver.
  *
- * \param opts NVMe controller initialization options that were actually used.  Options may differ
- * from the requested options from the probe call depending on what the controller supports.
+ * \param cb_ctx Opaque value passed to spdk_nvme_attach_cb().
+ * \param trid NVMe transport identifier.
+ * \param ctrlr Opaque handle to NVMe controller.
+ * \param opts NVMe controller initialization options that were actually used.
+ * Options may differ from the requested options from the attach call depending
+ * on what the controller supports.
  */
 typedef void (*spdk_nvme_attach_cb)(void *cb_ctx, const struct spdk_nvme_transport_id *trid,
 				    struct spdk_nvme_ctrlr *ctrlr,
 				    const struct spdk_nvme_ctrlr_opts *opts);
 
 /**
- * Callback for spdk_nvme_probe() to report that a device attached to the userspace NVMe driver
- * has been removed from the system.
+ * Callback for spdk_nvme_remove() to report that a device attached to the userspace
+ * NVMe driver has been removed from the system.
  *
  * The controller will remain in a failed state (any new I/O submitted will fail).
  *
  * The controller must be detached from the userspace driver by calling spdk_nvme_detach()
- * once the controller is no longer in use.  It is up to the library user to ensure that
- * no other threads are using the controller before calling spdk_nvme_detach().
+ * once the controller is no longer in use. It is up to the library user to ensure
+ * that no other threads are using the controller before calling spdk_nvme_detach().
  *
+ * \param cb_ctx Opaque value passed to spdk_nvme_remove_cb().
  * \param ctrlr NVMe controller instance that was removed.
  */
 typedef void (*spdk_nvme_remove_cb)(void *cb_ctx, struct spdk_nvme_ctrlr *ctrlr);
 
 /**
- * \brief Enumerate the bus indicated by the transport ID and attach the userspace NVMe driver
- * to each device found if desired.
- *
- * \param trid The transport ID indicating which bus to enumerate. If the trtype is PCIe or trid is NULL,
- * this will scan the local PCIe bus. If the trtype is RDMA, the traddr and trsvcid must point at the
- * location of an NVMe-oF discovery service.
- * \param cb_ctx Opaque value which will be passed back in cb_ctx parameter of the callbacks.
- * \param probe_cb will be called once per NVMe device found in the system.
- * \param attach_cb will be called for devices for which probe_cb returned true once that NVMe
- * controller has been attached to the userspace driver.
- * \param remove_cb will be called for devices that were attached in a previous spdk_nvme_probe()
- * call but are no longer attached to the system. Optional; specify NULL if removal notices are not
- * desired.
+ * Enumerate the bus indicated by the transport ID and attach the userspace NVMe
+ * driver to each device found if desired.
  *
- * This function is not thread safe and should only be called from one thread at a time while no
- * other threads are actively using any NVMe devices.
+ * This function is not thread safe and should only be called from one thread at
+ * a time while no other threads are actively using any NVMe devices.
  *
- * If called from a secondary process, only devices that have been attached to the userspace driver
- * in the primary process will be probed.
+ * If called from a secondary process, only devices that have been attached to
+ * the userspace driver in the primary process will be probed.
  *
- * If called more than once, only devices that are not already attached to the SPDK NVMe driver
- * will be reported.
+ * If called more than once, only devices that are not already attached to the
+ * SPDK NVMe driver will be reported.
  *
  * To stop using the the controller and release its associated resources,
- * call \ref spdk_nvme_detach with the spdk_nvme_ctrlr instance returned by this function.
+ * call spdk_nvme_detach() with the spdk_nvme_ctrlr instance from the attach_cb()
+ * function.
+ *
+ * \param trid The transport ID indicating which bus to enumerate. If the trtype
+ * is PCIe or trid is NULL, this will scan the local PCIe bus. If the trtype is
+ * RDMA, the traddr and trsvcid must point at the location of an NVMe-oF discovery
+ * service.
+ * \param cb_ctx Opaque value which will be passed back in cb_ctx parameter of
+ * the callbacks.
+ * \param probe_cb will be called once per NVMe device found in the system.
+ * \param attach_cb will be called for devices for which probe_cb returned true
+ * once that NVMe controller has been attached to the userspace driver.
+ * \param remove_cb will be called for devices that were attached in a previous
+ * spdk_nvme_probe() call but are no longer attached to the system. Optional;
+ * specify NULL if removal notices are not desired.
+ *
+ * \return 0 on success, -1 on failure.
  */
 int spdk_nvme_probe(const struct spdk_nvme_transport_id *trid,
 		    void *cb_ctx,
@@ -319,119 +544,344 @@ int spdk_nvme_probe(const struct spdk_nvme_transport_id *trid,
 		    spdk_nvme_remove_cb remove_cb);
 
 /**
- * \brief Detaches specified device returned by \ref spdk_nvme_probe()'s attach_cb from the NVMe driver.
+ * Connect the NVMe driver to the device located at the given transport ID.
+ *
+ * This function is not thread safe and should only be called from one thread at
+ * a time while no other threads are actively using this NVMe device.
+ *
+ * If called from a secondary process, only the device that has been attached to
+ * the userspace driver in the primary process will be connected.
+ *
+ * If connecting to multiple controllers, it is suggested to use spdk_nvme_probe()
+ * and filter the requested controllers with the probe callback. For PCIe controllers,
+ * spdk_nvme_probe() will be more efficient since the controller resets will happen
+ * in parallel.
+ *
+ * To stop using the the controller and release its associated resources, call
+ * spdk_nvme_detach() with the spdk_nvme_ctrlr instance returned by this function.
+ *
+ * \param trid The transport ID indicating which device to connect. If the trtype
+ * is PCIe, this will connect the local PCIe bus. If the trtype is RDMA, the traddr
+ * and trsvcid must point at the location of an NVMe-oF service.
+ * \param opts NVMe controller initialization options. Default values will be used
+ * if the user does not specify the options. The controller may not support all
+ * requested parameters.
+ * \param opts_size Must be set to sizeof(struct spdk_nvme_ctrlr_opts), or 0 if
+ * opts is NULL.
+ *
+ * \return pointer to the connected NVMe controller or NULL if there is any failure.
+ *
+ */
+struct spdk_nvme_ctrlr *spdk_nvme_connect(const struct spdk_nvme_transport_id *trid,
+		const struct spdk_nvme_ctrlr_opts *opts,
+		size_t opts_size);
+
+struct spdk_nvme_probe_ctx;
+
+/**
+ * Connect the NVMe driver to the device located at the given transport ID.
+ *
+ * The function will return a probe context on success, controller associates with
+ * the context is not ready for use, user must call spdk_nvme_probe_poll_async()
+ * until spdk_nvme_probe_poll_async() returns 0.
+ *
+ * \param trid The transport ID indicating which device to connect. If the trtype
+ * is PCIe, this will connect the local PCIe bus. If the trtype is RDMA, the traddr
+ * and trsvcid must point at the location of an NVMe-oF service.
+ * \param opts NVMe controller initialization options. Default values will be used
+ * if the user does not specify the options. The controller may not support all
+ * requested parameters.
+ * \param attach_cb will be called once the NVMe controller has been attached
+ * to the userspace driver.
+ *
+ * \return probe context on success, NULL on failure.
+ *
+ */
+struct spdk_nvme_probe_ctx *spdk_nvme_connect_async(const struct spdk_nvme_transport_id *trid,
+		const struct spdk_nvme_ctrlr_opts *opts,
+		spdk_nvme_attach_cb attach_cb);
+
+/**
+ * Probe and add controllers to the probe context list.
+ *
+ * Users must call spdk_nvme_probe_poll_async() to initialize
+ * controllers in the probe context list to the READY state.
+ *
+ * \param trid The transport ID indicating which bus to enumerate. If the trtype
+ * is PCIe or trid is NULL, this will scan the local PCIe bus. If the trtype is
+ * RDMA, the traddr and trsvcid must point at the location of an NVMe-oF discovery
+ * service.
+ * \param cb_ctx Opaque value which will be passed back in cb_ctx parameter of
+ * the callbacks.
+ * \param probe_cb will be called once per NVMe device found in the system.
+ * \param attach_cb will be called for devices for which probe_cb returned true
+ * once that NVMe controller has been attached to the userspace driver.
+ * \param remove_cb will be called for devices that were attached in a previous
+ * spdk_nvme_probe() call but are no longer attached to the system. Optional;
+ * specify NULL if removal notices are not desired.
+ *
+ * \return probe context on success, NULL on failure.
+ */
+struct spdk_nvme_probe_ctx *spdk_nvme_probe_async(const struct spdk_nvme_transport_id *trid,
+		void *cb_ctx,
+		spdk_nvme_probe_cb probe_cb,
+		spdk_nvme_attach_cb attach_cb,
+		spdk_nvme_remove_cb remove_cb);
+
+/**
+ * Start controllers in the context list.
+ *
+ * Users may call the function util it returns True.
+ *
+ * \param probe_ctx Context used to track probe actions.
+ *
+ * \return 0 if all probe operations are complete; the probe_ctx
+ * is also freed and no longer valid.
+ * \return -EAGAIN if there are still pending probe operations; user must call
+ * spdk_nvme_probe_poll_async again to continue progress.
+ * \return value other than 0 and -EAGAIN probe error with one controller.
+ */
+int spdk_nvme_probe_poll_async(struct spdk_nvme_probe_ctx *probe_ctx);
+
+/**
+ * Detach specified device returned by spdk_nvme_probe()'s attach_cb from the
+ * NVMe driver.
  *
  * On success, the spdk_nvme_ctrlr handle is no longer valid.
  *
  * This function should be called from a single thread while no other threads
  * are actively using the NVMe device.
  *
+ * \param ctrlr Opaque handle to NVMe controller.
+ *
+ * \return 0 on success, -1 on failure.
  */
 int spdk_nvme_detach(struct spdk_nvme_ctrlr *ctrlr);
 
 /**
- * \brief Perform a full hardware reset of the NVMe controller.
+ * Perform a full hardware reset of the NVMe controller.
  *
  * This function should be called from a single thread while no other threads
  * are actively using the NVMe device.
  *
- * Any pointers returned from spdk_nvme_ctrlr_get_ns() and spdk_nvme_ns_get_data() may be invalidated
- * by calling this function.  The number of namespaces as returned by spdk_nvme_ctrlr_get_num_ns() may
- * also change.
+ * Any pointers returned from spdk_nvme_ctrlr_get_ns() and spdk_nvme_ns_get_data()
+ * may be invalidated by calling this function. The number of namespaces as returned
+ * by spdk_nvme_ctrlr_get_num_ns() may also change.
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ *
+ * \return 0 on success, -1 on failure.
  */
 int spdk_nvme_ctrlr_reset(struct spdk_nvme_ctrlr *ctrlr);
 
 /**
- * \brief Get the identify controller data as defined by the NVMe specification.
+ * Get the identify controller data as defined by the NVMe specification.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * \param ctrlr Opaque handle to NVMe controller.
  *
+ * \return pointer to the identify controller data.
  */
 const struct spdk_nvme_ctrlr_data *spdk_nvme_ctrlr_get_data(struct spdk_nvme_ctrlr *ctrlr);
 
 /**
- * \brief Get the NVMe controller CSTS (Status) register.
+ * Get the NVMe controller CSTS (Status) register.
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ *
+ * \return the NVMe controller CSTS (Status) register.
  */
 union spdk_nvme_csts_register spdk_nvme_ctrlr_get_regs_csts(struct spdk_nvme_ctrlr *ctrlr);
 
 /**
- * \brief Get the NVMe controller CAP (Capabilities) register.
+ * Get the NVMe controller CAP (Capabilities) register.
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ *
+ * \return the NVMe controller CAP (Capabilities) register.
  */
 union spdk_nvme_cap_register spdk_nvme_ctrlr_get_regs_cap(struct spdk_nvme_ctrlr *ctrlr);
 
 /**
- * \brief Get the NVMe controller VS (Version) register.
+ * Get the NVMe controller VS (Version) register.
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ *
+ * \return the NVMe controller VS (Version) register.
  */
 union spdk_nvme_vs_register spdk_nvme_ctrlr_get_regs_vs(struct spdk_nvme_ctrlr *ctrlr);
 
 /**
- * \brief Get the number of namespaces for the given NVMe controller.
+ * Get the NVMe controller CMBSZ (Controller Memory Buffer Size) register
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ *
+ * \return the NVMe controller CMBSZ (Controller Memory Buffer Size) register.
+ */
+union spdk_nvme_cmbsz_register spdk_nvme_ctrlr_get_regs_cmbsz(struct spdk_nvme_ctrlr *ctrlr);
+
+/**
+ * Get the number of namespaces for the given NVMe controller.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * This function is thread safe and can be called at any point while the
+ * controller is attached to the SPDK NVMe driver.
  *
  * This is equivalent to calling spdk_nvme_ctrlr_get_data() to get the
  * spdk_nvme_ctrlr_data and then reading the nn field.
  *
+ * \param ctrlr Opaque handle to NVMe controller.
+ *
+ * \return the number of namespaces.
  */
 uint32_t spdk_nvme_ctrlr_get_num_ns(struct spdk_nvme_ctrlr *ctrlr);
 
 /**
- * \brief Determine if a particular log page is supported by the given NVMe controller.
+ * Get the PCI device of a given NVMe controller.
+ *
+ * This only works for local (PCIe-attached) NVMe controllers; other transports
+ * will return NULL.
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ *
+ * \return PCI device of the NVMe controller, or NULL if not available.
+ */
+struct spdk_pci_device *spdk_nvme_ctrlr_get_pci_device(struct spdk_nvme_ctrlr *ctrlr);
+
+/**
+ * Get the maximum data transfer size of a given NVMe controller.
+ *
+ * \return Maximum data transfer size of the NVMe controller in bytes.
+ *
+ * The I/O command helper functions, such as spdk_nvme_ns_cmd_read(), will split
+ * large I/Os automatically; however, it is up to the user to obey this limit for
+ * commands submitted with the raw command functions, such as spdk_nvme_ctrlr_cmd_io_raw().
+ */
+uint32_t spdk_nvme_ctrlr_get_max_xfer_size(const struct spdk_nvme_ctrlr *ctrlr);
+
+/**
+ * Check whether the nsid is an active nv for the given NVMe controller.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ * \param nsid Namespace id.
+ *
+ * \return true if nsid is an active ns, or false otherwise.
+ */
+bool spdk_nvme_ctrlr_is_active_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid);
+
+/**
+ * Get the nsid of the first active namespace.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ *
+ * \return the nsid of the first active namespace, 0 if there are no active namespaces.
+ */
+uint32_t spdk_nvme_ctrlr_get_first_active_ns(struct spdk_nvme_ctrlr *ctrlr);
+
+/**
+ * Get next active namespace given the previous nsid.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ * \param prev_nsid Namespace id.
+ *
+ * \return a next active namespace given the previous nsid, 0 when there are no
+ * more active namespaces.
+ */
+uint32_t spdk_nvme_ctrlr_get_next_active_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t prev_nsid);
+
+/**
+ * Determine if a particular log page is supported by the given NVMe controller.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * \sa spdk_nvme_ctrlr_cmd_get_log_page().
  *
- * \sa spdk_nvme_ctrlr_cmd_get_log_page()
+ * \param ctrlr Opaque handle to NVMe controller.
+ * \param log_page Log page to query.
+ *
+ * \return true if supported, or false otherwise.
  */
 bool spdk_nvme_ctrlr_is_log_page_supported(struct spdk_nvme_ctrlr *ctrlr, uint8_t log_page);
 
 /**
- * \brief Determine if a particular feature is supported by the given NVMe controller.
+ * Determine if a particular feature is supported by the given NVMe controller.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * \sa spdk_nvme_ctrlr_cmd_get_feature().
  *
- * \sa spdk_nvme_ctrlr_cmd_get_feature()
+ * \param ctrlr Opaque handle to NVMe controller.
+ * \param feature_code Feature to query.
+ *
+ * \return true if supported, or false otherwise.
  */
 bool spdk_nvme_ctrlr_is_feature_supported(struct spdk_nvme_ctrlr *ctrlr, uint8_t feature_code);
 
 /**
  * Signature for callback function invoked when a command is completed.
  *
- * The spdk_nvme_cpl parameter contains the completion status.
+ * \param spdk_nvme_cpl Completion queue entry that coontains the completion status.
  */
 typedef void (*spdk_nvme_cmd_cb)(void *, const struct spdk_nvme_cpl *);
 
 /**
- * Signature for callback function invoked when an asynchronous error
- *  request command is completed.
+ * Signature for callback function invoked when an asynchronous error request
+ * command is completed.
  *
- * The aer_cb_arg parameter is set to the context specified by
- *  spdk_nvme_register_aer_callback().
- * The spdk_nvme_cpl parameter contains the completion status of the
- *  asynchronous event request that was completed.
+ * \param ctrlr Opaque handle to NVMe controller.
+ * \param aer_cb_arg Context specified by spdk_nvme_register_aer_callback().
+ * \param spdk_nvme_cpl Completion queue entry that contains the completion status
+ * of the asynchronous event request that was completed.
  */
 typedef void (*spdk_nvme_aer_cb)(void *aer_cb_arg,
 				 const struct spdk_nvme_cpl *);
 
+/**
+ * Register callback function invoked when an AER command is completed for the
+ * given NVMe controller.
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ * \param aer_cb_fn Callback function invoked when an asynchronous error request
+ * command is completed.
+ * \param aer_cb_arg Argument passed to callback function.
+ */
 void spdk_nvme_ctrlr_register_aer_callback(struct spdk_nvme_ctrlr *ctrlr,
 		spdk_nvme_aer_cb aer_cb_fn,
 		void *aer_cb_arg);
 
 /**
- * \brief Opaque handle to a queue pair.
+ * Opaque handle to a queue pair.
  *
  * I/O queue pairs may be allocated using spdk_nvme_ctrlr_alloc_io_qpair().
  */
 struct spdk_nvme_qpair;
 
 /**
- * Signature for the callback function invoked when a timeout is
- * detected on a request.
- * For timeouts detected on the admin queue pair, the qpair returned
- * here will be NULL.
+ * Signature for the callback function invoked when a timeout is detected on a
+ * request.
+ *
+ * For timeouts detected on the admin queue pair, the qpair returned here will
+ * be NULL.  If the controller has a serious error condition and is unable to
+ * communicate with driver via completion queue, the controller can set Controller
+ * Fatal Status field to 1, then reset is required to recover from such error.
+ * Users may detect Controller Fatal Status when timeout happens.
+ *
+ * \param cb_arg Argument passed to callback funciton.
+ * \param ctrlr Opaque handle to NVMe controller.
+ * \param qpair Opaque handle to a queue pair.
+ * \param cid Command ID.
  */
 typedef void (*spdk_nvme_timeout_cb)(void *cb_arg,
 				     struct spdk_nvme_ctrlr *ctrlr,
@@ -439,39 +889,106 @@ typedef void (*spdk_nvme_timeout_cb)(void *cb_arg,
 				     uint16_t cid);
 
 /**
- * \brief Register for timeout callback on a controller.
+ * Register for timeout callback on a controller.
  *
  * The application can choose to register for timeout callback or not register
  * for timeout callback.
  *
  * \param ctrlr NVMe controller on which to monitor for timeout.
- * \param timeout_sec Timeout value in seconds.
- * \param cb_fn A function pointer that points to the callback function
+ * \param timeout_us Timeout value in microseconds.
+ * \param cb_fn A function pointer that points to the callback function.
  * \param cb_arg Argument to the callback function.
  */
 void spdk_nvme_ctrlr_register_timeout_callback(struct spdk_nvme_ctrlr *ctrlr,
-		uint32_t timeout_sec, spdk_nvme_timeout_cb cb_fn, void *cb_arg);
+		uint64_t timeout_us, spdk_nvme_timeout_cb cb_fn, void *cb_arg);
+
+/**
+ * NVMe I/O queue pair initialization options.
+ *
+ * These options may be passed to spdk_nvme_ctrlr_alloc_io_qpair() to configure queue pair
+ * options at queue creation time.
+ *
+ * The user may retrieve the default I/O queue pair creation options for a controller using
+ * spdk_nvme_ctrlr_get_default_io_qpair_opts().
+ */
+struct spdk_nvme_io_qpair_opts {
+	/**
+	 * Queue priority for weighted round robin arbitration.  If a different arbitration
+	 * method is in use, pass 0.
+	 */
+	enum spdk_nvme_qprio qprio;
+
+	/**
+	 * The queue depth of this NVMe I/O queue. Overrides spdk_nvme_ctrlr_opts::io_queue_size.
+	 */
+	uint32_t io_queue_size;
+
+	/**
+	 * The number of requests to allocate for this NVMe I/O queue.
+	 *
+	 * Overrides spdk_nvme_ctrlr_opts::io_queue_requests.
+	 *
+	 * This should be at least as large as io_queue_size.
+	 *
+	 * A single I/O may allocate more than one request, since splitting may be
+	 * necessary to conform to the device's maximum transfer size, PRP list
+	 * compatibility requirements, or driver-assisted striping.
+	 */
+	uint32_t io_queue_requests;
+
+	/**
+	 * When submitting I/O via spdk_nvme_ns_read/write and similar functions,
+	 * don't immediately write the submission queue doorbell. Instead, write
+	 * to the doorbell as necessary inside spdk_nvme_qpair_process_completions().
+	 *
+	 * This results in better batching of I/O submission and consequently fewer
+	 * MMIO writes to the doorbell, which may increase performance.
+	 *
+	 * This only applies to local PCIe devices. */
+	bool delay_pcie_doorbell;
+};
+
+/**
+ * Get the default options for I/O qpair creation for a specific NVMe controller.
+ *
+ * \param ctrlr NVMe controller to retrieve the defaults from.
+ * \param[out] opts Will be filled with the default options for
+ * spdk_nvme_ctrlr_alloc_io_qpair().
+ * \param opts_size Must be set to sizeof(struct spdk_nvme_io_qpair_opts).
+ */
+void spdk_nvme_ctrlr_get_default_io_qpair_opts(struct spdk_nvme_ctrlr *ctrlr,
+		struct spdk_nvme_io_qpair_opts *opts,
+		size_t opts_size);
 
 /**
- * \brief Allocate an I/O queue pair (submission and completion queue).
+ * Allocate an I/O queue pair (submission and completion queue).
  *
- * Each queue pair should only be used from a single thread at a time (mutual exclusion must be
- * enforced by the user).
+ * Each queue pair should only be used from a single thread at a time (mutual
+ * exclusion must be enforced by the user).
  *
  * \param ctrlr NVMe controller for which to allocate the I/O queue pair.
- * \param qprio Queue priority for weighted round robin arbitration.  If a different arbitration
- * method is in use, pass 0.
+ * \param opts I/O qpair creation options, or NULL to use the defaults as returned
+ * by spdk_nvme_ctrlr_alloc_io_qpair().
+ * \param opts_size Must be set to sizeof(struct spdk_nvme_io_qpair_opts), or 0
+ * if opts is NULL.
+ *
+ * \return a pointer to the allocated I/O queue pair.
  */
 struct spdk_nvme_qpair *spdk_nvme_ctrlr_alloc_io_qpair(struct spdk_nvme_ctrlr *ctrlr,
-		enum spdk_nvme_qprio qprio);
+		const struct spdk_nvme_io_qpair_opts *opts,
+		size_t opts_size);
 
 /**
- * \brief Free an I/O queue pair that was allocated by spdk_nvme_ctrlr_alloc_io_qpair().
+ * Free an I/O queue pair that was allocated by spdk_nvme_ctrlr_alloc_io_qpair().
+ *
+ * \param qpair I/O queue pair to free.
+ *
+ * \return 0 on success, -1 on failure.
  */
 int spdk_nvme_ctrlr_free_io_qpair(struct spdk_nvme_qpair *qpair);
 
 /**
- * \brief Send the given NVM I/O command to the NVMe controller.
+ * Send the given NVM I/O command to the NVMe controller.
  *
  * This is a low level interface for submitting I/O commands directly. Prefer
  * the spdk_nvme_ns_cmd_* functions instead. The validity of the command will
@@ -481,7 +998,18 @@ int spdk_nvme_ctrlr_free_io_qpair(struct spdk_nvme_qpair *qpair);
  * list/SGL or the CID. The driver will handle both of those for you.
  *
  * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ * \param qpair I/O qpair to submit command.
+ * \param cmd NVM I/O command to submit.
+ * \param buf Virtual memory address of a single physically contiguous buffer.
+ * \param len Size of buffer.
+ * \param cb_fn Callback function invoked when the I/O command completes.
+ * \param cb_arg Argument passed to callback function.
+ *
+ * \return 0 on success, negated errno on failure.
  */
 int spdk_nvme_ctrlr_cmd_io_raw(struct spdk_nvme_ctrlr *ctrlr,
 			       struct spdk_nvme_qpair *qpair,
@@ -490,33 +1018,66 @@ int spdk_nvme_ctrlr_cmd_io_raw(struct spdk_nvme_ctrlr *ctrlr,
 			       spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 
 /**
- * \brief Process any outstanding completions for I/O submitted on a queue pair.
+ * Send the given NVM I/O command with metadata to the NVMe controller.
  *
- * This call is non-blocking, i.e. it only
- * processes completions that are ready at the time of this function call. It does not
- * wait for outstanding commands to finish.
+ * This is a low level interface for submitting I/O commands directly. Prefer
+ * the spdk_nvme_ns_cmd_* functions instead. The validity of the command will
+ * not be checked!
  *
- * For each completed command, the request's callback function will
- *  be called if specified as non-NULL when the request was submitted.
+ * When constructing the nvme_command it is not necessary to fill out the PRP
+ * list/SGL or the CID. The driver will handle both of those for you.
  *
- * \param qpair Queue pair to check for completions.
- * \param max_completions Limit the number of completions to be processed in one call, or 0
- * for unlimited.
+ * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
  *
- * \return Number of completions processed (may be 0) or negative on error.
+ * \param ctrlr Opaque handle to NVMe controller.
+ * \param qpair I/O qpair to submit command.
+ * \param cmd NVM I/O command to submit.
+ * \param buf Virtual memory address of a single physically contiguous buffer.
+ * \param len Size of buffer.
+ * \param md_buf Virtual memory address of a single physically contiguous metadata
+ * buffer.
+ * \param cb_fn Callback function invoked when the I/O command completes.
+ * \param cb_arg Argument passed to callback function.
  *
- * \sa spdk_nvme_cmd_cb
+ * \return 0 on success, negated errno on failure.
+ */
+int spdk_nvme_ctrlr_cmd_io_raw_with_md(struct spdk_nvme_ctrlr *ctrlr,
+				       struct spdk_nvme_qpair *qpair,
+				       struct spdk_nvme_cmd *cmd,
+				       void *buf, uint32_t len, void *md_buf,
+				       spdk_nvme_cmd_cb cb_fn, void *cb_arg);
+
+/**
+ * Process any outstanding completions for I/O submitted on a queue pair.
+ *
+ * This call is non-blocking, i.e. it only processes completions that are ready
+ * at the time of this function call. It does not wait for outstanding commands
+ * to finish.
+ *
+ * For each completed command, the request's callback function will be called if
+ * specified as non-NULL when the request was submitted.
+ *
+ * The caller must ensure that each queue pair is only used from one thread at a
+ * time.
  *
  * This function may be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * the SPDK NVMe driver.
+ *
+ * \sa spdk_nvme_cmd_cb
+ *
+ * \param qpair Queue pair to check for completions.
+ * \param max_completions Limit the number of completions to be processed in one
+ * call, or 0 for unlimited.
  *
- * The caller must ensure that each queue pair is only used from one thread at a time.
+ * \return number of completions processed (may be 0) or negated on error.
  */
 int32_t spdk_nvme_qpair_process_completions(struct spdk_nvme_qpair *qpair,
 		uint32_t max_completions);
 
 /**
- * \brief Send the given admin command to the NVMe controller.
+ * Send the given admin command to the NVMe controller.
  *
  * This is a low level interface for submitting admin commands directly. Prefer
  * the spdk_nvme_ctrlr_cmd_* functions instead. The validity of the command will
@@ -525,11 +1086,20 @@ int32_t spdk_nvme_qpair_process_completions(struct spdk_nvme_qpair *qpair,
  * When constructing the nvme_command it is not necessary to fill out the PRP
  * list/SGL or the CID. The driver will handle both of those for you.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
  *
- * Call \ref spdk_nvme_ctrlr_process_admin_completions() to poll for completion
+ * Call spdk_nvme_ctrlr_process_admin_completions() to poll for completion
  * of commands submitted through this function.
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
+ * \param cmd NVM admin command to submit.
+ * \param buf Virtual memory address of a single physically contiguous buffer.
+ * \param len Size of buffer.
+ * \param cb_fn Callback function invoked when the admin command completes.
+ * \param cb_arg Argument passed to callback function.
+ *
+ * \return 0 on success, negated errno on failure.
  */
 int spdk_nvme_ctrlr_cmd_admin_raw(struct spdk_nvme_ctrlr *ctrlr,
 				  struct spdk_nvme_cmd *cmd,
@@ -537,60 +1107,71 @@ int spdk_nvme_ctrlr_cmd_admin_raw(struct spdk_nvme_ctrlr *ctrlr,
 				  spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 
 /**
- * \brief Process any outstanding completions for admin commands.
+ * Process any outstanding completions for admin commands.
  *
  * This will process completions for admin commands submitted on any thread.
  *
  * This call is non-blocking, i.e. it only processes completions that are ready
- * at the time of this function call. It does not wait for outstanding commands to
- * finish.
+ * at the time of this function call. It does not wait for outstanding commands
+ * to finish.
  *
- * \return Number of completions processed (may be 0) or negative on error.
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * \param ctrlr Opaque handle to NVMe controller.
+ *
+ * \return number of completions processed (may be 0) or negated on error.
  */
 int32_t spdk_nvme_ctrlr_process_admin_completions(struct spdk_nvme_ctrlr *ctrlr);
 
 
-/** \brief Opaque handle to a namespace. Obtained by calling spdk_nvme_ctrlr_get_ns(). */
+/**
+ * Opaque handle to a namespace. Obtained by calling spdk_nvme_ctrlr_get_ns().
+ */
 struct spdk_nvme_ns;
 
 /**
- * \brief Get a handle to a namespace for the given controller.
+ * Get a handle to a namespace for the given controller.
+ *
+ * Namespaces are numbered from 1 to the total number of namespaces. There will
+ * never be any gaps in the numbering. The number of namespaces is obtained by
+ * calling spdk_nvme_ctrlr_get_num_ns().
  *
- * Namespaces are numbered from 1 to the total number of namespaces. There will never
- * be any gaps in the numbering. The number of namespaces is obtained by calling
- * spdk_nvme_ctrlr_get_num_ns().
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * \param ctrlr Opaque handle to NVMe controller.
+ * \param ns_id Namespace id.
+ *
+ * \return a pointer to the namespace.
  */
 struct spdk_nvme_ns *spdk_nvme_ctrlr_get_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t ns_id);
 
 /**
- * \brief Get a specific log page from the NVMe controller.
+ * Get a specific log page from the NVMe controller.
  *
- * \param ctrlr NVMe controller to query.
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
+ *
+ * Call spdk_nvme_ctrlr_process_admin_completions() to poll for completion of
+ * commands submitted through this function.
+ *
+ * \sa spdk_nvme_ctrlr_is_log_page_supported()
+ *
+ * \param ctrlr Opaque handle to NVMe controller.
  * \param log_page The log page identifier.
- * \param nsid Depending on the log page, this may be 0, a namespace identifier, or SPDK_NVME_GLOBAL_NS_TAG.
+ * \param nsid Depending on the log page, this may be 0, a namespace identifier,
+ * or SPDK_NVME_GLOBAL_NS_TAG.
  * \param payload The pointer to the payload buffer.
  * \param payload_size The size of payload buffer.
- * \param offset Offset in bytes within the log page to start retrieving log page data.
- *               May only be non-zero if the controller supports extended data for Get Log Page
- *               as reported in the controller data log page attributes.
+ * \param offset Offset in bytes within the log page to start retrieving log page
+ * data. May only be non-zero if the controller supports extended data for Get Log
+ * Page as reported in the controller data log page attributes.
  * \param cb_fn Callback function to invoke when the log page has been retrieved.
  * \param cb_arg Argument to pass to the callback function.
  *
- * \return 0 if successfully submitted, ENOMEM if resources could not be allocated for this request
- *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
- *
- * Call \ref spdk_nvme_ctrlr_process_admin_completions() to poll for completion
- * of commands submitted through this function.
- *
- * \sa spdk_nvme_ctrlr_is_log_page_supported()
+ * \return 0 if successfully submitted, negated errno if resources could not be
+ * allocated for this request.
  */
 int spdk_nvme_ctrlr_cmd_get_log_page(struct spdk_nvme_ctrlr *ctrlr,
 				     uint8_t log_page, uint32_t nsid,
@@ -599,18 +1180,18 @@ int spdk_nvme_ctrlr_cmd_get_log_page(struct spdk_nvme_ctrlr *ctrlr,
 				     spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 
 /**
- * \brief Abort a specific previously-submitted NVMe command.
+ * Abort a specific previously-submitted NVMe command.
+ *
+ * \sa spdk_nvme_ctrlr_register_timeout_callback()
  *
  * \param ctrlr NVMe controller to which the command was submitted.
- * \param qpair NVMe queue pair to which the command was submitted.
- *              For admin commands, pass NULL for the qpair.
+ * \param qpair NVMe queue pair to which the command was submitted. For admin
+ *  commands, pass NULL for the qpair.
  * \param cid Command ID of the command to abort.
  * \param cb_fn Callback function to invoke when the abort has completed.
- * \param cb_arg Argument to pass to the callback function.\
+ * \param cb_arg Argument to pass to the callback function.
  *
  * \return 0 if successfully submitted, negated errno value otherwise.
- *
- * \sa spdk_nvme_ctrlr_register_timeout_callback()
  */
 int spdk_nvme_ctrlr_cmd_abort(struct spdk_nvme_ctrlr *ctrlr,
 			      struct spdk_nvme_qpair *qpair,
@@ -619,7 +1200,15 @@ int spdk_nvme_ctrlr_cmd_abort(struct spdk_nvme_ctrlr *ctrlr,
 			      void *cb_arg);
 
 /**
- * \brief Set specific feature for the given NVMe controller.
+ * Set specific feature for the given NVMe controller.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
+ *
+ * Call spdk_nvme_ctrlr_process_admin_completions() to poll for completion of
+ * commands submitted through this function.
+ *
+ * \sa spdk_nvme_ctrlr_cmd_get_feature().
  *
  * \param ctrlr NVMe controller to manipulate.
  * \param feature The feature identifier.
@@ -630,23 +1219,43 @@ int spdk_nvme_ctrlr_cmd_abort(struct spdk_nvme_ctrlr *ctrlr,
  * \param cb_fn Callback function to invoke when the feature has been set.
  * \param cb_arg Argument to pass to the callback function.
  *
- * \return 0 if successfully submitted, ENOMEM if resources could not be allocated for this request
+ * \return 0 if successfully submitted, negated errno if resources could not be
+ * allocated for this request.
+ */
+int spdk_nvme_ctrlr_cmd_set_feature(struct spdk_nvme_ctrlr *ctrlr,
+				    uint8_t feature, uint32_t cdw11, uint32_t cdw12,
+				    void *payload, uint32_t payload_size,
+				    spdk_nvme_cmd_cb cb_fn, void *cb_arg);
+
+/**
+ * Get specific feature from given NVMe controller.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * Call spdk_nvme_ctrlr_process_admin_completions() to poll for completion of
+ * commands submitted through this function.
  *
- * Call \ref spdk_nvme_ctrlr_process_admin_completions() to poll for completion
- * of commands submitted through this function.
+ * \sa spdk_nvme_ctrlr_cmd_set_feature()
  *
- * \sa spdk_nvme_ctrlr_cmd_get_feature()
+ * \param ctrlr NVMe controller to query.
+ * \param feature The feature identifier.
+ * \param cdw11 as defined by the specification for this command.
+ * \param payload The pointer to the payload buffer.
+ * \param payload_size The size of payload buffer.
+ * \param cb_fn Callback function to invoke when the feature has been retrieved.
+ * \param cb_arg Argument to pass to the callback function.
+ *
+ * \return 0 if successfully submitted, ENOMEM if resources could not be allocated
+ * for this request.
  */
-int spdk_nvme_ctrlr_cmd_set_feature(struct spdk_nvme_ctrlr *ctrlr,
-				    uint8_t feature, uint32_t cdw11, uint32_t cdw12,
+int spdk_nvme_ctrlr_cmd_get_feature(struct spdk_nvme_ctrlr *ctrlr,
+				    uint8_t feature, uint32_t cdw11,
 				    void *payload, uint32_t payload_size,
 				    spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 
 /**
- * \brief Get specific feature from given NVMe controller.
+ * Get specific feature from given NVMe controller.
  *
  * \param ctrlr NVMe controller to query.
  * \param feature The feature identifier.
@@ -655,195 +1264,424 @@ int spdk_nvme_ctrlr_cmd_set_feature(struct spdk_nvme_ctrlr *ctrlr,
  * \param payload_size The size of payload buffer.
  * \param cb_fn Callback function to invoke when the feature has been retrieved.
  * \param cb_arg Argument to pass to the callback function.
+ * \param ns_id The namespace identifier.
  *
- * \return 0 if successfully submitted, ENOMEM if resources could not be allocated for this request
+ * \return 0 if successfully submitted, ENOMEM if resources could not be allocated
+ * for this request
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
  *
  * Call \ref spdk_nvme_ctrlr_process_admin_completions() to poll for completion
  * of commands submitted through this function.
  *
- * \sa spdk_nvme_ctrlr_cmd_set_feature()
+ * \sa spdk_nvme_ctrlr_cmd_set_feature_ns()
  */
-int spdk_nvme_ctrlr_cmd_get_feature(struct spdk_nvme_ctrlr *ctrlr,
-				    uint8_t feature, uint32_t cdw11,
-				    void *payload, uint32_t payload_size,
-				    spdk_nvme_cmd_cb cb_fn, void *cb_arg);
+int spdk_nvme_ctrlr_cmd_get_feature_ns(struct spdk_nvme_ctrlr *ctrlr, uint8_t feature,
+				       uint32_t cdw11, void *payload, uint32_t payload_size,
+				       spdk_nvme_cmd_cb cb_fn, void *cb_arg, uint32_t ns_id);
 
 /**
- * \brief Attach the specified namespace to controllers.
+ * Set specific feature for the given NVMe controller and namespace ID.
  *
- * \param ctrlr NVMe controller to use for command submission.
- * \param nsid Namespace identifier for namespace to attach.
- * \param payload The pointer to the controller list.
+ * \param ctrlr NVMe controller to manipulate.
+ * \param feature The feature identifier.
+ * \param cdw11 as defined by the specification for this command.
+ * \param cdw12 as defined by the specification for this command.
+ * \param payload The pointer to the payload buffer.
+ * \param payload_size The size of payload buffer.
+ * \param cb_fn Callback function to invoke when the feature has been set.
+ * \param cb_arg Argument to pass to the callback function.
+ * \param ns_id The namespace identifier.
  *
- * \return 0 if successfully submitted, ENOMEM if resources could not be allocated for this request
+ * \return 0 if successfully submitted, ENOMEM if resources could not be allocated
+ * for this request.
  *
- * This function is thread safe and can be called at any point after spdk_nvme_attach().
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
  *
  * Call \ref spdk_nvme_ctrlr_process_admin_completions() to poll for completion
  * of commands submitted through this function.
+ *
+ * \sa spdk_nvme_ctrlr_cmd_get_feature_ns()
+ */
+int spdk_nvme_ctrlr_cmd_set_feature_ns(struct spdk_nvme_ctrlr *ctrlr, uint8_t feature,
+				       uint32_t cdw11, uint32_t cdw12, void *payload,
+				       uint32_t payload_size, spdk_nvme_cmd_cb cb_fn,
+				       void *cb_arg, uint32_t ns_id);
+
+/**
+ * Receive security protocol data from controller.
+ *
+ * This function is thread safe and can be called at any point after spdk_nvme_probe().
+ *
+ * Call spdk_nvme_ctrlr_process_admin_completions() to poll for completion of
+ * commands submitted through this function.
+ *
+ * \param ctrlr NVMe controller to use for security receive command submission.
+ * \param secp Security Protocol that is used.
+ * \param spsp Security Protocol Specific field.
+ * \param nssf NVMe Security Specific field. Indicate RPMB target when using Security
+ * Protocol EAh.
+ * \param payload The pointer to the payload buffer.
+ * \param size The size of payload buffer.
+ *
+ * \return 0 if successfully submitted, negated errno if resources could not be allocated
+ * for this request.
+ */
+int spdk_nvme_ctrlr_security_receive(struct spdk_nvme_ctrlr *ctrlr, uint8_t secp,
+				     uint16_t spsp, uint8_t nssf, void *payload, size_t size);
+
+/**
+ * Send security protocol data to controller.
+ *
+ * This function is thread safe and can be called at any point after spdk_nvme_probe().
+ *
+ * Call spdk_nvme_ctrlr_process_admin_completions() to poll for completion of
+ * commands submitted through this function.
+ *
+ * \param ctrlr NVMe controller to use for security send command submission.
+ * \param secp Security Protocol that is used.
+ * \param spsp Security Protocol Specific field.
+ * \param nssf NVMe Security Specific field. Indicate RPMB target when using Security
+ * Protocol EAh.
+ * \param payload The pointer to the payload buffer.
+ * \param size The size of payload buffer.
+ *
+ * \return 0 if successfully submitted, negated errno if resources could not be allocated
+ * for this request.
+ */
+int spdk_nvme_ctrlr_security_send(struct spdk_nvme_ctrlr *ctrlr, uint8_t secp,
+				  uint16_t spsp, uint8_t nssf, void *payload, size_t size);
+
+/**
+ * Get supported flags of the controller.
+ *
+ * \param ctrlr NVMe controller to get flags.
+ *
+ * \return supported flags of this controller.
+ */
+uint64_t spdk_nvme_ctrlr_get_flags(struct spdk_nvme_ctrlr *ctrlr);
+
+/**
+ * Attach the specified namespace to controllers.
+ *
+ * This function is thread safe and can be called at any point after spdk_nvme_probe().
+ *
+ * Call spdk_nvme_ctrlr_process_admin_completions() to poll for completion of
+ * commands submitted through this function.
+ *
+ * \param ctrlr NVMe controller to use for command submission.
+ * \param nsid Namespace identifier for namespace to attach.
+ * \param payload The pointer to the controller list.
+ *
+ * \return 0 if successfully submitted, ENOMEM if resources could not be allocated
+ * for this request.
  */
 int spdk_nvme_ctrlr_attach_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid,
 			      struct spdk_nvme_ctrlr_list *payload);
 
 /**
- * \brief Detach the specified namespace from controllers.
+ * Detach the specified namespace from controllers.
+ *
+ * This function is thread safe and can be called at any point after spdk_nvme_probe().
+ *
+ * Call spdk_nvme_ctrlr_process_admin_completions() to poll for completion of
+ * commands submitted through this function.
  *
  * \param ctrlr NVMe controller to use for command submission.
  * \param nsid Namespace ID to detach.
  * \param payload The pointer to the controller list.
  *
- * \return 0 if successfully submitted, ENOMEM if resources could not be allocated for this request
- *
- * This function is thread safe and can be called at any point after spdk_nvme_attach().
- *
- * Call \ref spdk_nvme_ctrlr_process_admin_completions() to poll for completion
- * of commands submitted through this function.
+ * \return 0 if successfully submitted, ENOMEM if resources could not be allocated
+ * for this request
  */
 int spdk_nvme_ctrlr_detach_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid,
 			      struct spdk_nvme_ctrlr_list *payload);
 
 /**
- * \brief Create a namespace.
+ * Create a namespace.
+ *
+ * This function is thread safe and can be called at any point after spdk_nvme_probe().
  *
  * \param ctrlr NVMe controller to create namespace on.
  * \param payload The pointer to the NVMe namespace data.
  *
  * \return Namespace ID (>= 1) if successfully created, or 0 if the request failed.
- *
- * This function is thread safe and can be called at any point after spdk_nvme_attach().
  */
 uint32_t spdk_nvme_ctrlr_create_ns(struct spdk_nvme_ctrlr *ctrlr,
 				   struct spdk_nvme_ns_data *payload);
 
 /**
- * \brief Delete a namespace.
+ * Delete a namespace.
  *
- * \param ctrlr NVMe controller to delete namespace from.
- * \param nsid The namespace identifier.
+ * This function is thread safe and can be called at any point after spdk_nvme_probe().
  *
- * \return 0 if successfully submitted, ENOMEM if resources could not be allocated for this request
+ * Call spdk_nvme_ctrlr_process_admin_completions() to poll for completion of
+ * commands submitted through this function.
  *
- * This function is thread safe and can be called at any point after spdk_nvme_attach().
+ * \param ctrlr NVMe controller to delete namespace from.
+ * \param nsid The namespace identifier.
  *
- * Call \ref spdk_nvme_ctrlr_process_admin_completions() to poll for completion
- * of commands submitted through this function.
+ * \return 0 if successfully submitted, negated errno if resources could not be
+ * allocated
+ * for this request
  */
 int spdk_nvme_ctrlr_delete_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid);
 
 /**
- * \brief Format NVM.
+ * Format NVM.
  *
  * This function requests a low-level format of the media.
  *
+ * This function is thread safe and can be called at any point after spdk_nvme_probe().
+ *
  * \param ctrlr NVMe controller to format.
- * \param nsid The namespace identifier.  May be SPDK_NVME_GLOBAL_NS_TAG to format all namespaces.
+ * \param nsid The namespace identifier. May be SPDK_NVME_GLOBAL_NS_TAG to format
+ * all namespaces.
  * \param format The format information for the command.
  *
- * \return 0 if successfully submitted, ENOMEM if resources could not be allocated for this request
- *
- * This function is thread safe and can be called at any point after spdk_nvme_attach().
+ * \return 0 if successfully submitted, negated errno if resources could not be
+ * allocated for this request
  */
 int spdk_nvme_ctrlr_format(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid,
 			   struct spdk_nvme_format *format);
 
 /**
- * \brief Download a new firmware image.
+ * Download a new firmware image.
+ *
+ * This function is thread safe and can be called at any point after spdk_nvme_probe().
  *
+ * \param ctrlr NVMe controller to perform firmware operation on.
  * \param payload The data buffer for the firmware image.
  * \param size The data size will be downloaded.
  * \param slot The slot that the firmware image will be committed to.
+ * \param commit_action The action to perform when firmware is committed.
+ * \param completion_status output parameter. Contains the completion status of
+ * the firmware commit operation.
  *
- * \return 0 if successfully submitted, ENOMEM if resources could not be allocated for this request,
- * -1 if the size is not multiple of 4.
- *
- * This function is thread safe and can be called at any point after spdk_nvme_attach().
+ * \return 0 if successfully submitted, ENOMEM if resources could not be allocated
+ * for this request, -1 if the size is not multiple of 4.
  */
 int spdk_nvme_ctrlr_update_firmware(struct spdk_nvme_ctrlr *ctrlr, void *payload, uint32_t size,
-				    int slot);
+				    int slot, enum spdk_nvme_fw_commit_action commit_action,
+				    struct spdk_nvme_status *completion_status);
+
+/**
+ * Allocate an I/O buffer from the controller memory buffer (Experimental).
+ *
+ * This function allocates registered memory which belongs to the Controller
+ * Memory Buffer (CMB) of the specified NVMe controller. Note that the CMB has
+ * to support the WDS and RDS capabilities for the allocation to be successful.
+ * Also, due to vtophys contraints the CMB must be at least 4MiB in size. Free
+ * memory allocated with this function using spdk_nvme_ctrlr_free_cmb_io_buffer().
+ *
+ * \param ctrlr Controller from which to allocate memory buffer.
+ * \param size Size of buffer to allocate in bytes.
+ *
+ * \return Pointer to controller memory buffer allocation, or NULL if allocation
+ * was not possible.
+ */
+void *spdk_nvme_ctrlr_alloc_cmb_io_buffer(struct spdk_nvme_ctrlr *ctrlr, size_t size);
 
 /**
- * \brief Get the identify namespace data as defined by the NVMe specification.
+ * Free a controller memory I/O buffer (Experimental).
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * Note this function is currently a NOP which is one reason why this and
+ * spdk_nvme_ctrlr_alloc_cmb_io_buffer() are currently marked as experimental.
+ *
+ * \param ctrlr Controller from which the buffer was allocated.
+ * \param buf Buffer previously allocated by spdk_nvme_ctrlr_alloc_cmb_io_buffer().
+ * \param size Size of buf in bytes.
+ */
+void spdk_nvme_ctrlr_free_cmb_io_buffer(struct spdk_nvme_ctrlr *ctrlr, void *buf, size_t size);
+
+/**
+ * Get the identify namespace data as defined by the NVMe specification.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
+ *
+ * \param ns Namespace.
+ *
+ * \return a pointer to the namespace data.
  */
 const struct spdk_nvme_ns_data *spdk_nvme_ns_get_data(struct spdk_nvme_ns *ns);
 
 /**
- * \brief Get the namespace id (index number) from the given namespace handle.
+ * Get the namespace id (index number) from the given namespace handle.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
+ *
+ * \param ns Namespace.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * \return namespace id.
  */
 uint32_t spdk_nvme_ns_get_id(struct spdk_nvme_ns *ns);
 
 /**
- * \brief Determine whether a namespace is active.
+ * Get the controller with which this namespace is associated.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
+ *
+ * \param ns Namespace.
+ *
+ * \return a pointer to the controller.
+ */
+struct spdk_nvme_ctrlr *spdk_nvme_ns_get_ctrlr(struct spdk_nvme_ns *ns);
+
+/**
+ * Determine whether a namespace is active.
  *
  * Inactive namespaces cannot be the target of I/O commands.
+ *
+ * \param ns Namespace to query.
+ *
+ * \return true if active, or false if inactive.
  */
 bool spdk_nvme_ns_is_active(struct spdk_nvme_ns *ns);
 
 /**
- * \brief Get the maximum transfer size, in bytes, for an I/O sent to the given namespace.
+ * Get the maximum transfer size, in bytes, for an I/O sent to the given namespace.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * \param ns Namespace to query.
+ *
+ * \return the maximum transfer size in bytes.
  */
 uint32_t spdk_nvme_ns_get_max_io_xfer_size(struct spdk_nvme_ns *ns);
 
 /**
- * \brief Get the sector size, in bytes, of the given namespace.
+ * Get the sector size, in bytes, of the given namespace.
+ *
+ * This function returns the size of the data sector only.  It does not
+ * include metadata size.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * \param ns Namespace to query.
+ *
+ * /return the sector size in bytes.
  */
 uint32_t spdk_nvme_ns_get_sector_size(struct spdk_nvme_ns *ns);
 
 /**
- * \brief Get the number of sectors for the given namespace.
+ * Get the extended sector size, in bytes, of the given namespace.
+ *
+ * This function returns the size of the data sector plus metadata.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
+ *
+ * \param ns Namespace to query.
+ *
+ * /return the extended sector size in bytes.
+ */
+uint32_t spdk_nvme_ns_get_extended_sector_size(struct spdk_nvme_ns *ns);
+
+/**
+ * Get the number of sectors for the given namespace.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * \param ns Namespace to query.
+ *
+ * \return the number of sectors.
  */
 uint64_t spdk_nvme_ns_get_num_sectors(struct spdk_nvme_ns *ns);
 
 /**
- * \brief Get the size, in bytes, of the given namespace.
+ * Get the size, in bytes, of the given namespace.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
+ *
+ * \param ns Namespace to query.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * \return the size of the given namespace in bytes.
  */
 uint64_t spdk_nvme_ns_get_size(struct spdk_nvme_ns *ns);
 
 /**
- * \brief Get the end-to-end data protection information type of the given namespace.
+ * Get the end-to-end data protection information type of the given namespace.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
+ *
+ * \param ns Namespace to query.
+ *
+ * \return the end-to-end data protection information type.
  */
 enum spdk_nvme_pi_type spdk_nvme_ns_get_pi_type(struct spdk_nvme_ns *ns);
 
 /**
- * \brief Get the metadata size, in bytes, of the given namespace.
+ * Get the metadata size, in bytes, of the given namespace.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * \param ns Namespace to query.
+ *
+ * \return the metadata size of the given namespace in bytes.
  */
 uint32_t spdk_nvme_ns_get_md_size(struct spdk_nvme_ns *ns);
 
 /**
- * \brief True if the namespace can support extended LBA when end-to-end data protection enabled.
+ * Check whether if the namespace can support extended LBA when end-to-end data
+ * protection enabled.
+ *
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
+ *
+ * \param ns Namespace to query.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * \return true if the namespace can support extended LBA when end-to-end data
+ * protection enabled, or false otherwise.
  */
 bool spdk_nvme_ns_supports_extended_lba(struct spdk_nvme_ns *ns);
 
 /**
+ * Determine the value returned when reading deallocated blocks.
+ *
+ * If deallocated blocks return 0, the deallocate command can be used as a more
+ * efficient alternative to the write_zeroes command, especially for large requests.
+ *
+ * \param ns Namespace.
+ *
+ * \return the logical block read value.
+ */
+enum spdk_nvme_dealloc_logical_block_read_value spdk_nvme_ns_get_dealloc_logical_block_read_value(
+	struct spdk_nvme_ns *ns);
+
+/**
+ * Get the optimal I/O boundary, in blocks, for the given namespace.
+ *
+ * Read and write commands should not cross the optimal I/O boundary for best
+ * performance.
+ *
+ * \param ns Namespace to query.
+ *
+ * \return Optimal granularity of I/O commands, in blocks, or 0 if no optimal
+ * granularity is reported.
+ */
+uint32_t spdk_nvme_ns_get_optimal_io_boundary(struct spdk_nvme_ns *ns);
+
+/**
+ * Get the UUID for the given namespace.
+ *
+ * \param ns Namespace to query.
+ *
+ * \return a pointer to namespace UUID, or NULL if ns does not have a UUID.
+ */
+const struct spdk_uuid *spdk_nvme_ns_get_uuid(const struct spdk_nvme_ns *ns);
+
+/**
  * \brief Namespace command support flags.
  */
 enum spdk_nvme_ns_flags {
@@ -858,19 +1696,24 @@ enum spdk_nvme_ns_flags {
 };
 
 /**
- * \brief Get the flags for the given namespace.
+ * Get the flags for the given namespace.
  *
  * See spdk_nvme_ns_flags for the possible flags returned.
  *
- * This function is thread safe and can be called at any point while the controller is attached to
- *  the SPDK NVMe driver.
+ * This function is thread safe and can be called at any point while the controller
+ * is attached to the SPDK NVMe driver.
+ *
+ * \param ns Namespace to query.
+ *
+ * \return the flags for the given namespace.
  */
 uint32_t spdk_nvme_ns_get_flags(struct spdk_nvme_ns *ns);
 
 /**
  * Restart the SGL walk to the specified offset when the command has scattered payloads.
  *
- * The cb_arg parameter is the value passed to readv/writev.
+ * \param cb_arg Argument passed to readv/writev.
+ * \param offset Offset for SGL.
  */
 typedef void (*spdk_nvme_req_reset_sgl_cb)(void *cb_arg, uint32_t offset);
 
@@ -878,54 +1721,58 @@ typedef void (*spdk_nvme_req_reset_sgl_cb)(void *cb_arg, uint32_t offset);
  * Fill out *address and *length with the current SGL entry and advance to the next
  * entry for the next time the callback is invoked.
  *
- * The cb_arg parameter is the value passed to readv/writev.
- * The address parameter contains the virtual address of this segment.
- * The length parameter contains the length of this physical segment.
+ * The described segment must be physically contiguous.
+ *
+ * \param cb_arg Argument passed to readv/writev.
+ * \param address Virtual address of this segment.
+ * \param length Length of this physical segment.
  */
 typedef int (*spdk_nvme_req_next_sge_cb)(void *cb_arg, void **address, uint32_t *length);
 
 /**
- * \brief Submits a write I/O to the specified NVMe namespace.
- *
- * \param ns NVMe namespace to submit the write I/O
- * \param qpair I/O queue pair to submit the request
- * \param payload virtual address pointer to the data payload
- * \param lba starting LBA to write the data
- * \param lba_count length (in sectors) for the write operation
- * \param cb_fn callback function to invoke when the I/O is completed
- * \param cb_arg argument to pass to the callback function
- * \param io_flags set flags, defined by the SPDK_NVME_IO_FLAGS_* entries
- * 			in spdk/nvme_spec.h, for this I/O.
- *
- * \return 0 if successfully submitted, ENOMEM if an nvme_request
- *	     structure cannot be allocated for the I/O request
+ * Submit a write I/O to the specified NVMe namespace.
  *
  * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ns NVMe namespace to submit the write I/O.
+ * \param qpair I/O queue pair to submit the request.
+ * \param payload Virtual address pointer to the data payload.
+ * \param lba Starting LBA to write the data.
+ * \param lba_count Length (in sectors) for the write operation.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ * \param io_flags Set flags, defined by the SPDK_NVME_IO_FLAGS_* entries in
+ * spdk/nvme_spec.h, for this I/O.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request
  */
 int spdk_nvme_ns_cmd_write(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair, void *payload,
 			   uint64_t lba, uint32_t lba_count, spdk_nvme_cmd_cb cb_fn,
 			   void *cb_arg, uint32_t io_flags);
 
 /**
- * \brief Submits a write I/O to the specified NVMe namespace.
- *
- * \param ns NVMe namespace to submit the write I/O
- * \param qpair I/O queue pair to submit the request
- * \param lba starting LBA to write the data
- * \param lba_count length (in sectors) for the write operation
- * \param cb_fn callback function to invoke when the I/O is completed
- * \param cb_arg argument to pass to the callback function
- * \param io_flags set flags, defined in nvme_spec.h, for this I/O
- * \param reset_sgl_fn callback function to reset scattered payload
- * \param next_sge_fn callback function to iterate each scattered
- * payload memory segment
- *
- * \return 0 if successfully submitted, ENOMEM if an nvme_request
- *	     structure cannot be allocated for the I/O request
+ * Submit a write I/O to the specified NVMe namespace.
  *
  * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ns NVMe namespace to submit the write I/O.
+ * \param qpair I/O queue pair to submit the request.
+ * \param lba Starting LBA to write the data.
+ * \param lba_count Length (in sectors) for the write operation.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ * \param io_flags Set flags, defined in nvme_spec.h, for this I/O.
+ * \param reset_sgl_fn Callback function to reset scattered payload.
+ * \param next_sge_fn Callback function to iterate each scattered payload memory
+ * segment.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
  */
 int spdk_nvme_ns_cmd_writev(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 			    uint64_t lba, uint32_t lba_count,
@@ -934,27 +1781,60 @@ int spdk_nvme_ns_cmd_writev(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpa
 			    spdk_nvme_req_next_sge_cb next_sge_fn);
 
 /**
- * \brief Submits a write I/O to the specified NVMe namespace.
+ * Submit a write I/O to the specified NVMe namespace.
+ *
+ * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
  *
  * \param ns NVMe namespace to submit the write I/O
  * \param qpair I/O queue pair to submit the request
- * \param payload virtual address pointer to the data payload
- * \param metadata virtual address pointer to the metadata payload, the length
- *	           of metadata is specified by spdk_nvme_ns_get_md_size()
  * \param lba starting LBA to write the data
  * \param lba_count length (in sectors) for the write operation
  * \param cb_fn callback function to invoke when the I/O is completed
  * \param cb_arg argument to pass to the callback function
- * \param io_flags set flags, defined by the SPDK_NVME_IO_FLAGS_* entries
- * 			in spdk/nvme_spec.h, for this I/O.
+ * \param io_flags set flags, defined in nvme_spec.h, for this I/O
+ * \param reset_sgl_fn callback function to reset scattered payload
+ * \param next_sge_fn callback function to iterate each scattered
+ * payload memory segment
+ * \param metadata virtual address pointer to the metadata payload, the length
+ * of metadata is specified by spdk_nvme_ns_get_md_size()
  * \param apptag_mask application tag mask.
  * \param apptag application tag to use end-to-end protection information.
  *
- * \return 0 if successfully submitted, ENOMEM if an nvme_request
- *	     structure cannot be allocated for the I/O request
+ * \return 0 if successfully submitted, ENOMEM if an nvme_request structure
+ * cannot be allocated for the I/O request.
+ */
+int spdk_nvme_ns_cmd_writev_with_md(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
+				    uint64_t lba, uint32_t lba_count,
+				    spdk_nvme_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+				    spdk_nvme_req_reset_sgl_cb reset_sgl_fn,
+				    spdk_nvme_req_next_sge_cb next_sge_fn, void *metadata,
+				    uint16_t apptag_mask, uint16_t apptag);
+
+/**
+ * Submit a write I/O to the specified NVMe namespace.
  *
  * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ns NVMe namespace to submit the write I/O.
+ * \param qpair I/O queue pair to submit the request.
+ * \param payload Virtual address pointer to the data payload.
+ * \param metadata Virtual address pointer to the metadata payload, the length
+ * of metadata is specified by spdk_nvme_ns_get_md_size().
+ * \param lba Starting LBA to write the data.
+ * \param lba_count Length (in sectors) for the write operation.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ * \param io_flags Set flags, defined by the SPDK_NVME_IO_FLAGS_* entries in
+ * spdk/nvme_spec.h, for this I/O.
+ * \param apptag_mask Application tag mask.
+ * \param apptag Application tag to use end-to-end protection information.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
  */
 int spdk_nvme_ns_cmd_write_with_md(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 				   void *payload, void *metadata,
@@ -963,22 +1843,23 @@ int spdk_nvme_ns_cmd_write_with_md(struct spdk_nvme_ns *ns, struct spdk_nvme_qpa
 				   uint16_t apptag_mask, uint16_t apptag);
 
 /**
- * \brief Submits a write zeroes I/O to the specified NVMe namespace.
- *
- * \param ns NVMe namespace to submit the write zeroes I/O
- * \param qpair I/O queue pair to submit the request
- * \param lba starting LBA for this command
- * \param lba_count length (in sectors) for the write zero operation
- * \param cb_fn callback function to invoke when the I/O is completed
- * \param cb_arg argument to pass to the callback function
- * \param io_flags set flags, defined by the SPDK_NVME_IO_FLAGS_* entries
- * 			in spdk/nvme_spec.h, for this I/O.
- *
- * \return 0 if successfully submitted, ENOMEM if an nvme_request
- *	     structure cannot be allocated for the I/O request
+ * Submit a write zeroes I/O to the specified NVMe namespace.
  *
  * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ns NVMe namespace to submit the write zeroes I/O.
+ * \param qpair I/O queue pair to submit the request.
+ * \param lba Starting LBA for this command.
+ * \param lba_count Length (in sectors) for the write zero operation.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ * \param io_flags Set flags, defined by the SPDK_NVME_IO_FLAGS_* entries in
+ * spdk/nvme_spec.h, for this I/O.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
  */
 int spdk_nvme_ns_cmd_write_zeroes(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 				  uint64_t lba, uint32_t lba_count,
@@ -988,44 +1869,46 @@ int spdk_nvme_ns_cmd_write_zeroes(struct spdk_nvme_ns *ns, struct spdk_nvme_qpai
 /**
  * \brief Submits a read I/O to the specified NVMe namespace.
  *
- * \param ns NVMe namespace to submit the read I/O
- * \param qpair I/O queue pair to submit the request
- * \param payload virtual address pointer to the data payload
- * \param lba starting LBA to read the data
- * \param lba_count length (in sectors) for the read operation
- * \param cb_fn callback function to invoke when the I/O is completed
- * \param cb_arg argument to pass to the callback function
- * \param io_flags set flags, defined in nvme_spec.h, for this I/O
- *
- * \return 0 if successfully submitted, ENOMEM if an nvme_request
- *	     structure cannot be allocated for the I/O request
- *
  * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ns NVMe namespace to submit the read I/O.
+ * \param qpair I/O queue pair to submit the request.
+ * \param payload Virtual address pointer to the data payload.
+ * \param lba Starting LBA to read the data.
+ * \param lba_count Length (in sectors) for the read operation.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ * \param io_flags Set flags, defined in nvme_spec.h, for this I/O.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
  */
 int spdk_nvme_ns_cmd_read(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair, void *payload,
 			  uint64_t lba, uint32_t lba_count, spdk_nvme_cmd_cb cb_fn,
 			  void *cb_arg, uint32_t io_flags);
 
 /**
- * \brief Submits a read I/O to the specified NVMe namespace.
- *
- * \param ns NVMe namespace to submit the read I/O
- * \param qpair I/O queue pair to submit the request
- * \param lba starting LBA to read the data
- * \param lba_count length (in sectors) for the read operation
- * \param cb_fn callback function to invoke when the I/O is completed
- * \param cb_arg argument to pass to the callback function
- * \param io_flags set flags, defined in nvme_spec.h, for this I/O
- * \param reset_sgl_fn callback function to reset scattered payload
- * \param next_sge_fn callback function to iterate each scattered
- * payload memory segment
- *
- * \return 0 if successfully submitted, ENOMEM if an nvme_request
- *	     structure cannot be allocated for the I/O request
+ * Submit a read I/O to the specified NVMe namespace.
  *
  * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ns NVMe namespace to submit the read I/O.
+ * \param qpair I/O queue pair to submit the request.
+ * \param lba Starting LBA to read the data.
+ * \param lba_count Length (in sectors) for the read operation.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ * \param io_flags Set flags, defined in nvme_spec.h, for this I/O.
+ * \param reset_sgl_fn Callback function to reset scattered payload.
+ * \param next_sge_fn Callback function to iterate each scattered payload memory
+ * segment.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
  */
 int spdk_nvme_ns_cmd_readv(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 			   uint64_t lba, uint32_t lba_count,
@@ -1034,18 +1917,20 @@ int spdk_nvme_ns_cmd_readv(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpai
 			   spdk_nvme_req_next_sge_cb next_sge_fn);
 
 /**
- * \brief Submits a read I/O to the specified NVMe namespace.
+ * Submit a read I/O to the specified NVMe namespace.
  *
  * \param ns NVMe namespace to submit the read I/O
  * \param qpair I/O queue pair to submit the request
- * \param payload virtual address pointer to the data payload
- * \param metadata virtual address pointer to the metadata payload, the length
- *	           of metadata is specified by spdk_nvme_ns_get_md_size()
  * \param lba starting LBA to read the data
  * \param lba_count length (in sectors) for the read operation
  * \param cb_fn callback function to invoke when the I/O is completed
  * \param cb_arg argument to pass to the callback function
  * \param io_flags set flags, defined in nvme_spec.h, for this I/O
+ * \param reset_sgl_fn callback function to reset scattered payload
+ * \param next_sge_fn callback function to iterate each scattered
+ * payload memory segment
+ * \param metadata virtual address pointer to the metadata payload, the length
+ *	           of metadata is specified by spdk_nvme_ns_get_md_size()
  * \param apptag_mask application tag mask.
  * \param apptag application tag to use end-to-end protection information.
  *
@@ -1055,6 +1940,32 @@ int spdk_nvme_ns_cmd_readv(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpai
  * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
  * The user must ensure that only one thread submits I/O on a given qpair at any given time.
  */
+int spdk_nvme_ns_cmd_readv_with_md(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
+				   uint64_t lba, uint32_t lba_count,
+				   spdk_nvme_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+				   spdk_nvme_req_reset_sgl_cb reset_sgl_fn,
+				   spdk_nvme_req_next_sge_cb next_sge_fn, void *metadata,
+				   uint16_t apptag_mask, uint16_t apptag);
+
+/**
+ * Submits a read I/O to the specified NVMe namespace.
+ *
+ * \param ns NVMe namespace to submit the read I/O
+ * \param qpair I/O queue pair to submit the request
+ * \param payload virtual address pointer to the data payload
+ * \param metadata virtual address pointer to the metadata payload, the length
+ * of metadata is specified by spdk_nvme_ns_get_md_size().
+ * \param lba starting LBA to read the data.
+ * \param lba_count Length (in sectors) for the read operation.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ * \param io_flags Set flags, defined in nvme_spec.h, for this I/O.
+ * \param apptag_mask Application tag mask.
+ * \param apptag Application tag to use end-to-end protection information.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
+ */
 int spdk_nvme_ns_cmd_read_with_md(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 				  void *payload, void *metadata,
 				  uint64_t lba, uint32_t lba_count, spdk_nvme_cmd_cb cb_fn,
@@ -1062,29 +1973,31 @@ int spdk_nvme_ns_cmd_read_with_md(struct spdk_nvme_ns *ns, struct spdk_nvme_qpai
 				  uint16_t apptag_mask, uint16_t apptag);
 
 /**
- * \brief Submits a data set management request to the specified NVMe namespace. Data set
- *        management operations are designed to optimize interaction with the block
- *        translation layer inside the device. The most common type of operation is
- *        deallocate, which is often referred to as TRIM or UNMAP.
+ * Submit a data set management request to the specified NVMe namespace. Data set
+ * management operations are designed to optimize interaction with the block
+ * translation layer inside the device. The most common type of operation is
+ * deallocate, which is often referred to as TRIM or UNMAP.
+ *
+ * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * This is a convenience wrapper that will automatically allocate and construct
+ * the correct data buffers. Therefore, ranges does not need to be allocated from
+ * pinned memory and can be placed on the stack. If a higher performance, zero-copy
+ * version of DSM is required, simply build and submit a raw command using
+ * spdk_nvme_ctrlr_cmd_io_raw().
  *
  * \param ns NVMe namespace to submit the DSM request
- * \param type A bit field constructed from \ref enum spdk_nvme_dsm_attribute.
+ * \param type A bit field constructed from \ref spdk_nvme_dsm_attribute.
  * \param qpair I/O queue pair to submit the request
- * \param ranges An array of \ref spdk_nvme_dsm_range elements describing
- 		 the LBAs to operate on.
+ * \param ranges An array of \ref spdk_nvme_dsm_range elements describing the LBAs
+ * to operate on.
  * \param num_ranges The number of elements in the ranges array.
- * \param cb_fn callback function to invoke when the I/O is completed
- * \param cb_arg argument to pass to the callback function
+ * \param cb_fn Callback function to invoke when the I/O is completed
+ * \param cb_arg Argument to pass to the callback function
  *
  * \return 0 if successfully submitted, negated POSIX errno values otherwise.
- *
- * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
- *
- * This is a convenience wrapper that will automatically allocate and construct the correct
- * data buffers. Therefore, ranges does not need to be allocated from pinned memory and
- * can be placed on the stack. If a higher performance, zero-copy version of DSM is
- * required, simply build and submit a raw command using spdk_nvme_ctrlr_cmd_io_raw().
  */
 int spdk_nvme_ns_cmd_dataset_management(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 					uint32_t type,
@@ -1094,39 +2007,41 @@ int spdk_nvme_ns_cmd_dataset_management(struct spdk_nvme_ns *ns, struct spdk_nvm
 					void *cb_arg);
 
 /**
- * \brief Submits a flush request to the specified NVMe namespace.
+ * Submit a flush request to the specified NVMe namespace.
  *
- * \param ns NVMe namespace to submit the flush request
- * \param qpair I/O queue pair to submit the request
- * \param cb_fn callback function to invoke when the I/O is completed
- * \param cb_arg argument to pass to the callback function
+ * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
  *
- * \return 0 if successfully submitted, ENOMEM if an nvme_request
- *	     structure cannot be allocated for the I/O request
+ * \param ns NVMe namespace to submit the flush request.
+ * \param qpair I/O queue pair to submit the request.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
  *
- * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
  */
 int spdk_nvme_ns_cmd_flush(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
 			   spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 
 /**
- * \brief Submits a reservation register to the specified NVMe namespace.
- *
- * \param ns NVMe namespace to submit the reservation register request
- * \param qpair I/O queue pair to submit the request
- * \param payload virtual address pointer to the reservation register data
- * \param ignore_key '1' the current reservation key check is disabled
- * \param action specifies the registration action
- * \param cptpl change the Persist Through Power Loss state
- * \param cb_fn callback function to invoke when the I/O is completed
- * \param cb_arg argument to pass to the callback function
- *
- * \return 0 if successfully submitted, ENOMEM if an nvme_request
- *	     structure cannot be allocated for the I/O request
+ * Submit a reservation register to the specified NVMe namespace.
  *
  * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ns NVMe namespace to submit the reservation register request.
+ * \param qpair I/O queue pair to submit the request.
+ * \param payload Virtual address pointer to the reservation register data.
+ * \param ignore_key '1' the current reservation key check is disabled.
+ * \param action Specifies the registration action.
+ * \param cptpl Change the Persist Through Power Loss state.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
  */
 int spdk_nvme_ns_cmd_reservation_register(struct spdk_nvme_ns *ns,
 		struct spdk_nvme_qpair *qpair,
@@ -1137,22 +2052,23 @@ int spdk_nvme_ns_cmd_reservation_register(struct spdk_nvme_ns *ns,
 		spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 
 /**
- * \brief Submits a reservation release to the specified NVMe namespace.
- *
- * \param ns NVMe namespace to submit the reservation release request
- * \param qpair I/O queue pair to submit the request
- * \param payload virtual address pointer to current reservation key
- * \param ignore_key '1' the current reservation key check is disabled
- * \param action specifies the reservation release action
- * \param type reservation type for the namespace
- * \param cb_fn callback function to invoke when the I/O is completed
- * \param cb_arg argument to pass to the callback function
- *
- * \return 0 if successfully submitted, ENOMEM if an nvme_request
- *	     structure cannot be allocated for the I/O request
+ * Submits a reservation release to the specified NVMe namespace.
  *
  * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ns NVMe namespace to submit the reservation release request.
+ * \param qpair I/O queue pair to submit the request.
+ * \param payload Virtual address pointer to current reservation key.
+ * \param ignore_key '1' the current reservation key check is disabled.
+ * \param action Specifies the reservation release action.
+ * \param type Reservation type for the namespace.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
  */
 int spdk_nvme_ns_cmd_reservation_release(struct spdk_nvme_ns *ns,
 		struct spdk_nvme_qpair *qpair,
@@ -1163,22 +2079,23 @@ int spdk_nvme_ns_cmd_reservation_release(struct spdk_nvme_ns *ns,
 		spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 
 /**
- * \brief Submits a reservation acquire to the specified NVMe namespace.
- *
- * \param ns NVMe namespace to submit the reservation acquire request
- * \param qpair I/O queue pair to submit the request
- * \param payload virtual address pointer to reservation acquire data
- * \param ignore_key '1' the current reservation key check is disabled
- * \param action specifies the reservation acquire action
- * \param type reservation type for the namespace
- * \param cb_fn callback function to invoke when the I/O is completed
- * \param cb_arg argument to pass to the callback function
- *
- * \return 0 if successfully submitted, ENOMEM if an nvme_request
- *	     structure cannot be allocated for the I/O request
+ * Submits a reservation acquire to the specified NVMe namespace.
  *
  * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ns NVMe namespace to submit the reservation acquire request.
+ * \param qpair I/O queue pair to submit the request.
+ * \param payload Virtual address pointer to reservation acquire data.
+ * \param ignore_key '1' the current reservation key check is disabled.
+ * \param action Specifies the reservation acquire action.
+ * \param type Reservation type for the namespace.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
  */
 int spdk_nvme_ns_cmd_reservation_acquire(struct spdk_nvme_ns *ns,
 		struct spdk_nvme_qpair *qpair,
@@ -1189,26 +2106,201 @@ int spdk_nvme_ns_cmd_reservation_acquire(struct spdk_nvme_ns *ns,
 		spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 
 /**
- * \brief Submits a reservation report to the specified NVMe namespace.
- *
- * \param ns NVMe namespace to submit the reservation report request
- * \param qpair I/O queue pair to submit the request
- * \param payload virtual address pointer for reservation status data
- * \param len length bytes for reservation status data structure
- * \param cb_fn callback function to invoke when the I/O is completed
- * \param cb_arg argument to pass to the callback function
- *
- * \return 0 if successfully submitted, ENOMEM if an nvme_request
- *	     structure cannot be allocated for the I/O request
+ * Submit a reservation report to the specified NVMe namespace.
  *
  * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
- * The user must ensure that only one thread submits I/O on a given qpair at any given time.
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ns NVMe namespace to submit the reservation report request.
+ * \param qpair I/O queue pair to submit the request.
+ * \param payload Virtual address pointer for reservation status data.
+ * \param len Length bytes for reservation status data structure.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
  */
 int spdk_nvme_ns_cmd_reservation_report(struct spdk_nvme_ns *ns,
 					struct spdk_nvme_qpair *qpair,
 					void *payload, uint32_t len,
 					spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 
+/**
+ * Submit a compare I/O to the specified NVMe namespace.
+ *
+ * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ns NVMe namespace to submit the compare I/O.
+ * \param qpair I/O queue pair to submit the request.
+ * \param payload Virtual address pointer to the data payload.
+ * \param lba Starting LBA to compare the data.
+ * \param lba_count Length (in sectors) for the compare operation.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ * \param io_flags Set flags, defined in nvme_spec.h, for this I/O.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
+ */
+int spdk_nvme_ns_cmd_compare(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair, void *payload,
+			     uint64_t lba, uint32_t lba_count, spdk_nvme_cmd_cb cb_fn,
+			     void *cb_arg, uint32_t io_flags);
+
+/**
+ * Submit a compare I/O to the specified NVMe namespace.
+ *
+ * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ns NVMe namespace to submit the compare I/O.
+ * \param qpair I/O queue pair to submit the request.
+ * \param lba Starting LBA to compare the data.
+ * \param lba_count Length (in sectors) for the compare operation.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ * \param io_flags Set flags, defined in nvme_spec.h, for this I/O.
+ * \param reset_sgl_fn Callback function to reset scattered payload.
+ * \param next_sge_fn Callback function to iterate each scattered payload memory
+ * segment.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
+ */
+int spdk_nvme_ns_cmd_comparev(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
+			      uint64_t lba, uint32_t lba_count,
+			      spdk_nvme_cmd_cb cb_fn, void *cb_arg, uint32_t io_flags,
+			      spdk_nvme_req_reset_sgl_cb reset_sgl_fn,
+			      spdk_nvme_req_next_sge_cb next_sge_fn);
+
+/**
+ * Submit a compare I/O to the specified NVMe namespace.
+ *
+ * The command is submitted to a qpair allocated by spdk_nvme_ctrlr_alloc_io_qpair().
+ * The user must ensure that only one thread submits I/O on a given qpair at any
+ * given time.
+ *
+ * \param ns NVMe namespace to submit the compare I/O.
+ * \param qpair I/O queue pair to submit the request.
+ * \param payload Virtual address pointer to the data payload.
+ * \param metadata Virtual address pointer to the metadata payload, the length
+ * of metadata is specified by spdk_nvme_ns_get_md_size().
+ * \param lba Starting LBA to compare the data.
+ * \param lba_count Length (in sectors) for the compare operation.
+ * \param cb_fn Callback function to invoke when the I/O is completed.
+ * \param cb_arg Argument to pass to the callback function.
+ * \param io_flags Set flags, defined in nvme_spec.h, for this I/O.
+ * \param apptag_mask Application tag mask.
+ * \param apptag Application tag to use end-to-end protection information.
+ *
+ * \return 0 if successfully submitted, negated errno if an nvme_request structure
+ * cannot be allocated for the I/O request.
+ */
+int spdk_nvme_ns_cmd_compare_with_md(struct spdk_nvme_ns *ns, struct spdk_nvme_qpair *qpair,
+				     void *payload, void *metadata,
+				     uint64_t lba, uint32_t lba_count, spdk_nvme_cmd_cb cb_fn,
+				     void *cb_arg, uint32_t io_flags,
+				     uint16_t apptag_mask, uint16_t apptag);
+
+/**
+ * \brief Inject an error for the next request with a given opcode.
+ *
+ * \param ctrlr NVMe controller.
+ * \param qpair I/O queue pair to add the error command,
+ *              NULL for Admin queue pair.
+ * \param opc Opcode for Admin or I/O commands.
+ * \param do_not_submit True if matching requests should not be submitted
+ *                      to the controller, but instead completed manually
+ *                      after timeout_in_us has expired.  False if matching
+ *                      requests should be submitted to the controller and
+ *                      have their completion status modified after the
+ *                      controller completes the request.
+ * \param timeout_in_us Wait specified microseconds when do_not_submit is true.
+ * \param err_count Number of matching requests to inject errors.
+ * \param sct Status code type.
+ * \param sc Status code.
+ *
+ * \return 0 if successfully enabled, ENOMEM if an error command
+ *	     structure cannot be allocated.
+ *
+ * The function can be called multiple times to inject errors for different
+ * commands.  If the opcode matches an existing entry, the existing entry
+ * will be updated with the values specified.
+ */
+int spdk_nvme_qpair_add_cmd_error_injection(struct spdk_nvme_ctrlr *ctrlr,
+		struct spdk_nvme_qpair *qpair,
+		uint8_t opc,
+		bool do_not_submit,
+		uint64_t timeout_in_us,
+		uint32_t err_count,
+		uint8_t sct, uint8_t sc);
+
+/**
+ * \brief Clear the specified NVMe command with error status.
+ *
+ * \param ctrlr NVMe controller.
+ * \param qpair I/O queue pair to remove the error command,
+ * \            NULL for Admin queue pair.
+ * \param opc Opcode for Admin or I/O commands.
+ *
+ * The function will remove specified command in the error list.
+ */
+void spdk_nvme_qpair_remove_cmd_error_injection(struct spdk_nvme_ctrlr *ctrlr,
+		struct spdk_nvme_qpair *qpair,
+		uint8_t opc);
+
+#ifdef SPDK_CONFIG_RDMA
+struct ibv_context;
+struct ibv_pd;
+struct ibv_mr;
+
+/**
+ * RDMA Transport Hooks
+ */
+struct spdk_nvme_rdma_hooks {
+	/**
+	 * \brief Get an InfiniBand Verbs protection domain.
+	 *
+	 * \param trid the transport id
+	 * \param verbs Infiniband verbs context
+	 *
+	 * \return pd of the nvme ctrlr
+	 */
+	struct ibv_pd *(*get_ibv_pd)(const struct spdk_nvme_transport_id *trid,
+				     struct ibv_context *verbs);
+
+	/**
+	 * \brief Get an InfiniBand Verbs memory region for a buffer.
+	 *
+	 * \param pd The protection domain returned from get_ibv_pd
+	 * \param buf Memory buffer for which an rkey should be returned.
+	 * \param size size of buf
+	 *
+	 * \return Infiniband remote key (rkey) for this buf
+	 */
+	uint64_t (*get_rkey)(struct ibv_pd *pd, void *buf, size_t size);
+};
+
+/**
+ * \brief Set the global hooks for the RDMA transport, if necessary.
+ *
+ * This call is optional and must be performed prior to probing for
+ * any devices. By default, the RDMA transport will use the ibverbs
+ * library to create protection domains and register memory. This
+ * is a mechanism to subvert that and use an existing registration.
+ *
+ * This function may only be called one time per process.
+ *
+ * \param hooks for initializing global hooks
+ */
+void spdk_nvme_rdma_init_hooks(struct spdk_nvme_rdma_hooks *hooks);
+
+#endif
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/PDK/core/src/api/include/udd/spdk/nvme_internal.h b/PDK/core/src/api/include/udd/spdk/nvme_internal.h
index 98dbbfd..95dda6a 100644
--- a/PDK/core/src/api/include/udd/spdk/nvme_internal.h
+++ b/PDK/core/src/api/include/udd/spdk/nvme_internal.h
@@ -34,6 +34,8 @@
 #ifndef __NVME_INTERNAL_H__
 #define __NVME_INTERNAL_H__
 
+#include "spdk/config.h"
+#include "spdk/likely.h"
 #include "spdk/stdinc.h"
 
 #include "spdk/nvme.h"
@@ -50,11 +52,15 @@
 #include "spdk/util.h"
 #include "spdk/nvme_intel.h"
 #include "spdk/nvmf_spec.h"
+#include "spdk/uuid.h"
 
 #include "spdk_internal/assert.h"
 #include "spdk_internal/log.h"
+#include "spdk_internal/memory.h"
 
-#include "spdk/kvnvme_spdk.h"
+#include "kv_types.h"
+
+extern pid_t g_spdk_nvme_pid;
 
 /*
  * Some Intel devices support vendor-unique read latency log page even
@@ -86,10 +92,42 @@
  */
 #define NVME_QUIRK_DELAY_AFTER_QUEUE_ALLOC 0x10
 
+/*
+ * Earlier NVMe devices do not indicate whether unmapped blocks
+ * will read all zeroes or not. This define indicates that the
+ * device does in fact read all zeroes after an unmap event
+ */
+#define NVME_QUIRK_READ_ZERO_AFTER_DEALLOCATE 0x20
+
+/*
+ * The controller doesn't handle Identify value others than 0 or 1 correctly.
+ */
+#define NVME_QUIRK_IDENTIFY_CNS 0x40
+
+/*
+ * The controller supports Open Channel command set if matching additional
+ * condition, like the first byte (value 0x1) in the vendor specific
+ * bits of the namespace identify structure is set.
+ */
+#define NVME_QUIRK_OCSSD 0x80
+
+/*
+ * The controller has an Intel vendor ID but does not support Intel vendor-specific
+ * log pages.  This is primarily for QEMU emulated SSDs which report an Intel vendor
+ * ID but do not support these log pages.
+ */
+#define NVME_INTEL_QUIRK_NO_LOG_PAGES 0x100
+
+/*
+ * The controller does not set SHST_COMPLETE in a reasonable amount of time.  This
+ * is primarily seen in virtual VMWare NVMe SSDs.  This quirk merely adds an additional
+ * error message that on VMWare NVMe SSDs, the shutdown timeout may be expected.
+ */
+#define NVME_QUIRK_SHST_COMPLETE 0x200
+
 #define NVME_MAX_ASYNC_EVENTS	(8)
 
-#define NVME_MIN_TIMEOUT_PERIOD		(5)
-#define NVME_MAX_TIMEOUT_PERIOD		(120)
+#define NVME_MAX_ADMIN_TIMEOUT_IN_SECS	(30)
 
 /* Maximum log page size to fetch for AERs. */
 #define NVME_MAX_AER_LOG_SIZE		(4096)
@@ -105,7 +143,14 @@
 #define DEFAULT_ADMIN_QUEUE_REQUESTS	(32)
 #define DEFAULT_IO_QUEUE_REQUESTS	(512)
 
-#define DEFAULT_HOSTNQN			"nqn.2016-06.io.spdk:host"
+#define MIN_KEEP_ALIVE_TIMEOUT_IN_MS	(10000)
+
+/* We want to fit submission and completion rings each in a single 2MB
+ * hugepage to ensure physical address contiguity.
+ */
+#define MAX_IO_QUEUE_ENTRIES		(VALUE_2MB / spdk_max( \
+						sizeof(struct spdk_nvme_cmd), \
+						sizeof(struct spdk_nvme_cpl)))
 
 enum nvme_payload_type {
 	NVME_PAYLOAD_TYPE_INVALID = 0,
@@ -117,56 +162,71 @@ enum nvme_payload_type {
 	NVME_PAYLOAD_TYPE_SGL,
 };
 
-/*
- * Controller support flags.
- */
-enum spdk_nvme_ctrlr_flags {
-	SPDK_NVME_CTRLR_SGL_SUPPORTED		= 0x1, /**< The SGL is supported */
-};
-
 /**
  * Descriptor for a request data payload.
- *
- * This struct is arranged so that it fits nicely in struct nvme_request.
  */
-struct __attribute__((packed)) nvme_payload {
-	union {
-		/** Virtual memory address of a single physically contiguous buffer */
-		void *contig;
-
-		/**
-		 * Functions for retrieving physical addresses for scattered payloads.
-		 */
-		struct nvme_sgl_args {
-			spdk_nvme_req_reset_sgl_cb reset_sgl_fn;
-			spdk_nvme_req_next_sge_cb next_sge_fn;
-			void *cb_arg;
-		} sgl;
-	} u;
-
-	/** Virtual memory address of a single physically contiguous metadata buffer */
+struct nvme_payload {
+	/**
+	 * Functions for retrieving physical addresses for scattered payloads.
+	 */
+	spdk_nvme_req_reset_sgl_cb reset_sgl_fn;
+	spdk_nvme_req_next_sge_cb next_sge_fn;
+
+	/**
+	 * If reset_sgl_fn == NULL, this is a contig payload, and contig_or_cb_arg contains the
+	 * virtual memory address of a single virtually contiguous buffer.
+	 *
+	 * If reset_sgl_fn != NULL, this is a SGL payload, and contig_or_cb_arg contains the
+	 * cb_arg that will be passed to the SGL callback functions.
+	 */
+	void *contig_or_cb_arg;
+
+	/** Virtual memory address of a single virtually contiguous metadata buffer */
 	void *md;
+};
+
+#define NVME_PAYLOAD_CONTIG(contig_, md_) \
+	(struct nvme_payload) { \
+		.reset_sgl_fn = NULL, \
+		.next_sge_fn = NULL, \
+		.contig_or_cb_arg = (contig_), \
+		.md = (md_), \
+	}
+
+#define NVME_PAYLOAD_SGL(reset_sgl_fn_, next_sge_fn_, cb_arg_, md_) \
+	(struct nvme_payload) { \
+		.reset_sgl_fn = (reset_sgl_fn_), \
+		.next_sge_fn = (next_sge_fn_), \
+		.contig_or_cb_arg = (cb_arg_), \
+		.md = (md_), \
+	}
+
+static inline enum nvme_payload_type
+nvme_payload_type(const struct nvme_payload *payload) {
+	return payload->reset_sgl_fn ? NVME_PAYLOAD_TYPE_SGL : NVME_PAYLOAD_TYPE_CONTIG;
+}
 
-	/** \ref nvme_payload_type */
-	uint8_t type;
+struct nvme_error_cmd {
+	bool				do_not_submit;
+	uint64_t			timeout_tsc;
+	uint32_t			err_count;
+	uint8_t				opc;
+	struct spdk_nvme_status		status;
+	TAILQ_ENTRY(nvme_error_cmd)	link;
 };
 
 struct nvme_request {
 	struct spdk_nvme_cmd		cmd;
 
-	/**
-	 * Data payload for this request's command.
-	 */
-	struct nvme_payload		payload;
-
 	uint8_t				retries;
 
+	bool				timed_out;
+
 	/**
 	 * Number of children requests still outstanding for this
 	 *  request which was split into multiple child requests.
 	 */
-	uint8_t				num_children;
-	uint32_t			payload_size;
+	uint16_t			num_children;
 
 	/**
 	 * Offset in bytes from the beginning of payload for this request.
@@ -175,12 +235,31 @@ struct nvme_request {
 	uint32_t			payload_offset;
 	uint32_t			md_offset;
 
+	uint32_t			payload_size;
+
+	/**
+	 * Timeout ticks for error injection requests, can be extended in future
+	 * to support per-request timeout feature.
+	 */
+	uint64_t			timeout_tsc;
+
+	/**
+	 * Data payload for this request's command.
+	 */
+	struct nvme_payload		payload;
+
 	spdk_nvme_cmd_cb		cb_fn;
 	void				*cb_arg;
 	STAILQ_ENTRY(nvme_request)	stailq;
 
 	struct spdk_nvme_qpair		*qpair;
 
+	/*
+	 * The value of spdk_get_ticks() when the request was submitted to the hardware.
+	 * Only set if ctrlr->timeout_enabled is true.
+	 */
+	uint64_t			submit_tick;
+
 	/**
 	 * The active admin request can be moved to a per process pending
 	 *  list based on the saved pid to tell which process it belongs
@@ -226,6 +305,8 @@ struct nvme_request {
 	 */
 	struct spdk_nvme_cpl		parent_status;
 
+	char key_data[KV_MAX_KEY_LEN];
+
 	/**
 	 * The user_cb_fn and user_cb_arg fields are used for holding the original
 	 * callback data when using nvme_allocate_request_user_copy.
@@ -233,6 +314,7 @@ struct nvme_request {
 	spdk_nvme_cmd_cb		user_cb_fn;
 	void				*user_cb_arg;
 	void				*user_buffer;
+
 };
 
 struct nvme_completion_poll_status {
@@ -247,16 +329,15 @@ struct nvme_async_event_request {
 };
 
 struct spdk_nvme_qpair {
-	pthread_spinlock_t		req_lock;
-	STAILQ_HEAD(, nvme_request)	free_req;
-	STAILQ_HEAD(, nvme_request)	queued_req;
-
-	enum spdk_nvme_transport_type	trtype;
+	struct spdk_nvme_ctrlr		*ctrlr;
 
 	uint16_t			id;
 
 	uint8_t				qprio;
 
+	uint8_t				is_enabled : 1;
+	uint8_t				is_connecting: 1;
+
 	/*
 	 * Members for handling IO qpair deletion inside of a completion context.
 	 * These are specifically defined as single bits, so that they do not
@@ -265,7 +346,28 @@ struct spdk_nvme_qpair {
 	uint8_t				in_completion_context : 1;
 	uint8_t				delete_after_completion_context: 1;
 
-	struct spdk_nvme_ctrlr		*ctrlr;
+	/*
+	 * Set when no deletion notification is needed. For example, the process
+	 * which allocated this qpair exited unexpectedly.
+	 */
+	uint8_t				no_deletion_notification_needed: 1;
+
+	enum spdk_nvme_transport_type	trtype;
+
+    pthread_spinlock_t req_lock;
+    pthread_spinlock_t sq_lock;
+    pthread_spinlock_t cq_lock;
+
+    uint32_t current_qd;
+
+
+	STAILQ_HEAD(, nvme_request)	free_req;
+	STAILQ_HEAD(, nvme_request)	queued_req;
+
+	/** Commands opcode in this list will return error */
+	TAILQ_HEAD(, nvme_error_cmd)	err_cmd_head;
+	/** Requests in this list will return error */
+	STAILQ_HEAD(, nvme_request)	err_req_head;
 
 	/* List entry for spdk_nvme_ctrlr::active_io_qpairs */
 	TAILQ_ENTRY(spdk_nvme_qpair)	tailq;
@@ -273,15 +375,13 @@ struct spdk_nvme_qpair {
 	/* List entry for spdk_nvme_ctrlr_process::allocated_io_qpairs */
 	TAILQ_ENTRY(spdk_nvme_qpair)	per_process_tailq;
 
+	struct spdk_nvme_ctrlr_process	*active_proc;
+
 	void				*req_buf;
-	pthread_spinlock_t		sq_lock;
-	pthread_spinlock_t		cq_lock;
-	uint16_t 			current_qd;
 };
 
 struct spdk_nvme_ns {
 	struct spdk_nvme_ctrlr		*ctrlr;
-	uint32_t			stripe_size;
 	uint32_t			sector_size;
 
 	/*
@@ -295,8 +395,11 @@ struct spdk_nvme_ns {
 	uint32_t			pi_type;
 	uint32_t			sectors_per_max_io;
 	uint32_t			sectors_per_stripe;
-	uint16_t			id;
+	uint32_t			id;
 	uint16_t			flags;
+
+	/* Namespace Identification Descriptor List (CNS = 03h) */
+	uint8_t				id_desc_list[4096];
 };
 
 /**
@@ -304,6 +407,11 @@ struct spdk_nvme_ns {
  */
 enum nvme_ctrlr_state {
 	/**
+	 * Wait before initializing the controller.
+	 */
+	NVME_CTRLR_STATE_INIT_DELAY,
+
+	/**
 	 * Controller has not been initialized yet.
 	 */
 	NVME_CTRLR_STATE_INIT,
@@ -329,9 +437,130 @@ enum nvme_ctrlr_state {
 	NVME_CTRLR_STATE_ENABLE_WAIT_FOR_READY_1,
 
 	/**
+	 * Enable the Admin queue of the controller.
+	 */
+	NVME_CTRLR_STATE_ENABLE_ADMIN_QUEUE,
+
+	/**
+	 * Identify Controller command will be sent to then controller.
+	 */
+	NVME_CTRLR_STATE_IDENTIFY,
+
+	/**
+	 * Waiting for Identify Controller command be completed.
+	 */
+	NVME_CTRLR_STATE_WAIT_FOR_IDENTIFY,
+
+	/**
+	 * Set Number of Queues of the controller.
+	 */
+	NVME_CTRLR_STATE_SET_NUM_QUEUES,
+
+	/**
+	 * Waiting for Set Num of Queues command to be completed.
+	 */
+	NVME_CTRLR_STATE_WAIT_FOR_SET_NUM_QUEUES,
+
+	/**
+	 * Get Number of Queues of the controller.
+	 */
+	NVME_CTRLR_STATE_GET_NUM_QUEUES,
+
+	/**
+	 * Waiting for Get Num of Queues command to be completed.
+	 */
+	NVME_CTRLR_STATE_WAIT_FOR_GET_NUM_QUEUES,
+
+	/**
+	 * Construct Namespace data structures of the controller.
+	 */
+	NVME_CTRLR_STATE_CONSTRUCT_NS,
+
+	/**
+	 * Get active Namespace list of the controller.
+	 */
+	NVME_CTRLR_STATE_IDENTIFY_ACTIVE_NS,
+
+	/**
+	 * Get Identify Namespace Data structure for each NS.
+	 */
+	NVME_CTRLR_STATE_IDENTIFY_NS,
+
+	/**
+	 * Waiting for the Identify Namespace commands to be completed.
+	 */
+	NVME_CTRLR_STATE_WAIT_FOR_IDENTIFY_NS,
+
+	/**
+	 * Get Identify Namespace Identification Descriptors.
+	 */
+	NVME_CTRLR_STATE_IDENTIFY_ID_DESCS,
+
+	/**
+	 * Waiting for the Identify Namespace Identification
+	 * Descriptors to be completed.
+	 */
+	NVME_CTRLR_STATE_WAIT_FOR_IDENTIFY_ID_DESCS,
+
+	/**
+	 * Configure AER of the controller.
+	 */
+	NVME_CTRLR_STATE_CONFIGURE_AER,
+
+	/**
+	 * Waiting for the Configure AER to be completed.
+	 */
+	NVME_CTRLR_STATE_WAIT_FOR_CONFIGURE_AER,
+
+	/**
+	 * Set supported log pages of the controller.
+	 */
+	NVME_CTRLR_STATE_SET_SUPPORTED_LOG_PAGES,
+
+	/**
+	 * Set supported features of the controller.
+	 */
+	NVME_CTRLR_STATE_SET_SUPPORTED_FEATURES,
+
+	/**
+	 * Set Doorbell Buffer Config of the controller.
+	 */
+	NVME_CTRLR_STATE_SET_DB_BUF_CFG,
+
+	/**
+	 * Waiting for Doorbell Buffer Config to be completed.
+	 */
+	NVME_CTRLR_STATE_WAIT_FOR_DB_BUF_CFG,
+
+	/**
+	 * Set Keep Alive Timeout of the controller.
+	 */
+	NVME_CTRLR_STATE_SET_KEEP_ALIVE_TIMEOUT,
+
+	/**
+	 * Waiting for Set Keep Alive Timeout to be completed.
+	 */
+	NVME_CTRLR_STATE_WAIT_FOR_KEEP_ALIVE_TIMEOUT,
+
+	/**
+	 * Set Host ID of the controller.
+	 */
+	NVME_CTRLR_STATE_SET_HOST_ID,
+
+	/**
+	 * Waiting for Set Host ID to be completed.
+	 */
+	NVME_CTRLR_STATE_WAIT_FOR_HOST_ID,
+
+	/**
 	 * Controller initialization has completed and the controller is ready.
 	 */
-	NVME_CTRLR_STATE_READY
+	NVME_CTRLR_STATE_READY,
+
+	/**
+	 * Controller inilialization has an error.
+	 */
+	NVME_CTRLR_STATE_ERROR
 };
 
 #define NVME_TIMEOUT_INFINITE	UINT64_MAX
@@ -359,6 +588,16 @@ struct spdk_nvme_ctrlr_process {
 
 	/** Allocated IO qpairs */
 	TAILQ_HEAD(, spdk_nvme_qpair)			allocated_io_qpairs;
+
+	spdk_nvme_aer_cb				aer_cb_fn;
+	void						*aer_cb_arg;
+
+	/**
+	 * A function pointer to timeout callback function
+	 */
+	spdk_nvme_timeout_cb		timeout_cb_fn;
+	void				*timeout_cb_arg;
+	uint64_t			timeout_ticks;
 };
 
 /*
@@ -380,12 +619,21 @@ struct spdk_nvme_ctrlr {
 
 	bool				is_failed;
 
+	bool				is_shutdown;
+
+	bool				timeout_enabled;
+
+	uint16_t			max_sges;
+
+	uint16_t			cntlid;
+
 	/** Controller support flags */
 	uint64_t			flags;
 
 	/* Cold data (not accessed in normal I/O path) is after this point. */
 
 	union spdk_nvme_cap_register	cap;
+	union spdk_nvme_vs_register	vs;
 
 	enum nvme_ctrlr_state		state;
 	uint64_t			state_timeout_tsc;
@@ -407,10 +655,11 @@ struct spdk_nvme_ctrlr {
 	/** minimum page size supported by this controller in bytes */
 	uint32_t			min_page_size;
 
+	/** selected memory page size for this controller in bytes */
+	uint32_t			page_size;
+
 	uint32_t			num_aers;
 	struct nvme_async_event_request	aer[NVME_MAX_ASYNC_EVENTS];
-	spdk_nvme_aer_cb		aer_cb_fn;
-	void				*aer_cb_arg;
 
 	/** guards access to the controller itself, including admin queues */
 	pthread_mutex_t			ctrlr_lock;
@@ -418,12 +667,22 @@ struct spdk_nvme_ctrlr {
 
 	struct spdk_nvme_qpair		*adminq;
 
+	/** shadow doorbell buffer */
+	uint32_t			*shadow_doorbell;
+	/** eventidx buffer */
+	uint32_t			*eventidx;
+
 	/**
 	 * Identify Controller data.
 	 */
 	struct spdk_nvme_ctrlr_data	cdata;
 
 	/**
+	 * Keep track of active namespaces
+	 */
+	uint32_t			*active_ns_list;
+
+	/**
 	 * Array of Identify Namespace data.
 	 *
 	 * Stored separately from ns since nsdata should not normally be accessed during I/O.
@@ -443,26 +702,34 @@ struct spdk_nvme_ctrlr {
 	/** Track all the processes manage this controller */
 	TAILQ_HEAD(, spdk_nvme_ctrlr_process)	active_procs;
 
-	/**
-	 * A function pointer to timeout callback function
-	 */
-	spdk_nvme_timeout_cb		timeout_cb_fn;
-	void				*timeout_cb_arg;
-	uint64_t			timeout_ticks;
 
 	STAILQ_HEAD(, nvme_request)	queued_aborts;
 	uint32_t			outstanding_aborts;
 };
 
+struct spdk_nvme_probe_ctx {
+	struct spdk_nvme_transport_id		trid;
+	void					*cb_ctx;
+	spdk_nvme_probe_cb			probe_cb;
+	spdk_nvme_attach_cb			attach_cb;
+	spdk_nvme_remove_cb			remove_cb;
+	TAILQ_HEAD(, spdk_nvme_ctrlr)		init_ctrlrs;
+};
+
 struct nvme_driver {
 	pthread_mutex_t			lock;
-	TAILQ_HEAD(, spdk_nvme_ctrlr)	init_ctrlrs;
-	TAILQ_HEAD(, spdk_nvme_ctrlr)	attached_ctrlrs;
+
+	/** Multi-process shared attached controller list */
+	TAILQ_HEAD(, spdk_nvme_ctrlr)	shared_attached_ctrlrs;
+
 	bool				initialized;
+	struct spdk_uuid		default_extended_host_id;
 };
 
 extern struct nvme_driver *g_spdk_nvme_driver;
 
+int nvme_driver_init(void);
+
 #define nvme_delay		usleep
 
 static inline bool
@@ -498,24 +765,29 @@ nvme_robust_mutex_unlock(pthread_mutex_t *mtx)
 }
 
 /* Admin functions */
-int	nvme_ctrlr_cmd_identify_controller(struct spdk_nvme_ctrlr *ctrlr,
-		void *payload,
-		spdk_nvme_cmd_cb cb_fn, void *cb_arg);
-int	nvme_ctrlr_cmd_identify_namespace(struct spdk_nvme_ctrlr *ctrlr,
-		uint16_t nsid, void *payload,
-		spdk_nvme_cmd_cb cb_fn, void *cb_arg);
+int	nvme_ctrlr_cmd_identify(struct spdk_nvme_ctrlr *ctrlr,
+				uint8_t cns, uint16_t cntid, uint32_t nsid,
+				void *payload, size_t payload_size,
+				spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 int	nvme_ctrlr_cmd_set_num_queues(struct spdk_nvme_ctrlr *ctrlr,
 				      uint32_t num_queues, spdk_nvme_cmd_cb cb_fn,
 				      void *cb_arg);
+int	nvme_ctrlr_cmd_get_num_queues(struct spdk_nvme_ctrlr *ctrlr,
+				      spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 int	nvme_ctrlr_cmd_set_async_event_config(struct spdk_nvme_ctrlr *ctrlr,
-		union spdk_nvme_critical_warning_state state,
+		union spdk_nvme_feat_async_event_configuration config,
 		spdk_nvme_cmd_cb cb_fn, void *cb_arg);
+int	nvme_ctrlr_cmd_set_host_id(struct spdk_nvme_ctrlr *ctrlr, void *host_id, uint32_t host_id_size,
+				   spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 int	nvme_ctrlr_cmd_attach_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid,
 				 struct spdk_nvme_ctrlr_list *payload, spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 int	nvme_ctrlr_cmd_detach_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid,
 				 struct spdk_nvme_ctrlr_list *payload, spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 int	nvme_ctrlr_cmd_create_ns(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_ns_data *payload,
 				 spdk_nvme_cmd_cb cb_fn, void *cb_arg);
+int	nvme_ctrlr_cmd_doorbell_buffer_config(struct spdk_nvme_ctrlr *ctrlr,
+		uint64_t prp1, uint64_t prp2,
+		spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 int	nvme_ctrlr_cmd_delete_ns(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid, spdk_nvme_cmd_cb cb_fn,
 				 void *cb_arg);
 int	nvme_ctrlr_cmd_format(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid,
@@ -526,87 +798,234 @@ int	nvme_ctrlr_cmd_fw_commit(struct spdk_nvme_ctrlr *ctrlr,
 int	nvme_ctrlr_cmd_fw_image_download(struct spdk_nvme_ctrlr *ctrlr,
 		uint32_t size, uint32_t offset, void *payload,
 		spdk_nvme_cmd_cb cb_fn, void *cb_arg);
+int	nvme_ctrlr_cmd_security_receive(struct spdk_nvme_ctrlr *ctrlr, uint8_t secp, uint16_t spsp,
+					uint8_t nssf, void *payload, uint32_t payload_size,
+					spdk_nvme_cmd_cb cb_fn, void *cb_arg);
+int	nvme_ctrlr_cmd_security_send(struct spdk_nvme_ctrlr *ctrlr, uint8_t secp,
+				     uint16_t spsp, uint8_t nssf, void *payload,
+				     uint32_t payload_size, spdk_nvme_cmd_cb cb_fn, void *cb_arg);
+int	nvme_ctrlr_cmd_sanitize(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid,
+				struct spdk_nvme_sanitize *sanitize, uint32_t cdw11,
+				spdk_nvme_cmd_cb cb_fn, void *cb_arg);
 void	nvme_completion_poll_cb(void *arg, const struct spdk_nvme_cpl *cpl);
-
+int	spdk_nvme_wait_for_completion(struct spdk_nvme_qpair *qpair,
+				      struct nvme_completion_poll_status *status);
+int	spdk_nvme_wait_for_completion_robust_lock(struct spdk_nvme_qpair *qpair,
+		struct nvme_completion_poll_status *status,
+		pthread_mutex_t *robust_mutex);
+int	spdk_nvme_wait_for_completion_timeout(struct spdk_nvme_qpair *qpair,
+		struct nvme_completion_poll_status *status,
+		uint64_t timeout_in_secs);
+
+struct spdk_nvme_ctrlr_process *spdk_nvme_ctrlr_get_process(struct spdk_nvme_ctrlr *ctrlr,
+		pid_t pid);
+struct spdk_nvme_ctrlr_process *spdk_nvme_ctrlr_get_current_process(struct spdk_nvme_ctrlr *ctrlr);
 int	nvme_ctrlr_add_process(struct spdk_nvme_ctrlr *ctrlr, void *devhandle);
 void	nvme_ctrlr_free_processes(struct spdk_nvme_ctrlr *ctrlr);
 struct spdk_pci_device *nvme_ctrlr_proc_get_devhandle(struct spdk_nvme_ctrlr *ctrlr);
 
-int	nvme_ctrlr_probe(const struct spdk_nvme_transport_id *trid, void *devhandle,
-			 spdk_nvme_probe_cb probe_cb, void *cb_ctx);
+int	nvme_ctrlr_probe(const struct spdk_nvme_transport_id *trid,
+			 struct spdk_nvme_probe_ctx *probe_ctx, void *devhandle);
 
 int	nvme_ctrlr_construct(struct spdk_nvme_ctrlr *ctrlr);
+void	nvme_ctrlr_destruct_finish(struct spdk_nvme_ctrlr *ctrlr);
 void	nvme_ctrlr_destruct(struct spdk_nvme_ctrlr *ctrlr);
 void	nvme_ctrlr_fail(struct spdk_nvme_ctrlr *ctrlr, bool hot_remove);
 int	nvme_ctrlr_process_init(struct spdk_nvme_ctrlr *ctrlr);
-int	nvme_ctrlr_start(struct spdk_nvme_ctrlr *ctrlr);
+void	nvme_ctrlr_connected(struct spdk_nvme_probe_ctx *probe_ctx,
+			     struct spdk_nvme_ctrlr *ctrlr);
 
 int	nvme_ctrlr_submit_admin_request(struct spdk_nvme_ctrlr *ctrlr,
 					struct nvme_request *req);
 int	nvme_ctrlr_get_cap(struct spdk_nvme_ctrlr *ctrlr, union spdk_nvme_cap_register *cap);
-void	nvme_ctrlr_init_cap(struct spdk_nvme_ctrlr *ctrlr, const union spdk_nvme_cap_register *cap);
+int	nvme_ctrlr_get_vs(struct spdk_nvme_ctrlr *ctrlr, union spdk_nvme_vs_register *vs);
+int	nvme_ctrlr_get_cmbsz(struct spdk_nvme_ctrlr *ctrlr, union spdk_nvme_cmbsz_register *cmbsz);
+void	nvme_ctrlr_init_cap(struct spdk_nvme_ctrlr *ctrlr, const union spdk_nvme_cap_register *cap,
+			    const union spdk_nvme_vs_register *vs);
 int	nvme_qpair_init(struct spdk_nvme_qpair *qpair, uint16_t id,
 			struct spdk_nvme_ctrlr *ctrlr,
 			enum spdk_nvme_qprio qprio,
 			uint32_t num_requests);
+void	nvme_qpair_deinit(struct spdk_nvme_qpair *qpair);
 void	nvme_qpair_enable(struct spdk_nvme_qpair *qpair);
 void	nvme_qpair_disable(struct spdk_nvme_qpair *qpair);
+void	nvme_qpair_complete_error_reqs(struct spdk_nvme_qpair *qpair);
 int	nvme_qpair_submit_request(struct spdk_nvme_qpair *qpair,
 				  struct nvme_request *req);
 
-int	nvme_ns_construct(struct spdk_nvme_ns *ns, uint16_t id,
+int	nvme_ctrlr_identify_active_ns(struct spdk_nvme_ctrlr *ctrlr);
+void	nvme_ns_set_identify_data(struct spdk_nvme_ns *ns);
+int	nvme_ns_construct(struct spdk_nvme_ns *ns, uint32_t id,
 			  struct spdk_nvme_ctrlr *ctrlr);
 void	nvme_ns_destruct(struct spdk_nvme_ns *ns);
 
-struct nvme_request *nvme_allocate_request(struct spdk_nvme_qpair *qpair,
-		const struct nvme_payload *payload,
-		uint32_t payload_size, spdk_nvme_cmd_cb cb_fn, void *cb_arg);
-struct nvme_request *nvme_allocate_request_null(struct spdk_nvme_qpair *qpair,
-		spdk_nvme_cmd_cb cb_fn, void *cb_arg);
-struct nvme_request *nvme_allocate_request_contig(struct spdk_nvme_qpair *qpair,
-		void *buffer, uint32_t payload_size,
-		spdk_nvme_cmd_cb cb_fn, void *cb_arg);
+int	nvme_fabric_ctrlr_set_reg_4(struct spdk_nvme_ctrlr *ctrlr, uint32_t offset, uint32_t value);
+int	nvme_fabric_ctrlr_set_reg_8(struct spdk_nvme_ctrlr *ctrlr, uint32_t offset, uint64_t value);
+int	nvme_fabric_ctrlr_get_reg_4(struct spdk_nvme_ctrlr *ctrlr, uint32_t offset, uint32_t *value);
+int	nvme_fabric_ctrlr_get_reg_8(struct spdk_nvme_ctrlr *ctrlr, uint32_t offset, uint64_t *value);
+int	nvme_fabric_ctrlr_discover(struct spdk_nvme_ctrlr *ctrlr,
+				   struct spdk_nvme_probe_ctx *probe_ctx);
+int	nvme_fabric_qpair_connect(struct spdk_nvme_qpair *qpair, uint32_t num_entries);
+
+static inline struct nvme_request *
+nvme_allocate_request(struct spdk_nvme_qpair *qpair,
+		      const struct nvme_payload *payload, uint32_t payload_size,
+		      spdk_nvme_cmd_cb cb_fn, void *cb_arg)
+{
+	struct nvme_request *req;
+
+	req = STAILQ_FIRST(&qpair->free_req);
+	if (req == NULL) {
+		return req;
+	}
+
+	STAILQ_REMOVE_HEAD(&qpair->free_req, stailq);
+
+	/*
+	 * Only memset/zero fields that need it.  All other fields
+	 *  will be initialized appropriately either later in this
+	 *  function, or before they are needed later in the
+	 *  submission patch.  For example, the children
+	 *  TAILQ_ENTRY and following members are
+	 *  only used as part of I/O splitting so we avoid
+	 *  memsetting them until it is actually needed.
+	 *  They will be initialized in nvme_request_add_child()
+	 *  if the request is split.
+	 */
+	memset(req, 0, offsetof(struct nvme_request, payload_size));
+
+	req->cb_fn = cb_fn;
+	req->cb_arg = cb_arg;
+	req->payload = *payload;
+	req->payload_size = payload_size;
+	req->pid = g_spdk_nvme_pid;
+	req->submit_tick = 0;
+
+	return req;
+}
+
+static inline struct nvme_request *
+nvme_allocate_request_contig(struct spdk_nvme_qpair *qpair,
+			     void *buffer, uint32_t payload_size,
+			     spdk_nvme_cmd_cb cb_fn, void *cb_arg)
+{
+	struct nvme_payload payload;
+
+	payload = NVME_PAYLOAD_CONTIG(buffer, NULL);
+
+	return nvme_allocate_request(qpair, &payload, payload_size, cb_fn, cb_arg);
+}
+
+static inline struct nvme_request *
+nvme_allocate_request_null(struct spdk_nvme_qpair *qpair, spdk_nvme_cmd_cb cb_fn, void *cb_arg)
+{
+	return nvme_allocate_request_contig(qpair, NULL, 0, cb_fn, cb_arg);
+}
+
 struct nvme_request *nvme_allocate_request_user_copy(struct spdk_nvme_qpair *qpair,
 		void *buffer, uint32_t payload_size,
 		spdk_nvme_cmd_cb cb_fn, void *cb_arg, bool host_to_controller);
-void	nvme_free_request(struct nvme_request *req);
+
+static inline void
+nvme_complete_request(spdk_nvme_cmd_cb cb_fn, void *cb_arg, struct spdk_nvme_qpair *qpair,
+		      struct nvme_request *req, struct spdk_nvme_cpl *cpl)
+{
+	struct spdk_nvme_cpl            err_cpl;
+	struct nvme_error_cmd           *cmd;
+
+	/* error injection at completion path,
+	 * only inject for successful completed commands
+	 */
+	if (spdk_unlikely(!TAILQ_EMPTY(&qpair->err_cmd_head) &&
+			  !spdk_nvme_cpl_is_error(cpl))) {
+		TAILQ_FOREACH(cmd, &qpair->err_cmd_head, link) {
+
+			if (cmd->do_not_submit) {
+				continue;
+			}
+
+			if ((cmd->opc == req->cmd.opc) && cmd->err_count) {
+
+				err_cpl = *cpl;
+				err_cpl.status.sct = cmd->status.sct;
+				err_cpl.status.sc = cmd->status.sc;
+
+				cpl = &err_cpl;
+				cmd->err_count--;
+				break;
+			}
+		}
+	}
+
+	if (cb_fn) {
+		cb_fn(cb_arg, cpl);
+	}
+}
+
+static inline void
+nvme_free_request(struct nvme_request *req)
+{
+	assert(req != NULL);
+	assert(req->num_children == 0);
+	assert(req->qpair != NULL);
+
+	STAILQ_INSERT_HEAD(&req->qpair->free_req, req, stailq);
+}
+
+static inline void
+nvme_qpair_free_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req)
+{
+	assert(req != NULL);
+	assert(req->num_children == 0);
+
+	STAILQ_INSERT_HEAD(&qpair->free_req, req, stailq);
+}
+
 void	nvme_request_remove_child(struct nvme_request *parent, struct nvme_request *child);
+int	nvme_request_check_timeout(struct nvme_request *req, uint16_t cid,
+				   struct spdk_nvme_ctrlr_process *active_proc, uint64_t now_tick);
 uint64_t nvme_get_quirks(const struct spdk_pci_id *id);
 
-void	spdk_nvme_ctrlr_opts_set_defaults(struct spdk_nvme_ctrlr_opts *opts);
-
 int	nvme_robust_mutex_init_shared(pthread_mutex_t *mtx);
 int	nvme_robust_mutex_init_recursive_shared(pthread_mutex_t *mtx);
 
+const char *spdk_nvme_cpl_get_status_string(const struct spdk_nvme_status *status);
 bool	nvme_completion_is_retry(const struct spdk_nvme_cpl *cpl);
 void	nvme_qpair_print_command(struct spdk_nvme_qpair *qpair, struct spdk_nvme_cmd *cmd);
 void	nvme_qpair_print_completion(struct spdk_nvme_qpair *qpair, struct spdk_nvme_cpl *cpl);
 
+struct spdk_nvme_ctrlr *spdk_nvme_get_ctrlr_by_trid_unsafe(
+	const struct spdk_nvme_transport_id *trid);
+
 /* Transport specific functions */
 #define DECLARE_TRANSPORT(name) \
 	struct spdk_nvme_ctrlr *nvme_ ## name ## _ctrlr_construct(const struct spdk_nvme_transport_id *trid, const struct spdk_nvme_ctrlr_opts *opts, \
 		void *devhandle); \
 	int nvme_ ## name ## _ctrlr_destruct(struct spdk_nvme_ctrlr *ctrlr); \
-	int nvme_ ## name ## _ctrlr_scan(const struct spdk_nvme_transport_id *trid, void *cb_ctx, spdk_nvme_probe_cb probe_cb, spdk_nvme_remove_cb remove_cb); \
+	int nvme_ ## name ## _ctrlr_scan(struct spdk_nvme_probe_ctx *probe_ctx, bool direct_connect); \
 	int nvme_ ## name ## _ctrlr_enable(struct spdk_nvme_ctrlr *ctrlr); \
 	int nvme_ ## name ## _ctrlr_set_reg_4(struct spdk_nvme_ctrlr *ctrlr, uint32_t offset, uint32_t value); \
 	int nvme_ ## name ## _ctrlr_set_reg_8(struct spdk_nvme_ctrlr *ctrlr, uint32_t offset, uint64_t value); \
 	int nvme_ ## name ## _ctrlr_get_reg_4(struct spdk_nvme_ctrlr *ctrlr, uint32_t offset, uint32_t *value); \
 	int nvme_ ## name ## _ctrlr_get_reg_8(struct spdk_nvme_ctrlr *ctrlr, uint32_t offset, uint64_t *value); \
 	uint32_t nvme_ ## name ## _ctrlr_get_max_xfer_size(struct spdk_nvme_ctrlr *ctrlr); \
-	uint32_t nvme_ ## name ## _ctrlr_get_max_io_queue_size(struct spdk_nvme_ctrlr *ctrlr); \
-	struct spdk_nvme_qpair *nvme_ ## name ## _ctrlr_create_io_qpair(struct spdk_nvme_ctrlr *ctrlr, uint16_t qid, enum spdk_nvme_qprio qprio); \
+	uint16_t nvme_ ## name ## _ctrlr_get_max_sges(struct spdk_nvme_ctrlr *ctrlr); \
+	struct spdk_nvme_qpair *nvme_ ## name ## _ctrlr_create_io_qpair(struct spdk_nvme_ctrlr *ctrlr, uint16_t qid, const struct spdk_nvme_io_qpair_opts *opts); \
+	void *nvme_ ## name ## _ctrlr_alloc_cmb_io_buffer(struct spdk_nvme_ctrlr *ctrlr, size_t size); \
+	int nvme_ ## name ## _ctrlr_free_cmb_io_buffer(struct spdk_nvme_ctrlr *ctrlr, void *buf, size_t size); \
+	volatile struct spdk_nvme_registers *nvme_ ## name ## _ctrlr_get_registers(struct spdk_nvme_ctrlr *ctrlr); \
 	int nvme_ ## name ## _ctrlr_delete_io_qpair(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_qpair *qpair); \
-	int nvme_ ## name ## _ctrlr_reinit_io_qpair(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_qpair *qpair); \
-	int nvme_ ## name ## _qpair_enable(struct spdk_nvme_qpair *qpair); \
-	int nvme_ ## name ## _qpair_disable(struct spdk_nvme_qpair *qpair); \
+	int nvme_ ## name ## _ctrlr_connect_qpair(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_qpair *qpair); \
+	void nvme_ ## name ## _ctrlr_disconnect_qpair(struct spdk_nvme_ctrlr *ctrlr, struct spdk_nvme_qpair *qpair); \
+	void nvme_ ## name ## _qpair_abort_reqs(struct spdk_nvme_qpair *qpair, uint32_t dnr); \
 	int nvme_ ## name ## _qpair_reset(struct spdk_nvme_qpair *qpair); \
-	int nvme_ ## name ## _qpair_fail(struct spdk_nvme_qpair *qpair); \
 	int nvme_ ## name ## _qpair_submit_request(struct spdk_nvme_qpair *qpair, struct nvme_request *req); \
-	int32_t nvme_ ## name ## _qpair_process_completions(struct spdk_nvme_qpair *qpair, uint32_t max_completions);
+	int32_t nvme_ ## name ## _qpair_process_completions(struct spdk_nvme_qpair *qpair, uint32_t max_completions); \
+	void nvme_ ## name ## _admin_qpair_abort_aers(struct spdk_nvme_qpair *qpair); \
 
 DECLARE_TRANSPORT(transport) /* generic transport dispatch functions */
 DECLARE_TRANSPORT(pcie)
+DECLARE_TRANSPORT(tcp)
 #ifdef  SPDK_CONFIG_RDMA
 DECLARE_TRANSPORT(rdma)
 #endif
@@ -624,9 +1043,9 @@ void	nvme_ctrlr_proc_put_ref(struct spdk_nvme_ctrlr *ctrlr);
 int	nvme_ctrlr_get_ref_count(struct spdk_nvme_ctrlr *ctrlr);
 
 static inline bool
-_is_page_aligned(uint64_t address)
+_is_page_aligned(uint64_t address, uint64_t page_size)
 {
-	return (address & (PAGE_SIZE - 1)) == 0;
+	return (address & (page_size - 1)) == 0;
 }
 
 #endif /* __NVME_INTERNAL_H__ */
diff --git a/PDK/core/src/api/include/udd/spdk/nvme_ocssd.h b/PDK/core/src/api/include/udd/spdk/nvme_ocssd.h
new file mode 100644
index 0000000..7ebb079
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/nvme_ocssd.h
@@ -0,0 +1,227 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * \file
+ * NVMe driver public API extension for Open-Channel
+ */
+
+#ifndef SPDK_NVME_OCSSD_H
+#define SPDK_NVME_OCSSD_H
+
+#include "spdk/stdinc.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include "spdk/nvme.h"
+#include "spdk/nvme_ocssd_spec.h"
+
+/**
+ * \brief Determine if OpenChannel is supported by the given NVMe controller.
+ * \param ctrlr NVMe controller to check.
+ *
+ * \return true if support OpenChannel
+ */
+bool spdk_nvme_ctrlr_is_ocssd_supported(struct spdk_nvme_ctrlr *ctrlr);
+
+/**
+ * \brief Identify geometry of the given namespace.
+ * \param ctrlr NVMe controller to query.
+ * \param nsid Id of the given namesapce.
+ * \param payload The pointer to the payload buffer.
+ * \param payload_size The size of payload buffer. Shall be multiple of 4K.
+ * \param cb_fn Callback function to invoke when the feature has been retrieved.
+ * \param cb_arg Argument to pass to the callback function.
+ *
+ * \return 0 if successfully submitted, ENOMEM if resources could not be
+ * allocated for this request, EINVAL if wrong payload size.
+ *
+ */
+int spdk_nvme_ocssd_ctrlr_cmd_geometry(struct spdk_nvme_ctrlr *ctrlr, uint32_t nsid,
+				       void *payload, uint32_t payload_size,
+				       spdk_nvme_cmd_cb cb_fn, void *cb_arg);
+
+/**
+ * \brief Submits a vector reset command to the specified NVMe namespace.
+ *
+ * \param ns NVMe namespace to submit the command
+ * \param qpair I/O queue pair to submit the request
+ * \param lba_list an array of LBAs for processing.
+ * LBAs must correspond to the start of chunks to reset.
+ * Must be allocated through spdk_dma_malloc() or its variants
+ * \param num_lbas number of LBAs stored in lba_list
+ * \param chunk_info an array of chunk info on DMA-able memory
+ * \param cb_fn callback function to invoke when the I/O is completed
+ * \param cb_arg argument to pass to the callback function
+ *
+ * \return 0 if successfully submitted, ENOMEM if an nvme_request
+ *	     structure cannot be allocated for the I/O request
+ */
+int spdk_nvme_ocssd_ns_cmd_vector_reset(struct spdk_nvme_ns *ns,
+					struct spdk_nvme_qpair *qpair,
+					uint64_t *lba_list, uint32_t num_lbas,
+					struct spdk_ocssd_chunk_information_entry *chunk_info,
+					spdk_nvme_cmd_cb cb_fn, void *cb_arg);
+
+/**
+ * \brief Submits a vector write command to the specified NVMe namespace.
+ *
+ * \param ns NVMe namespace to submit the command
+ * \param qpair I/O queue pair to submit the request
+ * \param buffer virtual address pointer to the data payload
+ * \param lba_list an array of LBAs for processing.
+ * Must be allocated through spdk_dma_malloc() or its variants
+ * \param num_lbas number of LBAs stored in lba_list
+ * \param cb_fn callback function to invoke when the I/O is completed
+ * \param cb_arg argument to pass to the callback function
+ * \param io_flags set flags, defined by the SPDK_OCSSD_IO_FLAGS_* entries
+ * in spdk/nvme_ocssd_spec.h, for this I/O.
+ *
+ * \return 0 if successfully submitted, ENOMEM if an nvme_request
+ *	     structure cannot be allocated for the I/O request
+ */
+int spdk_nvme_ocssd_ns_cmd_vector_write(struct spdk_nvme_ns *ns,
+					struct spdk_nvme_qpair *qpair,
+					void *buffer,
+					uint64_t *lba_list, uint32_t num_lbas,
+					spdk_nvme_cmd_cb cb_fn, void *cb_arg,
+					uint32_t io_flags);
+
+/**
+ * \brief Submits a vector write command to the specified NVMe namespace.
+ *
+ * \param ns NVMe namespace to submit the command
+ * \param qpair I/O queue pair to submit the request
+ * \param buffer virtual address pointer to the data payload
+ * \param metadata virtual address pointer to the metadata payload, the length
+ * of metadata is specified by spdk_nvme_ns_get_md_size()
+ * \param lba_list an array of LBAs for processing.
+ * Must be allocated through spdk_dma_malloc() or its variants
+ * \param num_lbas number of LBAs stored in lba_list
+ * \param cb_fn callback function to invoke when the I/O is completed
+ * \param cb_arg argument to pass to the callback function
+ * \param io_flags set flags, defined by the SPDK_OCSSD_IO_FLAGS_* entries
+ * in spdk/nvme_ocssd_spec.h, for this I/O.
+ *
+ * \return 0 if successfully submitted, ENOMEM if an nvme_request
+ *	     structure cannot be allocated for the I/O request
+ */
+int spdk_nvme_ocssd_ns_cmd_vector_write_with_md(struct spdk_nvme_ns *ns,
+		struct spdk_nvme_qpair *qpair,
+		void *buffer, void *metadata,
+		uint64_t *lba_list, uint32_t num_lbas,
+		spdk_nvme_cmd_cb cb_fn, void *cb_arg,
+		uint32_t io_flags);
+
+/**
+ * \brief Submits a vector read command to the specified NVMe namespace.
+ *
+ * \param ns NVMe namespace to submit the command
+ * \param qpair I/O queue pair to submit the request
+ * \param buffer virtual address pointer to the data payload
+ * \param lba_list an array of LBAs for processing.
+ * Must be allocated through spdk_dma_malloc() or its variants
+ * \param num_lbas number of LBAs stored in lba_list
+ * \param cb_fn callback function to invoke when the I/O is completed
+ * \param cb_arg argument to pass to the callback function
+ * \param io_flags set flags, defined by the SPDK_OCSSD_IO_FLAGS_* entries
+ * in spdk/nvme_ocssd_spec.h, for this I/O.
+ *
+ * \return 0 if successfully submitted, ENOMEM if an nvme_request
+ *	     structure cannot be allocated for the I/O request
+ */
+int spdk_nvme_ocssd_ns_cmd_vector_read(struct spdk_nvme_ns *ns,
+				       struct spdk_nvme_qpair *qpair,
+				       void *buffer,
+				       uint64_t *lba_list, uint32_t num_lbas,
+				       spdk_nvme_cmd_cb cb_fn, void *cb_arg,
+				       uint32_t io_flags);
+
+/**
+ * \brief Submits a vector read command to the specified NVMe namespace.
+ *
+ * \param ns NVMe namespace to submit the command
+ * \param qpair I/O queue pair to submit the request
+ * \param buffer virtual address pointer to the data payload
+ * \param metadata virtual address pointer to the metadata payload, the length
+ * of metadata is specified by spdk_nvme_ns_get_md_size()
+ * \param lba_list an array of LBAs for processing.
+ * Must be allocated through spdk_dma_malloc() or its variants
+ * \param num_lbas number of LBAs stored in lba_list
+ * \param cb_fn callback function to invoke when the I/O is completed
+ * \param cb_arg argument to pass to the callback function
+ * \param io_flags set flags, defined by the SPDK_OCSSD_IO_FLAGS_* entries
+ * in spdk/nvme_ocssd_spec.h, for this I/O.
+ *
+ * \return 0 if successfully submitted, ENOMEM if an nvme_request
+ *	     structure cannot be allocated for the I/O request
+ */
+int spdk_nvme_ocssd_ns_cmd_vector_read_with_md(struct spdk_nvme_ns *ns,
+		struct spdk_nvme_qpair *qpair,
+		void *buffer, void *metadata,
+		uint64_t *lba_list, uint32_t num_lbas,
+		spdk_nvme_cmd_cb cb_fn, void *cb_arg,
+		uint32_t io_flags);
+
+/**
+ * \brief Submits a vector copy command to the specified NVMe namespace.
+ *
+ * \param ns NVMe namespace to submit the command
+ * \param qpair I/O queue pair to submit the request
+ * \param dst_lba_list an array of destination LBAs for processing.
+ * Must be allocated through spdk_dma_malloc() or its variants
+ * \param src_lba_list an array of source LBAs for processing.
+ * Must be allocated through spdk_dma_malloc() or its variants
+ * \param num_lbas number of LBAs stored in src_lba_list and dst_lba_list
+ * \param cb_fn callback function to invoke when the I/O is completed
+ * \param cb_arg argument to pass to the callback function
+ * \param io_flags set flags, defined by the SPDK_OCSSD_IO_FLAGS_* entries
+ * in spdk/nvme_ocssd_spec.h, for this I/O.
+ *
+ * \return 0 if successfully submitted, ENOMEM if an nvme_request
+ *	     structure cannot be allocated for the I/O request
+ */
+int spdk_nvme_ocssd_ns_cmd_vector_copy(struct spdk_nvme_ns *ns,
+				       struct spdk_nvme_qpair *qpair,
+				       uint64_t *dst_lba_list, uint64_t *src_lba_list,
+				       uint32_t num_lbas,
+				       spdk_nvme_cmd_cb cb_fn, void *cb_arg,
+				       uint32_t io_flags);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/PDK/core/src/api/include/udd/spdk/nvme_ocssd_spec.h b/PDK/core/src/api/include/udd/spdk/nvme_ocssd_spec.h
new file mode 100644
index 0000000..21e9bce
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/nvme_ocssd_spec.h
@@ -0,0 +1,414 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * \file
+ * Open-Channel specification definitions
+ */
+
+#ifndef SPDK_NVME_OCSSD_SPEC_H
+#define SPDK_NVME_OCSSD_SPEC_H
+
+#include "spdk/stdinc.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include "spdk/assert.h"
+#include "spdk/nvme_spec.h"
+
+/** A maximum number of LBAs that can be issued by vector I/O commands */
+#define SPDK_NVME_OCSSD_MAX_LBAL_ENTRIES	64
+
+struct spdk_ocssd_dev_lba_fmt {
+	/**  Contiguous number of bits assigned to Group addressing */
+	uint8_t grp_len;
+
+	/** Contiguous number of bits assigned to PU addressing */
+	uint8_t pu_len;
+
+	/** Contiguous number of bits assigned to Chunk addressing */
+	uint8_t chk_len;
+
+	/** Contiguous number of bits assigned to logical blocks within Chunk */
+	uint8_t lbk_len;
+
+	uint8_t reserved[4];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_ocssd_dev_lba_fmt) == 8, "Incorrect size");
+
+struct spdk_ocssd_geometry_data {
+	/** Major Version Number */
+	uint8_t		mjr;
+
+	/** Minor Version Number */
+	uint8_t		mnr;
+
+	uint8_t		reserved1[6];
+
+	/** LBA format */
+	struct spdk_ocssd_dev_lba_fmt	lbaf;
+
+	/** Media and Controller Capabilities */
+	struct {
+		/* Supports the Vector Chunk Copy I/O Command */
+		uint32_t	vec_chk_cpy	: 1;
+
+		/* Supports multiple resets when a chunk is in its free state */
+		uint32_t	multi_reset	: 1;
+
+		uint32_t	reserved	: 30;
+	} mccap;
+
+	uint8_t		reserved2[12];
+
+	/** Wear-level Index Delta Threshold */
+	uint8_t		wit;
+
+	uint8_t		reserved3[31];
+
+	/** Number of Groups */
+	uint16_t	num_grp;
+
+	/** Number of parallel units per group */
+	uint16_t	num_pu;
+
+	/** Number of chunks per parallel unit */
+	uint32_t	num_chk;
+
+	/** Chunk Size */
+	uint32_t	clba;
+
+	uint8_t		reserved4[52];
+
+	/** Minimum Write Size */
+	uint32_t	ws_min;
+
+	/** Optimal Write Size */
+	uint32_t	ws_opt;
+
+	/** Cache Minimum Write Size Units */
+	uint32_t	mw_cunits;
+
+	/** Maximum Open Chunks */
+	uint32_t	maxoc;
+
+	/** Maximum Open Chunks per PU */
+	uint32_t	maxocpu;
+
+	uint8_t		reserved5[44];
+
+	/** tRD Typical */
+	uint32_t	trdt;
+
+	/** tRD Max */
+	uint32_t	trdm;
+
+	/** tWR Typical */
+	uint32_t	twrt;
+
+	/** tWR Max */
+	uint32_t	twrm;
+
+	/** tCRS Typical */
+	uint32_t	tcrst;
+
+	/** tCRS Max */
+	uint32_t	tcrsm;
+
+	/** bytes 216-255: reserved for performance related metrics */
+	uint8_t		reserved6[40];
+
+	uint8_t		reserved7[3071 - 255];
+
+	/** bytes 3072-4095: Vendor Specific */
+	uint8_t		vs[4095 - 3071];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_ocssd_geometry_data) == 4096, "Incorrect size");
+
+struct spdk_ocssd_chunk_information_entry {
+	/** Chunk State */
+	struct {
+		/** if set to 1 chunk is free */
+		uint8_t free		: 1;
+
+		/** if set to 1 chunk is closed */
+		uint8_t closed		: 1;
+
+		/** if set to 1 chunk is open */
+		uint8_t open		: 1;
+
+		/** if set to 1 chunk is offline */
+		uint8_t offline		: 1;
+
+		uint8_t reserved	: 4;
+	} cs;
+
+	/** Chunk Type */
+	struct {
+		/** If set to 1 chunk must be written sequentially */
+		uint8_t seq_write		: 1;
+
+		/** If set to 1 chunk allows random writes */
+		uint8_t rnd_write		: 1;
+
+		uint8_t reserved1		: 2;
+
+		/**
+		 * If set to 1 chunk deviates from the chunk size reported
+		 * in identify geometry command.
+		 */
+		uint8_t size_deviate		: 1;
+
+		uint8_t reserved2		: 3;
+	} ct;
+
+	/** Wear-level Index */
+	uint8_t wli;
+
+	uint8_t reserved[5];
+
+	/** Starting LBA */
+	uint64_t slba;
+
+	/** Number of blocks in chunk */
+	uint64_t cnlb;
+
+	/** Write Pointer */
+	uint64_t wp;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_ocssd_chunk_information_entry) == 32, "Incorrect size");
+
+struct spdk_ocssd_chunk_notification_entry {
+
+	/**
+	 * This is a 64-bit incrementing notification count, indicating a
+	 * unique identifier for this notification. The counter begins at 1h
+	 * and is incremented for each unique event
+	 */
+	uint64_t		nc;
+
+	/** This field points to the chunk that has its state updated */
+	uint64_t		lba;
+
+	/**
+	 * This field indicates the namespace id that the event is associated
+	 * with
+	 */
+	uint32_t		nsid;
+
+	/** Field that indicate the state of the block */
+	struct {
+
+		/**
+		 * If set to 1, then the error rate of the chunk has been
+		 * changed to low
+		 */
+		uint8_t error_rate_low : 1;
+
+		/**
+		 * If set to 1, then the error rate of the chunk has been
+		 * changed to medium
+		 */
+		uint8_t error_rate_medium : 1;
+
+		/**
+		 * If set to 1, then the error rate of the chunk has been
+		 * changed to high
+		 */
+		uint8_t error_rate_high : 1;
+
+		/**
+		 * If set to 1, then the error rate of the chunk has been
+		 * changed to unrecoverable
+		 */
+		uint8_t unrecoverable : 1;
+
+		/**
+		 * If set to 1, then the chunk has been refreshed by the
+		 * device
+		 */
+		uint8_t refreshed : 1;
+
+		uint8_t rsvd : 3;
+
+		/**
+		 * If set to 1 then the chunk's wear-level index is outside
+		 * the average wear-level index threshold defined by the
+		 * controller
+		 */
+		uint8_t wit_exceeded : 1;
+
+		uint8_t rsvd2 : 7;
+	} state;
+
+	/**
+	 * The address provided is covering either logical block, chunk, or
+	 * parallel unit
+	 */
+	struct {
+
+		/** If set to 1, the LBA covers the logical block */
+		uint8_t lblk : 1;
+
+		/** If set to 1, the LBA covers the respecting chunk */
+		uint8_t chunk : 1;
+
+		/**
+		 * If set to 1, the LBA covers the respecting parallel unit
+		 * including all chunks
+		 */
+		uint8_t pu : 1;
+
+		uint8_t rsvd : 5;
+	} mask;
+
+	uint8_t			rsvd[9];
+
+	/**
+	 * This field indicates the number of logical chunks to be written.
+	 * This is a 0's based value. This field is only valid if mask bit 0 is
+	 * set. The number of blocks addressed shall not be outside the boundary
+	 * of the specified chunk.
+	 */
+	uint16_t		nlb;
+
+	uint8_t			rsvd2[30];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_ocssd_chunk_notification_entry) == 64, "Incorrect size");
+
+/**
+ * Vector completion queue entry
+ */
+struct spdk_ocssd_vector_cpl {
+	/* dword 0,1 */
+	uint64_t		lba_status;	/* completion status bit array */
+
+	/* dword 2 */
+	uint16_t		sqhd;	/* submission queue head pointer */
+	uint16_t		sqid;	/* submission queue identifier */
+
+	/* dword 3 */
+	uint16_t		cid;	/* command identifier */
+	struct spdk_nvme_status	status;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_ocssd_vector_cpl) == 16, "Incorrect size");
+
+/**
+ * OCSSD admin command set opcodes
+ */
+enum spdk_ocssd_admin_opcode {
+	SPDK_OCSSD_OPC_GEOMETRY	= 0xE2
+};
+
+/**
+ * OCSSD I/O command set opcodes
+ */
+enum spdk_ocssd_io_opcode {
+	SPDK_OCSSD_OPC_VECTOR_RESET	= 0x90,
+	SPDK_OCSSD_OPC_VECTOR_WRITE	= 0x91,
+	SPDK_OCSSD_OPC_VECTOR_READ	= 0x92,
+	SPDK_OCSSD_OPC_VECTOR_COPY	= 0x93
+};
+
+/**
+ * Log page identifiers for SPDK_NVME_OPC_GET_LOG_PAGE
+ */
+enum spdk_ocssd_log_page {
+	/** Chunk Information */
+	SPDK_OCSSD_LOG_CHUNK_INFO		= 0xCA,
+
+	/** Chunk Notification Log */
+	SPDK_OCSSD_LOG_CHUNK_NOTIFICATION	= 0xD0,
+};
+
+/**
+ * OCSSD feature identifiers
+ * Defines OCSSD specific features that may be configured with Set Features and
+ * retrieved with Get Features.
+ */
+enum spdk_ocssd_feat {
+	/**  Media Feedback feature identifier */
+	SPDK_OCSSD_FEAT_MEDIA_FEEDBACK	= 0xCA
+};
+
+/**
+ * OCSSD media error status codes extension.
+ * Additional error codes for status code type â€œ2hâ€ (media errors)
+ */
+enum spdk_ocssd_media_error_status_code {
+	/**
+	 * The chunk was either marked offline by the reset or the state
+	 * of the chunk is already offline.
+	 */
+	SPDK_OCSSD_SC_OFFLINE_CHUNK			= 0xC0,
+
+	/**
+	 * Invalid reset if chunk state is either â€œFreeâ€ or â€œOpenâ€
+	 */
+	SPDK_OCSSD_SC_INVALID_RESET			= 0xC1,
+
+	/**
+	 * Write failed, chunk remains open.
+	 * Host should proceed to write to next write unit.
+	 */
+	SPDK_OCSSD_SC_WRITE_FAIL_WRITE_NEXT_UNIT	= 0xF0,
+
+	/**
+	 * The writes ended prematurely. The chunk state is set to closed.
+	 * The host can read up to the value of the write pointer.
+	 */
+	SPDK_OCSSD_SC_WRITE_FAIL_CHUNK_EARLY_CLOSE	= 0xF1,
+
+	/**
+	 * The write corresponds to a write out of order within an open
+	 * chunk or the write is to a closed or offline chunk.
+	 */
+	SPDK_OCSSD_SC_OUT_OF_ORDER_WRITE		= 0xF2,
+
+	/**
+	 * The data retrieved is nearing its limit for reading.
+	 * The limit is vendor specific, and only provides a hint
+	 * to the host that should refresh its data in the future.
+	 */
+	SPDK_OCSSD_SC_READ_HIGH_ECC			= 0xD0,
+};
+
+#define SPDK_OCSSD_IO_FLAGS_LIMITED_RETRY (1U << 31)
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/PDK/core/src/api/include/udd/spdk/nvme_spec.h b/PDK/core/src/api/include/udd/spdk/nvme_spec.h
index ceed1cf..ae5f550 100644
--- a/PDK/core/src/api/include/udd/spdk/nvme_spec.h
+++ b/PDK/core/src/api/include/udd/spdk/nvme_spec.h
@@ -67,6 +67,11 @@ extern "C" {
  */
 #define SPDK_NVME_DATASET_MANAGEMENT_MAX_RANGES	256
 
+/**
+ * Maximum number of blocks that may be specified in a single dataset management range.
+ */
+#define SPDK_NVME_DATASET_MANAGEMENT_RANGE_MAX_BLOCKS	0xFFFFFFFFu
+
 union spdk_nvme_cap_register {
 	uint64_t	raw;
 	struct {
@@ -91,10 +96,12 @@ union spdk_nvme_cap_register {
 		uint32_t nssrs		: 1;
 
 		/** command sets supported */
-		uint32_t css_nvm	: 1;
+		uint32_t css		: 8;
 
-		uint32_t css_reserved	: 3;
-		uint32_t reserved2	: 7;
+		/** boot partition support */
+		uint32_t bps		: 1;
+
+		uint32_t reserved2	: 2;
 
 		/** memory page size minimum */
 		uint32_t mpsmin		: 4;
@@ -107,6 +114,17 @@ union spdk_nvme_cap_register {
 };
 SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_cap_register) == 8, "Incorrect size");
 
+/**
+ * I/O Command Set Selected
+ *
+ * Only a single command set is defined as of NVMe 1.3 (NVM).
+ */
+enum spdk_nvme_cc_css {
+	SPDK_NVME_CC_CSS_NVM		= 0x0,	/**< NVM command set */
+};
+
+#define SPDK_NVME_CAP_CSS_NVM (1u << SPDK_NVME_CC_CSS_NVM) /**< NVM command set supported */
+
 union spdk_nvme_cc_register {
 	uint32_t	raw;
 	struct {
@@ -155,7 +173,13 @@ union spdk_nvme_csts_register {
 		/** shutdown status */
 		uint32_t shst		: 2;
 
-		uint32_t reserved1	: 28;
+		/** NVM subsystem reset occurred */
+		uint32_t nssro		: 1;
+
+		/** Processing paused */
+		uint32_t pp		: 1;
+
+		uint32_t reserved1	: 26;
 	} bits;
 };
 SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_csts_register) == 4, "Incorrect size");
@@ -239,6 +263,53 @@ union spdk_nvme_cmbsz_register {
 };
 SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_cmbsz_register) == 4, "Incorrect size");
 
+/** Boot partition information */
+union spdk_nvme_bpinfo_register	{
+	uint32_t	raw;
+	struct {
+		/** Boot partition size in 128KB multiples */
+		uint32_t bpsz		: 15;
+
+		uint32_t reserved1	: 9;
+
+		/**
+		 * Boot read status
+		 * 00b: No Boot Partition read operation requested
+		 * 01b: Boot Partition read in progress
+		 * 10b: Boot Partition read completed successfully
+		 * 11b: Error completing Boot Partition read
+		 */
+		uint32_t brs		: 2;
+
+		uint32_t reserved2	: 5;
+
+		/** Active Boot Partition ID */
+		uint32_t abpid		: 1;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_bpinfo_register) == 4, "Incorrect size");
+
+/** Boot partition read select */
+union spdk_nvme_bprsel_register {
+	uint32_t	raw;
+	struct {
+		/** Boot partition read size in multiples of 4KB */
+		uint32_t bprsz		: 10;
+
+		/** Boot partition read offset in multiples of 4KB */
+		uint32_t bprof		: 20;
+
+		uint32_t reserved	: 1;
+
+		/** Boot Partition Identifier */
+		uint32_t bpid		: 1;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_bprsel_register) == 4, "Incorrect size");
+
+/** Value to write to NSSR to indicate a NVM subsystem reset ("NVMe") */
+#define SPDK_NVME_NSSR_VALUE	0x4E564D65
+
 struct spdk_nvme_registers {
 	/** controller capabilities */
 	union spdk_nvme_cap_register	cap;
@@ -264,7 +335,17 @@ struct spdk_nvme_registers {
 	union spdk_nvme_cmbloc_register	cmbloc;
 	/** controller memory buffer size */
 	union spdk_nvme_cmbsz_register cmbsz;
-	uint32_t	reserved3[0x3f0];
+
+	/** boot partition information */
+	union spdk_nvme_bpinfo_register	bpinfo;
+
+	/** boot partition read select */
+	union spdk_nvme_bprsel_register	bprsel;
+
+	/** boot partition memory buffer location (must be 4KB aligned) */
+	uint64_t			bpmbl;
+
+	uint32_t	reserved3[0x3ec];
 
 	struct {
 		uint32_t	sq_tdbl;	/* submission queue tail doorbell */
@@ -290,6 +371,12 @@ SPDK_STATIC_ASSERT(0x38 == offsetof(struct spdk_nvme_registers, cmbloc),
 		   "Incorrect register offset");
 SPDK_STATIC_ASSERT(0x3C == offsetof(struct spdk_nvme_registers, cmbsz),
 		   "Incorrect register offset");
+SPDK_STATIC_ASSERT(0x40 == offsetof(struct spdk_nvme_registers, bpinfo),
+		   "Incorrect register offset");
+SPDK_STATIC_ASSERT(0x44 == offsetof(struct spdk_nvme_registers, bprsel),
+		   "Incorrect register offset");
+SPDK_STATIC_ASSERT(0x48 == offsetof(struct spdk_nvme_registers, bpmbl),
+		   "Incorrect register offset");
 
 enum spdk_nvme_sgl_descriptor_type {
 	SPDK_NVME_SGL_TYPE_DATA_BLOCK		= 0x0,
@@ -297,13 +384,15 @@ enum spdk_nvme_sgl_descriptor_type {
 	SPDK_NVME_SGL_TYPE_SEGMENT		= 0x2,
 	SPDK_NVME_SGL_TYPE_LAST_SEGMENT		= 0x3,
 	SPDK_NVME_SGL_TYPE_KEYED_DATA_BLOCK	= 0x4,
-	/* 0x5 - 0xE reserved */
+	SPDK_NVME_SGL_TYPE_TRANSPORT_DATA_BLOCK	= 0x5,
+	/* 0x6 - 0xE reserved */
 	SPDK_NVME_SGL_TYPE_VENDOR_SPECIFIC	= 0xF
 };
 
 enum spdk_nvme_sgl_descriptor_subtype {
 	SPDK_NVME_SGL_SUBTYPE_ADDRESS		= 0x0,
 	SPDK_NVME_SGL_SUBTYPE_OFFSET		= 0x1,
+	SPDK_NVME_SGL_SUBTYPE_TRANSPORT		= 0xa,
 };
 
 struct __attribute__((packed)) spdk_nvme_sgl_descriptor {
@@ -323,7 +412,7 @@ struct __attribute__((packed)) spdk_nvme_sgl_descriptor {
 		} unkeyed;
 
 		struct {
-			uint64_t length 	: 24;
+			uint64_t length		: 24;
 			uint64_t key		: 32;
 			uint64_t subtype	: 4;
 			uint64_t type		: 4;
@@ -474,40 +563,11 @@ enum spdk_nvme_status_code_type {
 	SPDK_NVME_SCT_GENERIC		= 0x0,
 	SPDK_NVME_SCT_COMMAND_SPECIFIC	= 0x1,
 	SPDK_NVME_SCT_MEDIA_ERROR	= 0x2,
-	KV_NVME_SCT_ERROR	= 0x3,
-	/* 0x3-0x6 - reserved */
+	SPDK_NVME_SCT_PATH		= 0x3,
+	/* 0x4-0x6 - reserved */
 	SPDK_NVME_SCT_VENDOR_SPECIFIC	= 0x7,
 };
 
-enum samsung_kv_nvme_command_status_code {
-	// 0x00h is reserved
-	KV_NVME_SC_INVALID_VALUE_SIZE			= 0x01,
-	KV_NVME_SC_INVALID_VALUE_OFFSET			= 0x02,
-	KV_NVME_SC_INVALID_KEY_SIZE			= 0x03,
-	KV_NVME_SC_INVALID_OPTION			= 0x04,
-
-	// 0x05h ~ 0x07h are reserved
-	KV_NVME_SC_MISALIGNED_VALUE_SIZE		= 0x08,
-	KV_NVME_SC_MISALIGNED_VALUE_OFFSET		= 0x09,
-	KV_NVME_SC_MISALIGNED_KEY_SIZE			= 0x0A,
-
-	// 0x0Bh ~ 0x0F are reserved
-	KV_NVME_SC_NOT_EXIST_KEY			= 0x10,
-	KV_NVME_SC_UNRECOVERED_ERROR			= 0x11,
-	KV_NVME_SC_CAPACITY_EXCEEDED			= 0x12,
-
-	// 0x13b ~ 0x7Fh are reserved
-	KV_NVME_SC_IDEMPOTENT_STORE_FAIL		= 0x80,
-	KV_NVME_SC_MAXIMUM_VALUE_SIZE_LIMIT_EXCEEDED	= 0x81,
-
-	// 0x82 ~ 0x91 are reserved
-	KV_NVME_SC_INVALID_ITERATE_HANDLE		= 0x90,
-	KV_NVME_SC_NO_AVAILABLE_ITERATE_HANDLE		= 0x91,
-	KV_NVME_SC_ITERATE_HANDLE_ALREADY_OPENED	= 0x92,
-	KV_NVME_SC_ITERATE_READ_EOF			= 0x93,
-	KV_NVME_SC_ITERATE_REQUEST_FAIL			= 0x94,
-};
-
 /**
  * Generic command status codes
  */
@@ -533,11 +593,17 @@ enum spdk_nvme_generic_command_status_code {
 	SPDK_NVME_SC_INVALID_CONTROLLER_MEM_BUF		= 0x12,
 	SPDK_NVME_SC_INVALID_PRP_OFFSET			= 0x13,
 	SPDK_NVME_SC_ATOMIC_WRITE_UNIT_EXCEEDED		= 0x14,
+	SPDK_NVME_SC_OPERATION_DENIED			= 0x15,
 	SPDK_NVME_SC_INVALID_SGL_OFFSET			= 0x16,
-	SPDK_NVME_SC_INVALID_SGL_SUBTYPE		= 0x17,
+	/* 0x17 - reserved */
 	SPDK_NVME_SC_HOSTID_INCONSISTENT_FORMAT		= 0x18,
 	SPDK_NVME_SC_KEEP_ALIVE_EXPIRED			= 0x19,
 	SPDK_NVME_SC_KEEP_ALIVE_INVALID			= 0x1a,
+	SPDK_NVME_SC_ABORTED_PREEMPT			= 0x1b,
+	SPDK_NVME_SC_SANITIZE_FAILED			= 0x1c,
+	SPDK_NVME_SC_SANITIZE_IN_PROGRESS		= 0x1d,
+	SPDK_NVME_SC_SGL_DATA_BLOCK_GRANULARITY_INVALID	= 0x1e,
+	SPDK_NVME_SC_COMMAND_INVALID_IN_CMB		= 0x1f,
 
 	SPDK_NVME_SC_LBA_OUT_OF_RANGE			= 0x80,
 	SPDK_NVME_SC_CAPACITY_EXCEEDED			= 0x81,
@@ -579,6 +645,12 @@ enum spdk_nvme_command_specific_status_code {
 	SPDK_NVME_SC_NAMESPACE_NOT_ATTACHED             = 0x1a,
 	SPDK_NVME_SC_THINPROVISIONING_NOT_SUPPORTED     = 0x1b,
 	SPDK_NVME_SC_CONTROLLER_LIST_INVALID            = 0x1c,
+	SPDK_NVME_SC_DEVICE_SELF_TEST_IN_PROGRESS	= 0x1d,
+	SPDK_NVME_SC_BOOT_PARTITION_WRITE_PROHIBITED	= 0x1e,
+	SPDK_NVME_SC_INVALID_CTRLR_ID			= 0x1f,
+	SPDK_NVME_SC_INVALID_SECONDARY_CTRLR_STATE	= 0x20,
+	SPDK_NVME_SC_INVALID_NUM_CTRLR_RESOURCES	= 0x21,
+	SPDK_NVME_SC_INVALID_RESOURCE_ID		= 0x22,
 
 	SPDK_NVME_SC_CONFLICTING_ATTRIBUTES		= 0x80,
 	SPDK_NVME_SC_INVALID_PROTECTION_INFO		= 0x81,
@@ -600,6 +672,18 @@ enum spdk_nvme_media_error_status_code {
 };
 
 /**
+ * Path related status codes
+ */
+enum spdk_nvme_path_status_code {
+	SPDK_NVME_SC_INTERNAL_PATH_ERROR		= 0x00,
+
+	SPDK_NVME_SC_CONTROLLER_PATH_ERROR		= 0x60,
+
+	SPDK_NVME_SC_HOST_PATH_ERROR			= 0x70,
+	SPDK_NVME_SC_ABORTED_BY_HOST			= 0x71,
+};
+
+/**
  * Admin opcodes
  */
 enum spdk_nvme_admin_opcode {
@@ -621,13 +705,24 @@ enum spdk_nvme_admin_opcode {
 	SPDK_NVME_OPC_FIRMWARE_COMMIT			= 0x10,
 	SPDK_NVME_OPC_FIRMWARE_IMAGE_DOWNLOAD		= 0x11,
 
+	SPDK_NVME_OPC_DEVICE_SELF_TEST			= 0x14,
 	SPDK_NVME_OPC_NS_ATTACHMENT			= 0x15,
 
 	SPDK_NVME_OPC_KEEP_ALIVE			= 0x18,
+	SPDK_NVME_OPC_DIRECTIVE_SEND			= 0x19,
+	SPDK_NVME_OPC_DIRECTIVE_RECEIVE			= 0x1a,
+
+	SPDK_NVME_OPC_VIRTUALIZATION_MANAGEMENT		= 0x1c,
+	SPDK_NVME_OPC_NVME_MI_SEND			= 0x1d,
+	SPDK_NVME_OPC_NVME_MI_RECEIVE			= 0x1e,
+
+	SPDK_NVME_OPC_DOORBELL_BUFFER_CONFIG		= 0x7c,
 
 	SPDK_NVME_OPC_FORMAT_NVM			= 0x80,
 	SPDK_NVME_OPC_SECURITY_SEND			= 0x81,
 	SPDK_NVME_OPC_SECURITY_RECEIVE			= 0x82,
+
+	SPDK_NVME_OPC_SANITIZE				= 0x84,
 };
 
 /**
@@ -681,29 +776,59 @@ static inline enum spdk_nvme_data_transfer spdk_nvme_opc_get_data_transfer(uint8
 
 enum spdk_nvme_feat {
 	/* 0x00 - reserved */
+
+	/** cdw11 layout defined by \ref spdk_nvme_feat_arbitration */
 	SPDK_NVME_FEAT_ARBITRATION				= 0x01,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_power_management */
 	SPDK_NVME_FEAT_POWER_MANAGEMENT				= 0x02,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_lba_range_type */
 	SPDK_NVME_FEAT_LBA_RANGE_TYPE				= 0x03,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_temperature_threshold */
 	SPDK_NVME_FEAT_TEMPERATURE_THRESHOLD			= 0x04,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_error_recovery */
 	SPDK_NVME_FEAT_ERROR_RECOVERY				= 0x05,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_volatile_write_cache */
 	SPDK_NVME_FEAT_VOLATILE_WRITE_CACHE			= 0x06,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_number_of_queues */
 	SPDK_NVME_FEAT_NUMBER_OF_QUEUES				= 0x07,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_interrupt_coalescing */
 	SPDK_NVME_FEAT_INTERRUPT_COALESCING			= 0x08,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_interrupt_vector_configuration */
 	SPDK_NVME_FEAT_INTERRUPT_VECTOR_CONFIGURATION		= 0x09,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_write_atomicity */
 	SPDK_NVME_FEAT_WRITE_ATOMICITY				= 0x0A,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_async_event_configuration */
 	SPDK_NVME_FEAT_ASYNC_EVENT_CONFIGURATION		= 0x0B,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_autonomous_power_state_transition */
 	SPDK_NVME_FEAT_AUTONOMOUS_POWER_STATE_TRANSITION	= 0x0C,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_host_mem_buffer */
 	SPDK_NVME_FEAT_HOST_MEM_BUFFER				= 0x0D,
+	SPDK_NVME_FEAT_TIMESTAMP				= 0x0E,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_keep_alive_timer */
 	SPDK_NVME_FEAT_KEEP_ALIVE_TIMER				= 0x0F,
-	/* 0x0C-0x7F - reserved */
+	/** cdw11 layout defined by \ref spdk_nvme_feat_host_controlled_thermal_management */
+	SPDK_NVME_FEAT_HOST_CONTROLLED_THERMAL_MANAGEMENT	= 0x10,
+	/** cdw11 layout defined by \ref spdk_nvme_feat_non_operational_power_state_config */
+	SPDK_NVME_FEAT_NON_OPERATIONAL_POWER_STATE_CONFIG	= 0x11,
+
+	/* 0x12-0x77 - reserved */
+
+	/* 0x78-0x7F - NVMe-MI features */
+
+	/** cdw11 layout defined by \ref spdk_nvme_feat_software_progress_marker */
 	SPDK_NVME_FEAT_SOFTWARE_PROGRESS_MARKER			= 0x80,
-	/* 0x81-0xBF - command set specific */
+
+	/** cdw11 layout defined by \ref spdk_nvme_feat_host_identifier */
 	SPDK_NVME_FEAT_HOST_IDENTIFIER				= 0x81,
 	SPDK_NVME_FEAT_HOST_RESERVE_MASK			= 0x82,
 	SPDK_NVME_FEAT_HOST_RESERVE_PERSIST			= 0x83,
+
+	/* 0x84-0xBF - command set specific (reserved) */
+
 	/* 0xC0-0xFF - vendor specific */
 };
 
+/** Bit set of attributes for DATASET MANAGEMENT commands. */
 enum spdk_nvme_dsm_attribute {
 	SPDK_NVME_DSM_ATTR_INTEGRAL_READ		= 0x1,
 	SPDK_NVME_DSM_ATTR_INTEGRAL_WRITE		= 0x2,
@@ -749,6 +874,9 @@ enum spdk_nvme_identify_cns {
 	/** List active NSIDs greater than CDW1.NSID */
 	SPDK_NVME_IDENTIFY_ACTIVE_NS_LIST		= 0x02,
 
+	/** List namespace identification descriptors */
+	SPDK_NVME_IDENTIFY_NS_ID_DESCRIPTOR_LIST	= 0x03,
+
 	/** List allocated NSIDs greater than CDW1.NSID */
 	SPDK_NVME_IDENTIFY_ALLOCATED_NS_LIST		= 0x10,
 
@@ -760,6 +888,12 @@ enum spdk_nvme_identify_cns {
 
 	/** Get list of controllers starting at CDW10.CNTID */
 	SPDK_NVME_IDENTIFY_CTRLR_LIST			= 0x13,
+
+	/** Get primary controller capabilities structure */
+	SPDK_NVME_IDENTIFY_PRIMARY_CTRLR_CAP		= 0x14,
+
+	/** Get secondary controller list */
+	SPDK_NVME_IDENTIFY_SECONDARY_CTRLR_LIST		= 0x15,
 };
 
 /** NVMe over Fabrics controller model */
@@ -771,6 +905,36 @@ enum spdk_nvmf_ctrlr_model {
 	SPDK_NVMF_CTRLR_MODEL_STATIC			= 1,
 };
 
+#define SPDK_NVME_CTRLR_SN_LEN	20
+#define SPDK_NVME_CTRLR_MN_LEN	40
+#define SPDK_NVME_CTRLR_FR_LEN	8
+
+/** Identify Controller data sgls.supported values */
+enum spdk_nvme_sgls_supported {
+	/** SGLs are not supported */
+	SPDK_NVME_SGLS_NOT_SUPPORTED			= 0,
+
+	/** SGLs are supported with no alignment or granularity requirement. */
+	SPDK_NVME_SGLS_SUPPORTED			= 1,
+
+	/** SGLs are supported with a DWORD alignment and granularity requirement. */
+	SPDK_NVME_SGLS_SUPPORTED_DWORD_ALIGNED		= 2,
+};
+
+/** Identify Controller data vwc.flush_broadcast values */
+enum spdk_nvme_flush_broadcast {
+	/** Support for NSID=FFFFFFFFh with Flush is not indicated. */
+	SPDK_NVME_FLUSH_BROADCAST_NOT_INDICATED		= 0,
+
+	/* 01b: Reserved */
+
+	/** Flush does not support NSID set to FFFFFFFFh. */
+	SPDK_NVME_FLUSH_BROADCAST_NOT_SUPPORTED		= 2,
+
+	/** Flush supports NSID set to FFFFFFFFh. */
+	SPDK_NVME_FLUSH_BROADCAST_SUPPORTED		= 3
+};
+
 struct __attribute__((packed)) spdk_nvme_ctrlr_data {
 	/* bytes 0-255: controller capabilities and features */
 
@@ -781,13 +945,13 @@ struct __attribute__((packed)) spdk_nvme_ctrlr_data {
 	uint16_t		ssvid;
 
 	/** serial number */
-	int8_t			sn[20];
+	int8_t			sn[SPDK_NVME_CTRLR_SN_LEN];
 
 	/** model number */
-	int8_t			mn[40];
+	int8_t			mn[SPDK_NVME_CTRLR_MN_LEN];
 
 	/** firmware revision */
-	uint8_t			fr[8];
+	uint8_t			fr[SPDK_NVME_CTRLR_FR_LEN];
 
 	/** recommended arbitration burst */
 	uint8_t			rab;
@@ -833,11 +997,21 @@ struct __attribute__((packed)) spdk_nvme_ctrlr_data {
 
 	/** controller attributes */
 	struct {
+		/** Supports 128-bit host identifier */
 		uint32_t	host_id_exhid_supported: 1;
-		uint32_t	reserved: 31;
+
+		/** Supports non-operational power state permissive mode */
+		uint32_t	non_operational_power_state_permissive_mode: 1;
+
+		uint32_t	reserved: 30;
 	} ctratt;
 
-	uint8_t			reserved1[156];
+	uint8_t			reserved_100[12];
+
+	/** FRU globally unique identifier */
+	uint8_t			fguid[16];
+
+	uint8_t			reserved_128[128];
 
 	/* bytes 256-511: admin command set attributes */
 
@@ -855,7 +1029,22 @@ struct __attribute__((packed)) spdk_nvme_ctrlr_data {
 		/* supports ns manage/ns attach commands */
 		uint16_t	ns_manage  : 1;
 
-		uint16_t	oacs_rsvd : 12;
+		/** Supports device self-test command (SPDK_NVME_OPC_DEVICE_SELF_TEST) */
+		uint16_t	device_self_test : 1;
+
+		/** Supports SPDK_NVME_OPC_DIRECTIVE_SEND and SPDK_NVME_OPC_DIRECTIVE_RECEIVE */
+		uint16_t	directives : 1;
+
+		/** Supports NVMe-MI (SPDK_NVME_OPC_NVME_MI_SEND, SPDK_NVME_OPC_NVME_MI_RECEIVE) */
+		uint16_t	nvme_mi : 1;
+
+		/** Supports SPDK_NVME_OPC_VIRTUALIZATION_MANAGEMENT */
+		uint16_t	virtualization_management : 1;
+
+		/** Supports SPDK_NVME_OPC_DOORBELL_BUFFER_CONFIG */
+		uint16_t	doorbell_buffer_config : 1;
+
+		uint16_t	oacs_rsvd : 7;
 	} oacs;
 
 	/** abort command limit */
@@ -886,7 +1075,9 @@ struct __attribute__((packed)) spdk_nvme_ctrlr_data {
 		uint8_t		celp : 1;
 		/* extended data for get log page */
 		uint8_t		edlp: 1;
-		uint8_t		lpa_rsvd : 5;
+		/** telemetry log pages and notices */
+		uint8_t		telemetry : 1;
+		uint8_t		lpa_rsvd : 4;
 	} lpa;
 
 	/** error log page entries */
@@ -944,11 +1135,64 @@ struct __attribute__((packed)) spdk_nvme_ctrlr_data {
 		uint8_t		access_size;
 	} rpmbs;
 
-	uint8_t			reserved2[4];
+	/** extended device self-test time (in minutes) */
+	uint16_t		edstt;
 
+	/** device self-test options */
+	union {
+		uint8_t	raw;
+		struct {
+			/** Device supports only one device self-test operation at a time */
+			uint8_t	one_only : 1;
+
+			uint8_t	reserved : 7;
+		} bits;
+	} dsto;
+
+	/**
+	 * Firmware update granularity
+	 *
+	 * 4KB units
+	 * 0x00 = no information provided
+	 * 0xFF = no restriction
+	 */
+	uint8_t			fwug;
+
+	/**
+	 * Keep Alive Support
+	 *
+	 * Granularity of keep alive timer in 100 ms units
+	 * 0 = keep alive not supported
+	 */
 	uint16_t		kas;
 
-	uint8_t			reserved3[190];
+	/** Host controlled thermal management attributes */
+	union {
+		uint16_t		raw;
+		struct {
+			uint16_t	supported : 1;
+			uint16_t	reserved : 15;
+		} bits;
+	} hctma;
+
+	/** Minimum thermal management temperature */
+	uint16_t		mntmt;
+
+	/** Maximum thermal management temperature */
+	uint16_t		mxtmt;
+
+	/** Sanitize capabilities */
+	union {
+		uint32_t	raw;
+		struct {
+			uint32_t	crypto_erase : 1;
+			uint32_t	block_erase : 1;
+			uint32_t	overwrite : 1;
+			uint32_t	reserved : 29;
+		} bits;
+	} sanicap;
+
+	uint8_t			reserved3[180];
 
 	/* bytes 512-703: nvm command set attributes */
 
@@ -977,7 +1221,8 @@ struct __attribute__((packed)) spdk_nvme_ctrlr_data {
 		uint16_t	write_zeroes: 1;
 		uint16_t	set_features_save: 1;
 		uint16_t	reservations: 1;
-		uint16_t	reserved: 10;
+		uint16_t	timestamp: 1;
+		uint16_t	reserved: 9;
 	} oncs;
 
 	/** fused operation support */
@@ -994,7 +1239,8 @@ struct __attribute__((packed)) spdk_nvme_ctrlr_data {
 	/** volatile write cache */
 	struct {
 		uint8_t		present : 1;
-		uint8_t		reserved : 7;
+		uint8_t		flush_broadcast : 2;
+		uint8_t		reserved : 5;
 	} vwc;
 
 	/** atomic write unit normal */
@@ -1015,8 +1261,7 @@ struct __attribute__((packed)) spdk_nvme_ctrlr_data {
 
 	/** SGL support */
 	struct {
-		uint32_t	supported : 1;
-		uint32_t	reserved0 : 1;
+		uint32_t	supported : 2;
 		uint32_t	keyed_sgl : 1;
 		uint32_t	reserved1 : 13;
 		uint32_t	bit_bucket_descriptor : 1;
@@ -1024,7 +1269,8 @@ struct __attribute__((packed)) spdk_nvme_ctrlr_data {
 		uint32_t	oversized_sgl : 1;
 		uint32_t	metadata_address : 1;
 		uint32_t	sgl_offset : 1;
-		uint32_t	reserved2: 11;
+		uint32_t	transport_sgl : 1;
+		uint32_t	reserved2 : 10;
 	} sgls;
 
 	uint8_t			reserved4[228];
@@ -1065,6 +1311,76 @@ struct __attribute__((packed)) spdk_nvme_ctrlr_data {
 };
 SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_ctrlr_data) == 4096, "Incorrect size");
 
+struct __attribute__((packed)) spdk_nvme_primary_ctrl_capabilities {
+	/**  controller id */
+	uint16_t		cntlid;
+	/**  port identifier */
+	uint16_t		portid;
+	/**  controller resource types */
+	struct {
+		uint8_t vq_supported	: 1;
+		uint8_t vi_supported	: 1;
+		uint8_t reserved	: 6;
+	} crt;
+	uint8_t			reserved[27];
+	/** total number of VQ flexible resources */
+	uint32_t		vqfrt;
+	/** total number of VQ flexible resources assigned to secondary controllers */
+	uint32_t		vqrfa;
+	/** total number of VQ flexible resources allocated to primary controller */
+	uint16_t		vqrfap;
+	/** total number of VQ Private resources for the primary controller */
+	uint16_t		vqprt;
+	/** max number of VQ flexible Resources that may be assigned to a secondary controller */
+	uint16_t		vqfrsm;
+	/** preferred granularity of assigning and removing VQ Flexible Resources */
+	uint16_t		vqgran;
+	uint8_t			reserved1[16];
+	/** total number of VI flexible resources for the primary and its secondary controllers */
+	uint32_t		vifrt;
+	/** total number of VI flexible resources assigned to the secondary controllers */
+	uint32_t		virfa;
+	/** total number of VI flexible resources currently allocated to the primary controller */
+	uint16_t		virfap;
+	/** total number of VI private resources for the primary controller */
+	uint16_t		viprt;
+	/** max number of VI flexible resources that may be assigned to a secondary controller */
+	uint16_t		vifrsm;
+	/** preferred granularity of assigning and removing VI flexible resources */
+	uint16_t		vigran;
+	uint8_t			reserved2[4016];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_primary_ctrl_capabilities) == 4096, "Incorrect size");
+
+struct __attribute__((packed)) spdk_nvme_secondary_ctrl_entry {
+	/** controller identifier of the secondary controller */
+	uint16_t		scid;
+	/** controller identifier of the associated primary controller */
+	uint16_t		pcid;
+	/** indicates the state of the secondary controller */
+	struct {
+		uint8_t is_online	: 1;
+		uint8_t reserved	: 7;
+	} scs;
+	uint8_t	reserved[3];
+	/** VF number if the secondary controller is an SR-IOV VF */
+	uint16_t		vfn;
+	/** number of VQ flexible resources assigned to the indicated secondary controller */
+	uint16_t		nvq;
+	/** number of VI flexible resources assigned to the indicated secondary controller */
+	uint16_t		nvi;
+	uint8_t			reserved1[18];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_secondary_ctrl_entry) == 32, "Incorrect size");
+
+struct __attribute__((packed)) spdk_nvme_secondary_ctrl_list {
+	/** number of Secondary controller entries in the list */
+	uint8_t					number;
+	uint8_t					reserved[31];
+	struct spdk_nvme_secondary_ctrl_entry	entries[127];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_secondary_ctrl_list) == 4096, "Incorrect size");
+
 struct spdk_nvme_ns_data {
 	/** namespace size */
 	uint64_t		nsze;
@@ -1079,7 +1395,17 @@ struct spdk_nvme_ns_data {
 	struct {
 		/** thin provisioning */
 		uint8_t		thin_prov : 1;
-		uint8_t		reserved1 : 7;
+
+		/** NAWUN, NAWUPF, and NACWU are defined for this namespace */
+		uint8_t		ns_atomic_write_unit : 1;
+
+		/** Supports Deallocated or Unwritten LBA error for this namespace */
+		uint8_t		dealloc_or_unwritten_error : 1;
+
+		/** Non-zero NGUID and EUI64 for namespace are never reused */
+		uint8_t		guid_never_reused : 1;
+
+		uint8_t		reserved1 : 4;
 	} nsfeat;
 
 	/** number of lba formats */
@@ -1164,7 +1490,8 @@ struct spdk_nvme_ns_data {
 			/** supports exclusive access - all registrants */
 			uint8_t		exclusive_access_all_reg : 1;
 
-			uint8_t		reserved : 1;
+			/** supports ignore existing key */
+			uint8_t		ignore_existing_key : 1;
 		} rescap;
 		uint8_t		raw;
 	} nsrescap;
@@ -1174,7 +1501,34 @@ struct spdk_nvme_ns_data {
 		uint8_t		fpi_supported : 1;
 	} fpi;
 
-	uint8_t			reserved33;
+	/** deallocate logical features */
+	union {
+		uint8_t		raw;
+		struct {
+			/**
+			 * Value read from deallocated blocks
+			 *
+			 * 000b = not reported
+			 * 001b = all bytes 0x00
+			 * 010b = all bytes 0xFF
+			 *
+			 * \ref spdk_nvme_dealloc_logical_block_read_value
+			 */
+			uint8_t	read_value : 3;
+
+			/** Supports Deallocate bit in Write Zeroes */
+			uint8_t	write_zero_deallocate : 1;
+
+			/**
+			 * Guard field behavior for deallocated logical blocks
+			 * 0: contains 0xFFFF
+			 * 1: contains CRC for read value
+			 */
+			uint8_t	guard_value : 1;
+
+			uint8_t	reserved : 3;
+		} bits;
+	} dlfeat;
 
 	/** namespace atomic write unit normal */
 	uint16_t		nawun;
@@ -1194,7 +1548,8 @@ struct spdk_nvme_ns_data {
 	/** namespace atomic boundary size power fail */
 	uint16_t		nabspf;
 
-	uint16_t		reserved46;
+	/** namespace optimal I/O boundary in logical blocks */
+	uint16_t		noiob;
 
 	/** NVM capacity */
 	uint64_t		nvmcap[2];
@@ -1228,6 +1583,20 @@ struct spdk_nvme_ns_data {
 SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_ns_data) == 4096, "Incorrect size");
 
 /**
+ * Deallocated logical block features - read value
+ */
+enum spdk_nvme_dealloc_logical_block_read_value {
+	/** Not reported */
+	SPDK_NVME_DEALLOC_NOT_REPORTED	= 0,
+
+	/** Deallocated blocks read 0x00 */
+	SPDK_NVME_DEALLOC_READ_00	= 1,
+
+	/** Deallocated blocks read 0xFF */
+	SPDK_NVME_DEALLOC_READ_FF	= 2,
+};
+
+/**
  * Reservation Type Encoding
  */
 enum spdk_nvme_reservation_type {
@@ -1273,32 +1642,57 @@ enum spdk_nvme_reservation_acquire_action {
 
 struct __attribute__((packed)) spdk_nvme_reservation_status_data {
 	/** reservation action generation counter */
-	uint32_t		generation;
+	uint32_t		gen;
 	/** reservation type */
-	uint8_t			type;
+	uint8_t			rtype;
 	/** number of registered controllers */
-	uint16_t		nr_regctl;
+	uint16_t		regctl;
 	uint16_t		reserved1;
 	/** persist through power loss state */
-	uint8_t			ptpl_state;
+	uint8_t			ptpls;
 	uint8_t			reserved[14];
 };
 SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_reservation_status_data) == 24, "Incorrect size");
 
-struct __attribute__((packed)) spdk_nvme_reservation_ctrlr_data {
-	uint16_t		ctrlr_id;
+struct __attribute__((packed)) spdk_nvme_reservation_status_extended_data {
+	struct spdk_nvme_reservation_status_data	data;
+	uint8_t						reserved[40];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_reservation_status_extended_data) == 64,
+		   "Incorrect size");
+
+struct __attribute__((packed)) spdk_nvme_registered_ctrlr_data {
+	/** controller id */
+	uint16_t		cntlid;
+	/** reservation status */
+	struct {
+		uint8_t		status    : 1;
+		uint8_t		reserved1 : 7;
+	} rcsts;
+	uint8_t			reserved2[5];
+	/** 64-bit host identifier */
+	uint64_t		hostid;
+	/** reservation key */
+	uint64_t		rkey;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_registered_ctrlr_data) == 24, "Incorrect size");
+
+struct __attribute__((packed)) spdk_nvme_registered_ctrlr_extended_data {
+	/** controller id */
+	uint16_t		cntlid;
 	/** reservation status */
 	struct {
 		uint8_t		status    : 1;
 		uint8_t		reserved1 : 7;
 	} rcsts;
 	uint8_t			reserved2[5];
-	/** host identifier */
-	uint64_t		host_id;
 	/** reservation key */
-	uint64_t		key;
+	uint64_t		rkey;
+	/** 128-bit host identifier */
+	uint8_t			hostid[16];
+	uint8_t			reserved3[32];
 };
-SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_reservation_ctrlr_data) == 24, "Incorrect size");
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_registered_ctrlr_extended_data) == 64, "Incorrect size");
 
 /**
  * Change persist through power loss state for
@@ -1342,6 +1736,39 @@ enum spdk_nvme_reservation_release_action {
 };
 
 /**
+ * Reservation notification log page type
+ */
+enum spdk_nvme_reservation_notification_log_page_type {
+	SPDK_NVME_RESERVATION_LOG_PAGE_EMPTY	= 0x0,
+	SPDK_NVME_REGISTRATION_PREEMPTED	= 0x1,
+	SPDK_NVME_RESERVATION_RELEASED		= 0x2,
+	SPDK_NVME_RESERVATION_PREEMPTED		= 0x3,
+};
+
+/**
+ * Reservation notification log
+ */
+struct spdk_nvme_reservation_notification_log {
+	/** 64-bit incrementing reservation notification log page count */
+	uint64_t	log_page_count;
+	/** Reservation notification log page type */
+	uint8_t		type;
+	/** Number of additional available reservation notification log pages */
+	uint8_t		num_avail_log_pages;
+	uint8_t		reserved[2];
+	uint32_t	nsid;
+	uint8_t		reserved1[48];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_reservation_notification_log) == 64, "Incorrect size");
+
+/* Mask Registration Preempted Notificaton */
+#define SPDK_NVME_REGISTRATION_PREEMPTED_MASK	(1U << 1)
+/* Mask Reservation Released Notification */
+#define SPDK_NVME_RESERVATION_RELEASED_MASK	(1U << 2)
+/* Mask Reservation Preempted Notification */
+#define SPDK_NVME_RESERVATION_PREEMPTED_MASK	(1U << 3)
+
+/**
  * Log page identifiers for SPDK_NVME_OPC_GET_LOG_PAGE
  */
 enum spdk_nvme_log_page {
@@ -1362,7 +1789,16 @@ enum spdk_nvme_log_page {
 	/** Command effects log (optional) */
 	SPDK_NVME_LOG_COMMAND_EFFECTS_LOG	= 0x05,
 
-	/* 0x06-0x6F - reserved */
+	/** Device self test (optional) */
+	SPDK_NVME_LOG_DEVICE_SELF_TEST	= 0x06,
+
+	/** Host initiated telemetry log (optional) */
+	SPDK_NVME_LOG_TELEMETRY_HOST_INITIATED	= 0x07,
+
+	/** Controller initiated telemetry log (optional) */
+	SPDK_NVME_LOG_TELEMETRY_CTRLR_INITIATED	= 0x08,
+
+	/* 0x09-0x6F - reserved */
 
 	/** Discovery(refer to the NVMe over Fabrics specification) */
 	SPDK_NVME_LOG_DISCOVERY		= 0x70,
@@ -1389,7 +1825,11 @@ struct spdk_nvme_error_information_entry {
 	uint64_t		lba;
 	uint32_t		nsid;
 	uint8_t			vendor_specific;
-	uint8_t			reserved[35];
+	uint8_t			trtype;
+	uint8_t			reserved30[2];
+	uint64_t		command_specific;
+	uint16_t		trtype_specific;
+	uint8_t			reserved42[22];
 };
 SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_error_information_entry) == 64, "Incorrect size");
 
@@ -1447,17 +1887,462 @@ struct __attribute__((packed)) spdk_nvme_health_information_page {
 };
 SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_health_information_page) == 512, "Incorrect size");
 
+/* Commands Supported and Effects Data Structure */
+struct spdk_nvme_cmds_and_effect_entry {
+	/** Command Supported */
+	uint16_t csupp : 1;
+
+	/** Logic Block Content Change  */
+	uint16_t lbcc  : 1;
+
+	/** Namespace Capability Change */
+	uint16_t ncc   : 1;
+
+	/** Namespace Inventory Change */
+	uint16_t nic   : 1;
+
+	/** Controller Capability Change */
+	uint16_t ccc   : 1;
+
+	uint16_t reserved1 : 11;
+
+	/* Command Submission and Execution recommendation
+	 * 000 - No command submission or execution restriction
+	 * 001 - Submitted when there is no outstanding command to same NS
+	 * 010 - Submitted when there is no outstanding command to any NS
+	 * others - Reserved
+	 * \ref command_submission_and_execution in section 5.14.1.5 NVMe Revision 1.3
+	 */
+	uint16_t cse : 3;
+
+	uint16_t reserved2 : 13;
+};
+
+/* Commands Supported and Effects Log Page */
+struct spdk_nvme_cmds_and_effect_log_page {
+	/** Commands Supported and Effects Data Structure for the Admin Commands */
+	struct spdk_nvme_cmds_and_effect_entry admin_cmds_supported[256];
+
+	/** Commands Supported and Effects Data Structure for the IO Commands */
+	struct spdk_nvme_cmds_and_effect_entry io_cmds_supported[256];
+
+	uint8_t reserved0[2048];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_cmds_and_effect_log_page) == 4096, "Incorrect size");
+
+/*
+ * Get Log Page â€“ Telemetry Host/Controller Initiated Log (Log Identifiers 07h/08h)
+ */
+struct spdk_nvme_telemetry_log_page_hdr {
+	uint8_t    lpi;			/* Log page identifier */
+	uint8_t    rsvd[4];
+	uint8_t    ieee_oui[3];
+	uint16_t   dalb1;		/* Data area 1 last block */
+	uint16_t   dalb2;		/* Data area 2 last block */
+	uint16_t   dalb3;		/* Data area 3 last block */
+	uint8_t    rsvd1[368];
+	uint8_t    ctrlr_avail;		/* Controller initiated data avail */
+	uint8_t    ctrlr_gen;		/* Controller initiated telemetry data generation */
+	uint8_t    rsnident[128];	/* Reason identifier */
+	uint8_t    telemetry_datablock[0];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_telemetry_log_page_hdr) == 512, "Incorrect size");
+
+/**
+ * Asynchronous Event Type
+ */
+enum spdk_nvme_async_event_type {
+	/* Error Status */
+	SPDK_NVME_ASYNC_EVENT_TYPE_ERROR	= 0x0,
+	/* SMART/Health Status */
+	SPDK_NVME_ASYNC_EVENT_TYPE_SMART	= 0x1,
+	/* Notice */
+	SPDK_NVME_ASYNC_EVENT_TYPE_NOTICE	= 0x2,
+	/* 0x3 - 0x5 Reserved */
+
+	/* I/O Command Set Specific Status */
+	SPDK_NVME_ASYNC_EVENT_TYPE_IO		= 0x6,
+	/* Vendor Specific */
+	SPDK_NVME_ASYNC_EVENT_TYPE_VENDOR	= 0x7,
+};
+
+/**
+ * Asynchronous Event Information for Error Status
+ */
+enum spdk_nvme_async_event_info_error {
+	/* Write to Invalid Doorbell Register */
+	SPDK_NVME_ASYNC_EVENT_WRITE_INVALID_DB		= 0x0,
+	/* Invalid Doorbell Register Write Value */
+	SPDK_NVME_ASYNC_EVENT_INVALID_DB_WRITE		= 0x1,
+	/* Diagnostic Failure */
+	SPDK_NVME_ASYNC_EVENT_DIAGNOSTIC_FAILURE	= 0x2,
+	/* Persistent Internal Error */
+	SPDK_NVME_ASYNC_EVENT_PERSISTENT_INTERNAL	= 0x3,
+	/* Transient Internal Error */
+	SPDK_NVME_ASYNC_EVENT_TRANSIENT_INTERNAL	= 0x4,
+	/* Firmware Image Load Error */
+	SPDK_NVME_ASYNC_EVENT_FW_IMAGE_LOAD		= 0x5,
+
+	/* 0x6 - 0xFF Reserved */
+};
+
+/**
+ * Asynchronous Event Information for SMART/Health Status
+ */
+enum spdk_nvme_async_event_info_smart {
+	/* NVM Subsystem Reliability */
+	SPDK_NVME_ASYNC_EVENT_SUBSYSTEM_RELIABILITY	= 0x0,
+	/* Temperature Threshold */
+	SPDK_NVME_ASYNC_EVENT_TEMPERATURE_THRESHOLD	= 0x1,
+	/* Spare Below Threshold */
+	SPDK_NVME_ASYNC_EVENT_SPARE_BELOW_THRESHOLD	= 0x2,
+
+	/* 0x3 - 0xFF Reserved */
+};
+
+/**
+ * Asynchronous Event Information for Notice
+ */
+enum spdk_nvme_async_event_info_notice {
+	/* Namespace Attribute Changed */
+	SPDK_NVME_ASYNC_EVENT_NS_ATTR_CHANGED		= 0x0,
+	/* Firmware Activation Starting */
+	SPDK_NVME_ASYNC_EVENT_FW_ACTIVATION_START	= 0x1,
+	/* Telemetry Log Changed */
+	SPDK_NVME_ASYNC_EVENT_TELEMETRY_LOG_CHANGED	= 0x2,
+
+	/* 0x3 - 0xFF Reserved */
+};
+
+/**
+ * Asynchronous Event Information for NVM Command Set Specific Status
+ */
+enum spdk_nvme_async_event_info_nvm_command_set {
+	/* Reservation Log Page Avaiable */
+	SPDK_NVME_ASYNC_EVENT_RESERVATION_LOG_AVAIL	= 0x0,
+	/* Sanitize Operation Completed */
+	SPDK_NVME_ASYNC_EVENT_SANITIZE_COMPLETED	= 0x1,
+
+	/* 0x2 - 0xFF Reserved */
+};
+
+/**
+ * Asynchronous Event Request Completion
+ */
+union spdk_nvme_async_event_completion {
+	uint32_t raw;
+	struct {
+		uint32_t async_event_type	: 3;
+		uint32_t reserved1		: 5;
+		uint32_t async_event_info	: 8;
+		uint32_t log_page_identifier	: 8;
+		uint32_t reserved2		: 8;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_async_event_completion) == 4, "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_ARBITRATION
+ */
+union spdk_nvme_feat_arbitration {
+	uint32_t raw;
+	struct {
+		/** Arbitration Burst */
+		uint32_t ab : 3;
+
+		uint32_t reserved : 5;
+
+		/** Low Priority Weight */
+		uint32_t lpw : 8;
+
+		/** Medium Priority Weight */
+		uint32_t mpw : 8;
+
+		/** High Priority Weight */
+		uint32_t hpw : 8;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_arbitration) == 4, "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_POWER_MANAGEMENT
+ */
+union spdk_nvme_feat_power_management {
+	uint32_t raw;
+	struct {
+		/** Power State */
+		uint32_t ps : 5;
+
+		/** Workload Hint */
+		uint32_t wh : 3;
+
+		uint32_t reserved : 24;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_power_management) == 4, "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_LBA_RANGE_TYPE
+ */
+union spdk_nvme_feat_lba_range_type {
+	uint32_t raw;
+	struct {
+		/** Number of LBA Ranges */
+		uint32_t num : 6;
+
+		uint32_t reserved : 26;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_lba_range_type) == 4, "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_TEMPERATURE_THRESHOLD
+ */
+union spdk_nvme_feat_temperature_threshold {
+	uint32_t raw;
+	struct {
+		/** Temperature Threshold */
+		uint32_t tmpth : 16;
+
+		/** Threshold Temperature Select */
+		uint32_t tmpsel : 4;
+
+		/** Threshold Type Select */
+		uint32_t thsel : 2;
+
+		uint32_t reserved : 10;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_temperature_threshold) == 4, "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_ERROR_RECOVERY
+ */
+union spdk_nvme_feat_error_recovery {
+	uint32_t raw;
+	struct {
+		/** Time Limited Error Recovery */
+		uint32_t tler : 16;
+
+		/** Deallocated or Unwritten Logical Block Error Enable */
+		uint32_t dulbe : 1;
+
+		uint32_t reserved : 15;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_error_recovery) == 4, "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_VOLATILE_WRITE_CACHE
+ */
+union spdk_nvme_feat_volatile_write_cache {
+	uint32_t raw;
+	struct {
+		/** Volatile Write Cache Enable */
+		uint32_t wce : 1;
+
+		uint32_t reserved : 31;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_volatile_write_cache) == 4, "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_NUMBER_OF_QUEUES
+ */
+union spdk_nvme_feat_number_of_queues {
+	uint32_t raw;
+	struct {
+		/** Number of I/O Submission Queues Requested */
+		uint32_t nsqr : 16;
+
+		/** Number of I/O Completion Queues Requested */
+		uint32_t ncqr : 16;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_number_of_queues) == 4, "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_INTERRUPT_COALESCING
+ */
+union spdk_nvme_feat_interrupt_coalescing {
+	uint32_t raw;
+	struct {
+		/** Aggregation Threshold */
+		uint32_t thr : 8;
+
+		/** Aggregration time */
+		uint32_t time : 8;
+
+		uint32_t reserved : 16;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_interrupt_coalescing) == 4, "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_INTERRUPT_VECTOR_CONFIGURATION
+ */
+union spdk_nvme_feat_interrupt_vector_configuration {
+	uint32_t raw;
+	struct {
+		/** Interrupt Vector */
+		uint32_t iv : 16;
+
+		/** Coalescing Disable */
+		uint32_t cd : 1;
+
+		uint32_t reserved : 15;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_interrupt_vector_configuration) == 4,
+		   "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_WRITE_ATOMICITY
+ */
+union spdk_nvme_feat_write_atomicity {
+	uint32_t raw;
+	struct {
+		/** Disable Normal */
+		uint32_t dn : 1;
+
+		uint32_t reserved : 31;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_write_atomicity) == 4, "Incorrect size");
+
+/**
+ * Data used by Set Features / Get Features \ref SPDK_NVME_FEAT_ASYNC_EVENT_CONFIGURATION
+ */
+union spdk_nvme_feat_async_event_configuration {
+	uint32_t raw;
+	struct {
+		union spdk_nvme_critical_warning_state crit_warn;
+		uint32_t ns_attr_notice		: 1;
+		uint32_t fw_activation_notice	: 1;
+		uint32_t telemetry_log_notice	: 1;
+		uint32_t reserved		: 21;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_async_event_configuration) == 4, "Incorrect size");
+/* Old name defined for compatibility */
+#define spdk_nvme_async_event_config spdk_nvme_feat_async_event_configuration
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_AUTONOMOUS_POWER_STATE_TRANSITION
+ */
+union spdk_nvme_feat_autonomous_power_state_transition {
+	uint32_t raw;
+	struct {
+		/** Autonomous Power State Transition Enable */
+		uint32_t apste : 1;
+
+		uint32_t reserved : 31;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_autonomous_power_state_transition) == 4,
+		   "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_HOST_MEM_BUFFER
+ */
+union spdk_nvme_feat_host_mem_buffer {
+	uint32_t raw;
+	struct {
+		/** Enable Host Memory */
+		uint32_t ehm : 1;
+
+		/** Memory Return */
+		uint32_t mr : 1;
+
+		uint32_t reserved : 30;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_host_mem_buffer) == 4, "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_KEEP_ALIVE_TIMER
+ */
+union spdk_nvme_feat_keep_alive_timer {
+	uint32_t raw;
+	struct {
+		/** Keep Alive Timeout */
+		uint32_t kato : 32;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_keep_alive_timer) == 4, "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_HOST_CONTROLLED_THERMAL_MANAGEMENT
+ */
+union spdk_nvme_feat_host_controlled_thermal_management {
+	uint32_t raw;
+	struct {
+		/** Thermal Management Temperature 2 */
+		uint32_t tmt2 : 16;
+
+		/** Thermal Management Temperature 1 */
+		uint32_t tmt1 : 16;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_host_controlled_thermal_management) == 4,
+		   "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_NON_OPERATIONAL_POWER_STATE_CONFIG
+ */
+union spdk_nvme_feat_non_operational_power_state_config {
+	uint32_t raw;
+	struct {
+		/** Non-Operational Power State Permissive Mode Enable */
+		uint32_t noppme : 1;
+
+		uint32_t reserved : 31;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_non_operational_power_state_config) == 4,
+		   "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_SOFTWARE_PROGRESS_MARKER
+ */
+union spdk_nvme_feat_software_progress_marker {
+	uint32_t raw;
+	struct {
+		/** Pre-boot Software Load Count */
+		uint32_t pbslc : 8;
+
+		uint32_t reserved : 24;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_software_progress_marker) == 4, "Incorrect size");
+
+/**
+ * Data used by Set Features/Get Features \ref SPDK_NVME_FEAT_HOST_IDENTIFIER
+ */
+union spdk_nvme_feat_host_identifier {
+	uint32_t raw;
+	struct {
+		/** Enable Extended Host Identifier */
+		uint32_t exhid : 1;
+
+		uint32_t reserved : 31;
+	} bits;
+};
+SPDK_STATIC_ASSERT(sizeof(union spdk_nvme_feat_host_identifier) == 4, "Incorrect size");
+
 /**
  * Firmware slot information page (\ref SPDK_NVME_LOG_FIRMWARE_SLOT)
  */
 struct spdk_nvme_firmware_page {
 	struct {
-		uint8_t	slot		: 3; /* slot for current FW */
-		uint8_t	reserved	: 5;
+		uint8_t	active_slot	: 3; /**< Slot for current FW */
+		uint8_t	reserved3	: 1;
+		uint8_t	next_reset_slot	: 3; /**< Slot that will be active at next controller reset */
+		uint8_t	reserved7	: 1;
 	} afi;
 
 	uint8_t			reserved[7];
-	uint64_t		revision[7]; /* revisions for 7 slots */
+	uint8_t			revision[7][8]; /** Revisions for 7 slots (ASCII strings) */
 	uint8_t			reserved2[448];
 };
 SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_firmware_page) == 512, "Incorrect size");
@@ -1493,6 +2378,37 @@ struct spdk_nvme_ns_list {
 };
 SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_ns_list) == 4096, "Incorrect size");
 
+/**
+ * Namespace identification descriptor type
+ *
+ * \sa spdk_nvme_ns_id_desc
+ */
+enum spdk_nvme_nidt {
+	/** IEEE Extended Unique Identifier */
+	SPDK_NVME_NIDT_EUI64		= 0x01,
+
+	/** Namespace GUID */
+	SPDK_NVME_NIDT_NGUID		= 0x02,
+
+	/** Namespace UUID */
+	SPDK_NVME_NIDT_UUID		= 0x03,
+};
+
+struct spdk_nvme_ns_id_desc {
+	/** Namespace identifier type */
+	uint8_t nidt;
+
+	/** Namespace identifier length (length of nid field) */
+	uint8_t nidl;
+
+	uint8_t reserved2;
+	uint8_t reserved3;
+
+	/** Namespace identifier */
+	uint8_t nid[];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_ns_id_desc) == 4, "Incorrect size");
+
 struct spdk_nvme_ctrlr_list {
 	uint16_t ctrlr_count;
 	uint16_t ctrlr_list[2047];
@@ -1581,8 +2497,17 @@ struct spdk_nvme_fw_commit {
 };
 SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_fw_commit) == 4, "Incorrect size");
 
-#define spdk_nvme_cpl_is_error(cpl)					\
-	((cpl)->status.sc != 0 || (cpl)->status.sct != 0)
+#define spdk_nvme_cpl_is_error(cpl)			\
+	((cpl)->status.sc != SPDK_NVME_SC_SUCCESS ||	\
+	 (cpl)->status.sct != SPDK_NVME_SCT_GENERIC)
+
+#define spdk_nvme_cpl_is_success(cpl)	(!spdk_nvme_cpl_is_error(cpl))
+
+#define spdk_nvme_cpl_is_pi_error(cpl)						\
+	((cpl)->status.sct == SPDK_NVME_SCT_MEDIA_ERROR &&			\
+	 ((cpl)->status.sc == SPDK_NVME_SC_GUARD_CHECK_ERROR ||			\
+	  (cpl)->status.sc == SPDK_NVME_SC_APPLICATION_TAG_CHECK_ERROR ||	\
+	  (cpl)->status.sc == SPDK_NVME_SC_REFERENCE_TAG_CHECK_ERROR))
 
 /** Enable protection information checking of the Logical Block Reference Tag field */
 #define SPDK_NVME_IO_FLAGS_PRCHK_REFTAG (1U << 26)
diff --git a/PDK/core/src/api/include/udd/spdk/nvmf.h b/PDK/core/src/api/include/udd/spdk/nvmf.h
index 45915c3..3cf7e47 100644
--- a/PDK/core/src/api/include/udd/spdk/nvmf.h
+++ b/PDK/core/src/api/include/udd/spdk/nvmf.h
@@ -1,8 +1,8 @@
 /*-
  *   BSD LICENSE
  *
- *   Copyright (c) Intel Corporation.
- *   All rights reserved.
+ *   Copyright (c) Intel Corporation. All rights reserved.
+ *   Copyright (c) 2018 Mellanox Technologies LTD. All rights reserved.
  *
  *   Redistribution and use in source and binary forms, with or without
  *   modification, are permitted provided that the following conditions
@@ -41,188 +41,821 @@
 #include "spdk/stdinc.h"
 
 #include "spdk/env.h"
+#include "spdk/nvme.h"
 #include "spdk/nvmf_spec.h"
 #include "spdk/queue.h"
+#include "spdk/uuid.h"
 
-#define MAX_VIRTUAL_NAMESPACE 16
-#define MAX_SN_LEN 20
-
-int spdk_nvmf_tgt_init(uint16_t max_queue_depth, uint16_t max_conn_per_sess,
-		       uint32_t in_capsule_data_size, uint32_t max_io_size);
-
-int spdk_nvmf_tgt_fini(void);
-
-int spdk_nvmf_check_pools(void);
+#ifdef __cplusplus
+extern "C" {
+#endif
 
+struct spdk_nvmf_tgt;
 struct spdk_nvmf_subsystem;
-struct spdk_nvmf_session;
-struct spdk_nvmf_conn;
+struct spdk_nvmf_ctrlr;
+struct spdk_nvmf_qpair;
 struct spdk_nvmf_request;
 struct spdk_bdev;
-struct spdk_nvme_ctrlr;
 struct spdk_nvmf_request;
-struct spdk_nvmf_conn;
+struct spdk_nvmf_host;
+struct spdk_nvmf_listener;
+struct spdk_nvmf_poll_group;
+struct spdk_json_write_ctx;
+struct spdk_nvmf_transport;
+
+struct spdk_nvmf_transport_opts {
+	uint16_t	max_queue_depth;
+	uint16_t	max_qpairs_per_ctrlr;
+	uint32_t	in_capsule_data_size;
+	uint32_t	max_io_size;
+	uint32_t	io_unit_size;
+	uint32_t	max_aq_depth;
+	uint32_t	num_shared_buffers;
+	uint32_t	buf_cache_size;
+	uint32_t	max_srq_depth;
+	bool		no_srq;
+};
 
-typedef void (*spdk_nvmf_subsystem_connect_fn)(void *cb_ctx, struct spdk_nvmf_request *req);
-typedef void (*spdk_nvmf_subsystem_disconnect_fn)(void *cb_ctx, struct spdk_nvmf_conn *conn);
+/**
+ * Construct an NVMe-oF target.
+ *
+ * \param max_subsystems the maximum number of subsystems allowed by the target.
+ *
+ * \return a pointer to a NVMe-oF target on success, or NULL on failure.
+ */
+struct spdk_nvmf_tgt *spdk_nvmf_tgt_create(uint32_t max_subsystems);
 
-enum spdk_nvmf_subsystem_mode {
-	NVMF_SUBSYSTEM_MODE_DIRECT	= 0,
-	NVMF_SUBSYSTEM_MODE_VIRTUAL	= 1,
-};
+typedef void (spdk_nvmf_tgt_destroy_done_fn)(void *ctx, int status);
 
-struct spdk_nvmf_listen_addr {
-	char					*traddr;
-	char					*trsvcid;
-	char					*trname;
-	TAILQ_ENTRY(spdk_nvmf_listen_addr)	link;
-};
+/**
+ * Destroy an NVMe-oF target.
+ *
+ * \param tgt The target to destroy. This releases all resources.
+ * \param cb_fn A callback that will be called once the target is destroyed
+ * \param cb_arg A context argument passed to cb_fn.
+ */
+void spdk_nvmf_tgt_destroy(struct spdk_nvmf_tgt *tgt,
+			   spdk_nvmf_tgt_destroy_done_fn cb_fn,
+			   void *cb_arg);
 
-struct spdk_nvmf_host {
-	char				*nqn;
-	TAILQ_ENTRY(spdk_nvmf_host)	link;
-};
+/**
+ * Write NVMe-oF target configuration into provided JSON context.
+ * \param w JSON write context
+ * \param tgt The NVMe-oF target
+ */
+void spdk_nvmf_tgt_write_config_json(struct spdk_json_write_ctx *w, struct spdk_nvmf_tgt *tgt);
 
-struct spdk_nvmf_ctrlr_ops {
-	/**
-	 * Initialize the controller.
-	 */
-	int (*attach)(struct spdk_nvmf_subsystem *subsystem);
+/**
+ * Function to be called once the target is listening.
+ *
+ * \param ctx Context argument passed to this function.
+ * \param status 0 if it completed successfully, or negative errno if it failed.
+ */
+typedef void (*spdk_nvmf_tgt_listen_done_fn)(void *ctx, int status);
 
-	/**
-	 * Get NVMe identify controller data.
-	 */
-	void (*ctrlr_get_data)(struct spdk_nvmf_session *session);
+/**
+ * Begin accepting new connections at the address provided.
+ *
+ * The connections will be matched with a subsystem, which may or may not allow
+ * the connection based on a subsystem-specific whitelist. See
+ * spdk_nvmf_subsystem_add_host() and spdk_nvmf_subsystem_add_listener()
+ *
+ * \param tgt The target associated with this listen address.
+ * \param trid The address to listen at.
+ * \param cb_fn A callback that will be called once the target is listening
+ * \param cb_arg A context argument passed to cb_fn.
+ *
+ * \return void. The callback status argument will be 0 on success
+ *	   or a negated errno on failure.
+ */
+void spdk_nvmf_tgt_listen(struct spdk_nvmf_tgt *tgt,
+			  struct spdk_nvme_transport_id *trid,
+			  spdk_nvmf_tgt_listen_done_fn cb_fn,
+			  void *cb_arg);
+
+/**
+ * Function to be called for each newly discovered qpair.
+ *
+ * \param qpair The newly discovered qpair.
+ */
+typedef void (*new_qpair_fn)(struct spdk_nvmf_qpair *qpair);
+
+/**
+ * Poll the target for incoming connections.
+ *
+ * The new_qpair_fn cb_fn will be called for each newly discovered
+ * qpair. The user is expected to add that qpair to a poll group
+ * to establish the connection.
+ *
+ * \param tgt The target associated with the listen address.
+ * \param cb_fn Called for each newly discovered qpair.
+ */
+void spdk_nvmf_tgt_accept(struct spdk_nvmf_tgt *tgt, new_qpair_fn cb_fn);
+
+/**
+ * Create a poll group.
+ *
+ * \param tgt The target to create a poll group.
+ *
+ * \return a poll group on success, or NULL on failure.
+ */
+struct spdk_nvmf_poll_group *spdk_nvmf_poll_group_create(struct spdk_nvmf_tgt *tgt);
+
+/**
+ * Destroy a poll group.
+ *
+ * \param group The poll group to destroy.
+ */
+void spdk_nvmf_poll_group_destroy(struct spdk_nvmf_poll_group *group);
+
+/**
+ * Add the given qpair to the poll group.
+ *
+ * \param group The group to add qpair to.
+ * \param qpair The qpair to add.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_nvmf_poll_group_add(struct spdk_nvmf_poll_group *group,
+			     struct spdk_nvmf_qpair *qpair);
+
+typedef void (*nvmf_qpair_disconnect_cb)(void *ctx);
+
+/**
+ * Disconnect an NVMe-oF qpair
+ *
+ * \param qpair The NVMe-oF qpair to disconnect.
+ * \param cb_fn The function to call upon completion of the disconnect.
+ * \param ctx The context to pass to the callback function.
+ *
+ * \return 0 upon success.
+ * \return -ENOMEM if the function specific context could not be allocated.
+ */
+int spdk_nvmf_qpair_disconnect(struct spdk_nvmf_qpair *qpair, nvmf_qpair_disconnect_cb cb_fn,
+			       void *ctx);
+
+/**
+ * Get the peer's transport ID for this queue pair.
+ *
+ * \param qpair The NVMe-oF qpair
+ * \param trid Output parameter that will contain the transport id.
+ *
+ * \return 0 for success.
+ * \return -EINVAL if the qpair is not connected.
+ */
+int spdk_nvmf_qpair_get_peer_trid(struct spdk_nvmf_qpair *qpair,
+				  struct spdk_nvme_transport_id *trid);
+
+/**
+ * Get the local transport ID for this queue pair.
+ *
+ * \param qpair The NVMe-oF qpair
+ * \param trid Output parameter that will contain the transport id.
+ *
+ * \return 0 for success.
+ * \return -EINVAL if the qpair is not connected.
+ */
+int spdk_nvmf_qpair_get_local_trid(struct spdk_nvmf_qpair *qpair,
+				   struct spdk_nvme_transport_id *trid);
+
+/**
+ * Get the associated listener transport ID for this queue pair.
+ *
+ * \param qpair The NVMe-oF qpair
+ * \param trid Output parameter that will contain the transport id.
+ *
+ * \return 0 for success.
+ * \return -EINVAL if the qpair is not connected.
+ */
+int spdk_nvmf_qpair_get_listen_trid(struct spdk_nvmf_qpair *qpair,
+				    struct spdk_nvme_transport_id *trid);
+
+/**
+ * Create an NVMe-oF subsystem.
+ *
+ * Subsystems are in one of three states: Inactive, Active, Paused. This
+ * state affects which operations may be performed on the subsystem. Upon
+ * creation, the subsystem will be in the Inactive state and may be activated
+ * by calling spdk_nvmf_subsystem_start(). No I/O will be processed in the Inactive
+ * or Paused states, but changes to the state of the subsystem may be made.
+ *
+ * \param tgt The NVMe-oF target that will own this subsystem.
+ * \param nqn The NVMe qualified name of this subsystem.
+ * \param type Whether this subsystem is an I/O subsystem or a Discovery subsystem.
+ * \param num_ns The number of namespaces this subsystem contains.
+ *
+ * \return a pointer to a NVMe-oF subsystem on success, or NULL on failure.
+ */
+struct spdk_nvmf_subsystem *spdk_nvmf_subsystem_create(struct spdk_nvmf_tgt *tgt,
+		const char *nqn,
+		enum spdk_nvmf_subtype type,
+		uint32_t num_ns);
+
+/**
+ * Destroy an NVMe-oF subsystem. A subsystem may only be destroyed when in
+ * the Inactive state. See spdk_nvmf_subsystem_stop().
+ *
+ * \param subsystem The NVMe-oF subsystem to destroy.
+ */
+void spdk_nvmf_subsystem_destroy(struct spdk_nvmf_subsystem *subsystem);
 
+/**
+ * Function to be called once the subsystem has changed state.
+ *
+ * \param subsytem NVMe-oF subsystem that has changed state.
+ * \param cb_arg Argument passed to callback function.
+ * \param status 0 if it completed successfully, or negative errno if it failed.
+ */
+typedef void (*spdk_nvmf_subsystem_state_change_done)(struct spdk_nvmf_subsystem *subsystem,
+		void *cb_arg, int status);
+
+/**
+ * Transition an NVMe-oF subsystem from Inactive to Active state.
+ *
+ * \param subsystem The NVMe-oF subsystem.
+ * \param cb_fn A function that will be called once the subsystem has changed state.
+ * \param cb_arg Argument passed to cb_fn.
+ *
+ * \return 0 on success, or negated errno on failure. The callback provided will only
+ * be called on success.
+ */
+int spdk_nvmf_subsystem_start(struct spdk_nvmf_subsystem *subsystem,
+			      spdk_nvmf_subsystem_state_change_done cb_fn,
+			      void *cb_arg);
+
+/**
+ * Transition an NVMe-oF subsystem from Active to Inactive state.
+ *
+ * \param subsystem The NVMe-oF subsystem.
+ * \param cb_fn A function that will be called once the subsystem has changed state.
+ * \param cb_arg Argument passed to cb_fn.
+ *
+ * \return 0 on success, or negated errno on failure. The callback provided will only
+ * be called on success.
+ */
+int spdk_nvmf_subsystem_stop(struct spdk_nvmf_subsystem *subsystem,
+			     spdk_nvmf_subsystem_state_change_done cb_fn,
+			     void *cb_arg);
+
+/**
+ * Transition an NVMe-oF subsystem from Active to Paused state.
+ *
+ * \param subsystem The NVMe-oF subsystem.
+ * \param cb_fn A function that will be called once the subsystem has changed state.
+ * \param cb_arg Argument passed to cb_fn.
+ *
+ * \return 0 on success, or negated errno on failure. The callback provided will only
+ * be called on success.
+ */
+int spdk_nvmf_subsystem_pause(struct spdk_nvmf_subsystem *subsystem,
+			      spdk_nvmf_subsystem_state_change_done cb_fn,
+			      void *cb_arg);
+
+/**
+ * Transition an NVMe-oF subsystem from Paused to Active state.
+ *
+ * \param subsystem The NVMe-oF subsystem.
+ * \param cb_fn A function that will be called once the subsystem has changed state.
+ * \param cb_arg Argument passed to cb_fn.
+ *
+ * \return 0 on success, or negated errno on failure. The callback provided will only
+ * be called on success.
+ */
+int spdk_nvmf_subsystem_resume(struct spdk_nvmf_subsystem *subsystem,
+			       spdk_nvmf_subsystem_state_change_done cb_fn,
+			       void *cb_arg);
+
+/**
+ * Search the target for a subsystem with the given NQN.
+ *
+ * \param tgt The NVMe-oF target to search from.
+ * \param subnqn NQN of the subsystem.
+ *
+ * \return a pointer to the NVMe-oF subsystem on success, or NULL on failure.
+ */
+struct spdk_nvmf_subsystem *spdk_nvmf_tgt_find_subsystem(struct spdk_nvmf_tgt *tgt,
+		const char *subnqn);
+
+/**
+ * Begin iterating over all known subsystems. If no subsystems are present, return NULL.
+ *
+ * \param tgt The NVMe-oF target to iterate.
+ *
+ * \return a pointer to the first NVMe-oF subsystem on success, or NULL on failure.
+ */
+struct spdk_nvmf_subsystem *spdk_nvmf_subsystem_get_first(struct spdk_nvmf_tgt *tgt);
+
+/**
+ * Continue iterating over all known subsystems. If no additional subsystems, return NULL.
+ *
+ * \param subsystem Previous subsystem returned from \ref spdk_nvmf_subsystem_get_first or
+ *                  \ref spdk_nvmf_subsystem_get_next.
+ *
+ * \return a pointer to the next NVMe-oF subsystem on success, or NULL on failure.
+ */
+struct spdk_nvmf_subsystem *spdk_nvmf_subsystem_get_next(struct spdk_nvmf_subsystem *subsystem);
+
+/**
+ * Allow the given host NQN to connect to the given subsystem.
+ *
+ * May only be performed on subsystems in the PAUSED or INACTIVE states.
+ *
+ * \param subsystem Subsystem to add host to.
+ * \param hostnqn The NQN for the host.
+ *
+ * \return 0 on success, or negated errno value on failure.
+ */
+int spdk_nvmf_subsystem_add_host(struct spdk_nvmf_subsystem *subsystem,
+				 const char *hostnqn);
+
+/**
+ * Remove the given host NQN from the allowed hosts whitelist.
+ *
+ * May only be performed on subsystems in the PAUSED or INACTIVE states.
+ *
+ * \param subsystem Subsystem to remove host from.
+ * \param hostnqn The NQN for the host.
+ *
+ * \return 0 on success, or negated errno value on failure.
+ */
+int spdk_nvmf_subsystem_remove_host(struct spdk_nvmf_subsystem *subsystem, const char *hostnqn);
+
+/**
+ * Set whether a subsystem should allow any host or only hosts in the allowed list.
+ *
+ * May only be performed on subsystems in the PAUSED or INACTIVE states.
+ *
+ * \param subsystem Subsystem to modify.
+ * \param allow_any_host true to allow any host to connect to this subsystem,
+ * or false to enforce the whitelist configured with spdk_nvmf_subsystem_add_host().
+ *
+ * \return 0 on success, or negated errno value on failure.
+ */
+int spdk_nvmf_subsystem_set_allow_any_host(struct spdk_nvmf_subsystem *subsystem,
+		bool allow_any_host);
+
+/**
+ * Check whether a subsystem should allow any host or only hosts in the allowed list.
+ *
+ * \param subsystem Subsystem to modify.
+ *
+ * \return true if any host is allowed to connect to this subsystem, or false if
+ * connecting hosts must be in the whitelist configured with spdk_nvmf_subsystem_add_host().
+ */
+bool spdk_nvmf_subsystem_get_allow_any_host(const struct spdk_nvmf_subsystem *subsystem);
+
+/**
+ * Check if the given host is allowed to connect to the subsystem.
+ *
+ * \param subsystem The subsystem to query.
+ * \param hostnqn The NQN of the host.
+ *
+ * \return true if allowed, false if not.
+ */
+bool spdk_nvmf_subsystem_host_allowed(struct spdk_nvmf_subsystem *subsystem, const char *hostnqn);
+
+/**
+ * Get the first allowed host in a subsystem.
+ *
+ * \param subsystem Subsystem to query.
+ *
+ * \return first allowed host in this subsystem, or NULL if none allowed.
+ */
+struct spdk_nvmf_host *spdk_nvmf_subsystem_get_first_host(struct spdk_nvmf_subsystem *subsystem);
+
+/**
+ * Get the next allowed host in a subsystem.
+ *
+ * \param subsystem Subsystem to query.
+ * \param prev_host Previous host returned from this function.
+ *
+ * \return next allowed host in this subsystem, or NULL if prev_host was the last host.
+ */
+struct spdk_nvmf_host *spdk_nvmf_subsystem_get_next_host(struct spdk_nvmf_subsystem *subsystem,
+		struct spdk_nvmf_host *prev_host);
+
+/**
+ * Get a host's NQN.
+ *
+ * \param host Host to query.
+ *
+ * \return NQN of host.
+ */
+const char *spdk_nvmf_host_get_nqn(struct spdk_nvmf_host *host);
+
+/**
+ * Accept new connections on the address provided.
+ *
+ * May only be performed on subsystems in the PAUSED or INACTIVE states.
+ *
+ * \param subsystem Subsystem to add listener to.
+ * \param trid The address to accept connections from.
+ *
+ * \return 0 on success, or negated errno value on failure.
+ */
+int spdk_nvmf_subsystem_add_listener(struct spdk_nvmf_subsystem *subsystem,
+				     struct spdk_nvme_transport_id *trid);
+
+/**
+ * Stop accepting new connections on the address provided
+ *
+ * May only be performed on subsystems in the PAUSED or INACTIVE states.
+ *
+ * \param subsystem Subsystem to remove listener from.
+ * \param trid The address to no longer accept connections from.
+ *
+ * \return 0 on success, or negated errno value on failure.
+ */
+int spdk_nvmf_subsystem_remove_listener(struct spdk_nvmf_subsystem *subsystem,
+					const struct spdk_nvme_transport_id *trid);
+
+/**
+ * Check if connections originated from the given address are allowed to connect
+ * to the subsystem.
+ *
+ * \param subsystem The subsystem to query.
+ * \param trid The listen address.
+ *
+ * \return true if allowed, or false if not.
+ */
+bool spdk_nvmf_subsystem_listener_allowed(struct spdk_nvmf_subsystem *subsystem,
+		struct spdk_nvme_transport_id *trid);
+
+/**
+ * Get the first allowed listen address in the subsystem.
+ *
+ * \param subsystem Subsystem to query.
+ *
+ * \return first allowed listen address in this subsystem, or NULL if none allowed.
+ */
+struct spdk_nvmf_listener *spdk_nvmf_subsystem_get_first_listener(
+	struct spdk_nvmf_subsystem *subsystem);
+
+/**
+ * Get the next allowed listen address in a subsystem.
+ *
+ * \param subsystem Subsystem to query.
+ * \param prev_listener Previous listen address for this subsystem.
+ *
+ * \return next allowed listen address in this subsystem, or NULL if prev_listener
+ * was the last address.
+ */
+struct spdk_nvmf_listener *spdk_nvmf_subsystem_get_next_listener(
+	struct spdk_nvmf_subsystem *subsystem,
+	struct spdk_nvmf_listener *prev_listener);
+
+/**
+ * Get a listen address' transport ID
+ *
+ * \param listener This listener.
+ *
+ * \return the transport ID for this listener.
+ */
+const struct spdk_nvme_transport_id *spdk_nvmf_listener_get_trid(
+	struct spdk_nvmf_listener *listener);
+
+/** NVMe-oF target namespace creation options */
+struct spdk_nvmf_ns_opts {
 	/**
-	 * Process admin command.
+	 * Namespace ID
+	 *
+	 * Set to 0 to automatically assign a free NSID.
 	 */
-	int (*process_admin_cmd)(struct spdk_nvmf_request *req);
+	uint32_t nsid;
 
 	/**
-	 * Process IO command.
+	 * Namespace Globally Unique Identifier
+	 *
+	 * Fill with 0s if not specified.
 	 */
-	int (*process_io_cmd)(struct spdk_nvmf_request *req);
+	uint8_t nguid[16];
 
 	/**
-	 * Poll for completions.
+	 * IEEE Extended Unique Identifier
+	 *
+	 * Fill with 0s if not specified.
 	 */
-	void (*poll_for_completions)(struct spdk_nvmf_subsystem *subsystem);
+	uint8_t eui64[8];
 
 	/**
-	 * Detach the controller.
+	 * Namespace UUID
+	 *
+	 * Fill with 0s if not specified.
 	 */
-	void (*detach)(struct spdk_nvmf_subsystem *subsystem);
+	struct spdk_uuid uuid;
 };
 
-struct spdk_nvmf_subsystem_allowed_listener {
-	struct spdk_nvmf_listen_addr				*listen_addr;
-	TAILQ_ENTRY(spdk_nvmf_subsystem_allowed_listener)	link;
-};
+/**
+ * Get default namespace creation options.
+ *
+ * \param opts Namespace options to fill with defaults.
+ * \param opts_size sizeof(struct spdk_nvmf_ns_opts)
+ */
+void spdk_nvmf_ns_opts_get_defaults(struct spdk_nvmf_ns_opts *opts, size_t opts_size);
 
-/*
- * The NVMf subsystem, as indicated in the specification, is a collection
- * of virtual controller sessions.  Any individual controller session has
- * access to all the NVMe device/namespaces maintained by the subsystem.
- */
-struct spdk_nvmf_subsystem {
-	uint32_t id;
-	uint32_t lcore;
-	char subnqn[SPDK_NVMF_NQN_MAX_LEN];
-	enum spdk_nvmf_subsystem_mode mode;
-	enum spdk_nvmf_subtype subtype;
-	bool is_removed;
-	union {
-		struct {
-			struct spdk_nvme_ctrlr	*ctrlr;
-			struct spdk_nvme_qpair	*io_qpair;
-			struct spdk_pci_addr	pci_addr;
-			struct spdk_poller	*admin_poller;
-			int32_t			outstanding_admin_cmd_count;
-		} direct;
-
-		struct {
-			char	sn[MAX_SN_LEN + 1];
-			struct spdk_bdev *ns_list[MAX_VIRTUAL_NAMESPACE];
-			struct spdk_io_channel *ch[MAX_VIRTUAL_NAMESPACE];
-			uint16_t ns_count;
-		} virt;
-	} dev;
-
-	const struct spdk_nvmf_ctrlr_ops *ops;
-
-	void					*cb_ctx;
-	spdk_nvmf_subsystem_connect_fn		connect_cb;
-	spdk_nvmf_subsystem_disconnect_fn	disconnect_cb;
-
-	TAILQ_HEAD(, spdk_nvmf_session)		sessions;
-
-	TAILQ_HEAD(, spdk_nvmf_host)		hosts;
-	uint32_t				num_hosts;
-
-	TAILQ_HEAD(, spdk_nvmf_subsystem_allowed_listener)	allowed_listeners;
-
-	TAILQ_ENTRY(spdk_nvmf_subsystem)	entries;
-};
+/**
+ * Add a namespace to a subsytem.
+ *
+ * May only be performed on subsystems in the PAUSED or INACTIVE states.
+ *
+ * \param subsystem Subsystem to add namespace to.
+ * \param bdev Block device to add as a namespace.
+ * \param opts Namespace options, or NULL to use defaults.
+ * \param opts_size sizeof(*opts)
+ *
+ * \return newly added NSID on success, or 0 on failure.
+ */
+uint32_t spdk_nvmf_subsystem_add_ns(struct spdk_nvmf_subsystem *subsystem, struct spdk_bdev *bdev,
+				    const struct spdk_nvmf_ns_opts *opts, size_t opts_size);
 
-struct spdk_nvmf_subsystem *spdk_nvmf_create_subsystem(const char *nqn,
-		enum spdk_nvmf_subtype type,
-		enum spdk_nvmf_subsystem_mode mode,
-		void *cb_ctx,
-		spdk_nvmf_subsystem_connect_fn connect_cb,
-		spdk_nvmf_subsystem_disconnect_fn disconnect_cb);
+/**
+ * Remove a namespace from a subsytem.
+ *
+ * May only be performed on subsystems in the PAUSED or INACTIVE states.
+ *
+ * \param subsystem Subsystem the namespace belong to.
+ * \param nsid Namespace ID to be removed.
+ * \param cb_fn Function to call when all thread local ns information has been updated
+ * \param cb_arg Context for the above cb_fn
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_nvmf_subsystem_remove_ns(struct spdk_nvmf_subsystem *subsystem, uint32_t nsid,
+				  spdk_nvmf_subsystem_state_change_done cb_fn, void *cb_arg);
 
 /**
- * Initialize the subsystem on the thread that will be used to poll it.
+ * Get the first allocated namespace in a subsystem.
+ *
+ * \param subsystem Subsystem to query.
  *
- * \param subsystem Subsystem that will be polled on this core.
+ * \return first allocated namespace in this subsystem, or NULL if this subsystem
+ * has no namespaces.
  */
-int spdk_nvmf_subsystem_start(struct spdk_nvmf_subsystem *subsystem);
+struct spdk_nvmf_ns *spdk_nvmf_subsystem_get_first_ns(struct spdk_nvmf_subsystem *subsystem);
 
-void spdk_nvmf_delete_subsystem(struct spdk_nvmf_subsystem *subsystem);
+/**
+ * Get the next allocated namespace in a subsystem.
+ *
+ * \param subsystem Subsystem to query.
+ * \param prev_ns Previous ns returned from this function.
+ *
+ * \return next allocated namespace in this subsystem, or NULL if prev_ns was the
+ * last namespace.
+ */
+struct spdk_nvmf_ns *spdk_nvmf_subsystem_get_next_ns(struct spdk_nvmf_subsystem *subsystem,
+		struct spdk_nvmf_ns *prev_ns);
 
-struct spdk_nvmf_subsystem *
-nvmf_find_subsystem(const char *subnqn);
+/**
+ * Get a namespace in a subsystem by NSID.
+ *
+ * \param subsystem Subsystem to search.
+ * \param nsid Namespace ID to find.
+ *
+ * \return namespace matching nsid, or NULL if nsid was not found.
+ */
+struct spdk_nvmf_ns *spdk_nvmf_subsystem_get_ns(struct spdk_nvmf_subsystem *subsystem,
+		uint32_t nsid);
 
-bool spdk_nvmf_subsystem_exists(const char *subnqn);
+/**
+ * Get the maximum number of namespaces allowed in a subsystem.
+ *
+ * \param subsystem Subsystem to query.
+ *
+ * \return Maximum number of namespaces allowed in the subsystem, or 0 for unlimited.
+ */
+uint32_t spdk_nvmf_subsystem_get_max_namespaces(const struct spdk_nvmf_subsystem *subsystem);
 
-bool spdk_nvmf_subsystem_host_allowed(struct spdk_nvmf_subsystem *subsystem, const char *hostnqn);
+/**
+ * Get a namespace's NSID.
+ *
+ * \param ns Namespace to query.
+ *
+ * \return NSID of ns.
+ */
+uint32_t spdk_nvmf_ns_get_id(const struct spdk_nvmf_ns *ns);
 
-struct spdk_nvmf_listen_addr *
-spdk_nvmf_tgt_listen(const char *trname, const char *traddr, const char *trsvcid);
+/**
+ * Get a namespace's associated bdev.
+ *
+ * \param ns Namespace to query.
+ *
+ * \return backing bdev of ns.
+ */
+struct spdk_bdev *spdk_nvmf_ns_get_bdev(struct spdk_nvmf_ns *ns);
 
-int
-spdk_nvmf_subsystem_add_listener(struct spdk_nvmf_subsystem *subsystem,
-				 struct spdk_nvmf_listen_addr *listen_addr);
+/**
+ * Get the options specified for a namespace.
+ *
+ * \param ns Namespace to query.
+ * \param opts Output parameter for options.
+ * \param opts_size sizeof(*opts)
+ */
+void spdk_nvmf_ns_get_opts(const struct spdk_nvmf_ns *ns, struct spdk_nvmf_ns_opts *opts,
+			   size_t opts_size);
 
-bool
-spdk_nvmf_subsystem_listener_allowed(struct spdk_nvmf_subsystem *subsystem,
-				     struct spdk_nvmf_listen_addr *listen_addr);
+/**
+ * Get the serial number of the specified subsystem.
+ *
+ * \param subsystem Subsystem to query.
+ *
+ * \return serial number of the specified subsystem.
+ */
+const char *spdk_nvmf_subsystem_get_sn(const struct spdk_nvmf_subsystem *subsystem);
 
-int
-spdk_nvmf_subsystem_add_host(struct spdk_nvmf_subsystem *subsystem,
-			     const char *host_nqn);
 
-int
-nvmf_subsystem_add_ctrlr(struct spdk_nvmf_subsystem *subsystem,
-			 struct spdk_nvme_ctrlr *ctrlr, const struct spdk_pci_addr *pci_addr);
+/**
+ * Set the serial number for the specified subsystem.
+ *
+ * \param subsystem Subsystem to set for.
+ * \param sn serial number to set.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_nvmf_subsystem_set_sn(struct spdk_nvmf_subsystem *subsystem, const char *sn);
 
-void spdk_nvmf_subsystem_poll(struct spdk_nvmf_subsystem *subsystem);
+/**
+ * Get the model number of the specified subsystem.
+ *
+ * \param subsystem Subsystem to query.
+ *
+ * \return model number of the specified subsystem.
+ */
+const char *spdk_nvmf_subsystem_get_mn(const struct spdk_nvmf_subsystem *subsystem);
 
-int
-spdk_nvmf_subsystem_add_ns(struct spdk_nvmf_subsystem *subsystem, struct spdk_bdev *bdev);
 
-int spdk_nvmf_subsystem_set_sn(struct spdk_nvmf_subsystem *subsystem, const char *sn);
+/**
+ * Set the model number for the specified subsystem.
+ *
+ * \param subsystem Subsystem to set for.
+ * \param mn model number to set.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_nvmf_subsystem_set_mn(struct spdk_nvmf_subsystem *subsystem, const char *mn);
 
+/**
+ * Get the NQN of the specified subsystem.
+ *
+ * \param subsystem Subsystem to query.
+ *
+ * \return NQN of the specified subsystem.
+ */
 const char *spdk_nvmf_subsystem_get_nqn(struct spdk_nvmf_subsystem *subsystem);
+
+/**
+ * Get the type of the specified subsystem.
+ *
+ * \param subsystem Subsystem to query.
+ *
+ * \return the type of the specified subsystem.
+ */
 enum spdk_nvmf_subtype spdk_nvmf_subsystem_get_type(struct spdk_nvmf_subsystem *subsystem);
-enum spdk_nvmf_subsystem_mode spdk_nvmf_subsystem_get_mode(struct spdk_nvmf_subsystem *subsystem);
 
-void spdk_nvmf_acceptor_poll(void);
+/**
+ * Initialize transport options
+ *
+ * \param type The transport type to create
+ * \param opts The transport options (e.g. max_io_size)
+ *
+ * \return bool. true if successful, false if transport type
+ *	   not found.
+ */
+bool
+spdk_nvmf_transport_opts_init(enum spdk_nvme_transport_type type,
+			      struct spdk_nvmf_transport_opts *opts);
+
+/**
+ * Create a protocol transport
+ *
+ * \param type The transport type to create
+ * \param opts The transport options (e.g. max_io_size)
+ *
+ * \return new transport or NULL if create fails
+ */
+struct spdk_nvmf_transport *spdk_nvmf_transport_create(enum spdk_nvme_transport_type type,
+		struct spdk_nvmf_transport_opts *opts);
 
-void spdk_nvmf_handle_connect(struct spdk_nvmf_request *req);
+/**
+ * Destroy a protocol transport
+ *
+ * \param transport The transport to destory
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_nvmf_transport_destroy(struct spdk_nvmf_transport *transport);
 
+/**
+ * Get an existing transport from the target
+ *
+ * \param tgt The NVMe-oF target
+ * \param type The transport type to get
+ *
+ * \return the transport or NULL if not found
+ */
+struct spdk_nvmf_transport *spdk_nvmf_tgt_get_transport(struct spdk_nvmf_tgt *tgt,
+		enum spdk_nvme_transport_type type);
+
+/**
+ * Get the first transport registered with the given target
+ *
+ * \param tgt The NVMe-oF target
+ *
+ * \return The first transport registered on the target
+ */
+struct spdk_nvmf_transport *spdk_nvmf_transport_get_first(struct spdk_nvmf_tgt *tgt);
+
+/**
+ * Get the next transport in a target's list.
+ *
+ * \param transport A handle to a transport object
+ *
+ * \return The next transport associated with the NVMe-oF target
+ */
+struct spdk_nvmf_transport *spdk_nvmf_transport_get_next(struct spdk_nvmf_transport *transport);
+
+/**
+ * Get the opts for a given transport.
+ *
+ * \param transport The transport to query
+ *
+ * \return The opts associated with the given transport
+ */
+const struct spdk_nvmf_transport_opts *spdk_nvmf_get_transport_opts(struct spdk_nvmf_transport
+		*transport);
+
+/**
+ * Get the transport type for a given transport.
+ *
+ * \param transport The transport to query
+ *
+ * \return the transport type for the given transport
+ */
+spdk_nvme_transport_type_t spdk_nvmf_get_transport_type(struct spdk_nvmf_transport *transport);
+
+/**
+ * Function to be called once transport add is complete
+ *
+ * \param cb_arg Callback argument passed to this function.
+ * \param status 0 if it completed successfully, or negative errno if it failed.
+ */
+typedef void (*spdk_nvmf_tgt_add_transport_done_fn)(void *cb_arg, int status);
+
+/**
+ * Add a transport to a target
+ *
+ * \param tgt The NVMe-oF target
+ * \param transport The transport to add
+ * \param cb_fn A callback that will be called once the transport is created
+ * \param cb_arg A context argument passed to cb_fn.
+ *
+ * \return void. The callback status argument will be 0 on success
+ *	   or a negated errno on failure.
+ */
+void spdk_nvmf_tgt_add_transport(struct spdk_nvmf_tgt *tgt,
+				 struct spdk_nvmf_transport *transport,
+				 spdk_nvmf_tgt_add_transport_done_fn cb_fn,
+				 void *cb_arg);
+
+/**
+ *
+ * Add listener to transport and begin accepting new connections.
+ *
+ * \param transport The transport to add listener to
+ * \param trid Address to listen at
+ *
+ * \return int. 0 if it completed successfully, or negative errno if it failed.
+ */
+
+int spdk_nvmf_transport_listen(struct spdk_nvmf_transport *transport,
+			       const struct spdk_nvme_transport_id *trid);
+
+/**
+ * Write NVMe-oF target's transport configurations into provided JSON context.
+ * \param w JSON write context
+ * \param tgt The NVMe-oF target
+ */
 void
-spdk_nvmf_session_disconnect(struct spdk_nvmf_conn *conn);
+spdk_nvmf_tgt_transport_write_config_json(struct spdk_json_write_ctx *w, struct spdk_nvmf_tgt *tgt);
+
+#ifdef SPDK_CONFIG_RDMA
+/**
+ * \brief Set the global hooks for the RDMA transport, if necessary.
+ *
+ * This call is optional and must be performed prior to probing for
+ * any devices. By default, the RDMA transport will use the ibverbs
+ * library to create protection domains and register memory. This
+ * is a mechanism to subvert that and use an existing registration.
+ *
+ * This function may only be called one time per process.
+ *
+ * \param hooks for initializing global hooks
+ */
+void
+spdk_nvmf_rdma_init_hooks(struct spdk_nvme_rdma_hooks *hooks);
+#endif
+
+#ifdef __cplusplus
+}
+#endif
 
 #endif
diff --git a/PDK/core/src/api/include/udd/spdk/nvmf_fc_spec.h b/PDK/core/src/api/include/udd/spdk/nvmf_fc_spec.h
new file mode 100644
index 0000000..2a3435a
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/nvmf_fc_spec.h
@@ -0,0 +1,403 @@
+/*
+ *   BSD LICENSE
+ *
+ *   Copyright (c) 2018 Broadcom.  All Rights Reserved.
+ *   The term "Broadcom" refers to Broadcom Inc. and/or its subsidiaries.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __NVMF_FC_SPEC_H__
+#define __NVMF_FC_SPEC_H__
+
+#include "spdk/env.h"
+#include "spdk/nvme.h"
+
+/*
+ * FC-NVMe Spec. Definitions
+ */
+
+#define FCNVME_R_CTL_CMD_REQ                   0x06
+#define FCNVME_R_CTL_DATA_OUT                  0x01
+#define FCNVME_R_CTL_CONFIRM                   0x03
+#define FCNVME_R_CTL_STATUS                    0x07
+#define FCNVME_R_CTL_ERSP_STATUS               0x08
+#define FCNVME_R_CTL_LS_REQUEST                0x32
+#define FCNVME_R_CTL_LS_RESPONSE               0x33
+#define FCNVME_R_CTL_BA_ABTS                   0x81
+
+#define FCNVME_F_CTL_END_SEQ                   0x080000
+#define FCNVME_F_CTL_SEQ_INIT                  0x010000
+
+/* END_SEQ | LAST_SEQ | Exchange Responder | SEQ init */
+#define FCNVME_F_CTL_RSP                       0x990000
+
+#define FCNVME_TYPE_BLS                        0x0
+#define FCNVME_TYPE_FC_EXCHANGE                0x08
+#define FCNVME_TYPE_NVMF_DATA                  0x28
+
+#define FCNVME_CMND_IU_FC_ID                   0x28
+#define FCNVME_CMND_IU_SCSI_ID                 0xFD
+
+#define FCNVME_CMND_IU_NODATA                  0x00
+#define FCNVME_CMND_IU_READ                    0x10
+#define FCNVME_CMND_IU_WRITE                   0x01
+
+/* BLS reject error codes */
+#define FCNVME_BLS_REJECT_UNABLE_TO_PERFORM    0x09
+#define FCNVME_BLS_REJECT_EXP_NOINFO           0x00
+#define FCNVME_BLS_REJECT_EXP_INVALID_OXID     0x03
+
+/*
+ * FC NVMe Link Services (LS) constants
+ */
+#define FCNVME_MAX_LS_REQ_SIZE                  1536
+#define FCNVME_MAX_LS_RSP_SIZE                  64
+
+#define FCNVME_LS_CA_CMD_MIN_LEN                592
+#define FCNVME_LS_CA_DESC_LIST_MIN_LEN          584
+#define FCNVME_LS_CA_DESC_MIN_LEN               576
+
+/* this value needs to be in sync with low level driver buffer size */
+#define FCNVME_MAX_LS_BUFFER_SIZE               2048
+
+#define FCNVME_GOOD_RSP_LEN                     12
+#define FCNVME_ASSOC_HOSTID_LEN                 16
+#define FCNVME_ASSOC_HOSTNQN_LEN                256
+#define FCNVME_ASSOC_SUBNQN_LEN                 256
+
+
+typedef uint64_t FCNVME_BE64;
+typedef uint32_t FCNVME_BE32;
+typedef uint16_t FCNVME_BE16;
+
+/*
+ * FC-NVME LS Commands
+ */
+enum {
+	FCNVME_LS_RSVD                = 0,
+	FCNVME_LS_RJT                 = 1,
+	FCNVME_LS_ACC                 = 2,
+	FCNVME_LS_CREATE_ASSOCIATION  = 3,
+	FCNVME_LS_CREATE_CONNECTION	  = 4,
+	FCNVME_LS_DISCONNECT          = 5,
+};
+
+/*
+ * FC-NVME Link Service Descriptors
+ */
+enum {
+	FCNVME_LSDESC_RSVD             = 0x0,
+	FCNVME_LSDESC_RQST             = 0x1,
+	FCNVME_LSDESC_RJT              = 0x2,
+	FCNVME_LSDESC_CREATE_ASSOC_CMD = 0x3,
+	FCNVME_LSDESC_CREATE_CONN_CMD  = 0x4,
+	FCNVME_LSDESC_DISCONN_CMD      = 0x5,
+	FCNVME_LSDESC_CONN_ID          = 0x6,
+	FCNVME_LSDESC_ASSOC_ID         = 0x7,
+};
+
+/*
+ * LS Reject reason_codes
+ */
+enum fcnvme_ls_rjt_reason {
+	FCNVME_RJT_RC_NONE         = 0,     /* no reason - not to be sent */
+	FCNVME_RJT_RC_INVAL        = 0x01,  /* invalid NVMe_LS command code */
+	FCNVME_RJT_RC_LOGIC        = 0x03,  /* logical error */
+	FCNVME_RJT_RC_UNAB         = 0x09,  /* unable to perform request */
+	FCNVME_RJT_RC_UNSUP        = 0x0b,  /* command not supported */
+	FCNVME_RJT_RC_INPROG       = 0x0e,  /* command already in progress */
+	FCNVME_RJT_RC_INV_ASSOC    = 0x40,  /* invalid Association ID */
+	FCNVME_RJT_RC_INV_CONN     = 0x41,  /* invalid Connection ID */
+	FCNVME_RJT_RC_INV_PARAM    = 0x42,  /* invalid parameters */
+	FCNVME_RJT_RC_INSUFF_RES   = 0x43,  /* insufficient resources */
+	FCNVME_RJT_RC_INV_HOST     = 0x44,  /* invalid or rejected host */
+	FCNVME_RJT_RC_VENDOR       = 0xff,  /* vendor specific error */
+};
+
+/*
+ * LS Reject reason_explanation codes
+ */
+enum fcnvme_ls_rjt_explan {
+	FCNVME_RJT_EXP_NONE	       = 0x00,  /* No additional explanation */
+	FCNVME_RJT_EXP_OXID_RXID   = 0x17,  /* invalid OX_ID-RX_ID combo */
+	FCNVME_RJT_EXP_UNAB_DATA   = 0x2a,  /* unable to supply data */
+	FCNVME_RJT_EXP_INV_LEN     = 0x2d,  /* invalid payload length */
+	FCNVME_RJT_EXP_INV_ESRP    = 0x40,  /* invalid ESRP ratio */
+	FCNVME_RJT_EXP_INV_CTL_ID  = 0x41,  /* invalid controller ID */
+	FCNVME_RJT_EXP_INV_Q_ID    = 0x42,  /* invalid queue ID */
+	FCNVME_RJT_EXP_SQ_SIZE     = 0x43,  /* invalid submission queue size */
+	FCNVME_RJT_EXP_INV_HOST_ID = 0x44,  /* invalid or rejected host ID */
+	FCNVME_RJT_EXP_INV_HOSTNQN = 0x45,  /* invalid or rejected host NQN */
+	FCNVME_RJT_EXP_INV_SUBNQN  = 0x46,  /* invalid or rejected subsys nqn */
+};
+
+/*
+ * NVMe over FC CMD IU
+ */
+struct spdk_nvmf_fc_cmnd_iu {
+	uint32_t scsi_id: 8,
+		 fc_id: 8,
+		 cmnd_iu_len: 16;
+	uint32_t rsvd0: 24,
+		 flags: 8;
+	uint64_t conn_id;
+	uint32_t cmnd_seq_num;
+	uint32_t data_len;
+	struct spdk_nvme_cmd cmd;
+	uint32_t rsvd1[2];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_cmnd_iu) == 96, "size_mismatch");
+
+/*
+ * NVMe over Extended Response IU
+ */
+struct spdk_nvmf_fc_ersp_iu {
+	uint32_t status_code: 8,
+		 rsvd0: 8,
+		 ersp_len: 16;
+	uint32_t response_seq_no;
+	uint32_t transferred_data_len;
+	uint32_t rsvd1;
+	struct spdk_nvme_cpl rsp;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_ersp_iu) == 32, "size_mismatch");
+
+/*
+ * Transfer ready IU
+ */
+struct spdk_nvmf_fc_xfer_rdy_iu {
+	uint32_t relative_offset;
+	uint32_t burst_len;
+	uint32_t rsvd;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_xfer_rdy_iu) == 12, "size_mismatch");
+
+/*
+ * FC NVME Frame Header
+ */
+struct spdk_nvmf_fc_frame_hdr {
+	FCNVME_BE32 r_ctl: 8,
+		    d_id: 24;
+	FCNVME_BE32 cs_ctl: 8,
+		    s_id: 24;
+	FCNVME_BE32 type: 8,
+		    f_ctl: 24;
+	FCNVME_BE32 seq_id: 8,
+		    df_ctl: 8,
+		    seq_cnt: 16;
+	FCNVME_BE32 ox_id: 16,
+		    rx_id: 16;
+	FCNVME_BE32 parameter;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_frame_hdr) == 24, "size_mismatch");
+
+/*
+ * Request payload word 0
+ */
+struct spdk_nvmf_fc_ls_rqst_w0 {
+	uint8_t	ls_cmd;			/* FCNVME_LS_xxx */
+	uint8_t zeros[3];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_ls_rqst_w0) == 4, "size_mismatch");
+
+/*
+ * LS request information descriptor
+ */
+struct spdk_nvmf_fc_lsdesc_rqst {
+	FCNVME_BE32 desc_tag;		/* FCNVME_LSDESC_xxx */
+	FCNVME_BE32 desc_len;
+	struct spdk_nvmf_fc_ls_rqst_w0 w0;
+	FCNVME_BE32 rsvd12;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_lsdesc_rqst) == 16, "size_mismatch");
+
+/*
+ * LS accept header
+ */
+struct spdk_nvmf_fc_ls_acc_hdr {
+	struct spdk_nvmf_fc_ls_rqst_w0 w0;
+	FCNVME_BE32 desc_list_len;
+	struct spdk_nvmf_fc_lsdesc_rqst rqst;
+	/* Followed by cmd-specific ACC descriptors, see next definitions */
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_ls_acc_hdr) == 24, "size_mismatch");
+
+/*
+ * LS descriptor connection id
+ */
+struct spdk_nvmf_fc_lsdesc_conn_id {
+	FCNVME_BE32 desc_tag;
+	FCNVME_BE32 desc_len;
+	FCNVME_BE64 connection_id;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_lsdesc_conn_id) == 16, "size_mismatch");
+
+/*
+ * LS decriptor association id
+ */
+struct spdk_nvmf_fc_lsdesc_assoc_id {
+	FCNVME_BE32 desc_tag;
+	FCNVME_BE32 desc_len;
+	FCNVME_BE64 association_id;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_lsdesc_assoc_id) == 16, "size_mismatch");
+
+/*
+ * LS Create Association descriptor
+ */
+struct spdk_nvmf_fc_lsdesc_cr_assoc_cmd {
+	FCNVME_BE32  desc_tag;
+	FCNVME_BE32  desc_len;
+	FCNVME_BE16  ersp_ratio;
+	FCNVME_BE16  rsvd10;
+	FCNVME_BE32  rsvd12[9];
+	FCNVME_BE16  cntlid;
+	FCNVME_BE16  sqsize;
+	FCNVME_BE32  rsvd52;
+	uint8_t hostid[FCNVME_ASSOC_HOSTID_LEN];
+	uint8_t hostnqn[FCNVME_ASSOC_HOSTNQN_LEN];
+	uint8_t subnqn[FCNVME_ASSOC_SUBNQN_LEN];
+	uint8_t rsvd584[432];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_lsdesc_cr_assoc_cmd) == 1016, "size_mismatch");
+
+/*
+ * LS Create Association reqeust payload
+ */
+struct spdk_nvmf_fc_ls_cr_assoc_rqst {
+	struct spdk_nvmf_fc_ls_rqst_w0 w0;
+	FCNVME_BE32 desc_list_len;
+	struct spdk_nvmf_fc_lsdesc_cr_assoc_cmd assoc_cmd;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_ls_cr_assoc_rqst) == 1024, "size_mismatch");
+
+/*
+ * LS Create Association accept payload
+ */
+struct spdk_nvmf_fc_ls_cr_assoc_acc {
+	struct spdk_nvmf_fc_ls_acc_hdr hdr;
+	struct spdk_nvmf_fc_lsdesc_assoc_id assoc_id;
+	struct spdk_nvmf_fc_lsdesc_conn_id conn_id;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_ls_cr_assoc_acc) == 56, "size_mismatch");
+
+/*
+ * LS Create IO Connection descriptor
+ */
+struct spdk_nvmf_fc_lsdesc_cr_conn_cmd {
+	FCNVME_BE32 desc_tag;
+	FCNVME_BE32 desc_len;
+	FCNVME_BE16 ersp_ratio;
+	FCNVME_BE16 rsvd10;
+	FCNVME_BE32 rsvd12[9];
+	FCNVME_BE16 qid;
+	FCNVME_BE16 sqsize;
+	FCNVME_BE32 rsvd52;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_ls_cr_assoc_acc) == 56, "size_mismatch");
+
+/*
+ * LS Create IO Connection payload
+ */
+struct spdk_nvmf_fc_ls_cr_conn_rqst {
+	struct spdk_nvmf_fc_ls_rqst_w0 w0;
+	FCNVME_BE32 desc_list_len;
+	struct spdk_nvmf_fc_lsdesc_assoc_id assoc_id;
+	struct spdk_nvmf_fc_lsdesc_cr_conn_cmd connect_cmd;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_ls_cr_conn_rqst) == 80, "size_mismatch");
+
+/*
+ * LS Create IO Connection accept payload
+ */
+struct spdk_nvmf_fc_ls_cr_conn_acc {
+	struct spdk_nvmf_fc_ls_acc_hdr hdr;
+	struct spdk_nvmf_fc_lsdesc_conn_id conn_id;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_ls_cr_conn_acc) == 40, "size_mismatch");
+
+/*
+ * LS Disconnect descriptor
+ */
+struct spdk_nvmf_fc_lsdesc_disconn_cmd {
+	FCNVME_BE32 desc_tag;
+	FCNVME_BE32 desc_len;
+	FCNVME_BE32 rsvd8;
+	FCNVME_BE32 rsvd12;
+	FCNVME_BE32 rsvd16;
+	FCNVME_BE32 rsvd20;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_lsdesc_disconn_cmd) == 24, "size_mismatch");
+
+/*
+ * LS Disconnect payload
+ */
+struct spdk_nvmf_fc_ls_disconnect_rqst {
+	struct spdk_nvmf_fc_ls_rqst_w0 w0;
+	FCNVME_BE32 desc_list_len;
+	struct spdk_nvmf_fc_lsdesc_assoc_id assoc_id;
+	struct spdk_nvmf_fc_lsdesc_disconn_cmd disconn_cmd;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_ls_disconnect_rqst) == 48, "size_mismatch");
+
+/*
+ * LS Disconnect accept payload
+ */
+struct spdk_nvmf_fc_ls_disconnect_acc {
+	struct spdk_nvmf_fc_ls_acc_hdr hdr;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_ls_disconnect_acc) == 24, "size_mismatch");
+
+/*
+ * LS Reject descriptor
+ */
+struct spdk_nvmf_fc_lsdesc_rjt {
+	FCNVME_BE32 desc_tag;
+	FCNVME_BE32 desc_len;
+	uint8_t rsvd8;
+
+	uint8_t reason_code;
+	uint8_t reason_explanation;
+
+	uint8_t vendor;
+	FCNVME_BE32 rsvd12;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_lsdesc_rjt) == 16, "size_mismatch");
+
+/*
+ * LS Reject payload
+ */
+struct spdk_nvmf_fc_ls_rjt {
+	struct spdk_nvmf_fc_ls_rqst_w0 w0;
+	FCNVME_BE32 desc_list_len;
+	struct spdk_nvmf_fc_lsdesc_rqst rqst;
+	struct spdk_nvmf_fc_lsdesc_rjt rjt;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fc_ls_rjt) == 40, "size_mismatch");
+
+#endif
diff --git a/PDK/core/src/api/include/udd/spdk/nvmf_spec.h b/PDK/core/src/api/include/udd/spdk/nvmf_spec.h
index 53f4ae7..5e4eb44 100644
--- a/PDK/core/src/api/include/udd/spdk/nvmf_spec.h
+++ b/PDK/core/src/api/include/udd/spdk/nvmf_spec.h
@@ -130,6 +130,9 @@ enum spdk_nvmf_trtype {
 	/** Fibre Channel */
 	SPDK_NVMF_TRTYPE_FC		= 0x2,
 
+	/** TCP */
+	SPDK_NVMF_TRTYPE_TCP		= 0x3,
+
 	/** Intra-host transport (loopback) */
 	SPDK_NVMF_TRTYPE_INTRA_HOST	= 0xfe,
 };
@@ -272,8 +275,8 @@ struct spdk_nvmf_fabric_prop_get_cmd {
 	uint8_t		fctype;
 	uint8_t		reserved2[35];
 	struct {
-		uint8_t size		: 2;
-		uint8_t reserved	: 6;
+		uint8_t size		: 3;
+		uint8_t reserved	: 5;
 	} attrib;
 	uint8_t		reserved3[3];
 	uint32_t	ofst;
@@ -304,8 +307,8 @@ struct spdk_nvmf_fabric_prop_set_cmd {
 	uint8_t		fctype;
 	uint8_t		reserved1[35];
 	struct {
-		uint8_t size		: 2;
-		uint8_t reserved	: 6;
+		uint8_t size		: 3;
+		uint8_t reserved	: 5;
 	} attrib;
 	uint8_t		reserved2[3];
 	uint32_t	ofst;
@@ -322,9 +325,15 @@ struct spdk_nvmf_fabric_prop_set_cmd {
 };
 SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_fabric_prop_set_cmd) == 64, "Incorrect size");
 
+#define SPDK_NVMF_NQN_MIN_LEN 11 /* The prefix in the spec is 11 characters */
 #define SPDK_NVMF_NQN_MAX_LEN 223
+#define SPDK_NVMF_NQN_UUID_PRE_LEN 32
+#define SPDK_NVMF_UUID_STRING_LEN 36
+#define SPDK_NVMF_NQN_UUID_PRE "nqn.2014-08.org.nvmexpress:uuid:"
 #define SPDK_NVMF_DISCOVERY_NQN "nqn.2014-08.org.nvmexpress.discovery"
 
+#define SPDK_DOMAIN_LABEL_MAX_LEN 63 /* RFC 1034 max domain label length */
+
 #define SPDK_NVMF_TRADDR_MAX_LEN 256
 #define SPDK_NVMF_TRSVCID_MAX_LEN 32
 
@@ -349,12 +358,34 @@ struct spdk_nvmf_rdma_transport_specific_address_subtype {
 SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_rdma_transport_specific_address_subtype) == 256,
 		   "Incorrect size");
 
+/** TCP Secure Socket Type */
+enum spdk_nvme_tcp_secure_socket_type {
+	/** No security */
+	SPDK_NVME_TCP_SECURITY_NONE			= 0,
+
+	/** TLS (Secure Sockets) */
+	SPDK_NVME_TCP_SECURITY_TLS			= 1,
+};
+
+/** TCP transport-specific address subtype */
+struct spdk_nvme_tcp_transport_specific_address_subtype {
+	/** Security type (\ref spdk_nvme_tcp_secure_socket_type) */
+	uint8_t		sectype;
+
+	uint8_t		reserved0[255];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_tcp_transport_specific_address_subtype) == 256,
+		   "Incorrect size");
+
 /** Transport-specific address subtype */
 union spdk_nvmf_transport_specific_address_subtype {
 	uint8_t raw[256];
 
 	/** RDMA */
 	struct spdk_nvmf_rdma_transport_specific_address_subtype rdma;
+
+	/** TCP */
+	struct spdk_nvme_tcp_transport_specific_address_subtype tcp;
 };
 SPDK_STATIC_ASSERT(sizeof(union spdk_nvmf_transport_specific_address_subtype) == 256,
 		   "Incorrect size");
@@ -425,8 +456,8 @@ struct spdk_nvmf_rdma_request_private_data {
 	uint16_t	qid;	/* queue id */
 	uint16_t	hrqsize;	/* host receive queue size */
 	uint16_t	hsqsize;	/* host send queue size */
-	uint16_t 	cntlid;		/* controller id */
-	uint8_t 	reserved[22];
+	uint16_t	cntlid;		/* controller id */
+	uint8_t		reserved[22];
 };
 SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_rdma_request_private_data) == 32, "Incorrect size");
 
@@ -439,7 +470,7 @@ SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_rdma_accept_private_data) == 32, "Inc
 
 struct spdk_nvmf_rdma_reject_private_data {
 	uint16_t	recfmt; /* record format */
-	struct spdk_nvme_status status;
+	uint16_t	sts; /* status */
 };
 SPDK_STATIC_ASSERT(sizeof(struct spdk_nvmf_rdma_reject_private_data) == 4, "Incorrect size");
 
@@ -450,7 +481,7 @@ union spdk_nvmf_rdma_private_data {
 };
 SPDK_STATIC_ASSERT(sizeof(union spdk_nvmf_rdma_private_data) == 32, "Incorrect size");
 
-enum spdk_nvmf_rdma_transport_errors {
+enum spdk_nvmf_rdma_transport_error {
 	SPDK_NVMF_RDMA_ERROR_INVALID_PRIVATE_DATA_LENGTH	= 0x1,
 	SPDK_NVMF_RDMA_ERROR_INVALID_RECFMT			= 0x2,
 	SPDK_NVMF_RDMA_ERROR_INVALID_QID			= 0x3,
@@ -459,9 +490,244 @@ enum spdk_nvmf_rdma_transport_errors {
 	SPDK_NVMF_RDMA_ERROR_NO_RESOURCES			= 0x6,
 	SPDK_NVMF_RDMA_ERROR_INVALID_IRD			= 0x7,
 	SPDK_NVMF_RDMA_ERROR_INVALID_ORD			= 0x8,
-	SPDK_NVMF_RDMA_ERROR_INVALID_CNTLID			= 0x9,
 };
 
+/* TCP transport specific definitions below */
+
+/** NVMe/TCP PDU type */
+enum spdk_nvme_tcp_pdu_type {
+	/** Initialize Connection Request (ICReq) */
+	SPDK_NVME_TCP_PDU_TYPE_IC_REQ			= 0x00,
+
+	/** Initialize Connection Response (ICResp) */
+	SPDK_NVME_TCP_PDU_TYPE_IC_RESP			= 0x01,
+
+	/** Terminate Connection Request (TermReq) */
+	SPDK_NVME_TCP_PDU_TYPE_H2C_TERM_REQ		= 0x02,
+
+	/** Terminate Connection Response (TermResp) */
+	SPDK_NVME_TCP_PDU_TYPE_C2H_TERM_REQ		= 0x03,
+
+	/** Command Capsule (CapsuleCmd) */
+	SPDK_NVME_TCP_PDU_TYPE_CAPSULE_CMD		= 0x04,
+
+	/** Response Capsule (CapsuleRsp) */
+	SPDK_NVME_TCP_PDU_TYPE_CAPSULE_RESP		= 0x05,
+
+	/** Host To Controller Data (H2CData) */
+	SPDK_NVME_TCP_PDU_TYPE_H2C_DATA			= 0x06,
+
+	/** Controller To Host Data (C2HData) */
+	SPDK_NVME_TCP_PDU_TYPE_C2H_DATA			= 0x07,
+
+	/** Ready to Transfer (R2T) */
+	SPDK_NVME_TCP_PDU_TYPE_R2T			= 0x09,
+};
+
+/** Common NVMe/TCP PDU header */
+struct spdk_nvme_tcp_common_pdu_hdr {
+	/** PDU type (\ref spdk_nvme_tcp_pdu_type) */
+	uint8_t				pdu_type;
+
+	/** pdu_type-specific flags */
+	uint8_t				flags;
+
+	/** Length of PDU header (not including the Header Digest) */
+	uint8_t				hlen;
+
+	/** PDU Data Offset from the start of the PDU */
+	uint8_t				pdo;
+
+	/** Total number of bytes in PDU, including pdu_hdr */
+	uint32_t			plen;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_tcp_common_pdu_hdr) == 8, "Incorrect size");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_common_pdu_hdr, pdu_type) == 0,
+		   "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_common_pdu_hdr, flags) == 1, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_common_pdu_hdr, hlen) == 2, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_common_pdu_hdr, pdo) == 3, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_common_pdu_hdr, plen) == 4, "Incorrect offset");
+
+#define SPDK_NVME_TCP_CH_FLAGS_HDGSTF		(1u << 0)
+#define SPDK_NVME_TCP_CH_FLAGS_DDGSTF		(1u << 1)
+
+/**
+ * ICReq
+ *
+ * common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_IC_REQ
+ */
+struct spdk_nvme_tcp_ic_req {
+	struct spdk_nvme_tcp_common_pdu_hdr	common;
+	uint16_t				pfv;
+	/** Specifies the data alignment for all PDUs transferred from the controller to the host that contain data */
+	uint8_t					hpda;
+	union {
+		uint8_t				raw;
+		struct {
+			uint8_t			hdgst_enable : 1;
+			uint8_t			ddgst_enable : 1;
+			uint8_t			reserved : 6;
+		} bits;
+	} dgst;
+	uint32_t				maxr2t;
+	uint8_t					reserved16[112];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_tcp_ic_req) == 128, "Incorrect size");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_ic_req, pfv) == 8, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_ic_req, hpda) == 10, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_ic_req, maxr2t) == 12, "Incorrect offset");
+
+#define SPDK_NVME_TCP_CPDA_MAX 31
+#define SPDK_NVME_TCP_PDU_PDO_MAX_OFFSET     ((SPDK_NVME_TCP_CPDA_MAX + 1) << 2)
+
+/**
+ * ICResp
+ *
+ * common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_IC_RESP
+ */
+struct spdk_nvme_tcp_ic_resp {
+	struct spdk_nvme_tcp_common_pdu_hdr	common;
+	uint16_t				pfv;
+	/** Specifies the data alignment for all PDUs transferred from the host to the controller that contain data */
+	uint8_t					cpda;
+	union {
+		uint8_t				raw;
+		struct {
+			uint8_t			hdgst_enable : 1;
+			uint8_t			ddgst_enable : 1;
+			uint8_t			reserved : 6;
+		} bits;
+	} dgst;
+	/** Specifies the maximum number of PDU-Data bytes per H2C Data Transfer PDU */
+	uint32_t				maxh2cdata;
+	uint8_t					reserved16[112];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_tcp_ic_resp) == 128, "Incorrect size");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_ic_resp, pfv) == 8, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_ic_resp, cpda) == 10, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_ic_resp, maxh2cdata) == 12, "Incorrect offset");
+
+/**
+ * TermReq
+ *
+ * common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_TERM_REQ
+ */
+struct spdk_nvme_tcp_term_req_hdr {
+	struct spdk_nvme_tcp_common_pdu_hdr	common;
+	uint16_t				fes;
+	uint8_t					fei[4];
+	uint8_t					reserved14[10];
+};
+
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_tcp_term_req_hdr) == 24, "Incorrect size");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_term_req_hdr, fes) == 8, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_term_req_hdr, fei) == 10, "Incorrect offset");
+
+enum spdk_nvme_tcp_term_req_fes {
+	SPDK_NVME_TCP_TERM_REQ_FES_INVALID_HEADER_FIELD				= 0x01,
+	SPDK_NVME_TCP_TERM_REQ_FES_PDU_SEQUENCE_ERROR				= 0x02,
+	SPDK_NVME_TCP_TERM_REQ_FES_HDGST_ERROR					= 0x03,
+	SPDK_NVME_TCP_TERM_REQ_FES_DATA_TRANSFER_OUT_OF_RANGE			= 0x04,
+	SPDK_NVME_TCP_TERM_REQ_FES_DATA_TRANSFER_LIMIT_EXCEEDED			= 0x05,
+	SPDK_NVME_TCP_TERM_REQ_FES_R2T_LIMIT_EXCEEDED				= 0x05,
+	SPDK_NVME_TCP_TERM_REQ_FES_INVALID_DATA_UNSUPPORTED_PARAMETER		= 0x06,
+};
+
+/* Total length of term req PDU (including PDU header and DATA) in bytes shall not exceed a limit of 152 bytes. */
+#define SPDK_NVME_TCP_TERM_REQ_ERROR_DATA_MAX_SIZE	128
+#define SPDK_NVME_TCP_TERM_REQ_PDU_MAX_SIZE		(SPDK_NVME_TCP_TERM_REQ_ERROR_DATA_MAX_SIZE + sizeof(struct spdk_nvme_tcp_term_req_hdr))
+
+/**
+ * CapsuleCmd
+ *
+ * common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_CAPSULE_CMD
+ */
+struct spdk_nvme_tcp_cmd {
+	struct spdk_nvme_tcp_common_pdu_hdr	common;
+	struct spdk_nvme_cmd			ccsqe;
+	/**< icdoff hdgest padding + in-capsule data + ddgst (if enabled) */
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_tcp_cmd) == 72, "Incorrect size");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_cmd, ccsqe) == 8, "Incorrect offset");
+
+/**
+ * CapsuleResp
+ *
+ * common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_CAPSULE_RESP
+ */
+struct spdk_nvme_tcp_rsp {
+	struct spdk_nvme_tcp_common_pdu_hdr	common;
+	struct spdk_nvme_cpl			rccqe;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_tcp_rsp) == 24, "incorrect size");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_rsp, rccqe) == 8, "Incorrect offset");
+
+
+/**
+ * H2CData
+ *
+ * hdr.pdu_type == SPDK_NVME_TCP_PDU_TYPE_H2C_DATA
+ */
+struct spdk_nvme_tcp_h2c_data_hdr {
+	struct spdk_nvme_tcp_common_pdu_hdr	common;
+	uint16_t				cccid;
+	uint16_t				ttag;
+	uint32_t				datao;
+	uint32_t				datal;
+	uint8_t					reserved20[4];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_tcp_h2c_data_hdr) == 24, "Incorrect size");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_h2c_data_hdr, cccid) == 8, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_h2c_data_hdr, ttag) == 10, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_h2c_data_hdr, datao) == 12, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_h2c_data_hdr, datal) == 16, "Incorrect offset");
+
+#define SPDK_NVME_TCP_H2C_DATA_FLAGS_LAST_PDU	(1u << 2)
+#define SPDK_NVME_TCP_H2C_DATA_FLAGS_SUCCESS	(1u << 3)
+#define SPDK_NVME_TCP_H2C_DATA_PDO_MULT		8u
+
+/**
+ * C2HData
+ *
+ * hdr.pdu_type == SPDK_NVME_TCP_PDU_TYPE_C2H_DATA
+ */
+struct spdk_nvme_tcp_c2h_data_hdr {
+	struct spdk_nvme_tcp_common_pdu_hdr	common;
+	uint16_t				cccid;
+	uint8_t					reserved10[2];
+	uint32_t				datao;
+	uint32_t				datal;
+	uint8_t					reserved20[4];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_tcp_c2h_data_hdr) == 24, "Incorrect size");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_c2h_data_hdr, cccid) == 8, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_c2h_data_hdr, datao) == 12, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_c2h_data_hdr, datal) == 16, "Incorrect offset");
+
+#define SPDK_NVME_TCP_C2H_DATA_FLAGS_SUCCESS	(1u << 3)
+#define SPDK_NVME_TCP_C2H_DATA_FLAGS_LAST_PDU	(1u << 2)
+#define SPDK_NVME_TCP_C2H_DATA_PDO_MULT		8u
+
+/**
+ * R2T
+ *
+ * common.pdu_type == SPDK_NVME_TCP_PDU_TYPE_R2T
+ */
+struct spdk_nvme_tcp_r2t_hdr {
+	struct spdk_nvme_tcp_common_pdu_hdr	common;
+	uint16_t				cccid;
+	uint16_t				ttag;
+	uint32_t				r2to;
+	uint32_t				r2tl;
+	uint8_t					reserved20[4];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_nvme_tcp_r2t_hdr) == 24, "Incorrect size");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_r2t_hdr, cccid) == 8, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_r2t_hdr, ttag) == 10, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_r2t_hdr, r2to) == 12, "Incorrect offset");
+SPDK_STATIC_ASSERT(offsetof(struct spdk_nvme_tcp_r2t_hdr, r2tl) == 16, "Incorrect offset");
+
 #pragma pack(pop)
 
 #endif /* __NVMF_SPEC_H__ */
diff --git a/PDK/core/src/api/include/udd/spdk/opal.h b/PDK/core/src/api/include/udd/spdk/opal.h
new file mode 100644
index 0000000..325b745
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/opal.h
@@ -0,0 +1,150 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef SPDK_OPAL_H
+#define SPDK_OPAL_H
+
+#include "spdk/stdinc.h"
+#include "spdk/nvme.h"
+#include "spdk/log.h"
+#include "spdk/endian.h"
+#include "spdk/string.h"
+
+#define SPDK_OPAL_NOT_SUPPORTED 0xFF
+
+#define MAX_PASSWORD_SIZE 32 /* in byte */
+
+/*
+ * TCG Storage Architecture Core Spec v2.01 r1.00
+ * 5.1.5 Method Status Codes
+ */
+#define SPDK_OPAL_FAILED 0x3F
+
+static const char *const spdk_opal_errors[] = {
+	"SUCCESS",
+	"NOT AUTHORIZED",
+	"OBSOLETE/UNKNOWN ERROR",
+	"SP BUSY",
+	"SP FAILED",
+	"SP DISABLED",
+	"SP FROZEN",
+	"NO SESSIONS AVAILABLE",
+	"UNIQUENESS CONFLICT",
+	"INSUFFICIENT SPACE",
+	"INSUFFICIENT ROWS",
+	"UNKNOWN ERROR",
+	"INVALID PARAMETER",
+	"OBSOLETE/UNKNOWN ERROR",
+	"UNKNOWN ERROR",
+	"TPER MALFUNCTION",
+	"TRANSACTION FAILURE",
+	"RESPONSE OVERFLOW",
+	"AUTHORITY LOCKED OUT",
+};
+
+enum spdk_opal_cmd {
+	OPAL_CMD_SAVE,
+	OPAL_CMD_LOCK_UNLOCK,
+	OPAL_CMD_TAKE_OWNERSHIP,
+	OPAL_CMD_ACTIVATE_LSP,	/* locking sp */
+	OPAL_CMD_SET_NEW_PASSWD,
+	OPAL_CMD_ACTIVATE_USER,
+	OPAL_CMD_REVERT_TPER,
+	OPAL_CMD_SETUP_LOCKING_RANGE,
+	OPAL_CMD_ADD_USER_TO_LOCKING_RANGE,
+	OPAL_CMD_ENABLE_DISABLE_SHADOW_MBR,
+	OPAL_CMD_ERASE_LOCKING_RANGE,
+	OPAL_CMD_SECURE_ERASE_LOCKING_RANGE,
+	OPAL_CMD_INITIAL_SETUP,
+	OPAL_CMD_SCAN,
+};
+
+struct spdk_opal_info {
+	uint8_t tper : 1;
+	uint8_t locking : 1;
+	uint8_t geometry : 1;
+	uint8_t single_user_mode : 1;
+	uint8_t datastore : 1;
+	uint8_t opal_v200 : 1;
+	uint8_t opal_v100 : 1;
+	uint8_t vendor_specific : 1;
+	uint8_t opal_ssc_dev : 1;
+	uint8_t tper_acknack : 1;
+	uint8_t tper_async : 1;
+	uint8_t tper_buffer_mgt : 1;
+	uint8_t tper_comid_mgt : 1;
+	uint8_t tper_streaming : 1;
+	uint8_t tper_sync : 1;
+	uint8_t locking_locked : 1;
+	uint8_t locking_locking_enabled : 1;
+	uint8_t locking_locking_supported : 1;
+	uint8_t locking_mbr_done : 1;
+	uint8_t locking_mbr_enabled : 1;
+	uint8_t locking_media_encrypt : 1;
+	uint8_t geometry_align : 1;
+	uint64_t geometry_alignment_granularity;
+	uint32_t geometry_logical_block_size;
+	uint64_t geometry_lowest_aligned_lba;
+	uint8_t single_user_any : 1;
+	uint8_t single_user_all : 1;
+	uint8_t single_user_policy : 1;
+	uint32_t single_user_locking_objects;
+	uint16_t datastore_max_tables;
+	uint32_t datastore_max_table_size;
+	uint32_t datastore_alignment;
+	uint16_t opal_v100_base_comid;
+	uint16_t opal_v100_num_comid;
+	uint8_t opal_v100_range_crossing : 1;
+	uint16_t opal_v200_base_comid;
+	uint16_t opal_v200_num_comid;
+	uint8_t opal_v200_initial_pin;
+	uint8_t opal_v200_reverted_pin;
+	uint16_t opal_v200_num_admin;
+	uint16_t opal_v200_num_user;
+	uint8_t opal_v200_range_crossing : 1;
+	uint16_t vu_feature_code; /* vendor specific feature */
+};
+
+struct spdk_opal_dev;
+
+struct spdk_opal_dev *spdk_opal_init_dev(void *dev_handler);
+
+int spdk_opal_scan(struct spdk_opal_dev *dev);
+void spdk_opal_close(struct spdk_opal_dev *dev);
+struct spdk_opal_info *spdk_opal_get_info(struct spdk_opal_dev *dev);
+
+bool spdk_opal_supported(struct spdk_opal_dev *dev);
+
+int spdk_opal_cmd(struct spdk_opal_dev *dev, unsigned int cmd, void *arg);
+
+#endif
diff --git a/PDK/core/src/api/include/udd/spdk/opal_spec.h b/PDK/core/src/api/include/udd/spdk/opal_spec.h
new file mode 100644
index 0000000..64d3553
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/opal_spec.h
@@ -0,0 +1,381 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef SPDK_OPAL_SPEC_H
+#define SPDK_OPAL_SPEC_H
+
+#include "spdk/stdinc.h"
+#include "spdk/assert.h"
+
+/*
+ * TCG Storage Architecture Core Spec v2.01 r1.00
+ * 3.2.2.3 Tokens
+ */
+#define SPDK_TINY_ATOM_TYPE_MAX			0x7F
+#define SPDK_SHORT_ATOM_TYPE_MAX		0xBF
+#define SPDK_MEDIUM_ATOM_TYPE_MAX		0xDF
+#define SPDK_LONG_ATOM_TYPE_MAX			0xE3
+
+#define SPDK_TINY_ATOM_SIGN_FLAG		0x40
+
+#define SPDK_TINY_ATOM_DATA_MASK		0x3F
+
+#define SPDK_SHORT_ATOM_ID			0x80
+#define SPDK_SHORT_ATOM_BYTESTRING_FLAG		0x20
+#define SPDK_SHORT_ATOM_SIGN_FLAG		0x10
+#define SPDK_SHORT_ATOM_LEN_MASK		0x0F
+
+#define SPDK_MEDIUM_ATOM_ID			0xC0
+#define SPDK_MEDIUM_ATOM_BYTESTRING_FLAG	0x10
+
+#define SPDK_MEDIUM_ATOM_SIGN_FLAG		0x08
+#define SPDK_MEDIUM_ATOM_LEN_MASK		0x07
+
+#define SPDK_LONG_ATOM_ID			0xE0
+#define SPDK_LONG_ATOM_BYTESTRING_FLAG		0x02
+#define SPDK_LONG_ATOM_SIGN_FLAG		0x01
+
+/*
+ * TCG Storage Architecture Core Spec v2.01 r1.00
+ * Table-26 ComID management
+ */
+#define LV0_DISCOVERY_COMID			0x01
+
+/*
+ * TCG Storage Opal v2.01 r1.00
+ * 5.2.3 Type Table Modification
+ */
+#define OPAL_MANUFACTURED_INACTIVE		0x08
+
+#define LOCKING_RANGE_NON_GLOBAL		0x03
+
+/*
+ * Feature Code
+ */
+enum spdk_lv0_discovery_feature_code {
+	/*
+	 * TCG Storage Architecture Core Spec v2.01 r1.00
+	 * 3.3.6 Level 0 Discovery
+	 */
+	FEATURECODE_TPER	= 0x0001,
+	FEATURECODE_LOCKING	= 0x0002,
+
+	/*
+	 * Opal SSC 1.00 r3.00 Final
+	 * 3.1.1.4 Opal SSC Feature
+	 */
+	FEATURECODE_OPALV100	= 0x0200,
+
+	/*
+	 * TCG Storage Opal v2.01 r1.00
+	 * 3.1.1.4 Geometry Reporting Feature
+	 * 3.1.1.5 Opal SSC V2.00 Feature
+	 */
+	FEATURECODE_OPALV200	= 0x0203,
+	FEATURECODE_GEOMETRY	= 0x0003,
+
+	/*
+	 * TCG Storage Opal Feature Set Single User Mode v1.00 r2.00
+	 * 4.2.1 Single User Mode Feature Descriptor
+	 */
+	FEATURECODE_SINGLEUSER	= 0x0201,
+
+	/*
+	 * TCG Storage Opal Feature Set Additional DataStore Tables v1.00 r1.00
+	 * 4.1.1 DataStore Table Feature Descriptor
+	 */
+	FEATURECODE_DATASTORE	= 0x0202,
+};
+
+/*
+ * TCG Storage Architecture Core Spec v2.01 r1.00
+ * 5.1.4 Abstract Type
+ */
+enum spdk_opal_token {
+	/* boolean */
+	SPDK_OPAL_TRUE			= 0x01,
+	SPDK_OPAL_FALSE			= 0x00,
+
+	/* cell_block
+	 * 5.1.4.2.3 */
+	SPDK_OPAL_TABLE			= 0x00,
+	SPDK_OPAL_STARTROW		= 0x01,
+	SPDK_OPAL_ENDROW		= 0x02,
+	SPDK_OPAL_STARTCOLUMN		= 0x03,
+	SPDK_OPAL_ENDCOLUMN		= 0x04,
+	SPDK_OPAL_VALUES		= 0x01,
+
+	/* C_PIN table
+	 * 5.3.2.12 */
+	SPDK_OPAL_PIN			= 0x03,
+
+	/* locking table
+	 * 5.7.2.2 */
+	SPDK_OPAL_RANGESTART		= 0x03,
+	SPDK_OPAL_RANGELENGTH		= 0x04,
+	SPDK_OPAL_READLOCKENABLED	= 0x05,
+	SPDK_OPAL_WRITELOCKENABLED	= 0x06,
+	SPDK_OPAL_READLOCKED		= 0x07,
+	SPDK_OPAL_WRITELOCKED		= 0x08,
+	SPDK_OPAL_ACTIVEKEY		= 0x0A,
+
+	/* locking info table */
+	SPDK_OPAL_MAXRANGES		= 0x04,
+
+	/* mbr control */
+	SPDK_OPAL_MBRENABLE		= 0x01,
+	SPDK_OPAL_MBRDONE		= 0x02,
+
+	/* properties */
+	SPDK_OPAL_HOSTPROPERTIES	= 0x00,
+
+	/* control tokens */
+	SPDK_OPAL_STARTLIST		= 0xF0,
+	SPDK_OPAL_ENDLIST		= 0xF1,
+	SPDK_OPAL_STARTNAME		= 0xF2,
+	SPDK_OPAL_ENDNAME		= 0xF3,
+	SPDK_OPAL_CALL			= 0xF8,
+	SPDK_OPAL_ENDOFDATA		= 0xF9,
+	SPDK_OPAL_ENDOFSESSION		= 0xFA,
+	SPDK_OPAL_STARTTRANSACTON	= 0xFB,
+	SPDK_OPAL_ENDTRANSACTON		= 0xFC,
+	SPDK_OPAL_EMPTYATOM		= 0xFF,
+	SPDK_OPAL_WHERE			= 0x00,
+
+	/* life cycle */
+	SPDK_OPAL_LIFECYCLE		= 0x06,
+
+	/* Autority table */
+	SPDK_OPAL_AUTH_ENABLE		= 0x05,
+};
+
+/*
+ * TCG Storage Architecture Core Spec v2.01 r1.00
+ * Table-39 Level0 Discovery Header Format
+ */
+struct spdk_d0_header {
+	uint32_t length;
+	uint32_t revision;
+	uint32_t reserved_0;
+	uint32_t reserved_1;
+	uint8_t vendor_specfic[32];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_d0_header) == 48, "Incorrect size");
+
+/*
+ * TCG Storage Architecture Core Spec v2.01 r1.00
+ * Table-42 TPer Feature Descriptor
+ */
+struct __attribute__((packed)) spdk_d0_tper_features {
+	uint16_t feature_code;
+	uint8_t reserved_0 : 4;
+	uint8_t version : 4;
+	uint8_t length;
+	uint8_t sync : 1;
+	uint8_t async : 1;
+	uint8_t acknack : 1;
+	uint8_t buffer_management : 1;
+	uint8_t streaming : 1;
+	uint8_t reserved_1 : 1;
+	uint8_t comid_management : 1;
+	uint8_t reserved_2 : 1;
+
+	uint8_t reserved_3[3];
+	uint32_t reserved_4;
+	uint32_t reserved_5;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_d0_tper_features) == 16, "Incorrect size");
+
+/*
+ * TCG Storage Architecture Core Spec v2.01 r1.00
+ * Table-43 Locking Feature Descriptor
+ */
+struct __attribute__((packed)) spdk_d0_locking_features {
+	uint16_t feature_code;
+	uint8_t reserved_0 : 4;
+	uint8_t version : 4;
+	uint8_t length;
+	uint8_t locking_supported : 1;
+	uint8_t locking_enabled : 1;
+	uint8_t locked : 1;
+	uint8_t media_encryption : 1;
+	uint8_t mbr_enabled : 1;
+	uint8_t mbr_done : 1;
+	uint8_t reserved_1 : 1;
+	uint8_t reserved_2 : 1;
+
+	uint8_t reserved_3[3];
+	uint32_t reserved_4;
+	uint32_t reserved_5;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_d0_locking_features) == 16, "Incorrect size");
+
+/*
+ * TCG Storage Opal Feature Set Single User Mode v1.00 r2.00
+ * 4.2.1 Single User Mode Feature Descriptor
+ */
+struct __attribute__((packed)) spdk_d0_sum {
+	uint16_t feature_code;
+	uint8_t reserved_0 : 4;
+	uint8_t version : 4;
+	uint8_t length;
+	uint32_t num_locking_objects;
+	uint8_t any : 1;
+	uint8_t all : 1;
+	uint8_t policy : 1;
+	uint8_t reserved_1 : 5;
+
+	uint8_t reserved_2;
+	uint16_t reserved_3;
+	uint32_t reserved_4;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_d0_sum) == 16, "Incorrect size");
+
+/*
+ * TCG Storage Opal v2.01 r1.00
+ * 3.1.1.4 Geometry Reporting Feature
+ */
+struct __attribute__((packed)) spdk_d0_geo_features {
+	uint16_t feature_code;
+	uint8_t reserved_0 : 4;
+	uint8_t version : 4;
+	uint8_t length;
+	uint8_t align : 1;
+	uint8_t reserved_1 : 7;
+	uint8_t reserved_2[7];
+	uint32_t logical_block_size;
+	uint64_t alignment_granularity;
+	uint64_t lowest_aligned_lba;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_d0_geo_features) == 32, "Incorrect size");
+
+/*
+ * TCG Storage Opal Feature Set Additional DataStore Tables v1.00 r1.00
+ * 4.1.1 DataStore Table Feature Descriptor
+ */
+struct __attribute__((packed)) spdk_d0_datastore_features {
+	uint16_t feature_code;
+	uint8_t reserved_0 : 4;
+	uint8_t version : 4;
+	uint8_t length;
+	uint16_t reserved_1;
+	uint16_t max_tables;
+	uint32_t max_table_size;
+	uint32_t alignment;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_d0_datastore_features) == 16, "Incorrect size");
+
+/*
+ * Opal SSC 1.00 r3.00 Final
+ * 3.1.1.4 Opal SSC Feature
+ */
+struct __attribute__((packed)) spdk_d0_opal_v100 {
+	uint16_t feature_code;
+	uint8_t reserved_0 : 4;
+	uint8_t version : 4;
+	uint8_t length;
+	uint16_t base_comid;
+	uint16_t number_comids;
+	uint8_t range_crossing : 1;
+
+	uint8_t reserved_1 : 7;
+	uint8_t reserved_2;
+	uint16_t reserved_3;
+	uint32_t reserved_4;
+	uint32_t reserved_5;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_d0_opal_v100) == 20, "Incorrect size");
+
+/*
+ * TCG Storage Opal v2.01 r1.00
+ * 3.1.1.4 Geometry Reporting Feature
+ * 3.1.1.5 Opal SSC V2.00 Feature
+ */
+struct __attribute__((packed)) spdk_d0_opal_v200 {
+	uint16_t featureCode;
+	uint8_t reserved_0 : 4;
+	uint8_t version : 4;
+	uint8_t length;
+	uint16_t base_comid;
+	uint16_t num_comids;
+	uint8_t range_crossing : 1;
+	uint8_t reserved_1 : 7;
+	uint16_t num_locking_admin_auth; /* Number of Locking SP Admin Authorities Supported */
+	uint16_t num_locking_user_auth;
+	uint8_t initial_pin;
+	uint8_t reverted_pin;
+
+	uint8_t reserved_2;
+	uint32_t reserved_3;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_d0_opal_v200) == 20, "Incorrect size");
+
+/*
+ * TCG Storage Architecture Core Spec v2.01 r1.00
+ * 3.2.3 ComPackets, Packets & Subpackets
+ */
+
+/* CommPacket header format
+ * (big-endian)
+ */
+struct __attribute__((packed)) spdk_opal_compacket {
+	uint32_t reserved;
+	uint8_t comid[2];
+	uint8_t extended_comid[2];
+	uint32_t outstanding_data;
+	uint32_t min_transfer;
+	uint32_t length;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_opal_compacket) == 20, "Incorrect size");
+
+/* packet header format */
+struct __attribute__((packed)) spdk_opal_packet {
+	uint32_t session_tsn;
+	uint32_t session_hsn;
+	uint32_t seq_number;
+	uint16_t reserved;
+	uint16_t ack_type;
+	uint32_t acknowledgment;
+	uint32_t length;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_opal_packet) == 24, "Incorrect size");
+
+/* data subpacket header */
+struct __attribute__((packed)) spdk_opal_data_subpacket {
+	uint8_t reserved[6];
+	uint16_t kind;
+	uint32_t length;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_opal_data_subpacket) == 12, "Incorrect size");
+
+#endif
diff --git a/PDK/core/src/api/include/udd/spdk/pci_ids.h b/PDK/core/src/api/include/udd/spdk/pci_ids.h
index 2ac94cf..b078dec 100644
--- a/PDK/core/src/api/include/udd/spdk/pci_ids.h
+++ b/PDK/core/src/api/include/udd/spdk/pci_ids.h
@@ -47,7 +47,11 @@ extern "C" {
 #define SPDK_PCI_ANY_ID			0xffff
 #define SPDK_PCI_VID_INTEL		0x8086
 #define SPDK_PCI_VID_MEMBLAZE		0x1c5f
+#define SPDK_PCI_VID_SAMSUNG		0x144d
 #define SPDK_PCI_VID_VIRTUALBOX		0x80ee
+#define SPDK_PCI_VID_VIRTIO		0x1af4
+#define SPDK_PCI_VID_CNEXLABS		0x1d1d
+#define SPDK_PCI_VID_VMWARE		0x15ad
 
 /**
  * PCI class code for NVMe devices.
@@ -114,6 +118,11 @@ extern "C" {
 
 #define PCI_DEVICE_ID_INTEL_IOAT_SKX	0x2021
 
+#define PCI_DEVICE_ID_INTEL_IOAT_ICX	0x0b00
+
+#define PCI_DEVICE_ID_VIRTIO_BLK_MODERN	0x1001
+#define PCI_DEVICE_ID_VIRTIO_SCSI_MODERN 0x1004
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/PDK/core/src/api/include/udd/spdk/queue.h b/PDK/core/src/api/include/udd/spdk/queue.h
index d3d8615..a24dd00 100644
--- a/PDK/core/src/api/include/udd/spdk/queue.h
+++ b/PDK/core/src/api/include/udd/spdk/queue.h
@@ -47,7 +47,7 @@ extern "C" {
  *  Include a header with these additional features on Linux only.
  */
 #ifndef __FreeBSD__
-#include <spdk/queue_extras.h>
+#include "spdk/queue_extras.h"
 #endif
 
 #ifdef __cplusplus
diff --git a/PDK/core/src/api/include/udd/spdk/queue_extras.h b/PDK/core/src/api/include/udd/spdk/queue_extras.h
index 7fa14b7..3ad31f6 100644
--- a/PDK/core/src/api/include/udd/spdk/queue_extras.h
+++ b/PDK/core/src/api/include/udd/spdk/queue_extras.h
@@ -197,7 +197,7 @@ struct {								\
 	if (LIST_NEXT((elm), field) != NULL &&				\
 	    LIST_NEXT((elm), field)->field.le_prev !=			\
 	     &((elm)->field.le_next))					\
-	     	panic("Bad link elm %p next->prev != elm", (elm));	\
+		panic("Bad link elm %p next->prev != elm", (elm));	\
 } while (0)
 
 #define	QMD_LIST_CHECK_PREV(elm, field) do {				\
@@ -258,7 +258,7 @@ struct {								\
 
 #define	QMD_TAILQ_CHECK_TAIL(head, field) do {				\
 	if (*(head)->tqh_last != NULL)					\
-	    	panic("Bad tailq NEXT(%p->tqh_last) != NULL", (head)); 	\
+		panic("Bad tailq NEXT(%p->tqh_last) != NULL", (head));	\
 } while (0)
 
 #define	QMD_TAILQ_CHECK_NEXT(elm, field) do {				\
diff --git a/PDK/core/src/api/include/udd/spdk/reduce.h b/PDK/core/src/api/include/udd/spdk/reduce.h
new file mode 100644
index 0000000..a35c5b9
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/reduce.h
@@ -0,0 +1,233 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/** \file
+ * SPDK block compression
+ */
+
+#ifndef SPDK_REDUCE_H_
+#define SPDK_REDUCE_H_
+
+#include "spdk/uuid.h"
+
+/**
+ * Describes the parameters of an spdk_reduce_vol.
+ */
+struct spdk_reduce_vol_params {
+	struct spdk_uuid	uuid;
+
+	/**
+	 * Size in bytes of the IO unit for the backing device.  This
+	 *  is the unit in which space is allocated from the backing
+	 *  device, and the unit in which data is read from of written
+	 *  to the backing device.  Must be greater than 0.
+	 */
+	uint32_t		backing_io_unit_size;
+
+	/**
+	 * Size in bytes of a logical block.  This is the unit in
+	 *  which users read or write data to the compressed volume.
+	 *  Must be greater than 0.
+	 */
+	uint32_t		logical_block_size;
+
+	/**
+	 * Size in bytes of a chunk on the compressed volume.  This
+	 *  is the unit in which data is compressed.  Must be an even
+	 *  multiple of backing_io_unit_size and logical_block_size.
+	 *  Must be greater than 0.
+	 */
+	uint32_t		chunk_size;
+
+	/**
+	 * Total size in bytes of the compressed volume.  During
+	 *  initialization, the size is calculated from the size of
+	 *  backing device size, so this must be set to 0 in the
+	 *  structure passed to spdk_reduce_vol_init().  After
+	 *  initialization, or a successful load, this field will
+	 *  contain the total size which will be an even multiple
+	 *  of the chunk size.
+	 */
+	uint64_t		vol_size;
+};
+
+struct spdk_reduce_vol;
+
+typedef void (*spdk_reduce_vol_op_complete)(void *ctx, int reduce_errno);
+typedef void (*spdk_reduce_vol_op_with_handle_complete)(void *ctx,
+		struct spdk_reduce_vol *vol,
+		int reduce_errno);
+
+/**
+ * Defines function type for callback functions called when backing_dev
+ *  operations are complete.
+ *
+ * \param cb_arg Callback argument
+ * \param reduce_errno Completion status of backing_dev operation
+ *		       Negative values indicate negated errno value
+ *		       0 indicates successful readv/writev/unmap operation
+ *		       Positive value indicates successful compress/decompress
+ *		       operations; number indicates number of bytes written to
+ *		       destination iovs
+ */
+typedef void (*spdk_reduce_dev_cpl)(void *cb_arg, int reduce_errno);
+
+struct spdk_reduce_vol_cb_args {
+	spdk_reduce_dev_cpl	cb_fn;
+	void			*cb_arg;
+};
+
+struct spdk_reduce_backing_dev {
+	void (*readv)(struct spdk_reduce_backing_dev *dev, struct iovec *iov, int iovcnt,
+		      uint64_t lba, uint32_t lba_count, struct spdk_reduce_vol_cb_args *args);
+
+	void (*writev)(struct spdk_reduce_backing_dev *dev, struct iovec *iov, int iovcnt,
+		       uint64_t lba, uint32_t lba_count, struct spdk_reduce_vol_cb_args *args);
+
+	void (*unmap)(struct spdk_reduce_backing_dev *dev,
+		      uint64_t lba, uint32_t lba_count, struct spdk_reduce_vol_cb_args *args);
+
+	void (*compress)(struct spdk_reduce_backing_dev *dev,
+			 struct iovec *src_iov, int src_iovcnt,
+			 struct iovec *dst_iov, int dst_iovcnt,
+			 struct spdk_reduce_vol_cb_args *args);
+
+	void (*decompress)(struct spdk_reduce_backing_dev *dev,
+			   struct iovec *src_iov, int src_iovcnt,
+			   struct iovec *dst_iov, int dst_iovcnt,
+			   struct spdk_reduce_vol_cb_args *args);
+
+	uint64_t	blockcnt;
+	uint32_t	blocklen;
+};
+
+/**
+ * Get the UUID for a libreduce compressed volume.
+ *
+ * \param vol Previously loaded or initialized compressed volume.
+ * \return UUID for the compressed volume.
+ */
+const struct spdk_uuid *spdk_reduce_vol_get_uuid(struct spdk_reduce_vol *vol);
+
+/**
+ * Initialize a new libreduce compressed volume.
+ *
+ * \param params Parameters for the new volume.
+ * \param backing_dev Structure describing the backing device to use for the new volume.
+ * \param pm_file_dir Directory to use for creation of the persistent memory file to
+ *                    use for the new volume.  This function will append the UUID as
+ *		      the filename to create in this directory.
+ * \param cb_fn Callback function to signal completion of the initialization process.
+ * \param cb_arg Argument to pass to the callback function.
+ */
+void spdk_reduce_vol_init(struct spdk_reduce_vol_params *params,
+			  struct spdk_reduce_backing_dev *backing_dev,
+			  const char *pm_file_dir,
+			  spdk_reduce_vol_op_with_handle_complete cb_fn,
+			  void *cb_arg);
+
+/**
+ * Load an existing libreduce compressed volume.
+ *
+ * \param backing_dev Structure describing the backing device containing the compressed volume.
+ * \param cb_fn Callback function to signal completion of the loading process.
+ * \param cb_arg Argument to pass to the callback function.
+ */
+void spdk_reduce_vol_load(struct spdk_reduce_backing_dev *backing_dev,
+			  spdk_reduce_vol_op_with_handle_complete cb_fn,
+			  void *cb_arg);
+
+/**
+ * Unload a previously initialized or loaded libreduce compressed volume.
+ *
+ * \param vol Volume to unload.
+ * \param cb_fn Callback function to signal completion of the unload process.
+ * \param cb_arg Argument to pass to the callback function.
+ */
+void spdk_reduce_vol_unload(struct spdk_reduce_vol *vol,
+			    spdk_reduce_vol_op_complete cb_fn,
+			    void *cb_arg);
+
+/**
+ * Destroy an existing libreduce compressed volume.
+ *
+ * This will zero the metadata region on the backing device and delete the associated
+ * pm metadata file.  If the backing device does not contain a compressed volume, the
+ * cb_fn will be called with error status without modifying the backing device nor
+ * deleting a pm file.
+ *
+ * \param backing_dev Structure describing the backing device containing the compressed volume.
+ * \param cb_fn Callback function to signal completion of the destruction process.
+ * \param cb_arg Argument to pass to the callback function.
+ */
+void spdk_reduce_vol_destroy(struct spdk_reduce_backing_dev *backing_dev,
+			     spdk_reduce_vol_op_complete cb_fn,
+			     void *cb_arg);
+
+/**
+ * Read data from a libreduce compressed volume.
+ *
+ * This function will only read from logical blocks on the comparessed volume that
+ * fall within the same chunk.
+ *
+ * \param vol Volume to read data.
+ * \param iov iovec array describing the data to be read
+ * \param iovcnt Number of elements in the iovec array
+ * \param offset Offset (in logical blocks) to read the data on the compressed volume
+ * \param length Length (in logical blocks) of the data to read
+ * \param cb_fn Callback function to signal completion of the readv operation.
+ * \param cb_arg Argument to pass to the callback function.
+ */
+void spdk_reduce_vol_readv(struct spdk_reduce_vol *vol,
+			   struct iovec *iov, int iovcnt, uint64_t offset, uint64_t length,
+			   spdk_reduce_vol_op_complete cb_fn, void *cb_arg);
+
+/**
+ * Write data to a libreduce compressed volume.
+ *
+ * This function will only write to logical blocks on the comparessed volume that
+ * fall within the same chunk.
+ *
+ * \param vol Volume to write data.
+ * \param iov iovec array describing the data to be written
+ * \param iovcnt Number of elements in the iovec array
+ * \param offset Offset (in logical blocks) to write the data on the compressed volume
+ * \param length Length (in logical blocks) of the data to write
+ * \param cb_fn Callback function to signal completion of the writev operation.
+ * \param cb_arg Argument to pass to the callback function.
+ */
+void spdk_reduce_vol_writev(struct spdk_reduce_vol *vol,
+			    struct iovec *iov, int iovcnt, uint64_t offset, uint64_t length,
+			    spdk_reduce_vol_op_complete cb_fn, void *cb_arg);
+
+#endif /* SPDK_REDUCE_H_ */
diff --git a/PDK/core/src/api/include/udd/spdk/rpc.h b/PDK/core/src/api/include/udd/spdk/rpc.h
index 4b3d5c5..8e5a52c 100644
--- a/PDK/core/src/api/include/udd/spdk/rpc.h
+++ b/PDK/core/src/api/include/udd/spdk/rpc.h
@@ -38,16 +38,86 @@
 
 #include "spdk/jsonrpc.h"
 
-typedef void (*spdk_rpc_method_handler)(struct spdk_jsonrpc_server_conn *conn,
-					const struct spdk_json_val *params,
-					const struct spdk_json_val *id);
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/**
+ * Start listening for RPC connections.
+ *
+ * \param listen_addr Listening address.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_rpc_listen(const char *listen_addr);
+
+/**
+ * Poll the RPC server.
+ */
+void spdk_rpc_accept(void);
 
-void spdk_rpc_register_method(const char *method, spdk_rpc_method_handler func);
+/**
+ * Stop listening for RPC connections.
+ */
+void spdk_rpc_close(void);
+
+/**
+ * Function signature for RPC request handlers.
+ *
+ * \param request RPC request to handle.
+ * \param params Parameters associated with the RPC request.
+ */
+typedef void (*spdk_rpc_method_handler)(struct spdk_jsonrpc_request *request,
+					const struct spdk_json_val *params);
+
+/**
+ * Register an RPC method.
+ *
+ * \param method Name for the registered method.
+ * \param func Function registered for this method to handle the RPC request.
+ * \param state_mask State mask of the registered method. If the bit of the state of
+ * the RPC server is set in the state_mask, the method is allowed. Otherwise, it is rejected.
+ */
+void spdk_rpc_register_method(const char *method, spdk_rpc_method_handler func,
+			      uint32_t state_mask);
 
-#define SPDK_RPC_REGISTER(method, func) \
+/**
+ * Check if \c method is allowed for \c state_mask
+ *
+ * \param method Method name
+ * \param state_mask state mask to check against
+ * \return 0 if method is allowed or negative error code:
+ * -EPERM method is not allowed
+ * -ENOENT method not found
+ */
+int spdk_rpc_is_method_allowed(const char *method, uint32_t state_mask);
+
+#define SPDK_RPC_STARTUP	0x1
+#define SPDK_RPC_RUNTIME	0x2
+
+#define SPDK_RPC_REGISTER(method, func, state_mask) \
 static void __attribute__((constructor)) rpc_register_##func(void) \
 { \
-	spdk_rpc_register_method(method, func); \
+	spdk_rpc_register_method(method, func, state_mask); \
 }
 
+/**
+ * Set the state mask of the RPC server. Any RPC method whose state mask is
+ * equal to the state of the RPC server is allowed.
+ *
+ * \param state_mask New state mask of the RPC server.
+ */
+void spdk_rpc_set_state(uint32_t state_mask);
+
+/**
+ * Get the current state of the RPC server.
+ *
+ * \return The current state of the RPC server.
+ */
+uint32_t spdk_rpc_get_state(void);
+
+#ifdef __cplusplus
+}
+#endif
+
 #endif
diff --git a/PDK/core/src/api/include/udd/spdk/scsi.h b/PDK/core/src/api/include/udd/spdk/scsi.h
index d245779..1e92270 100644
--- a/PDK/core/src/api/include/udd/spdk/scsi.h
+++ b/PDK/core/src/api/include/udd/spdk/scsi.h
@@ -33,7 +33,7 @@
 
 /**
  * \file
- * SCSI to blockdev translation layer
+ * SCSI to bdev translation layer
  */
 
 #ifndef SPDK_SCSI_H
@@ -41,8 +41,12 @@
 
 #include "spdk/stdinc.h"
 
+#include "spdk/bdev.h"
 #include "spdk/queue.h"
-#include "spdk/event.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
 
 /* Defines for SPDK tracing framework */
 #define OWNER_SCSI_DEV				0x10
@@ -57,8 +61,7 @@
 #define SPDK_SCSI_DEV_MAX_NAME			255
 
 #define SPDK_SCSI_PORT_MAX_NAME_LENGTH		255
-
-#define SPDK_SCSI_LUN_MAX_NAME_LENGTH		16
+#define SPDK_SCSI_MAX_TRANSPORT_ID_LENGTH	255
 
 enum spdk_scsi_data_dir {
 	SPDK_SCSI_DIR_NONE = 0,
@@ -73,11 +76,6 @@ enum spdk_scsi_task_func {
 	SPDK_SCSI_TASK_FUNC_LUN_RESET,
 };
 
-enum spdk_scsi_task_type {
-	SPDK_SCSI_TASK_TYPE_CMD = 0,
-	SPDK_SCSI_TASK_TYPE_MANAGE,
-};
-
 /*
  * SAM does not define the value for these service responses.  Each transport
  *  (i.e. SAS, FC, iSCSI) will map these value to transport-specific codes,
@@ -92,16 +90,21 @@ enum spdk_scsi_task_mgmt_resp {
 	SPDK_SCSI_TASK_MGMT_RESP_REJECT_FUNC_NOT_SUPPORTED
 };
 
+struct spdk_scsi_task;
+typedef void (*spdk_scsi_task_cpl)(struct spdk_scsi_task *task);
+typedef void (*spdk_scsi_task_free)(struct spdk_scsi_task *task);
+
 struct spdk_scsi_task {
-	uint8_t				type;
 	uint8_t				status;
 	uint8_t				function; /* task mgmt function */
 	uint8_t				response; /* task mgmt response */
+
 	struct spdk_scsi_lun		*lun;
-	struct spdk_io_channel		*ch;
 	struct spdk_scsi_port		*target_port;
 	struct spdk_scsi_port		*initiator_port;
-	struct spdk_event 		*cb_event;
+
+	spdk_scsi_task_cpl		cpl_fn;
+	spdk_scsi_task_free		free_fn;
 
 	uint32_t ref;
 	uint32_t transfer_len;
@@ -115,9 +118,6 @@ struct spdk_scsi_task {
 	uint32_t data_transferred;
 
 	uint64_t offset;
-	struct spdk_scsi_task *parent;
-
-	void (*free_fn)(struct spdk_scsi_task *);
 
 	uint8_t *cdb;
 
@@ -137,116 +137,433 @@ struct spdk_scsi_task {
 	uint8_t sense_data[32];
 	size_t sense_data_len;
 
-	void *blockdev_io;
+	void *bdev_io;
 
 	TAILQ_ENTRY(spdk_scsi_task) scsi_link;
 
 	uint32_t abort_id;
+	struct spdk_bdev_io_wait_entry bdev_io_wait;
 };
 
 struct spdk_scsi_port;
-
 struct spdk_scsi_dev;
+struct spdk_scsi_lun;
+struct spdk_scsi_lun_desc;
+
+typedef void (*spdk_scsi_lun_remove_cb_t)(struct spdk_scsi_lun *, void *);
+typedef void (*spdk_scsi_dev_destruct_cb_t)(void *cb_arg, int rc);
 
 /**
- * \brief Represents a SCSI LUN.
+ * Initialize SCSI layer.
  *
- * LUN modules will implement the function pointers specifically for the LUN
- * type.  For example, NVMe LUNs will implement scsi_execute to translate
- * the SCSI task to an NVMe command and post it to the NVMe controller.
- * malloc LUNs will implement scsi_execute to translate the SCSI task and
- * copy the task's data into or out of the allocated memory buffer.
+ * \return 0 on success, -1 on failure.
  */
-struct spdk_scsi_lun;
+int spdk_scsi_init(void);
 
+/**
+ * Stop and clean the SCSI layer.
+ */
+void spdk_scsi_fini(void);
+
+/**
+ * Get the LUN id of the given logical unit.
+ *
+ * \param lun Logical unit.
+ *
+ * \return LUN id of the logical unit.
+ */
 int spdk_scsi_lun_get_id(const struct spdk_scsi_lun *lun);
-const char *spdk_scsi_lun_get_name(const struct spdk_scsi_lun *lun);
 
+/**
+ * Get the name of the bdev associated with the given logical unit.
+ *
+ * \param lun Logical unit.
+ *
+ * \return the name of the bdev associated with the logical unit.
+ */
+const char *spdk_scsi_lun_get_bdev_name(const struct spdk_scsi_lun *lun);
+
+/**
+ * Get the SCSI device associated with the given logical unit.
+ *
+ * \param lun Logical unit.
+ *
+ * \return the SCSI device associated with the logical unit.
+ */
+const struct spdk_scsi_dev *spdk_scsi_lun_get_dev(const struct spdk_scsi_lun *lun);
+
+/**
+ * Check if the logical unit is hot removing.
+ *
+ * \param lun Logical unit
+ *
+ * \return true if removing, false otherwise.
+ */
+bool spdk_scsi_lun_is_removing(const struct spdk_scsi_lun *lun);
+
+/**
+ * Get the name of the given SCSI device.
+ *
+ * \param dev SCSI device.
+ *
+ * \return the name of the SCSI device on success, or NULL on failure.
+ */
 const char *spdk_scsi_dev_get_name(const struct spdk_scsi_dev *dev);
+
+/**
+ * Get the id of the given SCSI device.
+ *
+ * \param dev SCSI device.
+ *
+ * \return the id of the SCSI device.
+ */
 int spdk_scsi_dev_get_id(const struct spdk_scsi_dev *dev);
-int spdk_scsi_dev_get_max_lun(const struct spdk_scsi_dev *dev);
+
+/**
+ * Get the logical unit of the given SCSI device whose id is lun_id.
+ *
+ * \param dev SCSI device.
+ * \param lun_id Id of the logical unit.
+ *
+ * \return the logical unit on success, or NULL on failure.
+ */
 struct spdk_scsi_lun *spdk_scsi_dev_get_lun(struct spdk_scsi_dev *dev, int lun_id);
-void spdk_scsi_dev_destruct(struct spdk_scsi_dev *dev);
+
+/**
+ * Check whether the SCSI device has any pending task.
+ *
+ * \param dev SCSI device.
+ *
+ * \return true if the SCSI device has any pending task, or false otherwise.
+ */
+bool spdk_scsi_dev_has_pending_tasks(const struct spdk_scsi_dev *dev);
+
+/**
+ * Destruct the SCSI decice.
+ *
+ * \param dev SCSI device.
+ * \param cb_fn Callback function.
+ * \param cb_arg Argument to callback function.
+ */
+void spdk_scsi_dev_destruct(struct spdk_scsi_dev *dev,
+			    spdk_scsi_dev_destruct_cb_t cb_fn, void *cb_arg);
+
+/**
+ * Execute the SCSI management task.
+ *
+ * The task can be constructed by the function spdk_scsi_task_construct().
+ * Code of task management function to be executed is set before calling this API.
+ *
+ * \param dev SCSI device.
+ * \param task SCSI task to be executed.
+ */
 void spdk_scsi_dev_queue_mgmt_task(struct spdk_scsi_dev *dev, struct spdk_scsi_task *task);
+
+/**
+ * Execute the SCSI task.
+ *
+ * The task can be constructed by the function spdk_scsi_task_construct().
+ *
+ * \param dev SCSI device.
+ * \param task Task to be executed.
+ */
 void spdk_scsi_dev_queue_task(struct spdk_scsi_dev *dev, struct spdk_scsi_task *task);
+
+/**
+ * Add a new port to the given SCSI device.
+ *
+ * \param dev SCSI device.
+ * \param id Port id.
+ * \param name Port name.
+ *
+ * \return 0 on success, -1 on failure.
+ */
 int spdk_scsi_dev_add_port(struct spdk_scsi_dev *dev, uint64_t id, const char *name);
+
+/**
+ * Delete a specified port of the given SCSI device.
+ *
+ * \param dev SCSI device.
+ * \param id Port id.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_scsi_dev_delete_port(struct spdk_scsi_dev *dev, uint64_t id);
+
+/**
+ * Get the port of the given SCSI device whose port ID is id.
+ *
+ * \param dev SCSI device.
+ * \param id Port id.
+ *
+ * \return the port of the SCSI device on success, or NULL on failure.
+ */
 struct spdk_scsi_port *spdk_scsi_dev_find_port_by_id(struct spdk_scsi_dev *dev, uint64_t id);
-void spdk_scsi_dev_print(struct spdk_scsi_dev *dev);
+
+/**
+ * Allocate I/O channels for all LUNs of the given SCSI device.
+ *
+ * \param dev SCSI device.
+ *
+ * \return 0 on success, -1 on failure.
+ */
 int spdk_scsi_dev_allocate_io_channels(struct spdk_scsi_dev *dev);
+
+/**
+ * Free I/O channels from all LUNs of the given SCSI device.
+ */
 void spdk_scsi_dev_free_io_channels(struct spdk_scsi_dev *dev);
 
 /**
- * \brief Constructs a SCSI device object using the given parameters.
+ * Construct a SCSI device object using the given parameters.
  *
  * \param name Name for the SCSI device.
- * \param queue_depth Queue depth for the SCSI device.  This queue depth is
- * 		      a combined queue depth for all LUNs in the device.
- * \param lun_list List of LUN objects for the SCSI device.  Caller is
- * 		   responsible for managing the memory containing this list.
- * \param lun_id_list List of LUN IDs for the LUN in this SCSI device.  Caller is
- *		      responsible for managing the memory containing this list.
- *		      lun_id_list[x] is the LUN ID for lun_list[x].
+ * \param bdev_name_list List of bdev names to attach to the LUNs for this SCSI
+ * device.
+ * \param lun_id_list List of LUN IDs for the LUN in this SCSI device. Caller is
+ * responsible for managing the memory containing this list. lun_id_list[x] is
+ * the LUN ID for lun_list[x].
  * \param num_luns Number of entries in lun_list and lun_id_list.
- * \return The constructed spdk_scsi_dev object.
+ * \param protocol_id SCSI SPC protocol identifier to report in INQUIRY data
+ * \param hotremove_cb Callback to lun hotremoval. Will be called once hotremove
+ * is first triggered.
+ * \param hotremove_ctx Additional argument to hotremove_cb.
+ *
+ * \return the constructed spdk_scsi_dev object.
  */
 struct spdk_scsi_dev *spdk_scsi_dev_construct(const char *name,
-		char *lun_name_list[],
+		const char *bdev_name_list[],
 		int *lun_id_list,
-		int num_luns);
+		int num_luns,
+		uint8_t protocol_id,
+		void (*hotremove_cb)(const struct spdk_scsi_lun *, void *),
+		void *hotremove_ctx);
 
+/**
+ * Delete a logical unit of the given SCSI device.
+ *
+ * \param dev SCSI device.
+ * \param lun Logical unit to delete.
+ */
 void spdk_scsi_dev_delete_lun(struct spdk_scsi_dev *dev, struct spdk_scsi_lun *lun);
 
+/**
+ * Add a new logical unit to the given SCSI device.
+ *
+ * \param dev SCSI device.
+ * \param bdev_name Name of the bdev attached to the logical unit.
+ * \param lun_id LUN id for the new logical unit.
+ * \param hotremove_cb Callback to lun hotremoval. Will be called once hotremove
+ * is first triggered.
+ * \param hotremove_ctx Additional argument to hotremove_cb.
+ */
+int spdk_scsi_dev_add_lun(struct spdk_scsi_dev *dev, const char *bdev_name, int lun_id,
+			  void (*hotremove_cb)(const struct spdk_scsi_lun *, void *),
+			  void *hotremove_ctx);
 
+/**
+ * Create a new SCSI port.
+ *
+ * \param id Port id.
+ * \param index Port index.
+ * \param name Port Name.
+ *
+ * \return a pointer to the created SCSI port on success, or NULL on failure.
+ */
 struct spdk_scsi_port *spdk_scsi_port_create(uint64_t id, uint16_t index, const char *name);
+
+/**
+ * Free the SCSI port.
+ *
+ * \param pport SCSI port to free.
+ */
 void spdk_scsi_port_free(struct spdk_scsi_port **pport);
-const char *spdk_scsi_port_get_name(const struct spdk_scsi_port *port);
 
+/**
+ * Get the name of the SCSI port.
+ *
+ * \param port SCSI port to query.
+ *
+ * \return the name of the SCSI port.
+ */
+const char *spdk_scsi_port_get_name(const struct spdk_scsi_port *port);
 
+/**
+ * Construct a new SCSI task.
+ *
+ * \param task SCSI task to consturct.
+ * \param cpl_fn Called when the task is completed.
+ * \param free_fn Called when the task is freed
+ */
 void spdk_scsi_task_construct(struct spdk_scsi_task *task,
-			      void (*free_fn)(struct spdk_scsi_task *task),
-			      struct spdk_scsi_task *parent);
+			      spdk_scsi_task_cpl cpl_fn,
+			      spdk_scsi_task_free free_fn);
+
+/**
+ * Put the SCSI task.
+ *
+ * \param task SCSI task to put.
+ */
 void spdk_scsi_task_put(struct spdk_scsi_task *task);
 
-void spdk_scsi_task_free_data(struct spdk_scsi_task *task);
 /**
  * Set internal buffer to given one. Caller is owner of that buffer.
  *
- * \param task Task struct
- * \param data Pointer to buffer
- * \param len Buffer length
+ * \param task SCSI task.
+ * \param data Pointer to buffer.
+ * \param len Buffer length.
  */
 void spdk_scsi_task_set_data(struct spdk_scsi_task *task, void *data, uint32_t len);
 
 /**
- * Allocate internal buffer of requested size. Caller is not owner of
- * returned buffer and must not free it. Caller is permitted to call
- * spdk_scsi_task_free_data() to free internal buffer if it is not required
- * anymore, but must assert that task is done and not used by library.
+ * Single buffer -> vector of buffers.
  *
- * Allocated buffer is stored in iov field of task object.
+ * \param task SCSI task.
+ * \param src A pointer to the data buffer read from.
+ * \param len Length of the data buffer read from.
  *
- * \param task Task struct
- * \param alloc_len Size of allocated buffer.
- * \return Pointer to buffer or NULL on error.
+ * \return the total length of the vector of buffers written into on success, or
+ * -1 on failure.
  */
-void *spdk_scsi_task_alloc_data(struct spdk_scsi_task *task, uint32_t alloc_len);
-
 int spdk_scsi_task_scatter_data(struct spdk_scsi_task *task, const void *src, size_t len);
+
+/**
+ * Vector of buffers -> single buffer.
+ *
+ * \param task SCSI task,
+ * \param len Length of the buffer allocated and written into.
+ *
+ * \return a pointer to the buffer allocated and written into.
+ */
 void *spdk_scsi_task_gather_data(struct spdk_scsi_task *task, int *len);
+
+/**
+ * Build sense data for the SCSI task.
+ *
+ * \param task SCSI task.
+ * \param sk Sense key.
+ * \param asc Additional sense code.
+ * \param ascq Additional sense code qualifier.
+ */
 void spdk_scsi_task_build_sense_data(struct spdk_scsi_task *task, int sk, int asc,
 				     int ascq);
+
+/**
+ * Set SCSI status code to the SCSI task. When the status code is CHECK CONDITION,
+ * sense data is build too.
+ *
+ * \param task SCSI task.
+ * \param sc Sense code
+ * \param sk Sense key.
+ * \param asc Additional sense code.
+ * \param ascq Additional sense code qualifier.
+ */
 void spdk_scsi_task_set_status(struct spdk_scsi_task *task, int sc, int sk, int asc,
 			       int ascq);
+
+/**
+ * Copy SCSI status.
+ *
+ * \param dst SCSI task whose status is written to.
+ * \param src SCSI task whose status is read from.
+ */
+void spdk_scsi_task_copy_status(struct spdk_scsi_task *dst, struct spdk_scsi_task *src);
+
+/**
+ * Process the SCSI task when no LUN is attached.
+ *
+ * \param task SCSI task.
+ */
 void spdk_scsi_task_process_null_lun(struct spdk_scsi_task *task);
 
-static inline struct spdk_scsi_task *
-spdk_scsi_task_get_primary(struct spdk_scsi_task *task)
-{
-	if (task->parent) {
-		return task->parent;
-	} else {
-		return task;
-	}
+/**
+ * Process the aborted SCSI task.
+ *
+ * \param task SCSI task.
+ */
+void spdk_scsi_task_process_abort(struct spdk_scsi_task *task);
+
+/**
+ * Open a logical unit for I/O operations.
+ *
+ * The registered callback function must get all tasks from the upper layer
+ *  (e.g. iSCSI) to the LUN done, free the IO channel of the LUN if allocated,
+ *  and then close the LUN.
+ *
+ * \param lun Logical unit to open.
+ * \param hotremove_cb Callback function for hot removal of the logical unit.
+ * \param hotremove_ctx Param for hot removal callback function.
+ * \param desc Output parameter for the descriptor when operation is successful.
+ * \return 0 if operation is successful, suitable errno value otherwise
+ */
+int spdk_scsi_lun_open(struct spdk_scsi_lun *lun, spdk_scsi_lun_remove_cb_t hotremove_cb,
+		       void *hotremove_ctx, struct spdk_scsi_lun_desc **desc);
+
+/**
+ * Close an opened logical unit.
+ *
+ * \param desc Descriptor of the logical unit.
+ */
+void spdk_scsi_lun_close(struct spdk_scsi_lun_desc *desc);
+
+/**
+ * Allocate I/O channel for the LUN
+ *
+ * \param desc Descriptor of the logical unit.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_scsi_lun_allocate_io_channel(struct spdk_scsi_lun_desc *desc);
+
+/**
+ * Free I/O channel from the logical unit
+ *
+ * \param desc Descriptor of the logical unit.
+ */
+void spdk_scsi_lun_free_io_channel(struct spdk_scsi_lun_desc *desc);
+
+/**
+ * Get DIF context for SCSI LUN and SCSI command.
+ *
+ * \param lun Logical unit.
+ * \param cdb SCSI CDB.
+ * \param offset Offset in the payload.
+ * \param dif_ctx Output parameter which will contain initialized DIF context.
+ *
+ * \return true on success or false otherwise.
+ */
+bool spdk_scsi_lun_get_dif_ctx(struct spdk_scsi_lun *lun, uint8_t *cdb, uint32_t offset,
+			       struct spdk_dif_ctx *dif_ctx);
+
+/**
+ * Set iSCSI Initiator port TransportID
+ *
+ * \param port SCSI initiator port.
+ * \param iscsi_name Initiator name.
+ * \param isid Session ID.
+ */
+void spdk_scsi_port_set_iscsi_transport_id(struct spdk_scsi_port *port,
+		char *iscsi_name, uint64_t isid);
+
+/**
+ * Convert LUN ID from integer to LUN format
+ *
+ * \param lun_id Integer LUN ID
+ *
+ * \return LUN format of LUN ID
+ */
+uint64_t spdk_scsi_lun_id_int_to_fmt(int lun_id);
+
+/**
+ * Convert LUN ID from LUN format to integer
+ *
+ * \param fmt_lun LUN format of LUN ID
+ *
+ * \return integer LUN ID
+ */
+int spdk_scsi_lun_id_fmt_to_int(uint64_t fmt_lun);
+#ifdef __cplusplus
 }
+#endif
 
 #endif /* SPDK_SCSI_H */
diff --git a/PDK/core/src/api/include/udd/spdk/scsi_spec.h b/PDK/core/src/api/include/udd/spdk/scsi_spec.h
index 6163eb2..2bca212 100644
--- a/PDK/core/src/api/include/udd/spdk/scsi_spec.h
+++ b/PDK/core/src/api/include/udd/spdk/scsi_spec.h
@@ -232,6 +232,8 @@ enum spdk_sbc_opcode {
 	SPDK_SBC_VL_XPWRITE_32 = 0x0006,
 };
 
+#define SPDK_SBC_START_STOP_UNIT_START_BIT (1 << 0)
+
 enum spdk_mmc_opcode {
 	/* MMC6 */
 	SPDK_MMC_READ_DISC_STRUCTURE = 0xad,
@@ -328,6 +330,12 @@ enum spdk_spc_vpd {
 	SPDK_SPC_VPD_BLOCK_THIN_PROVISION = 0xb2,
 };
 
+enum spdk_spc_peripheral_qualifier {
+	SPDK_SPC_PERIPHERAL_QUALIFIER_CONNECTED = 0,
+	SPDK_SPC_PERIPHERAL_QUALIFIER_NOT_CONNECTED = 1,
+	SPDK_SPC_PERIPHERAL_QUALIFIER_NOT_CAPABLE = 3,
+};
+
 enum {
 	SPDK_SPC_PERIPHERAL_DEVICE_TYPE_DISK = 0x00,
 	SPDK_SPC_PERIPHERAL_DEVICE_TYPE_TAPE = 0x01,
@@ -379,7 +387,8 @@ struct spdk_scsi_cdb_inquiry {
 SPDK_STATIC_ASSERT(sizeof(struct spdk_scsi_cdb_inquiry) == 6, "incorrect CDB size");
 
 struct spdk_scsi_cdb_inquiry_data {
-	uint8_t peripheral;
+	uint8_t peripheral_device_type : 5;
+	uint8_t peripheral_qualifier : 3;
 	uint8_t rmb;
 	uint8_t version;
 	uint8_t response;
@@ -397,7 +406,8 @@ struct spdk_scsi_cdb_inquiry_data {
 };
 
 struct spdk_scsi_vpd_page {
-	uint8_t peripheral;
+	uint8_t peripheral_device_type : 5;
+	uint8_t peripheral_qualifier : 3;
 	uint8_t page_code;
 	uint8_t alloc_len[2];
 	uint8_t params[];
@@ -475,6 +485,17 @@ struct spdk_scsi_port_desc {
 	uint8_t tgt_desc[];
 };
 
+/* iSCSI initiator port TransportID header */
+struct spdk_scsi_iscsi_transport_id {
+	uint8_t protocol_id : 4;
+	uint8_t reserved1   : 2;
+	uint8_t format      : 2;
+	uint8_t reserved2;
+	uint16_t additional_len;
+	uint8_t name[];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_scsi_iscsi_transport_id) == 4, "Incorrect size");
+
 /* SCSI UNMAP block descriptor */
 struct spdk_scsi_unmap_bdesc {
 	/* UNMAP LOGICAL BLOCK ADDRESS */
@@ -487,7 +508,230 @@ struct spdk_scsi_unmap_bdesc {
 	uint32_t reserved;
 };
 
-#define SPDK_SCSI_UNMAP_LBPU  			1 << 7
+/* SCSI Persistent Reserve In action codes */
+enum spdk_scsi_pr_in_action_code {
+	/* Read all registered reservation keys */
+	SPDK_SCSI_PR_IN_READ_KEYS		= 0x00,
+	/* Read current persistent reservations */
+	SPDK_SCSI_PR_IN_READ_RESERVATION	= 0x01,
+	/* Return capabilities information */
+	SPDK_SCSI_PR_IN_REPORT_CAPABILITIES	= 0x02,
+	/* Read all registrations and persistent reservations */
+	SPDK_SCSI_PR_IN_READ_FULL_STATUS	= 0x03,
+	/* 0x04h - 0x1fh Reserved */
+};
+
+enum spdk_scsi_pr_scope_code {
+	/* Persistent reservation applies to full logical unit */
+	SPDK_SCSI_PR_LU_SCOPE			= 0x00,
+};
+
+/* SCSI Persistent Reservation type codes */
+enum spdk_scsi_pr_type_code {
+	/* Write Exclusive */
+	SPDK_SCSI_PR_WRITE_EXCLUSIVE		= 0x01,
+	/* Exclusive Access */
+	SPDK_SCSI_PR_EXCLUSIVE_ACCESS		= 0x03,
+	/* Write Exclusive - Registrants Only */
+	SPDK_SCSI_PR_WRITE_EXCLUSIVE_REGS_ONLY	= 0x05,
+	/* Exclusive Access - Registrants Only */
+	SPDK_SCSI_PR_EXCLUSIVE_ACCESS_REGS_ONLY	= 0x06,
+	/* Write Exclusive - All Registrants */
+	SPDK_SCSI_PR_WRITE_EXCLUSIVE_ALL_REGS	= 0x07,
+	/* Exclusive Access - All Registrants */
+	SPDK_SCSI_PR_EXCLUSIVE_ACCESS_ALL_REGS	= 0x08,
+};
+
+/* SCSI Persistent Reserve In header for
+ * Read Keys, Read Reservation, Read Full Status
+ */
+struct spdk_scsi_pr_in_read_header {
+	/* persistent reservation generation */
+	uint32_t pr_generation;
+	uint32_t addiontal_len;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_scsi_pr_in_read_header) == 8, "Incorrect size");
+
+/* SCSI Persistent Reserve In read keys data */
+struct spdk_scsi_pr_in_read_keys_data {
+	struct spdk_scsi_pr_in_read_header header;
+	/* reservation key list */
+	uint64_t rkeys[];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_scsi_pr_in_read_keys_data) == 8, "Incorrect size");
+
+/* SCSI Persistent Reserve In read reservations data */
+struct spdk_scsi_pr_in_read_reservations_data {
+	/* Fixed 0x10 with reservation and 0 for no reservation */
+	struct spdk_scsi_pr_in_read_header header;
+	/* reservation key */
+	uint64_t rkey;
+	uint32_t obsolete1;
+	uint8_t reserved;
+	uint8_t type  : 4;
+	uint8_t scope : 4;
+	uint16_t obsolete2;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_scsi_pr_in_read_reservations_data) == 24, "Incorrect size");
+
+/* SCSI Persistent Reserve In report capabilities data */
+struct spdk_scsi_pr_in_report_capabilities_data {
+	/* Fixed value 0x8 */
+	uint16_t length;
+
+	/* Persist through power loss capable */
+	uint8_t ptpl_c    : 1;
+	uint8_t reserved1 : 1;
+	/* All target ports capable */
+	uint8_t atp_c     : 1;
+	/* Specify initiator port capable */
+	uint8_t sip_c     : 1;
+	/* Compatible reservation handing bit to indicate
+	 * SPC-2 reserve/release is supported
+	 */
+	uint8_t crh       : 1;
+	uint8_t reserved2 : 3;
+	/* Persist through power loss activated */
+	uint8_t ptpl_a    : 1;
+	uint8_t reserved3 : 6;
+	/* Type mask valid */
+	uint8_t tmv       : 1;
+
+	/* Type mask format */
+	uint8_t reserved4 : 1;
+	/* Write Exclusive */
+	uint8_t wr_ex     : 1;
+	uint8_t reserved5 : 1;
+	/* Exclusive Access */
+	uint8_t ex_ac     : 1;
+	uint8_t reserved6 : 1;
+	/* Write Exclusive - Registrants Only */
+	uint8_t wr_ex_ro  : 1;
+	/* Exclusive Access - Registrants Only */
+	uint8_t ex_ac_ro  : 1;
+	/* Write Exclusive - All Registrants */
+	uint8_t wr_ex_ar  : 1;
+	/* Exclusive Access - All Registrants */
+	uint8_t ex_ac_ar  : 1;
+	uint8_t reserved7 : 7;
+
+	uint8_t reserved8[2];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_scsi_pr_in_report_capabilities_data) == 8, "Incorrect size");
+
+/* SCSI Persistent Reserve In full status descriptor */
+struct spdk_scsi_pr_in_full_status_desc {
+	/* Reservation key */
+	uint64_t rkey;
+	uint8_t reserved1[4];
+
+	/* 0 - Registrant only
+	 * 1 - Registrant and reservation holder
+	 */
+	uint8_t r_holder  : 1;
+	/* All target ports */
+	uint8_t all_tg_pt : 1;
+	uint8_t reserved2 : 6;
+
+	/* Reservation type */
+	uint8_t type      : 4;
+	/* Set to LU_SCOPE */
+	uint8_t scope     : 4;
+
+	uint8_t reserved3[4];
+	uint16_t relative_target_port_id;
+	/* Size of TransportID */
+	uint32_t desc_len;
+
+	uint8_t transport_id[];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_scsi_pr_in_full_status_desc) == 24, "Incorrect size");
+
+/* SCSI Persistent Reserve In full status data */
+struct spdk_scsi_pr_in_full_status_data {
+	struct spdk_scsi_pr_in_read_header header;
+	/* Full status descriptors */
+	struct spdk_scsi_pr_in_full_status_desc desc_list[];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_scsi_pr_in_full_status_data) == 8, "Incorrect size");
+
+/* SCSI Persistent Reserve Out service action codes */
+enum spdk_scsi_pr_out_service_action_code {
+	/* Register/unregister a reservation key */
+	SPDK_SCSI_PR_OUT_REGISTER		= 0x00,
+	/* Create a persistent reservation */
+	SPDK_SCSI_PR_OUT_RESERVE		= 0x01,
+	/* Release a persistent reservation */
+	SPDK_SCSI_PR_OUT_RELEASE		= 0x02,
+	/* Clear all reservation keys and persistent reservations */
+	SPDK_SCSI_PR_OUT_CLEAR			= 0x03,
+	/* Preempt persistent reservations and/or remove registrants */
+	SPDK_SCSI_PR_OUT_PREEMPT		= 0x04,
+	/* Preempt persistent reservations and or remove registrants
+	 * and abort all tasks for all preempted I_T nexuses
+	 */
+	SPDK_SCSI_PR_OUT_PREEMPT_AND_ABORT	= 0x05,
+	/* Register/unregister a reservation key based on the ignore bit */
+	SPDK_SCSI_PR_OUT_REG_AND_IGNORE_KEY	= 0x06,
+	/* Register a reservation key for another I_T nexus
+	 * and move a persistent reservation to that I_T nexus
+	 */
+	SPDK_SCSI_PR_OUT_REG_AND_MOVE		= 0x07,
+	/* 0x08 - 0x1f Reserved */
+};
+
+/* SCSI Persistent Reserve Out parameter list */
+struct spdk_scsi_pr_out_param_list {
+	/* Reservation key */
+	uint64_t rkey;
+	/* Service action reservation key */
+	uint64_t sa_rkey;
+	uint8_t obsolete1[4];
+
+	/* Active persist through power loss */
+	uint8_t aptpl     : 1;
+	uint8_t reserved1 : 1;
+	/* All target ports */
+	uint8_t all_tg_pt : 1;
+	/* Specify initiator ports */
+	uint8_t spec_i_pt : 1;
+	uint8_t reserved2 : 4;
+
+	uint8_t reserved3;
+	uint16_t obsolete2;
+
+	uint8_t param_data[];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_scsi_pr_out_param_list) == 24, "Incorrect size");
+
+struct spdk_scsi_pr_out_reg_and_move_param_list {
+	/* Reservation key */
+	uint64_t rkey;
+	/* Service action reservation key */
+	uint64_t sa_rkey;
+	uint8_t reserved1;
+
+	/* Active persist through power loss */
+	uint8_t aptpl     : 1;
+	/* Unregister */
+	uint8_t unreg     : 1;
+	uint8_t reserved2 : 6;
+
+	uint16_t relative_target_port_id;
+	/* TransportID parameter data length */
+	uint32_t transport_id_len;
+	uint8_t transport_id[];
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_scsi_pr_out_reg_and_move_param_list) == 24, "Incorrect size");
+
+/*
+ * SPC-4
+ * Table-258 SECURITY PROTOCOL field in SECURITY PROTOCOL IN command
+ */
+#define SPDK_SCSI_SECP_INFO	0x00
+#define SPDK_SCSI_SECP_TCG	0x01
+
+#define SPDK_SCSI_UNMAP_LBPU			1 << 7
 #define SPDK_SCSI_UNMAP_LBPWS			1 << 6
 #define SPDK_SCSI_UNMAP_LBPWS10			1 << 5
 
diff --git a/PDK/core/src/api/include/udd/spdk/sock.h b/PDK/core/src/api/include/udd/spdk/sock.h
new file mode 100644
index 0000000..6b84635
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/sock.h
@@ -0,0 +1,259 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/** \file
+ * TCP socket abstraction layer
+ */
+
+#ifndef SPDK_SOCK_H
+#define SPDK_SOCK_H
+
+#include "spdk/stdinc.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+struct spdk_sock;
+struct spdk_sock_group;
+
+/**
+ * Get client and server addresses of the given socket.
+ *
+ * \param sock Socket to get address.
+ * \param saddr A pointer to the buffer to hold the address of server.
+ * \param slen Length of the buffer 'saddr'.
+ * \param sport A pointer(May be NULL) to the buffer to hold the port info of server.
+ * \param caddr A pointer to the buffer to hold the address of client.
+ * \param clen Length of the buffer 'caddr'.
+ * \param cport A pointer(May be NULL) to the buffer to hold the port info of server.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_sock_getaddr(struct spdk_sock *sock, char *saddr, int slen, uint16_t *sport,
+		      char *caddr, int clen, uint16_t *cport);
+
+/**
+ * Create a socket, connect the socket to the specified address and port (of the
+ * server), and then return the socket. This function is used by client.
+ *
+ * \param ip IP address of the server.
+ * \param port Port number of the server.
+ *
+ * \return a pointer to the connected socket on success, or NULL on failure.
+ */
+struct spdk_sock *spdk_sock_connect(const char *ip, int port);
+
+/**
+ * Create a socket, bind the socket to the specified address and port and listen
+ * on the socket, and then return the socket. This function is used by server.
+ *
+ * \param ip IP address to listen on.
+ * \param port Port number.
+ *
+ * \return a pointer to the listened socket on success, or NULL on failure.
+ */
+struct spdk_sock *spdk_sock_listen(const char *ip, int port);
+
+/**
+ * Accept a new connection from a client on the specified socket and return a
+ * socket structure which holds the connection.
+ *
+ * \param sock Listening socket.
+ *
+ * \return a pointer to the accepted socket on success, or NULL on failure.
+ */
+struct spdk_sock *spdk_sock_accept(struct spdk_sock *sock);
+
+/**
+ * Close a socket.
+ *
+ * \param sock Socket to close.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_sock_close(struct spdk_sock **sock);
+
+/**
+ * Receive a message from the given socket.
+ *
+ * \param sock Socket to receive message.
+ * \param buf Pointer to a buffer to hold the data.
+ * \param len Length of the buffer.
+ *
+ * \return the length of the received message on success, -1 on failure.
+ */
+ssize_t spdk_sock_recv(struct spdk_sock *sock, void *buf, size_t len);
+
+/**
+ * Write message to the given socket from the I/O vector array.
+ *
+ * \param sock Socket to write to.
+ * \param iov I/O vector.
+ * \param iovcnt Number of I/O vectors in the array.
+ *
+ * \return the length of written message on success, -1 on failure.
+ */
+ssize_t spdk_sock_writev(struct spdk_sock *sock, struct iovec *iov, int iovcnt);
+
+/**
+ * Read message from the given socket to the I/O vector array.
+ *
+ * \param sock Socket to receive message.
+ * \param iov I/O vector.
+ * \param iovcnt Number of I/O vectors in the array.
+ *
+ * \return the length of the received message on success, -1 on failure.
+ */
+ssize_t spdk_sock_readv(struct spdk_sock *sock, struct iovec *iov, int iovcnt);
+
+/**
+ * Set the value used to specify the low water mark (in bytes) for this socket.
+ *
+ * \param sock Socket to set for.
+ * \param nbytes Value for recvlowat.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_sock_set_recvlowat(struct spdk_sock *sock, int nbytes);
+
+/**
+ * Set receive buffer size for the given socket.
+ *
+ * \param sock Socket to set buffer size for.
+ * \param sz Buffer size in bytes.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_sock_set_recvbuf(struct spdk_sock *sock, int sz);
+
+/**
+ * Set send buffer size for the given socket.
+ *
+ * \param sock Socket to set buffer size for.
+ * \param sz Buffer size in bytes.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_sock_set_sendbuf(struct spdk_sock *sock, int sz);
+
+/**
+ * Check whether the address of socket is ipv6.
+ *
+ * \param sock Socket to check.
+ *
+ * \return true if the address of socket is ipv6, or false otherwise.
+ */
+bool spdk_sock_is_ipv6(struct spdk_sock *sock);
+
+/**
+ * Check whether the address of socket is ipv4.
+ *
+ * \param sock Socket to check.
+ *
+ * \return true if the address of socket is ipv4, or false otherwise.
+ */
+bool spdk_sock_is_ipv4(struct spdk_sock *sock);
+
+/**
+ * Callback function for spdk_sock_group_add_sock().
+ *
+ * \param arg Argument for the callback function.
+ * \param group Socket group.
+ * \param sock Socket.
+ */
+typedef void (*spdk_sock_cb)(void *arg, struct spdk_sock_group *group, struct spdk_sock *sock);
+
+/**
+ * Create a new socket group.
+ *
+ * \return a pointer to the created group on success, or NULL on failure.
+ */
+struct spdk_sock_group *spdk_sock_group_create(void);
+
+/**
+ * Add a socket to the group.
+ *
+ * \param group Socket group.
+ * \param sock Socket to add.
+ * \param cb_fn Called when the operation completes.
+ * \param cb_arg Argument passed to the callback function.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_sock_group_add_sock(struct spdk_sock_group *group, struct spdk_sock *sock,
+			     spdk_sock_cb cb_fn, void *cb_arg);
+
+/**
+ * Remove a socket from the group.
+ *
+ * \param group Socket group.
+ * \param sock Socket to remove.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_sock_group_remove_sock(struct spdk_sock_group *group, struct spdk_sock *sock);
+
+/**
+ * Poll incoming events for each registered socket.
+ *
+ * \param group Group to poll.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_sock_group_poll(struct spdk_sock_group *group);
+
+/**
+ * Poll incoming events up to max_events for each registered socket.
+ *
+ * \param group Group to poll.
+ * \param max_events Number of maximum events to poll for each socket.
+ *
+ * \return the number of events on success, -1 on failure.
+ */
+int spdk_sock_group_poll_count(struct spdk_sock_group *group, int max_events);
+
+/**
+ * Close all registered sockets of the group and then remove the group.
+ *
+ * \param group Group to close.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_sock_group_close(struct spdk_sock_group **group);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* SPDK_SOCK_H */
diff --git a/PDK/core/src/api/include/udd/spdk/stdinc.h b/PDK/core/src/api/include/udd/spdk/stdinc.h
index 586a94f..f200223 100644
--- a/PDK/core/src/api/include/udd/spdk/stdinc.h
+++ b/PDK/core/src/api/include/udd/spdk/stdinc.h
@@ -85,6 +85,10 @@ extern "C" {
 #include <sys/un.h>
 #include <sys/user.h>
 #include <sys/wait.h>
+#include <regex.h>
+
+/* GNU extension */
+#include <getopt.h>
 
 #ifdef __cplusplus
 }
diff --git a/PDK/core/src/api/include/udd/spdk/string.h b/PDK/core/src/api/include/udd/spdk/string.h
index b0ee0ad..041010e 100644
--- a/PDK/core/src/api/include/udd/spdk/string.h
+++ b/PDK/core/src/api/include/udd/spdk/string.h
@@ -47,38 +47,85 @@ extern "C" {
 /**
  * sprintf with automatic buffer allocation.
  *
- * The return value is the formatted string,
- * which should be passed to free() when no longer needed,
- * or NULL on failure.
+ * The return value is the formatted string, which should be passed to free()
+ * when no longer needed.
+ *
+ * \param format Format for the string to print.
+ *
+ * \return the formatted string on success, or NULL on failure.
  */
 char *spdk_sprintf_alloc(const char *format, ...) __attribute__((format(printf, 1, 2)));
 
 /**
  * vsprintf with automatic buffer allocation.
  *
- * The return value is the formatted string,
- * which should be passed to free() when no longer needed,
- * or NULL on failure.
+ * The return value is the formatted string, which should be passed to free()
+ * when no longer needed.
+ *
+ * \param format Format for the string to print.
+ * \param args A value that identifies a variable arguments list.
+ *
+ * \return the formatted string on success, or NULL on failure.
  */
 char *spdk_vsprintf_alloc(const char *format, va_list args);
 
 /**
+ * Append string using vsprintf with automatic buffer re-allocation.
+ *
+ * The return value is the formatted string, in which the original string in
+ * buffer is unchanged and the specified formatted string is appended.
+ *
+ * The returned string should be passed to free() when no longer needed.
+ *
+ * If buffer is NULL, the call is equivalent to spdk_sprintf_alloc().
+ * If the call fails, the original buffer is left untouched.
+ *
+ * \param buffer Buffer which has a formatted string.
+ * \param format Format for the string to print.
+ *
+ * \return the formatted string on success, or NULL on failure.
+ */
+char *spdk_sprintf_append_realloc(char *buffer, const char *format, ...);
+
+/**
+ * Append string using vsprintf with automatic buffer re-allocation.
+ * The return value is the formatted string, in which the original string in
+ * buffer is unchanged and the specified formatted string is appended.
+ *
+ * The returned string should be passed to free() when no longer needed.
+ *
+ * If buffer is NULL, the call is equivalent to spdk_sprintf_alloc().
+ * If the call fails, the original buffer is left untouched.
+ *
+ * \param buffer Buffer which has a formatted string.
+ * \param format Format for the string to print.
+ * \param args A value that identifies a variable arguments list.
+ *
+ * \return the formatted string on success, or NULL on failure.
+ */
+char *spdk_vsprintf_append_realloc(char *buffer, const char *format, va_list args);
+
+/**
  * Convert string to lowercase in place.
  *
  * \param s String to convert to lowercase.
+ *
+ * \return the converted string.
  */
 char *spdk_strlwr(char *s);
 
 /**
  * Parse a delimited string with quote handling.
  *
- * \param stringp Pointer to starting location in string. *stringp will be updated to point to the
- * start of the next field, or NULL if the end of the string has been reached.
- * \param delim Null-terminated string containing the list of accepted delimiters.
+ * Note that the string will be modified in place to add the string terminator
+ * to each field.
  *
- * \return Pointer to beginning of the current field.
+ * \param stringp Pointer to starting location in string. *stringp will be updated
+ * to point to the start of the next field, or NULL if the end of the string has
+ * been reached.
+ * \param delim Null-terminated string containing the list of accepted delimiters.
  *
- * Note that the string will be modified in place to add the string terminator to each field.
+ * \return a pointer to beginning of the current field.
  */
 char *spdk_strsepq(char **stringp, const char *delim);
 
@@ -86,18 +133,52 @@ char *spdk_strsepq(char **stringp, const char *delim);
  * Trim whitespace from a string in place.
  *
  * \param s String to trim.
+ *
+ * \return the trimmed string.
  */
 char *spdk_str_trim(char *s);
 
 /**
- * Copy a string into a fixed-size buffer, padding extra bytes with a specific character.
+ * Copy the string version of an error into the user supplied buffer
+ *
+ * \param errnum Error code.
+ * \param buf Pointer to a buffer in which to place the error message.
+ * \param buflen The size of the buffer in bytes.
+ */
+void spdk_strerror_r(int errnum, char *buf, size_t buflen);
+
+/**
+ * Return the string version of an error from a static, thread-local buffer. This
+ * function is thread safe.
+ *
+ * \param errnum Error code.
+ *
+ * \return a pointer to buffer upon success.
+ */
+const char *spdk_strerror(int errnum);
+
+/**
+ * Remove trailing newlines from the end of a string in place.
+ *
+ * Any sequence of trailing \\r and \\n characters is removed from the end of the
+ * string.
+ *
+ * \param s String to remove newline from.
+ *
+ * \return the number of characters removed.
+ */
+size_t spdk_str_chomp(char *s);
+
+/**
+ * Copy a string into a fixed-size buffer, padding extra bytes with a specific
+ * character.
+ *
+ * If src is longer than size, only size bytes will be copied.
  *
  * \param dst Pointer to destination fixed-size buffer to fill.
  * \param src Pointer to source null-terminated string to copy into dst.
  * \param size Number of bytes to fill in dst.
  * \param pad Character to pad extra space in dst beyond the size of src.
- *
- * If src is longer than size, only size bytes will be copied.
  */
 void spdk_strcpy_pad(void *dst, const char *src, size_t size, int pad);
 
@@ -108,25 +189,81 @@ void spdk_strcpy_pad(void *dst, const char *src, size_t size, int pad);
  * \param size Size of the full string pointed to by str, including padding.
  * \param pad Character that was used to pad str up to size.
  *
- * \return Length of the non-padded portion of str.
+ * \return the length of the non-padded portion of str.
  */
 size_t spdk_strlen_pad(const void *str, size_t size, int pad);
 
 /**
- * Parse an IP address into its hostname and port components.
- * This modifies the IP address in place.
+ * Parse an IP address into its hostname and port components. This modifies the
+ * IP address in place.
  *
- * \param ip A null terminated IP address, including port.
- *           Both IPv4 and IPv6 are supported.
- * \param host Will point to the start of the hostname within ip.
- *             The string will be null terminated.
- * \param port Will point to the start of the port within ip.
- *             The string will be null terminated.
+ * \param ip A null terminated IP address, including port. Both IPv4 and IPv6
+ * are supported.
+ * \param host Will point to the start of the hostname within ip. The string will
+ * be null terminated.
+ * \param port Will point to the start of the port within ip. The string will be
+ * null terminated.
  *
- * \return 0 if successful. -1 on error.
+ * \return 0 on success. -EINVAL on failure.
  */
 int spdk_parse_ip_addr(char *ip, char **host, char **port);
 
+/**
+ * Parse a string representing a number possibly followed by a binary prefix.
+ *
+ * The string can contain a trailing "B" (KB,MB,GB) but it's not necessary.
+ * "128K" = 128 * 1024; "2G" = 2 * 1024 * 1024; "2GB" = 2 * 1024 * 1024;
+ * Additionally, lowercase "k", "m", "g" are parsed as well. They are processed
+ * the same as their uppercase equivalents.
+ *
+ * \param cap_str Null terminated string.
+ * \param cap Pointer where the parsed capacity (in bytes) will be put.
+ * \param has_prefix Pointer to a flag that will be set to describe whether given
+ * string contains a binary prefix.
+ *
+ * \return 0 on success, or negative errno on failure.
+ */
+int spdk_parse_capacity(const char *cap_str, uint64_t *cap, bool *has_prefix);
+
+/**
+ * Check if a buffer is all zero (0x00) bytes or not.
+ *
+ * \param data Buffer to check.
+ * \param size Size of data in bytes.
+ *
+ * \return true if data consists entirely of zeroes, or false if any byte in data
+ * is not zero.
+ */
+bool spdk_mem_all_zero(const void *data, size_t size);
+
+/**
+ * Convert the string in nptr to a long integer value according to the given base.
+ *
+ * spdk_strtol() does the additional error checking and allows only strings that
+ * contains only numbers and is positive number or zero. The caller only has to check
+ * if the return value is not negative.
+ *
+ * \param nptr String containing numbers.
+ * \param base Base which must be between 2 and 32 inclusive, or be the special value 0.
+ *
+ * \return positive number or zero on success, or negative errno on failure.
+ */
+long int spdk_strtol(const char *nptr, int base);
+
+/**
+ * Convert the string in nptr to a long long integer value according to the given base.
+ *
+ * spdk_strtoll() does the additional error checking and allows only strings that
+ * contains only numbers and is positive number or zero. The caller only has to check
+ * if the return value is not negative.
+ *
+ * \param nptr String containing numbers.
+ * \param base Base which must be between 2 and 32 inclusive, or be the special value 0.
+ *
+ * \return positive number or zero on success, or negative errno on failure.
+ */
+long long int spdk_strtoll(const char *nptr, int base);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/PDK/core/src/api/include/udd/spdk/thread.h b/PDK/core/src/api/include/udd/spdk/thread.h
new file mode 100644
index 0000000..281aec3
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/thread.h
@@ -0,0 +1,533 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/** \file
+ * Thread
+ */
+
+#ifndef SPDK_THREAD_H_
+#define SPDK_THREAD_H_
+
+#include "spdk/stdinc.h"
+
+#include "spdk/cpuset.h"
+#include "spdk/queue.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+struct spdk_thread;
+struct spdk_io_channel_iter;
+struct spdk_poller;
+
+/**
+ * A function that is called each time a new thread is created.
+ * The implementor of this function should frequently call
+ * spdk_thread_poll() on the thread provided.
+ *
+ * \param thread The new spdk_thread.
+ */
+typedef void (*spdk_new_thread_fn)(struct spdk_thread *thread);
+
+/**
+ * A function that will be called on the target thread.
+ *
+ * \param ctx Context passed as arg to spdk_thread_pass_msg().
+ */
+typedef void (*spdk_msg_fn)(void *ctx);
+
+/**
+ * Function to be called to pass a message to a thread.
+ *
+ * \param fn Callback function for a thread.
+ * \param ctx Context passed to fn.
+ * \param thread_ctx Context for the thread.
+ */
+typedef void (*spdk_thread_pass_msg)(spdk_msg_fn fn, void *ctx,
+				     void *thread_ctx);
+
+/**
+ * Callback function for a poller.
+ *
+ * \param ctx Context passed as arg to spdk_poller_register().
+ * \return 0 to indicate that polling took place but no events were found;
+ * positive to indicate that polling took place and some events were processed;
+ * negative if the poller does not provide spin-wait information.
+ */
+typedef int (*spdk_poller_fn)(void *ctx);
+
+/**
+ * Function to be called to start a poller for the thread.
+ *
+ * \param thread_ctx Context for the thread.
+ * \param fn Callback function for a poller.
+ * \param arg Argument passed to callback.
+ * \param period Polling period in microseconds.
+ *
+ * \return a pointer to the poller on success, or NULL on failure.
+ */
+typedef struct spdk_poller *(*spdk_start_poller)(void *thread_ctx,
+		spdk_poller_fn fn,
+		void *arg,
+		uint64_t period_microseconds);
+
+/**
+ * Function to be called to stop a poller.
+ *
+ * \param poller Poller to stop.
+ * \param thread_ctx Context for the thread.
+ */
+typedef void (*spdk_stop_poller)(struct spdk_poller *poller, void *thread_ctx);
+
+/**
+ * I/O channel creation callback.
+ *
+ * \param io_device I/O device associated with this channel.
+ * \param ctx_buf Context for the I/O device.
+ */
+typedef int (*spdk_io_channel_create_cb)(void *io_device, void *ctx_buf);
+
+/**
+ * I/O channel destruction callback.
+ *
+ * \param io_device I/O device associated with this channel.
+ * \param ctx_buf Context for the I/O device.
+ */
+typedef void (*spdk_io_channel_destroy_cb)(void *io_device, void *ctx_buf);
+
+/**
+ * I/O device unregister callback.
+ *
+ * \param io_device Unregistered I/O device.
+ */
+typedef void (*spdk_io_device_unregister_cb)(void *io_device);
+
+/**
+ * Called on the appropriate thread for each channel associated with io_device.
+ *
+ * \param i I/O channel iterator.
+ */
+typedef void (*spdk_channel_msg)(struct spdk_io_channel_iter *i);
+
+/**
+ * spdk_for_each_channel() callback.
+ *
+ * \param i I/O channel iterator.
+ * \param status 0 if it completed successfully, or negative errno if it failed.
+ */
+typedef void (*spdk_channel_for_each_cpl)(struct spdk_io_channel_iter *i, int status);
+
+/**
+ * \brief Represents a per-thread channel for accessing an I/O device.
+ *
+ * An I/O device may be a physical entity (i.e. NVMe controller) or a software
+ *  entity (i.e. a blobstore).
+ *
+ * This structure is not part of the API - all accesses should be done through
+ *  spdk_io_channel function calls.
+ */
+struct spdk_io_channel {
+	struct spdk_thread		*thread;
+	struct io_device		*dev;
+	uint32_t            channel_id;
+	uint32_t			ref;
+	uint32_t			destroy_ref;
+	TAILQ_ENTRY(spdk_io_channel)	tailq;
+	spdk_io_channel_destroy_cb	destroy_cb;
+
+	/*
+	 * Modules will allocate extra memory off the end of this structure
+	 *  to store references to hardware-specific references (i.e. NVMe queue
+	 *  pairs, or references to child device spdk_io_channels (i.e.
+	 *  virtual bdevs).
+	 */
+};
+
+/**
+ * Initialize the threading library. Must be called once prior to allocating any threads.
+ *
+ * \param new_thread_fn Called each time a new SPDK thread is created. The implementor
+ * is expected to frequently call spdk_thread_poll() on the provided thread.
+ * \param ctx_sz For each thread allocated, an additional region of memory of
+ * size ctx_size will also be allocated, for use by the thread scheduler. A pointer
+ * to this region may be obtained by calling spdk_thread_get_ctx().
+ *
+ * \return 0 on success. Negated errno on failure.
+ */
+int spdk_thread_lib_init(spdk_new_thread_fn new_thread_fn, size_t ctx_sz);
+
+/**
+ * Release all resources associated with this library.
+ */
+void spdk_thread_lib_fini(void);
+
+/**
+ * Creates a new SPDK thread object.
+ *
+ * \param name Human-readable name for the thread; can be retrieved with spdk_thread_get_name().
+ * The string is copied, so the pointed-to data only needs to be valid during the
+ * spdk_thread_create() call. May be NULL to specify no name.
+ * \param cpumask Optional mask of CPU cores on which to schedule this thread. This is only
+ * a suggestion to the scheduler. The value is copied, so cpumask may be released when
+ * this function returns. May be NULL if no mask is required.
+ *
+ * \return a pointer to the allocated thread on success or NULL on failure..
+ */
+struct spdk_thread *spdk_thread_create(const char *name, struct spdk_cpuset *cpumask);
+
+/**
+ * Release any resources related to the given thread and destroy it. Execution
+ * continues on the current system thread after returning.
+ *
+ * \param thread The thread to destroy.
+ *
+ * All I/O channel references associated with the thread must be released using
+ * spdk_put_io_channel() prior to calling this function.
+ */
+void spdk_thread_exit(struct spdk_thread *thread);
+
+/**
+ * Return a pointer to this thread's context.
+ *
+ * \param thread The thread on which to get the context.
+ *
+ * \return a pointer to the per-thread context, or NULL if there is
+ * no per-thread context.
+ */
+void *spdk_thread_get_ctx(struct spdk_thread *thread);
+
+/**
+ * Return the thread object associated with the context handle previously
+ * obtained by calling spdk_thread_get_ctx().
+ *
+ * \param ctx A context previously obtained by calling spdk_thread_get_ctx()
+ *
+ * \return The associated thread.
+ */
+struct spdk_thread *spdk_thread_get_from_ctx(void *ctx);
+
+/**
+ * Perform one iteration worth of processing on the thread. This includes
+ * both expired and continuous pollers as well as messages.
+ *
+ * \param thread The thread to process
+ * \param max_msgs The maximum number of messages that will be processed.
+ *                 Use 0 to process the default number of messages (8).
+ * \param now The current time, in ticks. Optional. If 0 is passed, this
+ *            function may call spdk_get_ticks() to get the current time.
+ *
+ * \return 1 if work was done. 0 if no work was done. -1 if unknown.
+ */
+int spdk_thread_poll(struct spdk_thread *thread, uint32_t max_msgs, uint64_t now);
+
+/**
+ * Return the number of ticks until the next timed poller
+ * would expire. Timed pollers are pollers for which
+ * period_microseconds is greater than 0.
+ *
+ * \param thread The thread to check poller expiration times on
+ *
+ * \return Number of ticks. If no timed pollers, return 0.
+ */
+uint64_t spdk_thread_next_poller_expiration(struct spdk_thread *thread);
+
+/**
+ * Returns whether there are any active pollers (pollers for which
+ * period_microseconds equals 0) registered to be run on the thread.
+ *
+ * \param thread The thread to check.
+ *
+ * \return 1 if there is at least one active poller, 0 otherwise.
+ */
+int spdk_thread_has_active_pollers(struct spdk_thread *thread);
+
+/**
+ * Returns whether there are any pollers registered to be run
+ * on the thread.
+ *
+ * \param thread The thread to check.
+ *
+ * \return true if there is any active poller, false otherwise.
+ */
+bool spdk_thread_has_pollers(struct spdk_thread *thread);
+
+/**
+ * Returns whether there are scheduled operations to be run on the thread.
+ *
+ * \param thread The thread to check.
+ *
+ * \return true if there are no scheduled operations, false otherwise.
+ */
+bool spdk_thread_is_idle(struct spdk_thread *thread);
+
+/**
+ * Get count of allocated threads.
+ */
+uint32_t spdk_thread_get_count(void);
+
+/**
+ * Get a handle to the current thread.
+ *
+ * This handle may be passed to other threads and used as the target of
+ * spdk_thread_send_msg().
+ *
+ * \sa spdk_io_channel_get_thread()
+ *
+ * \return a pointer to the current thread on success or NULL on failure.
+ */
+struct spdk_thread *spdk_get_thread(void);
+
+/**
+ * Get a thread's name.
+ *
+ * \param thread Thread to query.
+ *
+ * \return the name of the thread.
+ */
+const char *spdk_thread_get_name(const struct spdk_thread *thread);
+
+struct spdk_thread_stats {
+	uint64_t busy_tsc;
+	uint64_t idle_tsc;
+	uint64_t unknown_tsc;
+};
+
+/**
+ * Get statistics about the current thread.
+ *
+ * Copy cumulative thread stats values to the provided thread stats structure.
+ *
+ * \param stats User's thread_stats structure.
+ */
+int spdk_thread_get_stats(struct spdk_thread_stats *stats);
+
+/**
+ * Send a message to the given thread.
+ *
+ * The message may be sent asynchronously - i.e. spdk_thread_send_msg may return
+ * prior to `fn` being called.
+ *
+ * \param thread The target thread.
+ * \param fn This function will be called on the given thread.
+ * \param ctx This context will be passed to fn when called.
+ */
+void spdk_thread_send_msg(const struct spdk_thread *thread, spdk_msg_fn fn, void *ctx);
+
+/**
+ * Send a message to each thread, serially.
+ *
+ * The message is sent asynchronously - i.e. spdk_for_each_thread will return
+ * prior to `fn` being called on each thread.
+ *
+ * \param fn This is the function that will be called on each thread.
+ * \param ctx This context will be passed to fn when called.
+ * \param cpl This will be called on the originating thread after `fn` has been
+ * called on each thread.
+ */
+void spdk_for_each_thread(spdk_msg_fn fn, void *ctx, spdk_msg_fn cpl);
+
+/**
+ * Register a poller on the current thread.
+ *
+ * The poller can be unregistered by calling spdk_poller_unregister().
+ *
+ * \param fn This function will be called every `period_microseconds`.
+ * \param arg Argument passed to fn.
+ * \param period_microseconds How often to call `fn`. If 0, call `fn` as often
+ *  as possible.
+ *
+ * \return a pointer to the poller registered on the current thread on success
+ * or NULL on failure.
+ */
+struct spdk_poller *spdk_poller_register(spdk_poller_fn fn,
+		void *arg,
+		uint64_t period_microseconds);
+
+/**
+ * Unregister a poller on the current thread.
+ *
+ * \param ppoller The poller to unregister.
+ */
+void spdk_poller_unregister(struct spdk_poller **ppoller);
+
+/**
+ * Register the opaque io_device context as an I/O device.
+ *
+ * After an I/O device is registered, it can return I/O channels using the
+ * spdk_get_io_channel() function.
+ *
+ * \param io_device The pointer to io_device context.
+ * \param create_cb Callback function invoked to allocate any resources required
+ * for a new I/O channel.
+ * \param destroy_cb Callback function invoked to release the resources for an
+ * I/O channel.
+ * \param ctx_size The size of the context buffer allocated to store references
+ * to allocated I/O channel resources.
+ * \param name A string name for the device used only for debugging. Optional -
+ * may be NULL.
+ */
+void spdk_io_device_register(void *io_device, spdk_io_channel_create_cb create_cb,
+			     spdk_io_channel_destroy_cb destroy_cb, uint32_t ctx_size,
+			     const char *name);
+
+/**
+ * Unregister the opaque io_device context as an I/O device.
+ *
+ * The actual unregistration might be deferred until all active I/O channels are
+ * destroyed.
+ *
+ * \param io_device The pointer to io_device context.
+ * \param unregister_cb An optional callback function invoked to release any
+ * references to this I/O device.
+ */
+void spdk_io_device_unregister(void *io_device, spdk_io_device_unregister_cb unregister_cb);
+
+/**
+ * Get an I/O channel for the specified io_device to be used by the calling thread.
+ *
+ * The io_device context pointer specified must have previously been registered
+ * using spdk_io_device_register(). If an existing I/O channel does not exist
+ * yet for the given io_device on the calling thread, it will allocate an I/O
+ * channel and invoke the create_cb function pointer specified in spdk_io_device_register().
+ * If an I/O channel already exists for the given io_device on the calling thread,
+ * its reference is returned rather than creating a new I/O channel.
+ *
+ * \param io_device The pointer to io_device context.
+ *
+ * \return a pointer to the I/O channel for this device on success or NULL on failure.
+ */
+struct spdk_io_channel *spdk_get_io_channel(void *io_device);
+
+/**
+ * Release a reference to an I/O channel. This happens asynchronously.
+ *
+ * Actual release will happen on the same thread that called spdk_get_io_channel()
+ * for the specified I/O channel. If this releases the last reference to the
+ * I/O channel, The destroy_cb function specified in spdk_io_device_register()
+ * will be invoked to release any associated resources.
+ *
+ * \param ch I/O channel to release a reference.
+ */
+void spdk_put_io_channel(struct spdk_io_channel *ch);
+
+/**
+ * Get the context buffer associated with an I/O channel.
+ *
+ * \param ch I/O channel.
+ *
+ * \return a pointer to the context buffer.
+ */
+static inline void *
+spdk_io_channel_get_ctx(struct spdk_io_channel *ch)
+{
+	return (uint8_t *)ch + sizeof(*ch);
+}
+
+/**
+ * Get I/O channel from the context buffer. This is the inverse of
+ * spdk_io_channel_get_ctx().
+ *
+ * \param ctx The pointer to the context buffer.
+ *
+ * \return a pointer to the I/O channel associated with the context buffer.
+ */
+struct spdk_io_channel *spdk_io_channel_from_ctx(void *ctx);
+
+/**
+ * Get the thread associated with an I/O channel.
+ *
+ * \param ch I/O channel.
+ *
+ * \return a pointer to the thread associated with the I/O channel
+ */
+struct spdk_thread *spdk_io_channel_get_thread(struct spdk_io_channel *ch);
+
+/**
+ * Call 'fn' on each channel associated with io_device.
+ *
+ * This happens asynchronously, so fn may be called after spdk_for_each_channel
+ * returns. 'fn' will be called for each channel serially, such that two calls
+ * to 'fn' will not overlap in time. After 'fn' has been called, call
+ * spdk_for_each_channel_continue() to continue iterating.
+ *
+ * \param io_device 'fn' will be called on each channel associated with this io_device.
+ * \param fn Called on the appropriate thread for each channel associated with io_device.
+ * \param ctx Context buffer registered to spdk_io_channel_iter that can be obatined
+ * form the function spdk_io_channel_iter_get_ctx().
+ * \param cpl Called on the thread that spdk_for_each_channel was initially called
+ * from when 'fn' has been called on each channel.
+ */
+void spdk_for_each_channel(void *io_device, spdk_channel_msg fn, void *ctx,
+			   spdk_channel_for_each_cpl cpl);
+
+/**
+ * Get io_device from the I/O channel iterator.
+ *
+ * \param i I/O channel iterator.
+ *
+ * \return a pointer to the io_device.
+ */
+void *spdk_io_channel_iter_get_io_device(struct spdk_io_channel_iter *i);
+
+/**
+ * Get I/O channel from the I/O channel iterator.
+ *
+ * \param i I/O channel iterator.
+ *
+ * \return a pointer to the I/O channel.
+ */
+struct spdk_io_channel *spdk_io_channel_iter_get_channel(struct spdk_io_channel_iter *i);
+
+/**
+ * Get context buffer from the I/O channel iterator.
+ *
+ * \param i I/O channel iterator.
+ *
+ * \return a pointer to the context buffer.
+ */
+void *spdk_io_channel_iter_get_ctx(struct spdk_io_channel_iter *i);
+
+/**
+ * Helper function to iterate all channels for spdk_for_each_channel().
+ *
+ * \param i I/O channel iterator.
+ * \param status Status for the I/O channel iterator.
+ */
+void spdk_for_each_channel_continue(struct spdk_io_channel_iter *i, int status);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* SPDK_THREAD_H_ */
diff --git a/PDK/core/src/api/include/udd/spdk/trace.h b/PDK/core/src/api/include/udd/spdk/trace.h
index a017498..ed85a0b 100644
--- a/PDK/core/src/api/include/udd/spdk/trace.h
+++ b/PDK/core/src/api/include/udd/spdk/trace.h
@@ -45,7 +45,7 @@
 extern "C" {
 #endif
 
-#define SPDK_TRACE_SIZE	 (32 * 1024)
+#define SPDK_DEFAULT_NUM_TRACE_ENTRIES	 (32 * 1024)
 
 struct spdk_trace_entry {
 	uint64_t	tsc;
@@ -84,7 +84,7 @@ struct spdk_trace_tpoint {
 	uint8_t		object_type;
 	uint8_t		new_object;
 	uint8_t		arg1_is_ptr;
-	uint8_t		arg1_is_alias;
+	uint8_t		reserved;
 	char		arg1_name[8];
 };
 
@@ -92,13 +92,8 @@ struct spdk_trace_history {
 	/** Logical core number associated with this structure instance. */
 	int				lcore;
 
-	/**
-	 * Circular buffer of spdk_trace_entry structures for tracing
-	 *  tpoints on this core.  Debug tool spdk_trace reads this
-	 *  buffer from shared memory to post-process the tpoint entries and
-	 *  display in a human-readable format.
-	 */
-	struct spdk_trace_entry		entries[SPDK_TRACE_SIZE];
+	/** Number of trace_entries contained in each trace_history. */
+	uint64_t			num_entries;
 
 	/**
 	 * Running count of number of occurrences of each tracepoint on this
@@ -107,65 +102,291 @@ struct spdk_trace_history {
 	 */
 	uint64_t			tpoint_count[SPDK_TRACE_MAX_TPOINT_ID];
 
-	/** Index to next spdk_trace_entry to fill in the circular buffer. */
-	uint32_t			next_entry;
+	/** Index to next spdk_trace_entry to fill. */
+	uint64_t			next_entry;
 
+	/**
+	 * Circular buffer of spdk_trace_entry structures for tracing
+	 *  tpoints on this core.  Debug tool spdk_trace reads this
+	 *  buffer from shared memory to post-process the tpoint entries and
+	 *  display in a human-readable format.
+	 */
+	struct spdk_trace_entry		entries[0];
 };
 
 #define SPDK_TRACE_MAX_LCORE		128
 
-struct spdk_trace_histories {
+struct spdk_trace_flags {
 	uint64_t			tsc_rate;
 	uint64_t			tpoint_mask[SPDK_TRACE_MAX_GROUP_ID];
-	struct spdk_trace_history	per_lcore_history[SPDK_TRACE_MAX_LCORE];
 	struct spdk_trace_owner		owner[UCHAR_MAX + 1];
 	struct spdk_trace_object	object[UCHAR_MAX + 1];
 	struct spdk_trace_tpoint	tpoint[SPDK_TRACE_MAX_TPOINT_ID];
+
+	/** Offset of each trace_history from the beginning of this data structure.
+	 * The last one is the offset of the file end.
+	 */
+	uint64_t			lcore_history_offsets[SPDK_TRACE_MAX_LCORE + 1];
 };
+extern struct spdk_trace_flags *g_trace_flags;
+extern struct spdk_trace_histories *g_trace_histories;
+
+
+struct spdk_trace_histories {
+	struct spdk_trace_flags flags;
+
+	/**
+	 * struct spdk_trace_history has a dynamic size determined by num_entries
+	 * in spdk_trace_init. Mark array size of per_lcore_history to be 0 in uint8_t
+	 * as a reminder that each per_lcore_history pointer should be gotten by
+	 * proper API, instead of directly referencing by struct element.
+	 */
+	uint8_t	per_lcore_history[0];
+};
+
+static inline uint64_t
+spdk_get_trace_history_size(uint64_t num_entries)
+{
+	return sizeof(struct spdk_trace_history) + num_entries * sizeof(struct spdk_trace_entry);
+}
+
+static inline uint64_t
+spdk_get_trace_histories_size(struct spdk_trace_histories *trace_histories)
+{
+	return trace_histories->flags.lcore_history_offsets[SPDK_TRACE_MAX_LCORE];
+}
 
+static inline struct spdk_trace_history *
+spdk_get_per_lcore_history(struct spdk_trace_histories *trace_histories, unsigned lcore)
+{
+	char *lcore_history_offset;
 
+	if (lcore >= SPDK_TRACE_MAX_LCORE) {
+		return NULL;
+	}
+
+	lcore_history_offset = (char *)trace_histories;
+	lcore_history_offset += trace_histories->flags.lcore_history_offsets[lcore];
+
+	return (struct spdk_trace_history *)lcore_history_offset;
+}
+
+void _spdk_trace_record(uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
+			uint32_t size, uint64_t object_id, uint64_t arg1);
+
+/**
+ * Record the current trace state for tracing tpoints. Debug tool can read the
+ * information from shared memory to post-process the tpoint entries and display
+ * in a human-readable format. This function will call spdk_get_ticks() to get
+ * the current tsc to save in the tracepoint.
+ *
+ * \param tpoint_id Tracepoint id to record.
+ * \param poller_id Poller id to record.
+ * \param size Size to record.
+ * \param object_id Object id to record.
+ * \param arg1 Argument to record.
+ */
+static inline
 void spdk_trace_record(uint16_t tpoint_id, uint16_t poller_id, uint32_t size,
-		       uint64_t object_id, uint64_t arg1);
+		       uint64_t object_id, uint64_t arg1)
+{
+	/*
+	 * Tracepoint group ID is encoded in the tpoint_id.  Lower 6 bits determine the tracepoint
+	 *  within the group, the remaining upper bits determine the tracepoint group.  Each
+	 *  tracepoint group has its own tracepoint mask.
+	 */
+	assert(tpoint_id < SPDK_TRACE_MAX_TPOINT_ID);
+	if (g_trace_histories == NULL ||
+	    !((1ULL << (tpoint_id & 0x3F)) & g_trace_histories->flags.tpoint_mask[tpoint_id >> 6])) {
+		return;
+	}
+
+	_spdk_trace_record(0, tpoint_id, poller_id, size, object_id, arg1);
+}
 
-/** Returns the current tpoint mask. */
+/**
+ * Record the current trace state for tracing tpoints. Debug tool can read the
+ * information from shared memory to post-process the tpoint entries and display
+ * in a human-readable format.
+ *
+ * \param tsc Current tsc.
+ * \param tpoint_id Tracepoint id to record.
+ * \param poller_id Poller id to record.
+ * \param size Size to record.
+ * \param object_id Object id to record.
+ * \param arg1 Argument to record.
+ */
+static inline
+void spdk_trace_record_tsc(uint64_t tsc, uint16_t tpoint_id, uint16_t poller_id,
+			   uint32_t size, uint64_t object_id, uint64_t arg1)
+{
+	/*
+	 * Tracepoint group ID is encoded in the tpoint_id.  Lower 6 bits determine the tracepoint
+	 *  within the group, the remaining upper bits determine the tracepoint group.  Each
+	 *  tracepoint group has its own tracepoint mask.
+	 */
+	assert(tpoint_id < SPDK_TRACE_MAX_TPOINT_ID);
+	if (g_trace_histories == NULL ||
+	    !((1ULL << (tpoint_id & 0x3F)) & g_trace_histories->flags.tpoint_mask[tpoint_id >> 6])) {
+		return;
+	}
+
+	_spdk_trace_record(tsc, tpoint_id, poller_id, size, object_id, arg1);
+}
+
+/**
+ * Get the current tpoint mask of the given tpoint group.
+ *
+ * \param group_id Tpoint group id associated with the tpoint mask.
+ *
+ * \return current tpoint mask.
+ */
 uint64_t spdk_trace_get_tpoint_mask(uint32_t group_id);
 
-/** Adds the specified tpoints to the current tpoint mask for the given tpoint group. */
+/**
+ * Add the specified tpoints to the current tpoint mask for the given tpoint group.
+ *
+ * \param group_id Tpoint group id associated with the tpoint mask.
+ * \param tpoint_mask Tpoint mask which indicates which tpoints to add to the
+ * current tpoint mask.
+ */
 void spdk_trace_set_tpoints(uint32_t group_id, uint64_t tpoint_mask);
 
-/** Clears the specified tpoints from the current tpoint mask for the given tpoint group. */
+/**
+ * Clear the specified tpoints from the current tpoint mask for the given tpoint group.
+ *
+ * \param group_id Tpoint group id associated with the tpoint mask.
+ * \param tpoint_mask Tpoint mask which indicates which tpoints to clear from
+ * the current tpoint mask.
+ */
 void spdk_trace_clear_tpoints(uint32_t group_id, uint64_t tpoint_mask);
 
-/** Returns a mask of all tracepoint groups which have at least one tracepoint enabled. */
+/**
+ * Get a mask of all tracepoint groups which have at least one tracepoint enabled.
+ *
+ * \return a mask of all tracepoint groups.
+ */
 uint64_t spdk_trace_get_tpoint_group_mask(void);
 
-/** For each tpoint group specified in the group mask, enable all of its tpoints. */
+/**
+ * For each tpoint group specified in the group mask, enable all of its tpoints.
+ *
+ * \param tpoint_group_mask Tpoint group mask that indicates which tpoints to enable.
+ */
 void spdk_trace_set_tpoint_group_mask(uint64_t tpoint_group_mask);
 
-void spdk_trace_init(const char *shm_name);
+/**
+ * For each tpoint group specified in the group mask, disable all of its tpoints.
+ *
+ * \param tpoint_group_mask Tpoint group mask that indicates which tpoints to disable.
+ */
+void spdk_trace_clear_tpoint_group_mask(uint64_t tpoint_group_mask);
+
+/**
+ * Initialize the trace environment. Debug tool can read the information from
+ * the given shared memory to post-process the tpoint entries and display in a
+ * human-readable format.
+ *
+ * \param shm_name Name of shared memory.
+ * \param num_entries Number of trace entries per lcore.
+ * \return 0 on success, else non-zero indicates a failure.
+ */
+int spdk_trace_init(const char *shm_name, uint64_t num_entries);
+
+/**
+ * Unmap global trace memory structs.
+ */
 void spdk_trace_cleanup(void);
 
+/**
+ * Initialize trace flags.
+ */
+void spdk_trace_flags_init(void);
+
 #define OWNER_NONE 0
 #define OBJECT_NONE 0
 
+/**
+ * Register the trace owner.
+ *
+ * \param type Type of the trace owner.
+ * \param id_prefix Prefix of id for the trace owner.
+ */
 void spdk_trace_register_owner(uint8_t type, char id_prefix);
+
+/**
+ * Register the trace object.
+ *
+ * \param type Type of the trace object.
+ * \param id_prefix Prefix of id for the trace object.
+ */
 void spdk_trace_register_object(uint8_t type, char id_prefix);
+
+/**
+ * Register the description for the tpoint.
+ *
+ * \param name Name for the tpoint.
+ * \param short_name Short name for the tpoint.
+ * \param tpoint_id Id for the tpoint.
+ * \param owner_type Owner type for the tpoint.
+ * \param object_type Object type for the tpoint.
+ * \param new_object New object for the tpoint.
+ * \param arg1_is_ptr This argument indicates whether argument1 is a pointer.
+ * \param arg1_name Name of argument.
+ */
 void spdk_trace_register_description(const char *name, const char *short_name,
 				     uint16_t tpoint_id, uint8_t owner_type,
 				     uint8_t object_type, uint8_t new_object,
-				     uint8_t arg1_is_ptr, uint8_t arg1_is_alias,
-				     const char *arg1_name);
+				     uint8_t arg1_is_ptr, const char *arg1_name);
+
+struct spdk_trace_register_fn *spdk_trace_get_first_register_fn(void);
+
+struct spdk_trace_register_fn *spdk_trace_get_next_register_fn(struct spdk_trace_register_fn
+		*register_fn);
+
+/**
+ * Enable trace on specific tpoint group
+ *
+ * \param group_name Name of group to enable, "all" for enabling all groups.
+ * \return 0 on success, else non-zero indicates a failure.
+ */
+int spdk_trace_enable_tpoint_group(const char *group_name);
+
+/**
+ * Disable trace on specific tpoint group
+ *
+ * \param group_name Name of group to disable, "all" for disabling all groups.
+ * \return 0 on success, else non-zero indicates a failure.
+ */
+int spdk_trace_disable_tpoint_group(const char *group_name);
+
+/**
+ * Show trace mask and its usage.
+ *
+ * \param f File to hold the mask's information.
+ * \param tmask_arg Command line option to set the trace group mask.
+ */
+void spdk_trace_mask_usage(FILE *f, const char *tmask_arg);
 
 struct spdk_trace_register_fn {
+	const char *name;
+	uint8_t tgroup_id;
 	void (*reg_fn)(void);
 	struct spdk_trace_register_fn *next;
 };
 
+/**
+ * Add new trace register function.
+ *
+ * \param reg_fn Trace register function to add.
+ */
 void spdk_trace_add_register_fn(struct spdk_trace_register_fn *reg_fn);
 
-#define SPDK_TRACE_REGISTER_FN(fn) 				\
+#define SPDK_TRACE_REGISTER_FN(fn, name_str, _tgroup_id)	\
 	static void fn(void);					\
 	struct spdk_trace_register_fn reg_ ## fn = {		\
+		.name = name_str,				\
+		.tgroup_id = _tgroup_id,			\
 		.reg_fn = fn,					\
 		.next = NULL,					\
 	};							\
diff --git a/PDK/core/src/api/include/udd/spdk/util.h b/PDK/core/src/api/include/udd/spdk/util.h
index 447ac1f..658a9b1 100644
--- a/PDK/core/src/api/include/udd/spdk/util.h
+++ b/PDK/core/src/api/include/udd/spdk/util.h
@@ -49,6 +49,11 @@ extern "C" {
 
 #define SPDK_COUNTOF(arr) (sizeof(arr) / sizeof((arr)[0]))
 
+#define SPDK_CONTAINEROF(ptr, type, member) ((type *)((uintptr_t)ptr - offsetof(type, member)))
+
+#define SPDK_SEC_TO_USEC 1000000ULL
+#define SPDK_SEC_TO_NSEC 1000000000ULL
+
 static inline uint32_t
 spdk_u32log2(uint32_t x)
 {
@@ -65,6 +70,41 @@ spdk_align32pow2(uint32_t x)
 	return 1u << (1 + spdk_u32log2(x - 1));
 }
 
+static inline uint64_t
+spdk_u64log2(uint64_t x)
+{
+	if (x == 0) {
+		/* log(0) is undefined */
+		return 0;
+	}
+	return 63u - __builtin_clzl(x);
+}
+
+static inline uint64_t
+spdk_align64pow2(uint64_t x)
+{
+	return 1u << (1 + spdk_u64log2(x - 1));
+}
+
+/**
+ * Check if a uint32_t is a power of 2.
+ */
+static inline bool
+spdk_u32_is_pow2(uint32_t x)
+{
+	if (x == 0) {
+		return false;
+	}
+
+	return (x & (x - 1)) == 0;
+}
+
+static inline uint64_t
+spdk_divide_round_up(uint64_t num, uint64_t divisor)
+{
+	return (num + divisor - 1) / divisor;
+}
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/PDK/core/src/api/include/udd/spdk/uuid.h b/PDK/core/src/api/include/udd/spdk/uuid.h
new file mode 100644
index 0000000..820944e
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/uuid.h
@@ -0,0 +1,108 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/** \file
+ * UUID types and functions
+ */
+
+#ifndef SPDK_UUID_H
+#define SPDK_UUID_H
+
+#include "spdk/stdinc.h"
+
+#include "spdk/assert.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+struct spdk_uuid {
+	union {
+		uint8_t raw[16];
+	} u;
+};
+SPDK_STATIC_ASSERT(sizeof(struct spdk_uuid) == 16, "Incorrect size");
+
+#define SPDK_UUID_STRING_LEN 37 /* 36 characters + null terminator */
+
+/**
+ * Convert UUID in textual format into a spdk_uuid.
+ *
+ * \param[out] uuid User-provided UUID buffer.
+ * \param uuid_str UUID in textual format in C string.
+ *
+ * \return 0 on success, or negative errno on failure.
+ */
+int spdk_uuid_parse(struct spdk_uuid *uuid, const char *uuid_str);
+
+/**
+ * Convert UUID in spdk_uuid into lowercase textual format.
+ *
+ * \param uuid_str User-provided string buffer to write the textual format into.
+ * \param uuid_str_size Size of uuid_str buffer. Must be at least SPDK_UUID_STRING_LEN.
+ * \param uuid UUID to convert to textual format.
+ *
+ * \return 0 on success, or negative errno on failure.
+ */
+int spdk_uuid_fmt_lower(char *uuid_str, size_t uuid_str_size, const struct spdk_uuid *uuid);
+
+/**
+ * Compare two UUIDs.
+ *
+ * \param u1 UUID 1.
+ * \param u2 UUID 2.
+ *
+ * \return 0 if u1 == u2, less than 0 if u1 < u2, greater than 0 if u1 > u2.
+ */
+int spdk_uuid_compare(const struct spdk_uuid *u1, const struct spdk_uuid *u2);
+
+/**
+ * Generate a new UUID.
+ *
+ * \param[out] uuid User-provided UUID buffer to fill.
+ */
+void spdk_uuid_generate(struct spdk_uuid *uuid);
+
+/**
+ * Copy a UUID.
+ *
+ * \param src Source UUID to copy from.
+ * \param dst Destination UUID to store.
+ */
+void spdk_uuid_copy(struct spdk_uuid *dst, const struct spdk_uuid *src);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif
diff --git a/PDK/core/src/api/include/udd/spdk/version.h b/PDK/core/src/api/include/udd/spdk/version.h
new file mode 100644
index 0000000..983abc5
--- /dev/null
+++ b/PDK/core/src/api/include/udd/spdk/version.h
@@ -0,0 +1,110 @@
+/*-
+ *   BSD LICENSE
+ *
+ *   Copyright (c) Intel Corporation.
+ *   All rights reserved.
+ *
+ *   Redistribution and use in source and binary forms, with or without
+ *   modification, are permitted provided that the following conditions
+ *   are met:
+ *
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in
+ *       the documentation and/or other materials provided with the
+ *       distribution.
+ *     * Neither the name of Intel Corporation nor the names of its
+ *       contributors may be used to endorse or promote products derived
+ *       from this software without specific prior written permission.
+ *
+ *   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ *   "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ *   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ *   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ *   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ *   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ *   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ *   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ *   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ *   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/** \file
+ * SPDK version number definitions
+ */
+
+#ifndef SPDK_VERSION_H
+#define SPDK_VERSION_H
+
+/**
+ * Major version number (year of original release minus 2000).
+ */
+#define SPDK_VERSION_MAJOR	19
+
+/**
+ * Minor version number (month of original release).
+ */
+#define SPDK_VERSION_MINOR	4
+
+/**
+ * Patch level.
+ *
+ * Patch level is incremented on maintenance branch releases and reset to 0 for each
+ * new major.minor release.
+ */
+#define SPDK_VERSION_PATCH	1
+
+/**
+ * Version string suffix.
+ */
+#define SPDK_VERSION_SUFFIX	""
+
+/**
+ * Single numeric value representing a version number for compile-time comparisons.
+ *
+ * Example usage:
+ *
+ * \code
+ * #if SPDK_VERSION >= SPDK_VERSION_NUM(17, 7, 0)
+ *   Use feature from SPDK v17.07
+ * #endif
+ * \endcode
+ */
+#define SPDK_VERSION_NUM(major, minor, patch) \
+	(((major) * 100 + (minor)) * 100 + (patch))
+
+/**
+ * Current version as a SPDK_VERSION_NUM.
+ */
+#define SPDK_VERSION	SPDK_VERSION_NUM(SPDK_VERSION_MAJOR, SPDK_VERSION_MINOR, SPDK_VERSION_PATCH)
+
+#define SPDK_VERSION_STRINGIFY_x(x)	#x
+#define SPDK_VERSION_STRINGIFY(x)	SPDK_VERSION_STRINGIFY_x(x)
+
+#define SPDK_VERSION_MAJOR_STRING	SPDK_VERSION_STRINGIFY(SPDK_VERSION_MAJOR)
+
+#if SPDK_VERSION_MINOR < 10
+#define SPDK_VERSION_MINOR_STRING	".0" SPDK_VERSION_STRINGIFY(SPDK_VERSION_MINOR)
+#else
+#define SPDK_VERSION_MINOR_STRING	"." SPDK_VERSION_STRINGIFY(SPDK_VERSION_MINOR)
+#endif
+
+#if SPDK_VERSION_PATCH != 0
+#define SPDK_VERSION_PATCH_STRING	"." SPDK_VERSION_STRINGIFY(SPDK_VERSION_PATCH)
+#else
+#define SPDK_VERSION_PATCH_STRING	""
+#endif
+
+/**
+ * Human-readable version string.
+ */
+#define SPDK_VERSION_STRING	\
+	"SPDK v" \
+	SPDK_VERSION_MAJOR_STRING \
+	SPDK_VERSION_MINOR_STRING \
+	SPDK_VERSION_PATCH_STRING \
+	SPDK_VERSION_SUFFIX
+
+#endif
diff --git a/PDK/core/src/api/include/udd/spdk/vhost.h b/PDK/core/src/api/include/udd/spdk/vhost.h
index 995caf8..5cfe14a 100644
--- a/PDK/core/src/api/include/udd/spdk/vhost.h
+++ b/PDK/core/src/api/include/udd/spdk/vhost.h
@@ -42,31 +42,299 @@
 #include "spdk/stdinc.h"
 
 #include "spdk/event.h"
+#include "spdk/json.h"
 
-#define SPDK_VHOST_SCSI_CTRLR_MAX_DEVS 8
+#ifdef __cplusplus
+extern "C" {
+#endif
 
 /**
- * \param event event object. event arg1 is optional path to vhost socket.
+ * Callback funcion for spdk_vhost_fini().
+ */
+typedef void (*spdk_vhost_fini_cb)(void);
+
+/**
+ * Set the path to the directory where vhost sockets will be created.
+ *
+ * This function must be called before spdk_vhost_init().
+ *
+ * \param basename Path to vhost socket directory
+ *
+ * \return 0 on success, negative errno on error.
+ */
+int spdk_vhost_set_socket_path(const char *basename);
+
+/**
+ * Init vhost environment.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+int spdk_vhost_init(void);
+
+/**
+ * Clean up the environment of vhost after finishing the vhost application.
+ *
+ * \param fini_cb Called when the cleanup operation completes.
+ */
+void spdk_vhost_fini(spdk_vhost_fini_cb fini_cb);
+
+
+/**
+ * Write vhost subsystem configuration into provided JSON context.
+ *
+ * \param w JSON write context
+ */
+void spdk_vhost_config_json(struct spdk_json_write_ctx *w);
+
+/**
+ * Deinit vhost application. This is called once by SPDK app layer.
  */
-void spdk_vhost_startup(void *arg1, void *arg2);
 void spdk_vhost_shutdown_cb(void);
 
-/* Forward declaration */
-struct spdk_vhost_scsi_ctrlr;
+/**
+ * SPDK vhost device (vdev).  An equivalent of Virtio device.
+ * Both virtio-blk and virtio-scsi devices are represented by this
+ * struct. For virtio-scsi a single vhost device (also called SCSI
+ * controller) may contain multiple SCSI targets (devices), each of
+ * which may contain multiple logical units (SCSI LUNs). For now
+ * only one LUN per target is available.
+ *
+ * All vdev-changing functions operate directly on this object.
+ * Note that \c spdk_vhost_dev cannot be acquired. This object is
+ * only accessible as a callback parameter via \c
+ * spdk_vhost_call_external_event and it's derivatives. This ensures
+ * that all access to the vdev is piped through a single,
+ * thread-safe API.
+ */
+struct spdk_vhost_dev;
+
+/**
+ * Lock the global vhost mutex synchronizing all the vhost device accesses.
+ */
+void spdk_vhost_lock(void);
+
+/**
+ * Lock the global vhost mutex synchronizing all the vhost device accesses.
+ *
+ * \return 0 if the mutex could be locked immediately, negative errno otherwise.
+ */
+int spdk_vhost_trylock(void);
+
+/**
+ * Unlock the global vhost mutex.
+ */
+void spdk_vhost_unlock(void);
+
+/**
+ * Find a vhost device by name.
+ *
+ * \return vhost device or NULL
+ */
+struct spdk_vhost_dev *spdk_vhost_dev_find(const char *name);
+
+/**
+ * Get the next vhost device. If there's no more devices to iterate
+ * through, NULL will be returned.
+ *
+ * \param vdev vhost device. If NULL, this function will return the
+ * very first device.
+ * \return vdev vhost device or NULL
+ */
+struct spdk_vhost_dev *spdk_vhost_dev_next(struct spdk_vhost_dev *vdev);
+
+/**
+ * Synchronized vhost event used for user callbacks.
+ *
+ * \param vdev vhost device.
+ * \param arg user-provided parameter.
+ *
+ * \return 0 on success, -1 on failure.
+ */
+typedef int (*spdk_vhost_event_fn)(struct spdk_vhost_dev *vdev, void *arg);
+
+/**
+ * Get the name of the vhost device.  This is equal to the filename
+ * of socket file. The name is constant throughout the lifetime of
+ * a vdev.
+ *
+ * \param vdev vhost device.
+ *
+ * \return name of the vdev.
+ */
+const char *spdk_vhost_dev_get_name(struct spdk_vhost_dev *vdev);
+
+/**
+ * Get cpuset of the vhost device.  The cpuset is constant throughout the lifetime
+ * of a vdev. It is a subset of SPDK app cpuset vhost was started with.
+ *
+ * \param vdev vhost device.
+ *
+ * \return cpuset of the vdev.
+ */
+const struct spdk_cpuset *spdk_vhost_dev_get_cpumask(struct spdk_vhost_dev *vdev);
+
+/**
+ * By default, events are generated when asked, but for high queue depth and
+ * high IOPS this prove to be inefficient both for guest kernel that have to
+ * handle a lot more IO completions and for SPDK vhost that need to make more
+ * syscalls. If enabled, limit amount of events (IRQs) sent to initiator by SPDK
+ * vhost effectively coalescing couple of completions. This of cource introduce
+ * IO latency penalty proportional to event delay time.
+ *
+ * Actual events delay time when is calculated according to below formula:
+ * if (delay_base == 0 || IOPS < iops_threshold) {
+ *   delay = 0;
+ * } else if (IOPS < iops_threshold) {
+ *   delay = delay_base * (iops - iops_threshold) / iops_threshold;
+ * }
+ *
+ * \param vdev vhost device.
+ * \param delay_base_us Base delay time in microseconds. If 0, coalescing is disabled.
+ * \param iops_threshold IOPS threshold when coalescing is activated.
+ */
+int spdk_vhost_set_coalescing(struct spdk_vhost_dev *vdev, uint32_t delay_base_us,
+			      uint32_t iops_threshold);
+
+/**
+ * Get coalescing parameters.
+ *
+ * \see spdk_vhost_set_coalescing
+ *
+ * \param vdev vhost device.
+ * \param delay_base_us Optional pointer to store base delay time.
+ * \param iops_threshold Optional pointer to store IOPS threshold.
+ */
+void spdk_vhost_get_coalescing(struct spdk_vhost_dev *vdev, uint32_t *delay_base_us,
+			       uint32_t *iops_threshold);
+
+/**
+ * Construct an empty vhost SCSI device.  This will create a
+ * Unix domain socket together with a vhost-user slave server waiting
+ * for a connection on this socket. Creating the vdev does not
+ * start any I/O pollers and does not hog the CPU. I/O processing
+ * starts after receiving proper message on the created socket.
+ * See QEMU's vhost-user documentation for details.
+ * All physical devices have to be separately attached to this
+ * vdev via \c spdk_vhost_scsi_dev_add_tgt().
+ *
+ * This function is thread-safe.
+ *
+ * \param name name of the vhost device. The name will also be used
+ * for socket name, which is exactly \c socket_base_dir/name
+ * \param cpumask string containing cpumask in hex. The leading *0x*
+ * is allowed but not required. The mask itself can be constructed as:
+ * ((1 << cpu0) | (1 << cpu1) | ... | (1 << cpuN)).
+ *
+ * \return 0 on success, negative errno on error.
+ */
+int spdk_vhost_scsi_dev_construct(const char *name, const char *cpumask);
+
+/**
+ * Construct and attach new SCSI target to the vhost SCSI device
+ * on given (unoccupied) slot.  The device will be created with a single
+ * LUN0 associated with given SPDK bdev. Currently only one LUN per
+ * device is supported.
+ *
+ * If the vhost SCSI device has an active connection and has negotiated
+ * \c VIRTIO_SCSI_F_HOTPLUG feature,  the new SCSI target should be
+ * automatically detected by the other side.
+ *
+ * \param vdev vhost SCSI device.
+ * \param scsi_tgt_num slot to attach to or negative value to use first free.
+ * \param bdev_name name of the SPDK bdev to associate with SCSI LUN0.
+ *
+ * \return value >= 0 on success - the SCSI target ID, negative errno code:
+ * -EINVAL - one of the arguments is invalid:
+ *   - vdev is not vhost SCSI device
+ *   - SCSI target ID is out of range
+ *   - bdev name is NULL
+ *   - can't create SCSI LUN because of other errors e.g.: bdev does not exist
+ * -ENOSPC - scsi_tgt_num is -1 and maximum targets in vhost SCSI device reached
+ * -EEXIST - SCSI target ID already exists
+ */
+int spdk_vhost_scsi_dev_add_tgt(struct spdk_vhost_dev *vdev, int scsi_tgt_num,
+				const char *bdev_name);
+
+/**
+ * Get SCSI target from vhost SCSI device on given slot. Max
+ * number of available slots is defined by.
+ * \c SPDK_VHOST_SCSI_CTRLR_MAX_DEVS.
+ *
+ * \param vdev vhost SCSI device.
+ * \param num slot id.
+ *
+ * \return SCSI device on given slot or NULL.
+ */
+struct spdk_scsi_dev *spdk_vhost_scsi_dev_get_tgt(struct spdk_vhost_dev *vdev, uint8_t num);
+
+/**
+ * Detach and destruct SCSI target from a vhost SCSI device.
+ *
+ * The device will be deleted after all pending I/O is finished.
+ * If the driver supports VIRTIO_SCSI_F_HOTPLUG, then a hotremove
+ * notification will be sent.
+ *
+ * \param vdev vhost SCSI device
+ * \param scsi_tgt_num slot id to delete target from
+ * \param cb_fn callback to be fired once target has been successfully
+ * deleted. The first parameter of callback function is the vhost SCSI
+ * device, the second is user provided argument *cb_arg*.
+ * \param cb_arg parameter to be passed to *cb_fn*.
+ *
+ * \return 0 on success, negative errno on error.
+ */
+int spdk_vhost_scsi_dev_remove_tgt(struct spdk_vhost_dev *vdev, unsigned scsi_tgt_num,
+				   spdk_vhost_event_fn cb_fn, void *cb_arg);
+
+/**
+ * Construct a vhost blk device.  This will create a Unix domain
+ * socket together with a vhost-user slave server waiting for a
+ * connection on this socket. Creating the vdev does not start
+ * any I/O pollers and does not hog the CPU. I/O processing starts
+ * after receiving proper message on the created socket.
+ * See QEMU's vhost-user documentation for details. Vhost blk
+ * device is tightly associated with given SPDK bdev. Given
+ * bdev can not be changed, unless it has been hotremoved. This
+ * would result in all I/O failing with virtio \c VIRTIO_BLK_S_IOERR
+ * error code.
+ *
+ * This function is thread-safe.
+ *
+ * \param name name of the vhost blk device. The name will also be
+ * used for socket name, which is exactly \c socket_base_dir/name
+ * \param cpumask string containing cpumask in hex. The leading *0x*
+ * is allowed but not required. The mask itself can be constructed as:
+ * ((1 << cpu0) | (1 << cpu1) | ... | (1 << cpuN)).
+ * \param dev_name bdev name to associate with this vhost device
+ * \param readonly if set, all writes to the device will fail with
+ * \c VIRTIO_BLK_S_IOERR error code.
+ *
+ * \return 0 on success, negative errno on error.
+ */
+int spdk_vhost_blk_construct(const char *name, const char *cpumask, const char *dev_name,
+			     bool readonly);
 
 /**
- * Get handle to next controller.
- * \param prev Previous controller or NULL to get first one.
- * \return handle to next controller ot NULL if prev was the last one.
+ * Remove a vhost device. The device must not have any open connections on it's socket.
+ *
+ * \param vdev vhost blk device.
+ *
+ * \return 0 on success, negative errno on error.
+ */
+int spdk_vhost_dev_remove(struct spdk_vhost_dev *vdev);
+
+/**
+ * Get underlying SPDK bdev from vhost blk device. The bdev might be NULL, as it
+ * could have been hotremoved.
+ *
+ * \param ctrlr vhost blk device.
+ *
+ * \return SPDK bdev associated with given vdev.
  */
-struct spdk_vhost_scsi_ctrlr *spdk_vhost_scsi_ctrlr_next(struct spdk_vhost_scsi_ctrlr *prev);
+struct spdk_bdev *spdk_vhost_blk_get_dev(struct spdk_vhost_dev *ctrlr);
 
-const char *spdk_vhost_scsi_ctrlr_get_name(struct spdk_vhost_scsi_ctrlr *ctrl);
-uint64_t spdk_vhost_scsi_ctrlr_get_cpumask(struct spdk_vhost_scsi_ctrlr *ctrl);
-int spdk_vhost_scsi_ctrlr_construct(const char *name, uint64_t cpumask);
-int spdk_vhost_parse_core_mask(const char *mask, uint64_t *cpumask);
-struct spdk_scsi_dev *spdk_vhost_scsi_ctrlr_get_dev(struct spdk_vhost_scsi_ctrlr *ctrl,
-		uint8_t num);
-int spdk_vhost_scsi_ctrlr_add_dev(const char *name, unsigned scsi_dev_num, const char *lun_name);
+#ifdef __cplusplus
+}
+#endif
 
 #endif /* SPDK_VHOST_H */
diff --git a/PDK/core/src/api/src/cfrontend.cpp b/PDK/core/src/api/src/cfrontend.cpp
index 7454c63..cd1b3b4 100644
--- a/PDK/core/src/api/src/cfrontend.cpp
+++ b/PDK/core/src/api/src/cfrontend.cpp
@@ -86,17 +86,8 @@ kvs_result kvs_close_container (kvs_container_handle cont_hd) {
   return (kvs_result)api_private::kvs_close_container( (api_private::kvs_container_handle)cont_hd);
 }
 
-kvs_result kvs_list_containers(kvs_device_handle dev_hd, uint32_t index,
-  uint32_t buffer_size, kvs_container_name *names, uint32_t *cont_cnt) {
-  return (kvs_result)api_private::kvs_list_containers(
-    (api_private::kvs_device_handle)dev_hd, index, buffer_size,
-    (api_private::kvs_container_name*)names, cont_cnt);
-}
-
 kvs_result kvs_get_container_info (kvs_container_handle cont_hd, kvs_container *cont) {
-  return (kvs_result)api_private::kvs_get_container_info(
-    (api_private::kvs_container_handle)cont_hd,
-    (api_private::kvs_container*)cont);
+  return (kvs_result)api_private::kvs_get_container_info( (api_private::kvs_container_handle)cont_hd, (api_private::kvs_container*)cont);
 }
 
 int32_t kvs_get_ioevents(kvs_container_handle cont_hd, int maxevents) {
@@ -141,6 +132,26 @@ kvs_result kvs_exist_tuples_async(kvs_container_handle cont_hd, uint32_t key_cnt
   return (kvs_result)api_private::kvs_exist_tuples_async((api_private::kvs_container_handle)cont_hd, key_cnt, (const api_private::kvs_key*)keys, buffer_size, result_buffer, (const api_private::kvs_exist_context*)ctx, (api_private::kvs_callback_function)cbfn);
 }
 
+#ifdef KVS_REMOTE
+kvs_result kvs_list_tuple(kvs_container_handle cont_hd, const kvs_key *prefix_key, const kvs_key *start_key, uint16_t max_keys_to_list, kvs_value *value, const kvs_list_context *ctx) {
+  return (kvs_result)api_private::kvs_list_tuple( (api_private::kvs_container_handle)cont_hd, (const api_private::kvs_key*)prefix_key, (const api_private::kvs_key*)start_key, max_keys_to_list, (api_private::kvs_value*)value, (const api_private::kvs_list_context*)ctx);
+}
+
+kvs_result kvs_list_tuple_async(kvs_container_handle cont_hd, const kvs_key *prefix_key, const kvs_key *start_key, uint16_t max_keys_to_list, kvs_value *value, const kvs_list_context *ctx, kvs_callback_function cbfn) {
+  return (kvs_result)api_private::kvs_list_tuple_async( (api_private::kvs_container_handle)cont_hd, (const api_private::kvs_key*)prefix_key, (const api_private::kvs_key*)start_key, max_keys_to_list, (api_private::kvs_value*)value, \
+      (const api_private::kvs_list_context*)ctx, (api_private::kvs_callback_function)cbfn);
+}
+
+kvs_result kvs_retrieve_tuple_direct(kvs_container_handle cont_hd, const kvs_key *key, const kvs_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kvs_retrieve_context *ctx) {
+  return (kvs_result)api_private::kvs_retrieve_tuple_direct( (api_private::kvs_container_handle)cont_hd, (const api_private::kvs_key*)key, (const api_private::kvs_value*)value, client_rdma_key, client_rdma_qhandle, (const api_private::kvs_retrieve_context*)ctx);
+}
+
+kvs_result kvs_store_tuple_direct(kvs_container_handle cont_hd, const kvs_key *key, const kvs_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kvs_store_context *ctx) {
+  return (kvs_result)api_private::kvs_store_tuple_direct( (api_private::kvs_container_handle)cont_hd, (const api_private::kvs_key*)key, (const api_private::kvs_value*)value, client_rdma_key, client_rdma_qhandle, (const api_private::kvs_store_context*)ctx);
+}
+
+#endif
+
 kvs_result kvs_open_iterator(kvs_container_handle cont_hd, const kvs_iterator_context *ctx, kvs_iterator_handle *iter_hd) {
   return (kvs_result)api_private::kvs_open_iterator( (api_private::kvs_container_handle)cont_hd, (const api_private::kvs_iterator_context*)ctx, (api_private::kvs_iterator_handle*)iter_hd);
 }
@@ -202,6 +213,22 @@ kvs_result kvs_get_optimal_value_length (kvs_device_handle dev_hd, int32_t *opt_
   return (kvs_result)api_private::kvs_get_optimal_value_length( (api_private::kvs_device_handle)dev_hd, opt_value_length);
 }
 
+#ifdef KVS_REMOTE
+
+  kvs_result kvs_lock_tuple(kvs_container_handle cont_hd, const kvs_key *key, uint64_t instance_uuid, const kvs_lock_context *ctx) {
+    return (kvs_result)api_private::kvs_lock_tuple((api_private::kvs_container_handle)cont_hd, (const api_private::kvs_key*)key, 
+                                                  instance_uuid, (const api_private::kvs_lock_context*)ctx);
+  }
+
+  kvs_result kvs_unlock_tuple(kvs_container_handle cont_hd, const kvs_key *key, uint64_t instance_uuid, const kvs_lock_context *ctx) {
+    return (kvs_result)api_private::kvs_unlock_tuple((api_private::kvs_container_handle)cont_hd, (const api_private::kvs_key*)key, instance_uuid,
+                                                    (const api_private::kvs_lock_context*)ctx);
+
+  }
+
+#endif
+
+
 const char *kvs_errstr(int32_t errorno) {
   return api_private::kvs_errstr(errorno);
 }
@@ -242,12 +269,14 @@ typedef struct {
 
 api_private::kvs_key* _get_private_key(kvs_key* k) {
   api_private::kvs_key* key = new api_private::kvs_key();
-  key->key = k->key;
-  key->length = (api_private::kvs_key_t)k->length;
+  if(k) {
+      key->key = k->key;
+      key->length = (api_private::kvs_key_t)k->length;
+  }
   return key;
 }
 
-kvs_result result_mapping[api_private::KVS_ERR_CONT_MAX + 1];  // mapping table of kvs_result
+kvs_result result_mapping[api_private::KVS_ERR_CONT_OPEN + 1];  // mapping table of kvs_result
 void init_result_mapping() {
 #define RES_MAP(res1, res2) (result_mapping[api_private::res1] = res2)
   RES_MAP(KVS_SUCCESS, KVS_SUCCESS);
@@ -285,7 +314,7 @@ void init_result_mapping() {
   RES_MAP(KVS_ERR_UNCORRECTIBLE, KVS_ERR_SYS_IO);
   RES_MAP(KVS_ERR_VALUE_LENGTH_INVALID, KVS_ERR_VALUE_LENGTH_INVALID);
   RES_MAP(KVS_ERR_VALUE_LENGTH_MISALIGNED, KVS_ERR_PARAM_INVALID);
-  RES_MAP(KVS_ERR_VALUE_OFFSET_INVALID, KVS_ERR_VALUE_OFFSET_INVALID);
+  RES_MAP(KVS_ERR_VALUE_OFFSET_INVALID, KVS_ERR_VALUE_OFFSET_MISALIGNED);
   RES_MAP(KVS_ERR_VALUE_UPDATE_NOT_ALLOWED, KVS_ERR_VALUE_UPDATE_NOT_ALLOWED);
   RES_MAP(KVS_ERR_VENDOR, KVS_ERR_SYS_IO);
   RES_MAP(KVS_ERR_PERMISSION, KVS_ERR_SYS_IO);
@@ -325,10 +354,7 @@ void init_result_mapping() {
   RES_MAP(KVS_ERR_CONT_NAME, KVS_ERR_KS_NAME);
   RES_MAP(KVS_ERR_CONT_NOT_EXIST, KVS_ERR_KS_NOT_EXIST);
   RES_MAP(KVS_ERR_CONT_OPEN, KVS_ERR_KS_OPEN);
-  RES_MAP(KVS_ERR_CONT_PATH_TOO_LONG, KVS_ERR_PARAM_INVALID);
-  RES_MAP(KVS_ERR_CONT_MAX, KVS_ERR_SYS_IO);
   RES_MAP(KVS_ERR_ITERATOR_BUFFER_SIZE, KVS_ERR_BUFFER_SMALL);
-  RES_MAP(KVS_ERR_CONT_INDEX, KVS_ERR_KS_INDEX);
 }
 
 static void filter2context(kvs_key_group_filter* fltr, api_private::kvs_iterator_context* ctx) {
@@ -337,7 +363,7 @@ static void filter2context(kvs_key_group_filter* fltr, api_private::kvs_iterator
 }
 
 static kvs_result _check_key_length(kvs_key* key) {
-  if (key->length > UINT8_MAX)
+  if (key->length > KVS_MAX_KEY_LENGTH)
     return KVS_ERR_KEY_LENGTH_INVALID;
   return KVS_SUCCESS;
 }
@@ -348,8 +374,8 @@ void init_default_option(api_private::kvs_init_options &options) {
   options.aio.queuedepth = 64;
   options.memory.use_dpdk = 0;
   const char* configfile = "../kvssd_emul.conf";
-  options.emul_config_file = (char*)malloc(PATH_MAX);
-  strncpy(options.emul_config_file, configfile, strlen(configfile) + 1);
+  options.emul_config_file = (char *)malloc(PATH_MAX);
+  strncpy((char *)options.emul_config_file, configfile, strlen(configfile) + 1);
   char* core;
   core = options.udd.core_mask_str;
   *core = '0';
@@ -381,13 +407,13 @@ void init_env_from_cfgfile(api_private::kvs_init_options &options) {
   options.aio.queuedepth = queue_depth == 0 ? options.aio.queuedepth : (uint32_t)queue_depth;
   std::string cfg_file_path = cfg.getkv("emu", "cfg_file");
   if (cfg_file_path != "") {
-    strncpy(options.emul_config_file, cfg_file_path.c_str(), cfg_file_path.length() + 1);
+    strncpy((char *)options.emul_config_file, cfg_file_path.c_str(), cfg_file_path.length() + 1);
   }
 #ifdef WITH_SPDK
   options.memory.use_dpdk = 1;
-  if (strcmp(cfg.getkv("udd", "core_mask_str").c_str(), ""))
+  if (cfg.getkv("udd", "core_mask_str").c_str() != "")
     strcpy(options.udd.core_mask_str, cfg.getkv("udd", "core_mask_str").c_str());
-  if (strcmp(cfg.getkv("udd", "cq_thread_mask").c_str(), ""))
+  if (cfg.getkv("udd", "cq_thread_mask").c_str() != "")
     strcpy(options.udd.cq_thread_mask, cfg.getkv("udd", "cq_thread_mask").c_str());
   int mem_size = atoi(cfg.getkv("udd", "memory_size").c_str());
   options.udd.mem_size_mb = mem_size == 0 ? options.udd.mem_size_mb : (uint32_t)mem_size;
@@ -412,7 +438,7 @@ api_private::kvs_result init_env() {
   api_private::kvs_init_options options;
   init_env_from_cfgfile(options);
   ret = api_private::kvs_init_env(&options);
-  if (options.emul_config_file) free(options.emul_config_file);
+  if (options.emul_config_file) free((void *)options.emul_config_file);
   if (ret != api_private::KVS_SUCCESS)
     return ret;
 
@@ -550,52 +576,24 @@ kvs_result kvs_get_optimal_value_length(kvs_device_handle dev_hd, uint32_t *opt_
   return convert_res(ret);
 }
 
-kvs_result kvs_create_key_space(kvs_device_handle dev_hd,
-  kvs_key_space_name *key_space_name, uint64_t size, kvs_option_key_space opt) {
-  if(key_space_name == NULL || key_space_name->name == NULL){
-    return KVS_ERR_PARAM_INVALID;
-  }
-  if(strlen(key_space_name->name) != key_space_name->name_len){
-    return KVS_ERR_KS_NAME;
-  }
-  api_private::kvs_container_context ctx;
-  ctx.option.ordering = (api_private::kvs_key_order)opt.ordering;
-  api_private::kvs_device_handle dev_handle = (api_private::kvs_device_handle)dev_hd;
-  api_private::kvs_result ret = api_private::kvs_create_container(dev_handle,
-    key_space_name->name, size, &ctx);
-  return convert_res(ret);
+kvs_result kvs_create_key_space(kvs_device_handle dev_hd, kvs_key_space_name *key_space_name, uint64_t size, kvs_option_key_space opt) {
+  return KVS_SUCCESS;
 }
 
-kvs_result kvs_delete_key_space(kvs_device_handle dev_hd,
-  kvs_key_space_name *key_space_name) {
-  if(!key_space_name){
-    return KVS_ERR_PARAM_INVALID;
-  }
-  api_private::kvs_device_handle dev_handle = (api_private::kvs_device_handle)dev_hd;
-  api_private::kvs_result ret = api_private::kvs_delete_container(dev_handle,
-    key_space_name->name);
-  return convert_res(ret);
+kvs_result kvs_delete_key_space(kvs_device_handle dev_hd, kvs_key_space_name *key_space_name) {
+  return KVS_SUCCESS;
 }
 
-kvs_result kvs_list_key_spaces(kvs_device_handle dev_hd, uint32_t index,
-  uint32_t buffer_size, kvs_key_space_name *names, uint32_t *ks_cnt) {
-  api_private::kvs_device_handle dev_handle = (api_private::kvs_device_handle)dev_hd;
-  api_private::kvs_container_name* names_ptr = (api_private::kvs_container_name*)names;
-  api_private::kvs_result ret = api_private::kvs_list_containers(dev_handle,
-    index, buffer_size, names_ptr, ks_cnt);
-  return convert_res(ret);
+kvs_result kvs_list_key_spaces(kvs_device_handle dev_hd, uint32_t index, uint32_t buffer_size, kvs_key_space_name *names, uint32_t *ks_cnt) {
+  return KVS_ERR_KS_EXIST;
 }
 
 kvs_result kvs_open_key_space(kvs_device_handle dev_hd, char *name, kvs_key_space_handle *ks_hd) {
-  if(!ks_hd){
-    return KVS_ERR_PARAM_INVALID;
-  }
   api_private::kvs_container_handle cont_handle;
+
   api_private::kvs_device_handle dev_handle = (api_private::kvs_device_handle)dev_hd;
   api_private::kvs_result ret = api_private::kvs_open_container(dev_handle, name, &cont_handle);
-  if (ret == KVS_SUCCESS) {
-    *ks_hd = (kvs_key_space_handle)cont_handle;
-  }
+  *ks_hd = (kvs_key_space_handle)cont_handle;
   return convert_res(ret);
 }
 
@@ -605,18 +603,21 @@ kvs_result kvs_close_key_space(kvs_key_space_handle ks_hd) {
 }
 
 kvs_result kvs_get_key_space_info(kvs_key_space_handle ks_hd, kvs_key_space *ks) {
+  return KVS_ERR_UNSUPPORTED;
+  api_private::kvs_container cont;
+  api_private::kvs_container_handle cont_handle = (api_private::kvs_container_handle)ks_hd;
+
   if (ks == NULL)
     return KVS_ERR_PARAM_INVALID;
 
-  api_private::kvs_container cont;
-  cont.name = (api_private::kvs_container_name*)ks->name;
-  api_private::kvs_container_handle cont_handle = (api_private::kvs_container_handle)ks_hd;
   api_private::kvs_result ret = api_private::kvs_get_container_info(cont_handle, &cont);
   ks->capacity = cont.capacity;
   ks->count = cont.count;
   ks->free_size = cont.free_size;
   ks->opened = cont.opened;
-
+  // ks->name = (kvs_key_space_name*)malloc(sizeof(kvs_key_space_name));
+  ks->name->name_len = cont.name->name_len;
+  ks->name->name = cont.name->name;
   return convert_res(ret);
 }
 
@@ -660,9 +661,7 @@ kvs_result kvs_retrieve_kvp(kvs_key_space_handle ks_hd, kvs_key *key, kvs_option
 }
 
 static kvs_context op2context(uint8_t op) {
-  static kvs_context op_code[KVS_CMD_STORE + 1] = {(kvs_context)0, KVS_CMD_STORE,
-    KVS_CMD_RETRIEVE, KVS_CMD_DELETE, KVS_CMD_EXIST, KVS_CMD_ITER_CREATE,
-    KVS_CMD_ITER_DELETE, KVS_CMD_ITER_NEXT};
+  static kvs_context op_code[KVS_CMD_LIST + 1] = {0, KVS_CMD_STORE, KVS_CMD_RETRIEVE, KVS_CMD_DELETE, KVS_CMD_EXIST, KVS_CMD_ITER_CREATE, KVS_CMD_ITER_DELETE, KVS_CMD_ITER_NEXT, KVS_CMD_LIST};
   return op_code[op];
 }
 
@@ -670,8 +669,8 @@ void private_callback_func(api_private::kvs_callback_context* ioctx) {
   kvs_postprocess_context postctx;
 
   postctx.context = op2context(ioctx->opcode);
-  postctx.iter_hd = (kvs_iterator_handle*)(&ioctx->iter_hd);
-  postctx.ks_hd = (kvs_key_space_handle*)(&ioctx->cont_hd);
+  postctx.iter_hd = (kvs_iterator_handle*)ioctx->iter_hd;
+  postctx.ks_hd = (kvs_key_space_handle*)ioctx->cont_hd;
   postctx.value = (kvs_value*)ioctx->value;
   // key is in private1
   postprocess_data* pd = (postprocess_data*)ioctx->private1;
@@ -805,12 +804,6 @@ kvs_result kvs_create_iterator(kvs_key_space_handle ks_hd, kvs_option_iterator *
 
   ctx.option.iter_type = (api_private::kvs_iterator_type)iter_op->iter_type;
   filter2context(iter_fltr, &ctx);
-  /* the Samsung iterator needs to convert the little endian to the big end in the adi layer,
-   and the snia does not need to be converted When the cpu is a little endian.  
-   In order to ensure that the result of snia iterator read is correct, 
-   the snia iterator is reversed in the API layer.*/
-  ctx.bitmask = htobe32(ctx.bitmask);
-  ctx.bit_pattern = htobe32(ctx.bit_pattern);
 
   return convert_res(api_private::kvs_open_iterator(cont_hd, &ctx, (api_private::kvs_iterator_handle*)iter_hd));
 }
@@ -852,12 +845,9 @@ kvs_result kvs_iterate_next_async(kvs_key_space_handle ks_hd, kvs_iterator_handl
   api_private::kvs_iterator_handle it_hd = (api_private::kvs_iterator_handle)iter_hd;
   api_private::kvs_iterator_context ctx;
   iterator_data *iter_data = (iterator_data*)malloc(sizeof(iterator_data));
-  if(iter_data == NULL){
-    return KVS_ERR_SYS_IO;
-  }
-  if (iter_list == NULL || post_fn == NULL){
+
+  if (iter_list == NULL || post_fn == NULL)
     return KVS_ERR_PARAM_INVALID;
-  }
 
   iter_data->iter_list = iter_list;
   iter_data->pri_list.it_list = iter_list->it_list;
@@ -909,9 +899,6 @@ kvs_result kvs_exist_kv_pairs_async(kvs_key_space_handle ks_hd, uint32_t key_cnt
   list->num_keys = key_cnt;
 
   exist_data* ed = (exist_data*)malloc(sizeof(exist_data));
-  if(ed == NULL){
-    return KVS_ERR_SYS_IO;
-  }
   ed->list = list;
   ed->key = key_list;
   ctx.private1 = (void*)(new postprocess_data(post_fn, keys));
@@ -1017,3 +1004,5 @@ kvs_result kvs_delete_key_group_async(kvs_key_space_handle ks_hd, kvs_key_group_
   return KVS_SUCCESS;
 }
 #endif
+
+
diff --git a/PDK/core/src/api/src/driver_adapter/kvemuldriver.cpp b/PDK/core/src/api/src/driver_adapter/kvemuldriver.cpp
index 1d45734..be500e2 100644
--- a/PDK/core/src/api/src/driver_adapter/kvemuldriver.cpp
+++ b/PDK/core/src/api/src/driver_adapter/kvemuldriver.cpp
@@ -45,7 +45,8 @@
 
 #define MAX_POOLSIZE 10240
 #define use_pool
-namespace api_private {
+namespace api_private
+{
 inline void malloc_context(KvEmulator::kv_emul_context **ctx,
   std::condition_variable* ctx_pool_notfull,
   std::queue<KvEmulator::kv_emul_context *> &pool, std::mutex& pool_lock){
@@ -109,7 +110,7 @@ void on_io_complete(kv_io_context *context){
   iocb->result = (kvs_result)context->retcode;
   
   if(context->opcode == KV_OPC_GET)
-    iocb->value->actual_value_size = context->value->actual_value_size - context->value->offset;
+    iocb->value->actual_value_size = context->value->actual_value_size;
   
   if(ctx->syncio) {
     if(context->opcode == KV_OPC_OPEN_ITERATOR) {
@@ -127,10 +128,12 @@ void on_io_complete(kv_io_context *context){
   } else {
     if(context->opcode != KV_OPC_OPEN_ITERATOR && context->opcode != KV_OPC_CLOSE_ITERATOR) {
       if(ctx->on_complete && iocb){
-        ctx->on_complete(iocb);
+	ctx->on_complete(iocb);
       }
     }
-    free_context(ctx, &owner->ctx_pool_notfull, owner->kv_ctx_pool, owner->lock);
+    if(!ctx->syncio) { 
+      free_context(ctx, &owner->ctx_pool_notfull, owner->kv_ctx_pool, owner->lock);
+    }
   }
 }
 
@@ -146,7 +149,7 @@ int KvEmulator::create_queue(int qdepth, uint16_t qtype, kv_queue_handle *handle
   if (ret != KV_SUCCESS) {fprintf(stderr, "kv_create_queue failed 0x%x\n", ret);}
 
   if(qtype == COMPLETION_Q_TYPE && is_polling == 0) {
-    // Interrupt mode
+    // Interrupt mode 
     // set up interrupt handler
     kv_interrupt_handler int_func = (kv_interrupt_handler)malloc(sizeof(_kv_interrupt_handler));
     int_func->handler = interrupt_func_emu;
@@ -192,14 +195,12 @@ int32_t KvEmulator::init(const char* devpath, const char* configfile, int queued
   return ret;
 }
 
-KvEmulator::kv_emul_context* KvEmulator::prep_io_context(int opcode,
-  kvs_container_handle cont_hd, const kvs_key *key, const kvs_value *value, void *private1,
-  void *private2, bool syncio, kvs_callback_function cbfn){
+KvEmulator::kv_emul_context* KvEmulator::prep_io_context(int opcode, int contid, const kvs_key *key, const kvs_value *value, void *private1, void *private2, bool syncio, kvs_callback_function cbfn){
   kv_emul_context *ctx = NULL;
   malloc_context(&ctx, &this->ctx_pool_notfull, this->kv_ctx_pool, this->lock);
   ctx->on_complete = cbfn;
   ctx->iocb.opcode = opcode;
-  ctx->iocb.cont_hd = cont_hd;
+  //ctx->iocb.contid = contid;
   if(key) {
     ctx->iocb.key = (kvs_key*)key;
    } else {
@@ -224,11 +225,9 @@ KvEmulator::kv_emul_context* KvEmulator::prep_io_context(int opcode,
 }
 
 /* MAIN ENTRY POINT */
-int32_t KvEmulator::store_tuple(kvs_container_handle cont_hd, const kvs_key *key,
-  const kvs_value *value, kvs_store_option option, void *private1, void *private2,
-  bool syncio, kvs_callback_function cbfn) {
-  auto ctx = prep_io_context(IOCB_ASYNC_PUT_CMD, cont_hd, key, value, private1,
-    private2, syncio, cbfn);
+int32_t KvEmulator::store_tuple(int contid, const kvs_key *key, const kvs_value *value, kvs_store_option option, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+
+  auto ctx = prep_io_context(IOCB_ASYNC_PUT_CMD, contid, key, value, private1, private2, syncio, /*this->user_io_complete*/cbfn);
   kv_postprocess_function f = {on_io_complete, (void*)ctx};
 
   kv_store_option option_adi;
@@ -249,7 +248,6 @@ int32_t KvEmulator::store_tuple(kvs_container_handle cont_hd, const kvs_key *key
       break;
     default:
       fprintf(stderr, "WARN: Wrong store option\n");
-      free_context(ctx, &this->ctx_pool_notfull, this->kv_ctx_pool, this->lock);
       return KVS_ERR_OPTION_INVALID;
     }
   } else {
@@ -269,21 +267,14 @@ int32_t KvEmulator::store_tuple(kvs_container_handle cont_hd, const kvs_key *key
       break;
     default:
       fprintf(stderr, "WARN: Wrong store option\n");
-      free_context(ctx, &this->ctx_pool_notfull, this->kv_ctx_pool, this->lock);
       return KVS_ERR_OPTION_INVALID;
     }
   }
   
   ctx->key = (kv_key*)key;
   ctx->value = (kv_value*)value;
-  int ret = kv_store(this->sqH, this->nsH, cont_hd->keyspace_id, (kv_key*)key,
-    (kv_value*)value, option_adi, &f);
-  if(ret != KV_SUCCESS) {
-    fprintf(stderr, "kv_store failed with error:  0x%X\n", ret);
-    free_context(ctx, &this->ctx_pool_notfull, this->kv_ctx_pool, this->lock);
-    return ret;
-  }
-  
+  int ret = kv_store(this->sqH, this->nsH, (kv_key*)key, (kv_value*)value, option_adi, &f);
+
   if(syncio) {
     std::unique_lock<std::mutex> lock_s(ctx->lock_sync);
     while(ctx->done_sync == 0)
@@ -297,11 +288,9 @@ int32_t KvEmulator::store_tuple(kvs_container_handle cont_hd, const kvs_key *key
 }
 
 
-int32_t KvEmulator::retrieve_tuple(kvs_container_handle cont_hd, const kvs_key *key,
-  kvs_value *value, kvs_retrieve_option option, void *private1, void *private2,
-  bool syncio, kvs_callback_function cbfn) {
-  auto ctx = prep_io_context(IOCB_ASYNC_GET_CMD, cont_hd, key, value, private1,
-    private2, syncio, cbfn);
+int32_t KvEmulator::retrieve_tuple(int contid, const kvs_key *key, kvs_value *value, kvs_retrieve_option option/*uint8_t option*/, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+
+  auto ctx = prep_io_context(IOCB_ASYNC_GET_CMD, contid, key, value, private1, private2, syncio, cbfn);
   kv_postprocess_function f = {on_io_complete, (void*)ctx};
 
   kv_retrieve_option option_adi;
@@ -319,14 +308,7 @@ int32_t KvEmulator::retrieve_tuple(kvs_container_handle cont_hd, const kvs_key *
 
   ctx->key = (kv_key*)key;
   ctx->value = (kv_value*)value;
-  int ret = kv_retrieve(this->sqH, this->nsH, cont_hd->keyspace_id, (kv_key*)key,
-    option_adi, (kv_value*)value, &f);
-  if(ret != KV_SUCCESS) {
-    fprintf(stderr, "kv_retrieve failed with error:  0x%X\n", ret);
-    free_context(ctx, &this->ctx_pool_notfull, this->kv_ctx_pool, this->lock);
-    return ret;
-  }
-  
+  int ret = kv_retrieve(this->sqH, this->nsH, (kv_key*)key, option_adi, (kv_value*)value, &f);
   if(syncio) {
     std::unique_lock<std::mutex> lock_s(ctx->lock_sync);
     while(ctx->done_sync == 0)
@@ -340,10 +322,8 @@ int32_t KvEmulator::retrieve_tuple(kvs_container_handle cont_hd, const kvs_key *
   return ret;
 }
 
-int32_t KvEmulator::delete_tuple(kvs_container_handle cont_hd, const kvs_key *key,
-  kvs_delete_option option, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
-  auto ctx = prep_io_context(IOCB_ASYNC_DEL_CMD, cont_hd, key, NULL, private1, private2,
-    syncio, cbfn);
+int32_t KvEmulator::delete_tuple(int contid, const kvs_key *key, kvs_delete_option option/*uint8_t option*/, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+  auto ctx = prep_io_context(IOCB_ASYNC_DEL_CMD, contid, key, NULL, private1, private2, syncio, cbfn);
   kv_postprocess_function f = {on_io_complete, (void*)ctx};
 
   kv_delete_option option_adi;
@@ -354,13 +334,7 @@ int32_t KvEmulator::delete_tuple(kvs_container_handle cont_hd, const kvs_key *ke
   
   ctx->key = (kv_key*)key;
   ctx->value = NULL;
-  int ret =  kv_delete(this->sqH, this->nsH, cont_hd->keyspace_id, (kv_key*)key, option_adi, &f);
-  if(ret != KV_SUCCESS) {
-    fprintf(stderr, "kv_delete failed with error:  0x%X\n", ret);
-    free_context(ctx, &this->ctx_pool_notfull, this->kv_ctx_pool, this->lock);
-    return ret;
-  }
-  
+  int ret =  kv_delete(this->sqH, this->nsH, (kv_key*)key, option_adi, &f);
   if(syncio) {
     std::unique_lock<std::mutex> lock_s(ctx->lock_sync);
     while(ctx->done_sync == 0)
@@ -373,11 +347,9 @@ int32_t KvEmulator::delete_tuple(kvs_container_handle cont_hd, const kvs_key *ke
   return ret;
 }
 
-int32_t KvEmulator::exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt,
-  const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1,
-  void *private2,bool syncio, kvs_callback_function cbfn) {
-  auto ctx = prep_io_context(IOCB_ASYNC_CHECK_KEY_EXIST_CMD, cont_hd, keys, NULL,
-    private1, private2, syncio, cbfn);
+int32_t KvEmulator::exist_tuple(int contid, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+
+  auto ctx = prep_io_context(IOCB_ASYNC_CHECK_KEY_EXIST_CMD, contid, keys, NULL, private1, private2, syncio, cbfn);
   ctx->iocb.key_cnt = key_cnt;
   ctx->iocb.result_buffer = result_buffer;
   kv_postprocess_function f = {on_io_complete, (void*)ctx};
@@ -386,13 +358,7 @@ int32_t KvEmulator::exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt,
   ctx->key = NULL;
   ctx->value = NULL;
   
-  int ret = kv_exist(this->sqH, this->nsH, cont_hd->keyspace_id, (kv_key*)keys,
-    key_cnt, buffer_size, result_buffer, &f);
-  if(ret != KV_SUCCESS) {
-    fprintf(stderr, "kv_exist failed with error:  0x%X\n", ret);
-    free_context(ctx, &this->ctx_pool_notfull, this->kv_ctx_pool, this->lock);
-    return ret;
-  }
+  int ret = kv_exist(this->sqH, this->nsH, (kv_key*)keys, key_cnt, buffer_size, result_buffer, &f);
 
   if(syncio) {
     std::unique_lock<std::mutex> lock_s(ctx->lock_sync);
@@ -406,58 +372,13 @@ int32_t KvEmulator::exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt,
   return ret;
 }
 
-int32_t KvEmulator::trans_store_cmd_opt(kvs_store_option kvs_opt,
-                                            kv_store_option *kv_opt){
-  if(!kvs_opt.kvs_store_compress) {
-    // Default: no compression
-    switch(kvs_opt.st_type) {
-    case KVS_STORE_POST:
-      *kv_opt = KV_STORE_OPT_DEFAULT;
-      break;
-    case KVS_STORE_UPDATE_ONLY:
-      *kv_opt = KV_STORE_OPT_UPDATE_ONLY;
-      break;
-    case KVS_STORE_NOOVERWRITE:
-      *kv_opt = KV_STORE_OPT_IDEMPOTENT;
-      break;
-    case KVS_STORE_APPEND:
-      *kv_opt = KV_STORE_OPT_APPEND;
-      break;
-    default:
-      fprintf(stderr, "WARN: Wrong store option\n");
-      return KVS_ERR_OPTION_INVALID;
-    }
-  } else {
-    // compression
-    switch(kvs_opt.st_type) {
-    case KVS_STORE_POST:
-      *kv_opt = KV_STORE_OPT_POST_WITH_COMPRESS;
-      break;
-    case KVS_STORE_UPDATE_ONLY:
-      *kv_opt = KV_STORE_OPT_UPDATE_ONLY_COMPRESS;
-      break;
-    case KVS_STORE_NOOVERWRITE:
-      *kv_opt = KV_STORE_OPT_NOOVERWRITE_COMPRESS;
-      break;
-    case KVS_STORE_APPEND:
-      *kv_opt = KV_STORE_OPT_APPEND_COMPRESS;
-      break;
-    default:
-      fprintf(stderr, "WARN: Wrong store option\n");
-      return KVS_ERR_OPTION_INVALID;
-    }
-  }
-
-  return KVS_SUCCESS;
-}
-
-int32_t KvEmulator::open_iterator(kvs_container_handle cont_hd, kvs_iterator_option option,
-  uint32_t bitmask, uint32_t bit_pattern, kvs_iterator_handle *iter_hd) {
+int32_t KvEmulator::open_iterator(int contid,  /*uint8_t option*/kvs_iterator_option option, uint32_t bitmask,
+				  uint32_t bit_pattern, kvs_iterator_handle *iter_hd) {
+  
   int ret = 0;
   //kvs_iterator_handle iterh = (kvs_iterator_handle)malloc(sizeof(struct _kvs_iterator_handle));
   
-  auto ctx = prep_io_context(IOCB_ASYNC_ITER_OPEN_CMD, cont_hd, 0, 0,
-    (void*)iter_hd, 0/*private1, private2*/, TRUE, 0);
+  auto ctx = prep_io_context(IOCB_ASYNC_ITER_OPEN_CMD, 0, 0, 0, (void*)iter_hd, 0/*private1, private2*/, TRUE, 0);
   kv_group_condition grp_cond = {bitmask, bit_pattern};
   kv_postprocess_function f = {on_io_complete, (void*)ctx};
 
@@ -468,7 +389,8 @@ int32_t KvEmulator::open_iterator(kvs_container_handle cont_hd, kvs_iterator_opt
   } else {
     option_adi = KV_ITERATOR_OPT_KV;
   }
-  ret = kv_open_iterator(this->sqH, this->nsH, cont_hd->keyspace_id, option_adi, &grp_cond, &f);
+  ret = kv_open_iterator(this->sqH, this->nsH, /*option.kvs_iterator_opt_key ? KV_ITERATOR_OPT_KEY : KV_ITERATOR_OPT_KV*/option_adi, &grp_cond, &f);
+
   if(ret != KV_SUCCESS) {
     fprintf(stderr, "kv_open_iterator failed with error:  0x%X\n", ret);
     free_context(ctx, &this->ctx_pool_notfull, this->kv_ctx_pool, this->lock);
@@ -492,7 +414,8 @@ int32_t KvEmulator::open_iterator(kvs_container_handle cont_hd, kvs_iterator_opt
   return ret;
 }
 
-int32_t KvEmulator::close_iterator(kvs_container_handle cont_hd, kvs_iterator_handle hiter) {
+int32_t KvEmulator::close_iterator(int contid, kvs_iterator_handle hiter) {
+
   int ret = 0;
   auto ctx = prep_io_context(IOCB_ASYNC_ITER_CLOSE_CMD, 0, 0, 0, 0,0/*private1, private2*/, TRUE, 0);
 
@@ -523,14 +446,14 @@ int32_t KvEmulator::close_iterator(kvs_container_handle cont_hd, kvs_iterator_ha
 }
 
 
-int32_t KvEmulator::close_iterator_all(kvs_container_handle cont_hd) {
+int32_t KvEmulator::close_iterator_all(int contid) {
 
   fprintf(stderr, "WARN: this feature is not supported in the emulator\n");
   return KVS_ERR_OPTION_INVALID;
 }
 
 
-int32_t KvEmulator::list_iterators(kvs_container_handle cont_hd, kvs_iterator_info *kvs_iters, uint32_t count) {
+int32_t KvEmulator::list_iterators(int contid, kvs_iterator_info *kvs_iters, uint32_t count) {
 
   //auto ctx = prep_io_context(IOCB_ASYNC_ITER_OPEN_CMD, 0, 0, 0, 0, 0, TRUE, 0);
   //uint32_t count = SAMSUNG_MAX_ITERATORS;
@@ -540,7 +463,7 @@ int32_t KvEmulator::list_iterators(kvs_container_handle cont_hd, kvs_iterator_in
 
   for(uint32_t i = 0; i< count; i++){
     if(kvs_iters[i].status == 1) fprintf(stdout, "found handler %d %x %x\n",
-      kvs_iters[i].iter_handle, kvs_iters[i].bit_pattern, kvs_iters[i].bitmask);
+					 kvs_iters[i].iter_handle, kvs_iters[i].bit_pattern, kvs_iters[i].bitmask);
   }
   
   //free(ctx);
@@ -548,12 +471,11 @@ int32_t KvEmulator::list_iterators(kvs_container_handle cont_hd, kvs_iterator_in
   return res;
 }
 
-int32_t KvEmulator::iterator_next(kvs_container_handle cont_hd, kvs_iterator_handle hiter, kvs_iterator_list *iter_list, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+int32_t KvEmulator::iterator_next(kvs_iterator_handle hiter, kvs_iterator_list *iter_list, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
 
   int ret = 0;
-  kv_emul_context* ctx = prep_io_context(IOCB_ASYNC_ITER_NEXT_CMD, cont_hd, 0, 0, private1, private2, syncio, cbfn);
+  auto ctx = prep_io_context(IOCB_ASYNC_ITER_NEXT_CMD, 0, 0, 0, private1, private2, syncio, cbfn);
   kv_postprocess_function f = {on_io_complete, (void*)ctx};
-  ctx->iocb.iter_hd = hiter;
 
   ret = kv_iterator_next(this->sqH, this->nsH, hiter/*iterh_adi*/, (kv_iterator_list *)iter_list, &f);
   if(ret != KV_SUCCESS) {
@@ -619,31 +541,23 @@ int32_t KvEmulator::get_total_size(int64_t *dev_capa){
 
 int32_t KvEmulator::process_completions(int max)
 {
-  int ret;  
-  uint32_t processed = 0;
-
-  ret = kv_poll_completion(this->cqH, 0, &processed);
-  if (ret != KV_SUCCESS && ret != KV_WRN_MORE)
-    fprintf(stdout, "Polling failed\n");
-  
-  return processed;
-}
+        int ret;  
+        uint32_t processed = 0;
 
-void KvEmulator::wait_for_io(kv_emul_context *ctx) {
-    std::unique_lock<std::mutex> lock(ctx->lock_sync);
+	ret = kv_poll_completion(this->cqH, 0, &processed);
+	if (ret != KV_SUCCESS && ret != KV_WRN_MORE)
+	  fprintf(stdout, "Polling failed\n");
 
-    while(!ctx->done_sync)
-        ctx->done_cond_sync.wait(lock);
+	return processed;
 }
 
+
 KvEmulator::~KvEmulator() {
-  std::unique_lock<std::mutex> lock(this->lock);
   while(!this->kv_ctx_pool.empty()) {
     auto p = this->kv_ctx_pool.front();
     this->kv_ctx_pool.pop();
     delete p;
   }
-  lock.unlock();
   
   // shutdown device
   if(this->int_handler) {
diff --git a/PDK/core/src/api/src/driver_adapter/kvkdd.cpp b/PDK/core/src/api/src/driver_adapter/kvkdd.cpp
index 98356eb..419083f 100644
--- a/PDK/core/src/api/src/driver_adapter/kvkdd.cpp
+++ b/PDK/core/src/api/src/driver_adapter/kvkdd.cpp
@@ -34,7 +34,6 @@
 
 #include <iostream>
 #include <fstream>
-#include <map>
 
 #include "kvs_utils.h"
 #include "kvkdd.hpp"
@@ -115,7 +114,15 @@ KvsRWLogger kvkdd_logger;
 namespace api_private {
 inline void free_if_error(int ret, KDDriver::kv_kdd_context *ctx) {
  if (ret != 0 && ctx) {
-    delete ctx;
+     if(ctx->iocb.opcode == IOCB_ASYNC_LIST_CMD){
+        if(ctx->iocb.key && ctx->iocb.key->key) {
+            kvs_free(ctx->iocb.key->key);
+            ctx->iocb.key->key = NULL;
+        }
+        if(ctx->iocb.key)delete ctx->iocb.key;
+        ctx->iocb.key = NULL;
+     }
+     delete ctx;
   }
 }
 
@@ -382,6 +389,121 @@ inline kvs_result convert_return_code(int opcode, int dev_status_code)
       return KVS_ERR_SYS_IO;
     }
   }
+  else if (opcode ==  IOCB_ASYNC_LIST_CMD)
+  {
+    if (dev_status_code == 0x301)
+    {
+      return KVS_ERR_VALUE_LENGTH_INVALID;
+    }
+    else if (dev_status_code == 0x302)
+    {
+      return KVS_ERR_VALUE_OFFSET_INVALID;
+    }
+    else if (dev_status_code == 0x303)
+    {
+      return KVS_ERR_KEY_LENGTH_INVALID;
+    }
+    else if (dev_status_code == 0x304)
+    {
+      return KVS_ERR_OPTION_INVALID; // for invalid option
+    }
+    else if (dev_status_code == 0x308)
+    {
+      return KVS_ERR_VALUE_LENGTH_MISALIGNED;
+    }
+    else if (dev_status_code == 0x310)
+    {
+      return KVS_ERR_KEY_NOT_EXIST;
+    }
+    else if (dev_status_code == 0x311)
+    {
+      return KVS_ERR_UNCORRECTIBLE;
+    }
+    else if (dev_status_code == 0x390)
+    {
+      return KVS_ERR_NONEXIST_PREFIX;
+    }
+    else if (dev_status_code == 0x391)
+    {
+      return KVS_ERR_NONEXIST_STARTKEY;
+    }
+    else if (dev_status_code == 0x392)
+    {
+      return KVS_ERR_UNSUPPORTED_OPTION;
+    }
+    else if (dev_status_code == 0x393)
+    {
+      return KVS_ERR_END_OF_LIST;
+    }
+    else
+    {
+      return KVS_ERR_SYS_IO;
+    }
+  }
+  else if (opcode ==  IOCB_ASYNC_LOCK_CMD)
+  {
+    if (dev_status_code == 0x303)
+    {
+      return KVS_ERR_KEY_LENGTH_INVALID;
+    }
+    else if (dev_status_code == 0x305)
+    {
+      return KVS_ERR_NS_INVALID;
+    }
+	else if (dev_status_code == 0x312)
+	{
+      return KVS_ERR_DEV_CAPACITY;
+	}
+    else if (dev_status_code == 0x397)
+    {
+      return KVS_ERR_KEY_IS_LOCKED;
+    }
+    else if (dev_status_code == 0x3A1)
+    {
+      return KVS_ERR_LOCK_EXPIRED;
+    }
+    else
+    {
+      return KVS_ERR_SYS_IO;
+    }
+  }
+  else if (opcode ==  IOCB_ASYNC_UNLOCK_CMD)
+  {
+    if (dev_status_code == 0x303)
+    {
+      return KVS_ERR_KEY_LENGTH_INVALID;
+    }
+    else if (dev_status_code == 0x305)
+    {
+      return KVS_ERR_NS_INVALID;
+    }
+    else if (dev_status_code == 0x310)
+    {
+      return KVS_ERR_KEY_NOT_EXIST;
+    }
+    else if (dev_status_code == 0x398)
+    {
+      return KVS_ERR_LOCK_UUID_MISMATCH;
+    }
+    else if (dev_status_code == 0x399)
+    {
+      return KVS_ERR_NONEXIST_WRITER;
+    }
+    else if (dev_status_code == 0x3A0)
+    {
+      return KVS_ERR_NONEXIST_READER;
+    }
+    else if (dev_status_code == 0x3A1)
+    {
+      return KVS_ERR_LOCK_EXPIRED;
+    }
+    else
+    {
+      return KVS_ERR_SYS_IO;
+    }
+  }
+
+
   return KVS_ERR_SYS_IO;
 
 }
@@ -403,10 +525,17 @@ void kdd_on_io_complete(kv_io_context *context){
   if(iocb->opcode == IOCB_ASYNC_GET_CMD) {
     iocb->value->actual_value_size = context->value->actual_value_size;
     iocb->value->length = context->value->length;
-    //when it is not a partial retrieve and actual length bigger than buffer length user inputted
-    if(iocb->value->length < iocb->value->actual_value_size)
+    if(iocb->value->length < iocb->value->actual_value_size)  //actual length bigger than buffer length user inputted
       iocb->result = KVS_ERR_BUFFER_SMALL;
   }
+  else if(iocb->opcode == IOCB_ASYNC_LIST_CMD) {
+    iocb->value->actual_value_size = context->value->actual_value_size;
+    iocb->value->length = context->value->length;
+    if(iocb->value->length < iocb->value->actual_value_size)  //actual length bigger than buffer length user inputted
+      iocb->result = KVS_ERR_BUFFER_SMALL;
+    //if(iocb->result == 0 && iocb->value->actual_value_size == 0)
+    //    iocb->result = KVS_ERR_END_OF_LIST; //this shouldnt happen
+  }
   else if (iocb->opcode == IOCB_ASYNC_ITER_NEXT_CMD && context->retcode == 0) {
 
     kvs_iterator_list* list = (kvs_iterator_list*) iocb->result_buffer;
@@ -439,9 +568,19 @@ void kdd_on_io_complete(kv_io_context *context){
     ctx->done_cond_sync.notify_one();
 
   } else {
+      if(iocb->opcode == IOCB_ASYNC_LIST_CMD){
+          if(iocb->key && iocb->key->key){
+              kvs_free(iocb->key->key);
+              iocb->key->key = NULL;
+          }
+          if(iocb->key) delete iocb->key;
+          iocb->key = NULL;
+      }
     if(ctx->on_complete && iocb) {
       ctx->on_complete(iocb);
     }
+    if (iocb->opcode == IOCB_ASYNC_ITER_NEXT_CMD && iocb->iter_hd)
+	  free(iocb->iter_hd);
     delete ctx;
     ctx = NULL;
   }
@@ -507,27 +646,64 @@ int32_t KDDriver::init(const char* devpath, const char* configfile, int queue_de
 
 /* MAIN ENTRY POINT */
 
-int32_t KDDriver::store_tuple(kvs_container_handle cont_hd, const kvs_key *key,
-  const kvs_value *value, kvs_store_option option, void *private1, void *private2,
-  bool syncio, kvs_callback_function cbfn) {
-  auto ctx = prep_io_context(IOCB_ASYNC_PUT_CMD, cont_hd, key, value, private1,
-    private2, syncio, cbfn);
+int32_t KDDriver::store_tuple(int contid, const kvs_key *key, const kvs_value *value, kvs_store_option option, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+
+  auto ctx = prep_io_context(IOCB_ASYNC_PUT_CMD, contid, key, value, private1, private2, syncio, cbfn);
   kv_postprocess_function f = {
     kdd_on_io_complete, (void*)ctx
   };
+
   kv_store_option option_adi;
-  if(trans_store_cmd_opt(option, &option_adi)) {
-    delete ctx;
-    return KVS_ERR_OPTION_INVALID;
+  if(!option.kvs_store_compress) {
+    // Default: no compression
+    switch(option.st_type) {
+    case KVS_STORE_POST:
+      option_adi = KV_STORE_OPT_DEFAULT;
+      break;
+    case KVS_STORE_UPDATE_ONLY:
+      option_adi = KV_STORE_OPT_UPDATE_ONLY;
+      break;
+    case KVS_STORE_NOOVERWRITE:
+      option_adi = KV_STORE_OPT_IDEMPOTENT;
+      break;
+    case KVS_STORE_APPEND:
+      option_adi = KV_STORE_OPT_APPEND;
+      break;
+    default:
+      fprintf(stderr, "WARN: Wrong store option\n");
+      delete ctx;
+      ctx = NULL;
+      return KVS_ERR_OPTION_INVALID;
+    }
+  } else {
+    // compression
+    switch(option.st_type) {
+    case KVS_STORE_POST:
+      option_adi = KV_STORE_OPT_POST_WITH_COMPRESS;
+      break;
+    case KVS_STORE_UPDATE_ONLY:
+      option_adi = KV_STORE_OPT_UPDATE_ONLY_COMPRESS;
+      break;
+    case KVS_STORE_NOOVERWRITE:
+      option_adi = KV_STORE_OPT_NOOVERWRITE_COMPRESS;
+      break;
+    case KVS_STORE_APPEND:
+      option_adi = KV_STORE_OPT_APPEND_COMPRESS;
+      break;
+    default:
+      delete ctx;
+      ctx = NULL;
+      fprintf(stderr, "WARN: Wrong store option\n");
+      return KVS_ERR_OPTION_INVALID;
+    }
   }
+ 
+  int ret = kv_store(this->sqH, this->nsH, (kv_key*)key, (kv_value*)value, option_adi, &f);
 
-  int ret = kv_store(this->sqH, this->nsH, cont_hd->keyspace_id, (kv_key*)key,
-    (kv_value*)value, option_adi, &f);
   while(ret == KV_ERR_QUEUE_IS_FULL) {
-    ret = kv_store(this->sqH, this->nsH, cont_hd->keyspace_id, (kv_key*)key,
-      (kv_value*)value, option_adi, &f);
+    ret = kv_store(this->sqH, this->nsH, (kv_key*)key, (kv_value*)value, option_adi, &f);
   }
-  
+  																								
   if(syncio && ret == 0) {
     wait_for_io(ctx);
     ret = ctx->iocb.result;
@@ -537,9 +713,178 @@ int32_t KDDriver::store_tuple(kvs_container_handle cont_hd, const kvs_key *key,
 
   free_if_error(ret, ctx);
 
+
+  return ret;
+}
+
+#ifdef KVS_REMOTE
+int32_t KDDriver::list_tuple(int contid, const kvs_key *prefix_key, const kvs_key *start_key, uint16_t max_keys_to_list, kvs_value *value, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+
+  uint16_t key_offset = 0, key_len = 0;
+  api_private::kvs_key* k = new api_private::kvs_key();
+  char *key_buff   = (char*)kvs_malloc(KVS_MAX_KEY_LENGTH, 4096);
+
+  if(prefix_key && prefix_key->length){
+      key_len = prefix_key->length;
+      key_offset = key_len;
+      memcpy(key_buff, prefix_key->key, prefix_key->length);
+  }
+  if(start_key && start_key->length){
+      key_len += start_key->length;
+      memcpy(key_buff+key_offset, start_key->key, start_key->length);
+  }
+
+  if(key_len){
+      k->key = key_buff;
+      k->length = key_len;
+  }else{
+    kvs_free(key_buff);
+    delete k;
+    k = NULL;
+  }
+
+  auto ctx = prep_io_context(IOCB_ASYNC_LIST_CMD, contid, k, value, private1, private2, syncio, cbfn, prefix_key, start_key, max_keys_to_list);
+  kv_postprocess_function f = {
+    kdd_on_io_complete, (void*)ctx
+  };
+
+
+  int ret = kv_list(this->sqH, this->nsH, (kv_key*)k, key_offset, max_keys_to_list, (kv_value*)value, &f);
+
+  while(ret == KV_ERR_QUEUE_IS_FULL) {
+    ret = kv_list(this->sqH, this->nsH, (kv_key*)k, key_offset, max_keys_to_list, (kv_value*)value, &f);
+  }
+
+  if(syncio && ret == 0) {
+    wait_for_io(ctx);
+    ret = ctx->iocb.result;
+    if(ctx->iocb.key && ctx->iocb.key->key){
+        kvs_free(ctx->iocb.key->key);
+        ctx->iocb.key->key = NULL;
+    }
+    if(ctx->iocb.key) delete ctx->iocb.key;
+    ctx->iocb.key = NULL;
+    delete ctx; ctx = NULL;
+  }
+
+  free_if_error(ret, ctx);
+
+
   return ret;
 }
 
+int32_t KDDriver::retrieve_tuple_direct(int contid, const kvs_key *key, const kvs_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, kvs_retrieve_option option/*uint8_t option*/,
+                                        void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+
+  auto ctx = prep_io_context(IOCB_ASYNC_GET_CMD, contid, key, value, private1, private2, syncio, cbfn);
+  kv_postprocess_function f = {kdd_on_io_complete, (void*)ctx};
+
+  kv_retrieve_option option_adi;
+  if(!option.kvs_retrieve_delete) {
+    if(!option.kvs_retrieve_decompress)
+      option_adi = KV_RETRIEVE_OPT_DEFAULT;
+    else
+      option_adi = KV_RETRIEVE_OPT_DECOMPRESS;
+  } else {
+    if(!option.kvs_retrieve_decompress)
+      option_adi = KV_RETRIEVE_OPT_DELETE;
+    else
+      option_adi = KV_RETRIEVE_OPT_DECOMPRESS_DELETE;
+  }
+
+  int ret = kv_retrieve_direct(this->sqH, this->nsH, (kv_key*)key, option_adi, (kv_value *)value , client_rdma_key, client_rdma_qhandle, &f);
+
+  while(ret == KV_ERR_QUEUE_IS_FULL) {
+    ret = kv_retrieve_direct(this->sqH, this->nsH, (kv_key*)key, option_adi, (kv_value *)value, client_rdma_key, client_rdma_qhandle, &f);
+  }
+
+  if(syncio && ret == 0) {
+
+     wait_for_io(ctx);
+     ret = ctx->iocb.result;
+
+    delete ctx;
+    ctx = NULL;
+  }
+
+  free_if_error(ret, ctx);
+  return ret;
+
+}
+
+int32_t KDDriver::store_tuple_direct(int contid, const kvs_key *key, const kvs_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, kvs_store_option option/*uint8_t option*/,
+                                        void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+
+  auto ctx = prep_io_context(IOCB_ASYNC_GET_CMD, contid, key, value, private1, private2, syncio, cbfn);
+  kv_postprocess_function f = {kdd_on_io_complete, (void*)ctx};
+
+  kv_store_option option_adi;
+  if(!option.kvs_store_compress) {
+
+    switch(option.st_type) {
+    case KVS_STORE_POST:
+      option_adi = KV_STORE_OPT_DEFAULT;
+      break;
+    case KVS_STORE_UPDATE_ONLY:
+      option_adi = KV_STORE_OPT_UPDATE_ONLY;
+      break;
+    case KVS_STORE_NOOVERWRITE:
+      option_adi = KV_STORE_OPT_IDEMPOTENT;
+      break;
+    case KVS_STORE_APPEND:
+      option_adi = KV_STORE_OPT_APPEND;
+      break;
+    default:
+      fprintf(stderr, "WARN: Wrong store option\n");
+      delete ctx;
+      ctx = NULL;
+      return KVS_ERR_OPTION_INVALID;
+    }
+  } else {
+
+    switch(option.st_type) {
+    case KVS_STORE_POST:
+      option_adi = KV_STORE_OPT_POST_WITH_COMPRESS;
+      break;
+    case KVS_STORE_UPDATE_ONLY:
+      option_adi = KV_STORE_OPT_UPDATE_ONLY_COMPRESS;
+      break;
+    case KVS_STORE_NOOVERWRITE:
+      option_adi = KV_STORE_OPT_NOOVERWRITE_COMPRESS;
+      break;
+    case KVS_STORE_APPEND:
+      option_adi = KV_STORE_OPT_APPEND_COMPRESS;
+      break;
+    default:
+      delete ctx;
+      ctx = NULL;
+      fprintf(stderr, "WARN: Wrong store option\n");
+      return KVS_ERR_OPTION_INVALID;
+    }
+  }
+
+  int ret = kv_store_direct(this->sqH, this->nsH, (kv_key*)key, option_adi, (kv_value *)value , client_rdma_key, client_rdma_qhandle, &f);
+
+  while(ret == KV_ERR_QUEUE_IS_FULL) {
+    ret = kv_store_direct(this->sqH, this->nsH, (kv_key*)key, option_adi, (kv_value *)value, client_rdma_key, client_rdma_qhandle, &f);
+  }
+
+  if(syncio && ret == 0) {
+
+     wait_for_io(ctx);
+     ret = ctx->iocb.result;
+
+    delete ctx;
+    ctx = NULL;
+  }
+
+  free_if_error(ret, ctx);
+  return ret;
+
+}
+
+#endif
+
 void KDDriver::wait_for_io(kv_kdd_context *ctx) {
     std::unique_lock<std::mutex> lock(ctx->lock_sync);
 
@@ -548,10 +893,9 @@ void KDDriver::wait_for_io(kv_kdd_context *ctx) {
 
 }
 
-int32_t KDDriver::retrieve_tuple(kvs_container_handle cont_hd, const kvs_key *key,
-  kvs_value *value, kvs_retrieve_option option, void *private1, void *private2,
-  bool syncio, kvs_callback_function cbfn) {
-  auto ctx = prep_io_context(IOCB_ASYNC_GET_CMD, cont_hd, key, value, private1, private2, syncio, cbfn);
+int32_t KDDriver::retrieve_tuple(int contid, const kvs_key *key, kvs_value *value, kvs_retrieve_option option, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+
+  auto ctx = prep_io_context(IOCB_ASYNC_GET_CMD, contid, key, value, private1, private2, syncio, cbfn);
   kv_postprocess_function f = {kdd_on_io_complete, (void*)ctx};
 
   kv_retrieve_option option_adi;
@@ -567,29 +911,27 @@ int32_t KDDriver::retrieve_tuple(kvs_container_handle cont_hd, const kvs_key *ke
       option_adi = KV_RETRIEVE_OPT_DECOMPRESS_DELETE;
   }
   
-  int ret = kv_retrieve(this->sqH, this->nsH, cont_hd->keyspace_id,
-    (kv_key*)key, option_adi, (kv_value*)value, &f);
+  int ret = kv_retrieve(this->sqH, this->nsH, (kv_key*)key, option_adi, (kv_value*)value, &f);
   
   while(ret == KV_ERR_QUEUE_IS_FULL) {
-    ret = kv_retrieve(this->sqH, this->nsH, cont_hd->keyspace_id,
-      (kv_key*)key, option_adi, (kv_value*)value, &f);
+    ret = kv_retrieve(this->sqH, this->nsH, (kv_key*)key, option_adi, (kv_value*)value, &f);
   }
 
   if(syncio && ret == 0) {
+   
      wait_for_io(ctx);  
      ret = ctx->iocb.result;
-     delete ctx;
-     ctx = NULL;
+
+    delete ctx;
+    ctx = NULL;
   }
 
   free_if_error(ret, ctx);
   return ret;
 }
 
-int32_t KDDriver::delete_tuple(kvs_container_handle cont_hd, const kvs_key *key,
-  kvs_delete_option option, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
-  auto ctx = prep_io_context(IOCB_ASYNC_DEL_CMD, cont_hd, key, NULL, private1, private2,
-    syncio, cbfn);
+int32_t KDDriver::delete_tuple(int contid, const kvs_key *key, kvs_delete_option option, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+  auto ctx = prep_io_context(IOCB_ASYNC_DEL_CMD, contid, key, NULL, private1, private2, syncio, cbfn);
   kv_postprocess_function f = {kdd_on_io_complete, (void*)ctx};
 
   kv_delete_option option_adi;
@@ -598,17 +940,17 @@ int32_t KDDriver::delete_tuple(kvs_container_handle cont_hd, const kvs_key *key,
   else
     option_adi = KV_DELETE_OPT_ERROR;
   
-  int ret =  kv_delete(this->sqH, this->nsH, cont_hd->keyspace_id,
-    (kv_key*)key, option_adi, &f);
+  int ret =  kv_delete(this->sqH, this->nsH, (kv_key*)key, option_adi, &f);
   
   while(ret == KV_ERR_QUEUE_IS_FULL) {
-    ret =  kv_delete(this->sqH, this->nsH, cont_hd->keyspace_id,
-      (kv_key*)key, option_adi, &f);
+    ret =  kv_delete(this->sqH, this->nsH, (kv_key*)key, option_adi, &f);
   }
   
   if(syncio && ret == 0) {
+    
     wait_for_io(ctx);  
     ret = ctx->iocb.result;
+
     delete ctx;
     ctx = NULL;
   }    
@@ -618,30 +960,28 @@ int32_t KDDriver::delete_tuple(kvs_container_handle cont_hd, const kvs_key *key,
 }
 
 
-int32_t KDDriver::exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt,
-  const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1,
-  void *private2, bool syncio, kvs_callback_function cbfn) {
+int32_t KDDriver::exist_tuple(int contid, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+
   if(key_cnt > 1) {
     fprintf(stderr, "WARN: kernel driver only supports one key check \n");
     return KV_ERR_PARAM_INVALID;
   }
-  auto ctx = prep_io_context(IOCB_ASYNC_CHECK_KEY_EXIST_CMD, cont_hd, keys, NULL,
-    private1, private2, syncio, cbfn);
+  auto ctx = prep_io_context(IOCB_ASYNC_CHECK_KEY_EXIST_CMD, contid, keys, NULL, private1, private2, syncio, cbfn);
   ctx->iocb.key_cnt = key_cnt;
   ctx->iocb.result_buffer = result_buffer;
   
   kv_postprocess_function f = {kdd_on_io_complete, (void*)ctx};
 
-  int ret = kv_exist(this->sqH, this->nsH, cont_hd->keyspace_id,
-    (kv_key*)keys, key_cnt, buffer_size, result_buffer, &f);
+  int ret = kv_exist(this->sqH, this->nsH, (kv_key*)keys, key_cnt, buffer_size, result_buffer, &f);
+  
   while(ret == KV_ERR_QUEUE_IS_FULL) {
-    ret = kv_exist(this->sqH, this->nsH, cont_hd->keyspace_id,
-      (kv_key*)keys, key_cnt, buffer_size, result_buffer, &f);
+    ret = kv_exist(this->sqH, this->nsH, (kv_key*)keys, key_cnt, buffer_size, result_buffer, &f);
   }
 
   if(syncio && ret == 0) {
     wait_for_io(ctx);  
     ret = ctx->iocb.result;
+
     delete ctx;
     ctx = NULL;
   }
@@ -651,8 +991,57 @@ int32_t KDDriver::exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt,
   return ret;
 }
 
-int KDDriver::check_opened_iterators(uint32_t bitmask, uint32_t bit_pattern,
-                                     kvs_iterator_handle *iter_hd) {
+#ifdef KVS_REMOTE
+int32_t KDDriver::lock_tuple(int contid, const kvs_key *key, uint64_t instance_uuid, kvs_lock_option option, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+  auto ctx = prep_io_context(IOCB_ASYNC_LOCK_CMD, contid, key, NULL, private1, private2, syncio, cbfn);
+  kv_postprocess_function f = {kdd_on_io_complete, (void*)ctx};
+
+    int ret = kv_lock(this->sqH, this->nsH, (kv_key*)key, option.kvs_reader_lock, option.kvs_blocking_lock, option.lock_priority, option.lock_duration, instance_uuid, &f);
+
+	while(ret == KV_ERR_QUEUE_IS_FULL) {
+    	ret = kv_lock(this->sqH, this->nsH, (kv_key*)key, option.kvs_reader_lock, option.kvs_blocking_lock, option.lock_priority, option.lock_duration, instance_uuid, &f);
+	}
+
+  if(syncio && ret == 0) {
+    
+    wait_for_io(ctx);  
+    ret = ctx->iocb.result;
+
+    delete ctx;
+    ctx = NULL;
+  }    
+
+  free_if_error(ret, ctx);
+  return ret;
+
+}
+
+int32_t KDDriver::unlock_tuple(int contid, const kvs_key *key, uint64_t instance_uuid, kvs_lock_option option, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+  auto ctx = prep_io_context(IOCB_ASYNC_UNLOCK_CMD, contid, key, NULL, private1, private2, syncio, cbfn);
+  kv_postprocess_function f = {kdd_on_io_complete, (void*)ctx};
+
+    int ret = kv_unlock(this->sqH, this->nsH, (kv_key*)key, option.kvs_reader_lock, instance_uuid, &f);
+
+	while(ret == KV_ERR_QUEUE_IS_FULL) {
+    	ret = kv_unlock(this->sqH, this->nsH, (kv_key*)key, option.kvs_reader_lock, instance_uuid, &f);
+	}
+
+  if(syncio && ret == 0) {
+    
+    wait_for_io(ctx);  
+    ret = ctx->iocb.result;
+
+    delete ctx;
+    ctx = NULL;
+  }    
+
+  free_if_error(ret, ctx);
+  return ret;
+
+}
+#endif
+
+int KDDriver::check_opened_iterators(uint32_t bitmask, uint32_t bit_pattern) {
   kv_iterator kv_iters[SAMSUNG_MAX_ITERATORS];
   memset(kv_iters, 0, sizeof(kv_iters));
   uint32_t count = SAMSUNG_MAX_ITERATORS;
@@ -666,7 +1055,6 @@ int KDDriver::check_opened_iterators(uint32_t bitmask, uint32_t bit_pattern,
       opened++;
       //fprintf(stdout, "found handler %d, prefix 0x%x 0x%x\n", kv_iters[i].handle_id, kv_iters[i].prefix, kv_iters[i].bitmask);
       if(kv_iters[i].prefix == bit_pattern && kv_iters[i].bitmask == bitmask) {
-        *iter_hd = kv_iters[i].handle_id;
 	      fprintf(stdout, "WARN: Iterator with same prefix/bitmask is already opened\n");
 	      return KVS_ERR_ITERATOR_OPEN;
       }
@@ -679,13 +1067,13 @@ int KDDriver::check_opened_iterators(uint32_t bitmask, uint32_t bit_pattern,
   return 0;
 }
 
-int32_t KDDriver::open_iterator(kvs_container_handle cont_hd, kvs_iterator_option option,
-  uint32_t bitmask, uint32_t bit_pattern, kvs_iterator_handle *iter_hd) {
-  int ret = check_opened_iterators(bitmask, bit_pattern, iter_hd);
+int32_t KDDriver::open_iterator(int contid, kvs_iterator_option option /*uint8_t option*/, uint32_t bitmask,
+				uint32_t bit_pattern, kvs_iterator_handle *iter_hd) {
+  int ret = check_opened_iterators(bitmask, bit_pattern);
   if (ret) {
     return ret;
   }
-
+  
   kv_group_condition grp_cond = {bitmask, bit_pattern};
   kv_iterator_option option_adi;
   switch(option.iter_type) {
@@ -703,23 +1091,23 @@ int32_t KDDriver::open_iterator(kvs_container_handle cont_hd, kvs_iterator_optio
     return KVS_ERR_OPTION_INVALID;
   }
   
-  ret = kv_open_iterator_sync(this->sqH, this->nsH, cont_hd->keyspace_id,
-    option_adi, &grp_cond, iter_hd);
+  ret = kv_open_iterator_sync(this->sqH, this->nsH, option_adi, &grp_cond, iter_hd);
   return ret;
 }
 
-int32_t KDDriver::close_iterator(kvs_container_handle cont_hd, kvs_iterator_handle hiter) {
+int32_t KDDriver::close_iterator(int contid, kvs_iterator_handle hiter) {
   int ret = kv_close_iterator_sync(this->sqH, this->nsH, hiter/*iterh_adi*/);
   return ret;
 }
 
-int32_t KDDriver::close_iterator_all(kvs_container_handle cont_hd) {
+int32_t KDDriver::close_iterator_all(int contid) {
+
   fprintf(stderr, "WARN: this feature is not supported in the kernel driver\n");
   return KVS_ERR_OPTION_INVALID;
+
 }
 
-int32_t KDDriver::list_iterators(kvs_container_handle cont_hd, kvs_iterator_info *kvs_iters,
-  uint32_t count) {
+int32_t KDDriver::list_iterators(int contid, kvs_iterator_info *kvs_iters, uint32_t count) {
   int ret = kv_list_iterators_sync(sqH, nsH, (kv_iterator *)kvs_iters, &count);
   if(ret == KV_SUCCESS){
      for(uint32_t idx = 0; idx < count; idx++){
@@ -731,10 +1119,6 @@ int32_t KDDriver::list_iterators(kvs_container_handle cont_hd, kvs_iterator_info
           ret = KVS_ERR_ITERATOR_COND_INVALID;
         break;
       }
-      /* The bitmask and bit_pattern are reversed when the iterator is opened in the case
-      of cpu for little endian mode,so it needs to be inverted again in get_iterator_info.
-      kvs_iters[idx].bitmask = htobe32(kvs_iters[idx].bitmask);
-      kvs_iters[idx].bit_pattern = htobe32(kvs_iters[idx].bit_pattern);*/
     }
   }
   if(ret == KV_ERR_PARAM_INVALID)
@@ -743,24 +1127,31 @@ int32_t KDDriver::list_iterators(kvs_container_handle cont_hd, kvs_iterator_info
 }
 
 
-int32_t KDDriver::iterator_next(kvs_container_handle cont_hd, kvs_iterator_handle hiter, kvs_iterator_list *iter_list, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+int32_t KDDriver::iterator_next(kvs_iterator_handle hiter, kvs_iterator_list *iter_list, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
   int ret;
   if (syncio) {   
-    ret = kv_iterator_next_sync(this->sqH, this->nsH, hiter, (kv_iterator_list *)iter_list);
+    ret = kv_iterator_next_sync(this->sqH, this->nsH, hiter/*hiter->iterh_adi*/, (kv_iterator_list *)iter_list);
+
     reformat_iterbuffer(iter_list);
+    /*if(ret != KV_SUCCESS) {
+      fprintf(stderr, "kv_iterator_next failed with error:  0x%X\n", ret);
+    }*/
     return ret;
   }
   else { /* async */
-    auto ctx = prep_io_context(IOCB_ASYNC_ITER_NEXT_CMD, cont_hd, 0, 0, private1, private2, syncio, cbfn);
+    auto ctx = prep_io_context(IOCB_ASYNC_ITER_NEXT_CMD, 0, 0, 0, private1, private2, syncio, cbfn);
     ctx->iocb.result_buffer = (uint8_t*)iter_list;
-    ctx->iocb.iter_hd = hiter;
+    ctx->iocb.iter_hd = (kvs_iterator_handle *)malloc(sizeof(kvs_iterator_handle));
+    *(ctx->iocb.iter_hd) = hiter;
     kv_postprocess_function f = {
       kdd_on_io_complete, (void*)ctx
     };
-    ret = kv_iterator_next(this->sqH, this->nsH, hiter, (kv_iterator_list *)iter_list, &f);
+    ret = kv_iterator_next(this->sqH, this->nsH, hiter/*hiter->iterh_adi*/, (kv_iterator_list *)iter_list, &f);
   
     if(ret != KV_SUCCESS) {
       fprintf(stderr, "kv_iterator_next failed with error:  0x%X\n", ret);
+      if (ctx->iocb.iter_hd)
+	    free(ctx->iocb.iter_hd);
       delete ctx;
       ctx = NULL;
     }
@@ -783,11 +1174,9 @@ int32_t KDDriver::get_used_size(int32_t *dev_util){
     if(stat) free(stat);
     return ret;
   }
+  *dev_util = stat->utilization;
 
-  if(stat){
-    *dev_util = stat->utilization;
-    free(stat);
-  }
+  if(stat) free(stat);
   
   return ret;
 }
@@ -805,10 +1194,9 @@ int32_t KDDriver::get_total_size(int64_t *dev_capa) {
     return ret;
   }
 
-  if(devinfo){
-    *dev_capa = devinfo->capacity;
-    free(devinfo);
-  }
+  *dev_capa = devinfo->capacity;
+
+  if(devinfo) free(devinfo);
   return ret;
 }
 
@@ -840,13 +1228,16 @@ KDDriver::~KDDriver() {
   kv_cleanup_device(devH);
 }
 
-KDDriver::kv_kdd_context* KDDriver::prep_io_context(int opcode, kvs_container_handle cont_hd,
-  const kvs_key *key, const kvs_value *value, void *private1, void *private2,
-  bool syncio, kvs_callback_function cbfn){
+KDDriver::kv_kdd_context* KDDriver::prep_io_context(int opcode, int contid, const kvs_key *key, const kvs_value *value, void *private1, void *private2, bool syncio, kvs_callback_function cbfn,
+                                                    const kvs_key *prefix_key, const kvs_key *start_key, uint16_t max_keys_to_list){
+
   kv_kdd_context *ctx = new kv_kdd_context();
+
+  memset(&ctx->iocb, 0, sizeof(kvs_callback_context));
+
   ctx->owner = this;
   ctx->iocb.opcode = opcode;
-  ctx->iocb.cont_hd = cont_hd;
+  //ctx->iocb.contid = contid;
   if(key) {
     ctx->iocb.key = (kvs_key*)key;
   } else {
@@ -859,6 +1250,9 @@ KDDriver::kv_kdd_context* KDDriver::prep_io_context(int opcode, kvs_container_ha
     ctx->iocb.value = 0;
   }
 
+  ctx->iocb.prefix_key = prefix_key;
+  ctx->iocb.start_key = start_key;
+  ctx->iocb.max_keys_to_list = max_keys_to_list;
   ctx->iocb.private1 = private1;
   ctx->iocb.private2 = private2;
   ctx->iocb.result_buffer = NULL;
@@ -893,51 +1287,6 @@ int32_t KDDriver::trans_iter_type(uint8_t dev_it_type, uint8_t* kvs_it_type){
   return ret;
 }
 
-int32_t KDDriver::trans_store_cmd_opt(kvs_store_option kvs_opt,
-                                            kv_store_option *kv_opt){
-  if(!kvs_opt.kvs_store_compress) {
-    // Default: no compression
-    switch(kvs_opt.st_type) {
-    case KVS_STORE_POST:
-      *kv_opt = KV_STORE_OPT_DEFAULT;
-      break;
-    case KVS_STORE_UPDATE_ONLY:
-      *kv_opt = KV_STORE_OPT_UPDATE_ONLY;
-      break;
-    case KVS_STORE_NOOVERWRITE:
-      *kv_opt = KV_STORE_OPT_IDEMPOTENT;
-      break;
-    case KVS_STORE_APPEND:
-      *kv_opt = KV_STORE_OPT_APPEND;
-      break;
-    default:
-      fprintf(stderr, "WARN: Wrong store option\n");
-      return KVS_ERR_OPTION_INVALID;
-    }
-  } else {
-    // compression
-    switch(kvs_opt.st_type) {
-    case KVS_STORE_POST:
-      *kv_opt = KV_STORE_OPT_POST_WITH_COMPRESS;
-      break;
-    case KVS_STORE_UPDATE_ONLY:
-      *kv_opt = KV_STORE_OPT_UPDATE_ONLY_COMPRESS;
-      break;
-    case KVS_STORE_NOOVERWRITE:
-      *kv_opt = KV_STORE_OPT_NOOVERWRITE_COMPRESS;
-      break;
-    case KVS_STORE_APPEND:
-      *kv_opt = KV_STORE_OPT_APPEND_COMPRESS;
-      break;
-    default:
-      fprintf(stderr, "WARN: Wrong store option\n");
-      return KVS_ERR_OPTION_INVALID;
-    }
-  }
-
-  return KVS_SUCCESS;
-}
-
 float  KDDriver::get_waf(){
 
   uint32_t tmp_waf;
diff --git a/PDK/core/src/api/src/driver_adapter/kvudd.cpp b/PDK/core/src/api/src/driver_adapter/kvudd.cpp
index 6e18ae7..abcc436 100644
--- a/PDK/core/src/api/src/driver_adapter/kvudd.cpp
+++ b/PDK/core/src/api/src/driver_adapter/kvudd.cpp
@@ -87,7 +87,7 @@ void udd_iterate_cb(kv_iterate *it, unsigned int result, unsigned int status) {
     unsigned int buffer_size = it->kv.value.length;
     char *current_ptr = data_buff;
     
-    iocb->iter_hd = it->iterator;
+    iocb->iter_hd = (kvs_iterator_handle *)&(it->iterator);
     unsigned int key_size = 0;
     int keydata_len_with_padding = 0;
     unsigned int buffdata_len = buffer_size;
@@ -96,8 +96,8 @@ void udd_iterate_cb(kv_iterate *it, unsigned int result, unsigned int status) {
     data_buff += KV_IT_READ_BUFFER_META_LEN;
     for (uint32_t i = 0; i < num_key && buffdata_len > 0; i++) {
       if (buffdata_len < KV_IT_READ_BUFFER_META_LEN) {
-        iocb->result = KVS_ERR_SYS_IO;
-        break;
+	iocb->result = KVS_ERR_SYS_IO;
+	break;
       }
 
       // move 4 byte key len
@@ -110,12 +110,12 @@ void udd_iterate_cb(kv_iterate *it, unsigned int result, unsigned int status) {
       data_buff += KV_IT_READ_BUFFER_META_LEN;
 
       if (key_size > buffdata_len) {
-        iocb->result = KVS_ERR_SYS_IO;
-        break;
+	iocb->result = KVS_ERR_SYS_IO;
+	break;
       }
       if (key_size >= 256) {
-        iocb->result = KVS_ERR_SYS_IO;
-        break;
+	iocb->result = KVS_ERR_SYS_IO;
+	break;
       }
 
       // move key data
@@ -223,52 +223,6 @@ void udd_write_cb(kv_pair *kv, unsigned int result, unsigned int status) {
   }
 }
 
-static std::map<int32_t, int32_t> udd_err_to_api_err_table = {
-  {KV_SUCCESS, KVS_SUCCESS},
-  {KV_ERR_INVALID_VALUE_SIZE, KVS_ERR_VALUE_LENGTH_INVALID},
-  {KV_ERR_INVALID_KEY_SIZE, KVS_ERR_KEY_LENGTH_INVALID},
-  {KV_ERR_INVALID_OPTION, KVS_ERR_OPTION_INVALID},
-  {KV_ERR_INVALID_KEYSPACE_ID, KVS_ERR_PARAM_INVALID},
-  {KV_ERR_NOT_EXIST_KEY, KVS_ERR_KEY_NOT_EXIST},
-  {KV_ERR_UNRECOVERED_ERROR, KVS_ERR_UNRECOVERED_ERROR},
-  {KV_ERR_CAPACITY_EXCEEDED, KVS_ERR_DEV_CAPACITY},
-  {KV_ERR_IDEMPOTENT_STORE_FAIL, KVS_ERR_KEY_EXIST},
-  {KV_ERR_MAXIMUM_VALUE_SIZE_LIMIT_EXCEEDED, KVS_ERR_OPTION_INVALID},
-  {KV_ERR_ITERATE_FAIL_TO_PROCESS_REQUEST, KVS_ERR_ITERATOR_NOT_EXIST},
-  {KV_ERR_ITERATE_NO_AVAILABLE_HANDLE, KVS_ERR_ITERATOR_MAX},
-  {KV_ERR_ITERATE_HANDLE_ALREADY_OPENED, KVS_ERR_ITERATOR_OPEN},
-  {KV_ERR_ITERATE_READ_EOF, KVS_SUCCESS},
-  {KV_ERR_ITERATE_REQUEST_FAIL, KVS_ERR_ITERATE_REQUEST_FAIL},
-  {KV_ERR_ITERATE_TCG_LOCKED, KVS_ERR_SYS_BUSY},
-  {KV_ERR_ITERATE_ERROR, KVS_ERR_ITERATE_REQUEST_FAIL},
-  {KV_ERR_DD_INVALID_PARAM, KVS_ERR_PARAM_INVALID},
-  {KV_ERR_DD_UNSUPPORTED_CMD, KVS_ERR_DD_UNSUPPORTED_CMD},
-  {KV_ERR_IO, KVS_ERR_SYS_IO},
-};
-
-kvs_result convert_udd_result_to_api_result(uint32_t udd_errcode) {
-  if (udd_errcode == KV_SUCCESS)
-    return KVS_SUCCESS;
-
-  auto iter = udd_err_to_api_err_table.find(udd_errcode);
-  if(iter != udd_err_to_api_err_table.end()) {
-    return (kvs_result)iter->second;
-  } else {
-    fprintf(stderr, "[%s] error. status code = 0x%x\n", __FUNCTION__, udd_errcode);
-    return KVS_ERR_SYS_IO;
-  }
-}
-
-kvs_result convert_subcmd_error_code(uint32_t cmd_cnt, uint32_t *buffer) {
-  uint32_t id = 0;
-  for (id = 0; id < cmd_cnt; id++) {
-    uint32_t *res = buffer + id;
-    *res = convert_udd_result_to_api_result(*res);
-  }
-
-  return KVS_SUCCESS;
-}
-
 void print_coremask(uint64_t x)
 {
   int z;
@@ -279,18 +233,15 @@ void print_coremask(uint64_t x)
   }
   printf("coremask = %s\n", b);
 }
+  
+int32_t KUDDriver::init(const char* devpath, bool syncio, uint64_t sq_core, uint64_t cq_core, uint32_t mem_size_mb, int queue_depth) {
 
-int32_t KUDDriver::init(const char* devpath, bool syncio, uint64_t sq_core,
-  uint64_t cq_core, uint32_t mem_size_mb, int queue_depth) {
   int ret;
+
   kv_nvme_io_options options = {0};
   options.core_mask = (1ULL << sq_core); //sq_core; 
-  core_mask = options.core_mask;
-  sync_io = syncio;
-  if (syncio){
+  if (syncio)
     options.sync_mask = (1ULL << sq_core);     // Use Sync I/O mode
-    sync_mask = options.sync_mask;
-  }
   else
     options.sync_mask = 0;     // Use Async I/O mode
   options.num_cq_threads = 1;  // Use only one CQ Processing Thread
@@ -335,40 +286,19 @@ int32_t KUDDriver::init(const char* devpath, bool syncio, uint64_t sq_core,
       fprintf(stderr, "Failed to allocate kv pair\n");
       exit(1);
     }
-    std::unique_lock<std::mutex> lock(this->lock);
     this->kv_pair_pool.push(kv);
-    lock.unlock();
   }
 
   return ret;
 }
 
-int16_t KUDDriver::_get_queue_id(kvs_container_handle cont_hd) {
-  int16_t core_id = 0;
-  int16_t qid = DEFAULT_IO_QUEUE_ID;
-  if(cont_hd->keyspace_id == META_DATA_KEYSPACE_ID){
-    uint64_t mask = sync_mask;
-    if(!sync_io){ 
-      mask = sync_mask ^ core_mask;
-    }
 
-    for(core_id = 0; core_id < MAX_CPU_CORES; core_id++) {
-      if(mask & (1ULL << core_id)){
-        break;
-      }  
-    }
-   qid = core_id;
-  }
-  return qid;
-}
+KUDDriver::kv_udd_context* KUDDriver::prep_io_context(int opcode, int contid, const kvs_key *key, const kvs_value *value, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
 
-KUDDriver::kv_udd_context* KUDDriver::prep_io_context(int opcode,
-  kvs_container_handle cont_hd, const kvs_key *key, const kvs_value *value,
-  void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
   kv_udd_context *ctx = (kv_udd_context*)calloc(1, sizeof(kv_udd_context));
+  
   ctx->on_complete = cbfn;
   ctx->iocb.opcode = opcode;
-  ctx->iocb.cont_hd = cont_hd;
   if(key) {
     ctx->iocb.key = (kvs_key*)key;
   } else {
@@ -414,28 +344,52 @@ int32_t KUDDriver::trans_iter_type(uint8_t dev_it_type, uint8_t* kvs_it_type){
 }
 
 /* MAIN ENTRY POINT */
-int32_t KUDDriver::store_tuple(kvs_container_handle cont_hd, const kvs_key *key,
-const kvs_value *value, kvs_store_option option, void *private1, void *private2,
-bool syncio, kvs_callback_function cbfn) {
+int32_t KUDDriver::store_tuple(int contid, const kvs_key *key, const kvs_value *value, kvs_store_option option, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+  
   int ret = -EINVAL;
-  auto ctx = prep_io_context(IOCB_ASYNC_PUT_CMD, cont_hd, key, value, private1, private2, syncio, cbfn);
+
+  auto ctx = prep_io_context(IOCB_ASYNC_PUT_CMD, contid, key, value, private1, private2, syncio, cbfn);
+  
   std::unique_lock<std::mutex> lock(this->lock);
   kv_pair *kv = this->kv_pair_pool.front();
   this->kv_pair_pool.pop();
   lock.unlock();
   if(!kv) {
     fprintf(stderr, "failed to allocate kv pairs\n");
-    free(ctx);
-    return KVS_ERR_QUEUE_IS_FULL;
+    exit(1);
   }
 
   int option_adi;
-  ret = trans_store_cmd_opt(option, &option_adi);
-  if (ret) {
-    free(ctx);
-    return ret;
+  if(!option.kvs_store_compress) {
+    // Default: no compression
+    switch(option.st_type) {
+    case KVS_STORE_POST:
+      option_adi = KV_STORE_DEFAULT;
+      break;
+    case KVS_STORE_NOOVERWRITE:
+      option_adi = KV_STORE_IDEMPOTENT;
+      break;
+    case KVS_STORE_APPEND:
+    case KVS_STORE_UPDATE_ONLY:
+    default:
+      fprintf(stderr, "WARN: Wrong store option\n");
+      return KVS_ERR_OPTION_INVALID;
+    }
+  } else {
+    // compression
+    switch(option.st_type) {
+    case KVS_STORE_POST:
+      option_adi = KV_STORE_COMPRESSION;
+      break;
+    case KVS_STORE_UPDATE_ONLY:
+    case KVS_STORE_NOOVERWRITE:
+    case KVS_STORE_APPEND:
+    default:
+      fprintf(stderr, "WARN: Wrong store option\n");
+      return KVS_ERR_OPTION_INVALID;
+    }
   }
-  kv->keyspace_id = cont_hd->keyspace_id;
+  
   kv->key.key = key->key;
   kv->key.length = key->length;
   
@@ -447,12 +401,9 @@ bool syncio, kvs_callback_function cbfn) {
   kv->param.private_data = ctx;
   kv->param.io_option.store_option = option_adi;//KV_STORE_DEFAULT;
 
-  int qid = _get_queue_id(cont_hd);
   if(syncio) {
-    ret = kv_nvme_write(handle, qid, kv);
-    std::unique_lock<std::mutex> lock(this->lock);
+    ret = kv_nvme_write(handle, DEFAULT_IO_QUEUE_ID, kv);
     this->kv_pair_pool.push(kv);
-    lock.unlock();
     free(ctx);
     ctx = NULL;
     
@@ -482,16 +433,13 @@ bool syncio, kvs_callback_function cbfn) {
       ret = KVS_ERR_BUFFER_SMALL;
     } else if (ret == KV_ERR_IDEMPOTENT_STORE_FAIL) {
       ret = KVS_ERR_KEY_EXIST;
-    } else if (ret == KV_ERR_DD_INVALID_QUEUE_TYPE) {
-      ret = KVS_ERR_DD_INVALID_QUEUE_TYPE;
     } else {
       fprintf(stderr, "[%s] error. key=%s option=%d value.length=%d value.offset=%d status code = 0x%x\n", __FUNCTION__, (char*)key->key, kv->param.io_option.store_option,kv->value.length, kv->value.offset, ret);
       ret = KVS_ERR_SYS_IO;
     }
   } else {
-    ret = -EINVAL;
     while (ret) {
-      ret = kv_nvme_write_async(handle, qid, kv);
+      ret = kv_nvme_write_async(handle, DEFAULT_IO_QUEUE_ID, kv);
       if(ret == KV_ERR_DD_NO_AVAILABLE_RESOURCE || ret == KV_ERR_DD_NO_AVAILABLE_QUEUE) {
         usleep(1);
       }
@@ -511,9 +459,7 @@ bool syncio, kvs_callback_function cbfn) {
           ret = KVS_ERR_SYS_IO;
         }
         if (ret != KV_SUCCESS) {
-          std::unique_lock<std::mutex> lock(this->lock);
           this->kv_pair_pool.push(kv);
-          lock.unlock();
           free(ctx);
           ctx = NULL;
         }
@@ -525,11 +471,11 @@ bool syncio, kvs_callback_function cbfn) {
   return ret;
 }
 
-int32_t KUDDriver::retrieve_tuple(kvs_container_handle cont_hd,
-  const kvs_key *key, kvs_value *value, kvs_retrieve_option option,
-  void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+int32_t KUDDriver::retrieve_tuple(int contid, const kvs_key *key, kvs_value *value, kvs_retrieve_option option, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+
   int ret = -EINVAL;
-  auto ctx = prep_io_context(IOCB_ASYNC_GET_CMD, cont_hd, key, value, private1, private2, syncio, cbfn);
+
+  auto ctx = prep_io_context(IOCB_ASYNC_GET_CMD, contid, key, value, private1, private2, syncio, cbfn);
   
   std::unique_lock<std::mutex> lock(this->lock);
   kv_pair *kv = this->kv_pair_pool.front();
@@ -537,8 +483,7 @@ int32_t KUDDriver::retrieve_tuple(kvs_container_handle cont_hd,
   lock.unlock();
   if(!kv) {
     fprintf(stderr, "failed to allocate kv pairs\n");
-    free(ctx);
-    return KVS_ERR_QUEUE_IS_FULL;
+    exit(1);
   }
 
   int option_adi;
@@ -548,10 +493,9 @@ int32_t KUDDriver::retrieve_tuple(kvs_container_handle cont_hd,
     else
       option_adi = KV_RETRIEVE_DECOMPRESSION;
   } else {
-    free(ctx);
     return KVS_ERR_OPTION_INVALID;
   }
-  kv->keyspace_id = cont_hd->keyspace_id;
+  
   kv->key.key = key->key;
   kv->key.length = key->length;
   
@@ -563,14 +507,11 @@ int32_t KUDDriver::retrieve_tuple(kvs_container_handle cont_hd,
   kv->param.async_cb = udd_write_cb;
   kv->param.private_data = ctx;
 
-  int qid = _get_queue_id(cont_hd);
   if(syncio) {
-    ret = kv_nvme_read(handle, qid, kv);
+    ret = kv_nvme_read(handle, DEFAULT_IO_QUEUE_ID, kv);
     value->actual_value_size = kv->value.actual_value_size;
     value->length = kv->value.length;
-    std::unique_lock<std::mutex> lock(this->lock);
     this->kv_pair_pool.push(kv);
-    lock.unlock();
     free(ctx);
     ctx = NULL;
 
@@ -600,15 +541,14 @@ int32_t KUDDriver::retrieve_tuple(kvs_container_handle cont_hd,
       ret = KVS_ERR_OPTION_INVALID;
     } else if (ret == KV_ERR_BUFFER) {
       ret = KVS_ERR_BUFFER_SMALL;
-    } else if (ret == KV_ERR_DD_INVALID_QUEUE_TYPE) {
-      ret = KVS_ERR_DD_INVALID_QUEUE_TYPE;
     } else {
       fprintf(stderr, "[%s] error. key=%s option=%d value.length=%d value.offset=%d status code = 0x%x\n", __FUNCTION__, (char*)key->key, kv->param.io_option.retrieve_option,kv->value.length, kv->value.offset, ret);
       ret = KVS_ERR_SYS_IO;
     }
+    
   } else {
     while (ret) {
-      ret = kv_nvme_read_async(handle, qid, kv);
+      ret = kv_nvme_read_async(handle, DEFAULT_IO_QUEUE_ID, kv);
       if(ret == KV_ERR_DD_NO_AVAILABLE_RESOURCE || ret == KV_ERR_DD_NO_AVAILABLE_QUEUE) {
         usleep(1);
       }
@@ -628,9 +568,7 @@ int32_t KUDDriver::retrieve_tuple(kvs_container_handle cont_hd,
           ret = KVS_ERR_SYS_IO;
         }
         if (ret != KV_SUCCESS) {
-          std::unique_lock<std::mutex> lock(this->lock);
           this->kv_pair_pool.push(kv);
-          lock.unlock();
           free(ctx);
           ctx = NULL;
         }
@@ -642,10 +580,10 @@ int32_t KUDDriver::retrieve_tuple(kvs_container_handle cont_hd,
   return ret;
 }
 
-int32_t KUDDriver::delete_tuple(kvs_container_handle cont_hd, const kvs_key *key, kvs_delete_option option, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+int32_t KUDDriver::delete_tuple(int contid, const kvs_key *key, kvs_delete_option option, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
 
   int ret = -EINVAL;
-  auto ctx = prep_io_context(IOCB_ASYNC_DEL_CMD, cont_hd, key, NULL, private1, private2, syncio, cbfn);
+  auto ctx = prep_io_context(IOCB_ASYNC_DEL_CMD, contid, key, NULL, private1, private2, syncio, cbfn);
 
   std::unique_lock<std::mutex> lock(this->lock);
   kv_pair *kv = this->kv_pair_pool.front();
@@ -653,8 +591,7 @@ int32_t KUDDriver::delete_tuple(kvs_container_handle cont_hd, const kvs_key *key
   lock.unlock();
   if(!kv) {
     fprintf(stderr, "failed to allocate kv pairs\n");
-    free(ctx);
-    return KVS_ERR_QUEUE_IS_FULL;
+    exit(1);
   }
 
   int option_adi;
@@ -666,18 +603,14 @@ int32_t KUDDriver::delete_tuple(kvs_container_handle cont_hd, const kvs_key *key
   kv->key.key = key->key;
   kv->key.length = key->length;
   kv->value.value = 0;
-  kv->keyspace_id = cont_hd->keyspace_id;
 
   kv->param.io_option.delete_option = option_adi;
   kv->param.async_cb = udd_write_cb;
   kv->param.private_data = ctx;
 
-  int qid = _get_queue_id(cont_hd);
   if(syncio){
-    ret = kv_nvme_delete(handle, qid, kv);
-    std::unique_lock<std::mutex> lock(this->lock);
+    ret = kv_nvme_delete(handle, DEFAULT_IO_QUEUE_ID, kv);
     this->kv_pair_pool.push(kv);
-    lock.unlock();
     free(ctx);
     ctx = NULL;
 
@@ -712,7 +645,7 @@ int32_t KUDDriver::delete_tuple(kvs_container_handle cont_hd, const kvs_key *key
     
   } else {
     while(ret){
-      ret = kv_nvme_delete_async(handle, qid, kv);
+      ret = kv_nvme_delete_async(handle, DEFAULT_IO_QUEUE_ID, kv);
       if(ret == KV_ERR_DD_NO_AVAILABLE_RESOURCE || ret == KV_ERR_DD_NO_AVAILABLE_QUEUE) {
         usleep(1);
       }
@@ -732,9 +665,7 @@ int32_t KUDDriver::delete_tuple(kvs_container_handle cont_hd, const kvs_key *key
           ret = KVS_ERR_SYS_IO;
         }
         if (ret != KV_SUCCESS) {
-         std::unique_lock<std::mutex> lock(this->lock);
           this->kv_pair_pool.push(kv);
-          lock.unlock();
           free(ctx);
           ctx = NULL;
         }
@@ -746,10 +677,10 @@ int32_t KUDDriver::delete_tuple(kvs_container_handle cont_hd, const kvs_key *key
   return ret;
 }
 
-int32_t KUDDriver::exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1, void *private2, bool syncio, kvs_callback_function cbfn ) {
+int32_t KUDDriver::exist_tuple(int contid, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, void *private1, void *private2, bool syncio, kvs_callback_function cbfn ) {
 
   int ret = 1;
-  auto ctx = prep_io_context(IOCB_ASYNC_CHECK_KEY_EXIST_CMD, cont_hd, keys, NULL, private1, private2, syncio, cbfn);
+  auto ctx = prep_io_context(IOCB_ASYNC_CHECK_KEY_EXIST_CMD, contid, keys, NULL, private1, private2, syncio, cbfn);
   ctx->iocb.result_buffer = result_buffer;
   
   std::unique_lock<std::mutex> lock(this->lock);
@@ -758,21 +689,18 @@ int32_t KUDDriver::exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt, c
   lock.unlock();
   if(!kv) {
     fprintf(stderr, "failed to allocate kv pairs\n");
-    free(ctx);
-    return KVS_ERR_QUEUE_IS_FULL;
+    exit(1);
   }
-  
+											       
   kv->key.key = keys->key;
   kv->key.length = keys->length;
 
   kv->param.io_option.exist_option = KV_EXIST_DEFAULT;
   kv->param.async_cb = udd_write_cb;
   kv->param.private_data = ctx;
-  kv->keyspace_id = cont_hd->keyspace_id;
 
-  int qid = _get_queue_id(cont_hd);
   if(syncio) {
-    ret = kv_nvme_exist(handle, qid, kv);
+    ret = kv_nvme_exist(handle, DEFAULT_IO_QUEUE_ID, kv);
     if(ret == KV_SUCCESS) {
       *result_buffer = 1;//ret;
       ret = KVS_SUCCESS;
@@ -804,14 +732,12 @@ int32_t KUDDriver::exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt, c
        *result_buffer = ret = KVS_ERR_SYS_IO;
     }
     
-    std::unique_lock<std::mutex> lock(this->lock);
     this->kv_pair_pool.push(kv);
-    lock.unlock();
     free(ctx);
     ctx = NULL;    
   } else {
     while(ret){
-      ret = kv_nvme_exist_async(handle, qid, kv);
+      ret = kv_nvme_exist_async(handle, DEFAULT_IO_QUEUE_ID, kv);
       if(ret == KV_ERR_DD_NO_AVAILABLE_RESOURCE || ret == KV_ERR_DD_NO_AVAILABLE_QUEUE) {
         usleep(1);
       }
@@ -831,9 +757,7 @@ int32_t KUDDriver::exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt, c
           ret = KVS_ERR_SYS_IO;
         }
         if (ret != KV_SUCCESS) {
-          std::unique_lock<std::mutex> lock(this->lock);
           this->kv_pair_pool.push(kv);
-          lock.unlock();
           free(ctx);
           ctx = NULL;
         }
@@ -845,10 +769,11 @@ int32_t KUDDriver::exist_tuple(kvs_container_handle cont_hd, uint32_t key_cnt, c
   return ret;
 }
 
-int32_t KUDDriver::open_iterator(kvs_container_handle cont_hd,
-  kvs_iterator_option option, uint32_t bitmask, uint32_t bit_pattern,
-  kvs_iterator_handle *iter_hd) {
+int32_t KUDDriver::open_iterator(int contid,  /*uint8_t option*/kvs_iterator_option option, uint32_t bitmask,
+				 uint32_t bit_pattern, kvs_iterator_handle *iter_hd) {
+  
   int ret = 0;
+
   uint8_t option_udd;
 
   switch(option.iter_type) {
@@ -873,15 +798,14 @@ int32_t KUDDriver::open_iterator(kvs_container_handle cont_hd,
   if (ret == KV_SUCCESS) {
     for(int i=0;i<nr_iterate_handle;i++){
       if(info[i].status == ITERATE_HANDLE_OPENED){
-        opened++;
-        if(info[i].bitmask == bitmask && info[i].prefix == bit_pattern) {
-          *iter_hd = info[i].handle_id;
-          //kv_nvme_iterate_close(handle, info[i].handle_id);
-          fprintf(stdout, "WARN: Iterator with same prefix/bitmask is already opened\n");
-          return KVS_ERR_ITERATOR_OPEN;
-        }
+	opened++;
+	if(info[i].bitmask == bitmask && info[i].prefix == bit_pattern) {	
+	  //kv_nvme_iterate_close(handle, info[i].handle_id);
+	  fprintf(stdout, "WARN: Iterator with same prefix/bitmask is already opened\n");
+	  return KVS_ERR_ITERATOR_OPEN;
+	}
       } else {
-        //fprintf(stdout, "iterate %d is closed\n", i);
+	//fprintf(stdout, "iterate %d is closed\n", i);
       }
     }
   }
@@ -890,8 +814,7 @@ int32_t KUDDriver::open_iterator(kvs_container_handle cont_hd,
     return KVS_ERR_ITERATOR_MAX;
   
   uint32_t iterator = KV_INVALID_ITERATE_HANDLE;
-  iterator = kv_nvme_iterate_open(handle, cont_hd->keyspace_id, bitmask, bit_pattern,
-    option_udd);
+  iterator = kv_nvme_iterate_open(handle, KV_KEYSPACE_IODATA, bitmask, bit_pattern, /*(option == KVS_ITERATOR_OPT_KEY ? KV_KEY_ITERATE : KV_KEY_ITERATE_WITH_RETRIEVE)*/option_udd);
 
   if(iterator > KV_INVALID_ITERATE_HANDLE && iterator <= KV_MAX_ITERATE_HANDLE){
     fprintf(stdout, "Iterate_Open Success: iterator id=0x%x\n", iterator);
@@ -911,8 +834,8 @@ int32_t KUDDriver::open_iterator(kvs_container_handle cont_hd,
   return ret;
 }
 
-int32_t KUDDriver::close_iterator(kvs_container_handle cont_hd,
-  kvs_iterator_handle hiter) {
+int32_t KUDDriver::close_iterator(int contid, kvs_iterator_handle hiter) {
+
   int ret = KVS_ERR_PARAM_INVALID;
   //if(hiter->iterator > 0)
   if(hiter > 0) {
@@ -933,7 +856,7 @@ int32_t KUDDriver::close_iterator(kvs_container_handle cont_hd,
 }
 
 
-int32_t KUDDriver::close_iterator_all(kvs_container_handle cont_hd) {
+int32_t KUDDriver::close_iterator_all(int contid) {
   int ret;
   int nr_iterate_handle = KV_MAX_ITERATE_HANDLE;
   kv_iterate_handle_info info[KV_MAX_ITERATE_HANDLE];
@@ -949,11 +872,11 @@ int32_t KUDDriver::close_iterator_all(kvs_container_handle cont_hd) {
   return KVS_SUCCESS;
 }
 
-int32_t KUDDriver::list_iterators(kvs_container_handle cont_hd,
-  kvs_iterator_info *kvs_iters, uint32_t count) {
+int32_t KUDDriver::list_iterators(int contid, kvs_iterator_info *kvs_iters, uint32_t count) {
+
   //int nr_iterate_handle = KV_MAX_ITERATE_HANDLE;
   //kv_iterate_handle_info info[KV_MAX_ITERATE_HANDLE];
-  kvs_iters->keyspace_id = cont_hd->keyspace_id;
+  
   int ret = kv_nvme_iterate_info(handle, (kv_iterate_handle_info*)kvs_iters, count);
   if(ret == KV_SUCCESS){
     for(uint32_t idx = 0; idx < count; idx++){
@@ -972,14 +895,14 @@ int32_t KUDDriver::list_iterators(kvs_container_handle cont_hd,
   return ret;
 }
 
-int32_t KUDDriver::iterator_next(kvs_container_handle cont_hd, kvs_iterator_handle hiter,
-  kvs_iterator_list *iter_list, void *private1, void *private2, bool syncio,
-  kvs_callback_function cbfn) {
+int32_t KUDDriver::iterator_next(kvs_iterator_handle hiter, kvs_iterator_list *iter_list, void *private1, void *private2, bool syncio, kvs_callback_function cbfn) {
+
   int ret = -EINVAL;
-  auto ctx = prep_io_context(IOCB_ASYNC_ITER_NEXT_CMD, cont_hd, 0, 0, private1, private2,
-    syncio, cbfn);
+
+  auto ctx = prep_io_context(IOCB_ASYNC_ITER_NEXT_CMD, 0, 0, 0, private1, private2, syncio, cbfn);
+
   ctx->iter_list = iter_list;
-  ctx->iocb.iter_hd = hiter;
+
   kv_iterate *it = (kv_iterate *)kv_zalloc(sizeof(kv_iterate)); 
   if(!it) {
     return -ENOMEM;
@@ -1017,6 +940,7 @@ int32_t KUDDriver::iterator_next(kvs_container_handle cont_hd, kvs_iterator_hand
 
 
     if(ret == KV_SUCCESS) {
+
       // first 4 bytes are for key counts
       uint32_t num_key = *((unsigned int*)it->kv.value.value);
       iter_list->num_entries = num_key;
@@ -1032,39 +956,39 @@ int32_t KUDDriver::iterator_next(kvs_container_handle cont_hd, kvs_iterator_hand
       buffdata_len -= KV_IT_READ_BUFFER_META_LEN;
       data_buff += KV_IT_READ_BUFFER_META_LEN;
       for (uint32_t i = 0; i < num_key && buffdata_len > 0; i++) {
-        if (buffdata_len < KV_IT_READ_BUFFER_META_LEN) {
-          ret = KVS_ERR_SYS_IO;
-          break;
-        }
-
-        // move 4 byte key len
-        memmove(current_ptr, data_buff, KV_IT_READ_BUFFER_META_LEN);
-        current_ptr += KV_IT_READ_BUFFER_META_LEN;
-
-        // get key size
-        key_size = *((uint32_t *)data_buff);
-        buffdata_len -= KV_IT_READ_BUFFER_META_LEN;
-        data_buff += KV_IT_READ_BUFFER_META_LEN;
-
-        if (key_size > buffdata_len) {
-          ret = KVS_ERR_SYS_IO;
-          break;
-        }
-        if (key_size >= 256) {
-          ret = KVS_ERR_SYS_IO;
-          break;
-        }
-
-        // move key data
-        memmove(current_ptr, data_buff, key_size);
-        current_ptr += key_size;
-
-        // calculate 4 byte aligned current key len including padding bytes
-        keydata_len_with_padding = (((key_size + 3) >> 2) << 2);
-
-        // skip to start position of next key
-        buffdata_len -= keydata_len_with_padding;
-        data_buff += keydata_len_with_padding;
+	if (buffdata_len < KV_IT_READ_BUFFER_META_LEN) {
+	  ret = KVS_ERR_SYS_IO;
+	  break;
+	}
+
+	// move 4 byte key len
+	memmove(current_ptr, data_buff, KV_IT_READ_BUFFER_META_LEN);
+	current_ptr += KV_IT_READ_BUFFER_META_LEN;
+
+	// get key size
+	key_size = *((uint32_t *)data_buff);
+	buffdata_len -= KV_IT_READ_BUFFER_META_LEN;
+	data_buff += KV_IT_READ_BUFFER_META_LEN;
+
+	if (key_size > buffdata_len) {
+	  ret = KVS_ERR_SYS_IO;
+	  break;
+	}
+	if (key_size >= 256) {
+	  ret = KVS_ERR_SYS_IO;
+	  break;
+	}
+
+	// move key data
+	memmove(current_ptr, data_buff, key_size);
+	current_ptr += key_size;
+
+	// calculate 4 byte aligned current key len including padding bytes
+	keydata_len_with_padding = (((key_size + 3) >> 2) << 2);
+
+	// skip to start position of next key
+	buffdata_len -= keydata_len_with_padding;
+	data_buff += keydata_len_with_padding;
       }
     }
     
@@ -1131,40 +1055,6 @@ int32_t KUDDriver::iterator_next(kvs_container_handle cont_hd, kvs_iterator_hand
   return ret;
 }  
 
-int32_t KUDDriver::trans_store_cmd_opt(kvs_store_option kvs_opt, int *kv_opt){
-  if(!kvs_opt.kvs_store_compress) {
-    // Default: no compression
-    switch(kvs_opt.st_type) {
-      case KVS_STORE_POST:
-        *kv_opt = KV_STORE_DEFAULT;
-        break;
-      case KVS_STORE_NOOVERWRITE:
-        *kv_opt = KV_STORE_IDEMPOTENT;
-        break;
-      case KVS_STORE_APPEND:
-      case KVS_STORE_UPDATE_ONLY:
-      default:
-        fprintf(stderr, "WARN: Wrong store option\n");
-        return KVS_ERR_OPTION_INVALID;
-    }
-  } else {
-    // compression
-    switch(kvs_opt.st_type) {
-      case KVS_STORE_POST:
-        *kv_opt = KV_STORE_COMPRESSION;
-        break;
-      case KVS_STORE_UPDATE_ONLY:
-      case KVS_STORE_NOOVERWRITE:
-      case KVS_STORE_APPEND:
-      default:
-        fprintf(stderr, "WARN: Wrong store option\n");
-        return KVS_ERR_OPTION_INVALID;
-    }
-  }
-
-  return KVS_SUCCESS;
-}
-
 float KUDDriver::get_waf(){
 
   return (float)kv_nvme_get_waf(handle) / 10;
@@ -1202,14 +1092,11 @@ KUDDriver::~KUDDriver() {
   }
   ret = kv_nvme_finalize(trid);
 
-  std::unique_lock<std::mutex> lock(this->lock);
   while(!this->kv_pair_pool.empty()) {
     auto p = this->kv_pair_pool.front();
     this->kv_pair_pool.pop();
     kv_free(p);
-    
   }
-  lock.unlock();
 
   /*
   while(!this->udd_context_pool.empty()){
diff --git a/PDK/core/src/api/src/private_cfrontend.cpp b/PDK/core/src/api/src/private_cfrontend.cpp
index d37d892..b946c03 100644
--- a/PDK/core/src/api/src/private_cfrontend.cpp
+++ b/PDK/core/src/api/src/private_cfrontend.cpp
@@ -31,7 +31,7 @@
  *   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  */
 
-#include <unistd.h>
+
 #include "kvs_utils.h"
 #include "uddenv.h"
 #include "private_types.h"
@@ -42,23 +42,22 @@
 #else
 #include "udd.hpp"
 #endif
-#include <string.h>
+
 #include <map>
 #include <list>
 //#include <regex>
 #include <string>
+#include <unistd.h>
 
 namespace api_private {
-const char* KEY_SPACE_LIST_KEY_NAME = "key_space_list"; //the key of kv pair that store key
-                                                        //spaces name list
+
 struct {
   bool initialized = false;
   bool use_spdk = false;
   int queuedepth;
   int is_polling = 0;
   std::map<std::string, kv_device_priv *> list_devices;
-  std::list<kvs_device_handle> open_devices;
-  std::list<kvs_container_handle> list_open_container;
+  std::list<kvs_device_handle > open_devices;
 #if defined WITH_SPDK
   struct {
     uint64_t cq_masks[NR_MAX_SSD];
@@ -73,37 +72,6 @@ struct {
 
 std::map<int, std::string> errortable;
 
-
-typedef uint8_t keyspace_id_t;
-typedef uint8_t cont_opened_t;
-typedef uint8_t cont_key_order_t;
-typedef uint64_t cont_capacity_t;
-typedef uint64_t cont_free_size_t;
-typedef uint64_t cont_kv_count_t;
-typedef struct {
-  keyspace_id_t keyspace_id;    // the keyspace id that this container is corresponding
-  cont_opened_t opened;         // is this container opened, 1:open, 0:close
-  cont_key_order_t key_order;   // key order, 0:default(order not defined), 1:ascend, 2:descend
-  cont_capacity_t capacity;     // container capacity in bytes
-  cont_free_size_t free_size;   // available space of container in bytes
-  cont_kv_count_t kv_count;     // # of Key Value tuples that exist in this container
-  const char* name;             // container name
-} cont_metadata;
-
-typedef struct {
-  char names_buffer[MAX_CONT_PATH_LEN];
-  keyspace_id_t keyspace_id;
-} cont_list_entry;
-
-typedef uint8_t container_id_t;
-typedef struct {
-  container_id_t cont_num;
-  cont_list_entry entries[NR_MAX_CONT];
-} cont_list;
-
-const keyspace_id_t _INVALID_USER_KEYSPACE_ID = 0;  //invalid keyspace id
-
-
 /*!Check wether the iterator bitmask is valid 
  * \desc:  bitmask should be set from the first bit of a key and it is not 
  *            allowed setting bitmask froma middle position of a key. Hence, 
@@ -136,7 +104,7 @@ inline bool _is_valid_bitmask(uint32_t bitmask){
   }
 
   return true;
-}
+};
 
 kvs_result kvs_exit_env() {
   g_env.initialized = false;
@@ -168,16 +136,15 @@ int kvs_list_kvdevices(kv_device_info **devs, int size) {
     kv_device_priv* dev_i = t.second;
     kv_device_info* dev = (kv_device_info*) malloc(sizeof(kv_device_info));
 
-    if(dev){
-      snprintf(dev->node, sizeof(dev->node), "%s", dev_i->node);
-      snprintf(dev->spdkpath, sizeof(dev->spdkpath), "%s", dev_i->spdkpath);
-      dev->nsid = dev_i->nsid;
-      snprintf(dev->pci_slot_name, sizeof(dev->pci_slot_name), "%s", dev_i->pci_slot_name);
-      dev->numanode = dev_i->numanode;
-      dev->vendorid = dev_i->vendorid;
-      dev->deviceid = dev_i->deviceid;
-      snprintf(dev->ven_dev_id, sizeof(dev->ven_dev_id), "%s", dev_i->ven_dev_id);
-    }
+    strcpy(dev->node, dev_i->node);
+    strcpy(dev->spdkpath, dev_i->spdkpath);
+    dev->nsid = dev_i->nsid;
+    strcpy(dev->pci_slot_name, dev_i->pci_slot_name);
+    dev->numanode = dev_i->numanode;
+    dev->vendorid = dev_i->vendorid;
+    dev->deviceid = dev_i->deviceid;
+    strcpy(dev->ven_dev_id, dev_i->ven_dev_id);
+
     devs[index++] = dev;
 
     if (index == max)
@@ -191,18 +158,17 @@ int initialize_udd_options(kvs_init_options* options){
   int i = 0;
   std::string delim = ",";
   char *pt;
-  char *saveptr = NULL;
-  pt = strtok_r(options->udd.core_mask_str, ",", &saveptr);
+  pt = strtok(options->udd.core_mask_str, ",");
   while(pt != NULL) {
     g_env.udd_option.core_masks[i++] = std::stol(pt);
-    pt = strtok_r(NULL, ",", &saveptr);
+    pt = strtok(NULL, ",");
   }
   
   i = 0;
-  pt = strtok_r(options->udd.cq_thread_mask, ",", &saveptr);
+  pt = strtok(options->udd.cq_thread_mask, ",");
   while(pt != NULL) {
     g_env.udd_option.cq_masks[i++] = std::stol(pt);
-    pt = strtok_r(NULL, ",", &saveptr);
+    pt = strtok(NULL, ",");
   }
   
   g_env.udd_option.num_devices = 0;
@@ -243,7 +209,7 @@ kvs_result kvs_init_env(kvs_init_options* options) {
         WRITE_WARNING("Emulator configure file can not be readed\n");
         return KVS_ERR_OPTION_INVALID;
       }
-      snprintf(g_env.configfile, sizeof(g_env.configfile), "%s", options->emul_config_file);
+      strcpy(g_env.configfile, options->emul_config_file);
 #endif
       // emulator or kdd
       //fprintf(stdout, "Using KV Emulator or Kernel\n");
@@ -279,7 +245,7 @@ kvs_result kvs_init_env(kvs_init_options* options) {
 
 
 kv_device_priv *_find_local_device_from_path(const std::string &devpath,
-		std::map<std::string, kv_device_priv *> *list_devices) {
+		std::map<std::string, kv_device_priv *> &list_devices) {
 
   //static std::regex emu_pattern("/dev/kvemul*");
   kv_device_priv *dev = 0;
@@ -304,7 +270,7 @@ kv_device_priv *_find_local_device_from_path(const std::string &devpath,
 #elif defined WITH_EMU
   static int emulnsid = 0;
   kv_device_priv *emul = new kv_device_priv();
-  snprintf(emul->node, sizeof(emul->node), "%s", devpath.c_str());
+  sprintf(emul->node, "%s", devpath.c_str());
   emul->nsid = emulnsid++;
   emul->isemul = true;
   emul->iskerneldev = false;
@@ -393,7 +359,6 @@ void build_error_table() {
   errortable[0x02B]="KVS_ERR_ITERATOR_NUM_OUT_RANGE";
   errortable[0x02C]="KVS_ERR_DD_UNSUPPORTED";
   errortable[0x02D]="KVS_ERR_ITERATOR_BUFFER_SIZE";
-  errortable[0x032]="KVS_ERR_MEMORY_MALLOC_FAIL";
   errortable[0x200]="KVS_ERR_CACHE_INVALID_PARAM";
   errortable[0x201]="KVS_ERR_CACHE_NO_CACHED_KEY";
   errortable[0x202]="KVS_ERR_DD_INVALID_QUEUE_TYPE";
@@ -426,8 +391,6 @@ void build_error_table() {
   errortable[0x405]="KVS_ERR_CONT_NAME";
   errortable[0x406]="KVS_ERR_CONT_NOT_EXIST";
   errortable[0x407]="KVS_ERR_CONT_OPEN";
-  errortable[0x408]="KVS_ERR_CONT_PATH_TOO_LONG";
-  errortable[0x409]="KVS_ERR_CONT_MAX";
 }
 
 const char *kvs_errstr(int32_t errorno) {
@@ -444,7 +407,7 @@ bool _device_opened(kvs_device_handle dev_hd){
 
 bool _device_opened(const char* dev_path){
   std::string dev(dev_path);
-  for (const auto &t : g_env.open_devices) {
+  for (const auto &t: g_env.open_devices) {
     if(t->dev_path == dev){
       return true;
     }
@@ -452,11 +415,8 @@ bool _device_opened(const char* dev_path){
   return false;
 }
 
-bool _container_opened(kvs_device_handle dev_hd, const char* name) {
-  if(dev_hd->open_cont_hds.empty()) {
-    return false;
-  }
-  for (const auto &t : dev_hd->open_cont_hds) {
+bool _container_opened(kvs_device_handle dev_hd, const char* name){
+  for (const auto &t: dev_hd->driver->open_containers) {
     if(strcmp(t->name, name) == 0) {
       return true;
     }
@@ -464,586 +424,18 @@ bool _container_opened(kvs_device_handle dev_hd, const char* name) {
   return false;
 }
 
-bool _container_opened(kvs_container_handle cont_hd) {
-  auto t = find(g_env.list_open_container.begin(), g_env.list_open_container.end(), cont_hd);
-  if (t == g_env.list_open_container.end()) {
-    return false;
-  }
-  return true;
-}
-
 inline kvs_result _check_container_handle(kvs_container_handle cont_hd) {
-  if (cont_hd == NULL) {
+  if((cont_hd == NULL) || (cont_hd->dev == NULL) 
+      || (cont_hd->dev->driver == NULL))
     return KVS_ERR_PARAM_INVALID;
-  }
-  if (!_container_opened(cont_hd)) {
-    return KVS_ERR_CONT_CLOSE;
-  }
-  if ((cont_hd->dev == NULL) || (cont_hd->dev->driver == NULL)) {
-    return KVS_ERR_PARAM_INVALID;
-  }
+
   if (!_device_opened(cont_hd->dev)) {
     return KVS_ERR_DEV_NOT_OPENED;
   }
-  return KVS_SUCCESS;
-}
-
-uint16_t _get_container_payload_size() {
-  cont_metadata cont;
-  uint16_t payload_size = sizeof(cont.keyspace_id);
-  payload_size += sizeof(cont.opened);
-  payload_size += sizeof(cont.key_order);
-  payload_size += sizeof(cont.capacity);
-  payload_size += sizeof(cont.free_size);
-  payload_size += sizeof(cont.kv_count);
-  return payload_size;
-}
-
-// translate byte order from host cpu end to little end, will change the content of buffer inputted
-// the size of of inputted interger should be in 1/2/4/8 byte
-bool _trans_host_to_little_end(void* data, uint8_t size) {
-  if(size == 1)
-    return true;
-  if(size == 2) {
-    uint16_t data_le = htole16(*((uint16_t*)data));
-    memcpy(data, (void*)(&data_le), size);
-    return true;
-  }
-  if(size == 4) {
-    uint32_t data_le = htole32(*((uint32_t*)data));
-    memcpy(data, (void*)(&data_le), size);
-    return true;
-  }
-  if(size == 8) {
-    uint64_t data_le = htole64(*((uint64_t*)data));
-    memcpy(data, (void*)(&data_le), size);
-    return true;
-  }
-  return false;
-}
-
-//copy a integer in to payload. Integer will translate to little end, and then copied to buffer 
-#define _copy_int_to_payload(integer, payload_buff, curr_posi) \
-do { \
-  uint8_t integer_size = sizeof(integer); \
-  _trans_host_to_little_end(&integer, integer_size); \
-  memcpy(payload_buff + curr_posi, &integer, integer_size); \
-  curr_posi += integer_size; \
-} while(0)
-
-void _construct_container_metadata_payload(const cont_metadata *cont,
-  char* payload_buff) {
-  cont_metadata cont_le = *cont;
-  uint16_t curr_posi = 0;
-  _copy_int_to_payload(cont_le.keyspace_id, payload_buff, curr_posi);
-  _copy_int_to_payload(cont_le.opened, payload_buff, curr_posi);
-  _copy_int_to_payload(cont_le.key_order, payload_buff, curr_posi);
-  _copy_int_to_payload(cont_le.capacity, payload_buff, curr_posi);
-  _copy_int_to_payload(cont_le.free_size, payload_buff, curr_posi);
-  _copy_int_to_payload(cont_le.kv_count, payload_buff, curr_posi);
-}
-
-// translate byte order from little end to host cpu end, will change the content of buffer inputted
-// the size of of inputted interger should be in 1/2/4/8 byte
-bool _trans_little_end_to_host(void* data, uint8_t size) {
-  if(size == 1)
-    return true;
-  if(size == 2) {
-    uint16_t data_le = le16toh(*((uint16_t*)data));
-    memcpy(data, (void*)(&data_le), size);
-    return true;
-  }
-  if(size == 4) {
-    uint32_t data_le = le32toh(*((uint32_t*)data));
-    memcpy(data, (void*)(&data_le), size);
-    return true;
-  }
-  if(size == 8) {
-    uint64_t data_le = le64toh(*((uint64_t*)data));
-    memcpy(data, (void*)(&data_le), size);
-    return true;
-  }
-  return false;
-}
-
-//Copy data from payload to a integer. 
-//Copied to integer type buffer, and  translate to host cpu end
-#define _copy_payload_to_int(integer, payload_buff, curr_posi) \
-do { \
-  uint16_t integer_size = sizeof(integer); \
-  memcpy(&integer, payload_buff + curr_posi, integer_size); \
-  _trans_little_end_to_host(&integer, integer_size); \
-  curr_posi += integer_size; \
-} while(0)
-
-void _parse_container_metadata_payload(cont_metadata *cont,
-  const char* payload_buff) {
-  uint16_t curr_posi = 0;
-  _copy_payload_to_int(cont->keyspace_id, payload_buff, curr_posi);
-  _copy_payload_to_int(cont->opened, payload_buff, curr_posi);
-  _copy_payload_to_int(cont->key_order, payload_buff, curr_posi);
-  _copy_payload_to_int(cont->capacity, payload_buff, curr_posi);
-  _copy_payload_to_int(cont->free_size, payload_buff, curr_posi);
-  _copy_payload_to_int(cont->kv_count, payload_buff, curr_posi); 
-}
-
-void _metadata_keyspace_aio_complete_handle(
-  kvs_callback_context* ioctx) {
-  uint32_t *complete_ptr = (uint32_t *)ioctx->private1;
-  *complete_ptr = 1;
-  kvs_result *result_ptr = (kvs_result *)ioctx->private2; 
-  *result_ptr = ioctx->result;
-}
-
-kvs_result _sync_io_to_meta_keyspace(kvs_device_handle dev_hd, 
-  const kvs_key *key, kvs_value *value, void* io_option, kvs_op io_op) {
-  kvs_result ret = KVS_SUCCESS;
-  bool syncio = 1;
-  kvs_callback_function cbfn = NULL;
-  /* as async_completed and async_result may be modify by other cpu(in complete handle),
-        so should be volatile type */
-  volatile uint32_t async_completed = 0; 
-  volatile kvs_result async_result = KVS_SUCCESS;
-
-  /*If use kdd or emulator sync store can be use always.
-       If use UDD should use corresponding interface, 
-       sync mode: sync interface, async mode: async interface*/
-#if defined WITH_SPDK
-  if(!g_env.udd_option.syncio) {//uses async interface
-    syncio = 0;
-    cbfn = _metadata_keyspace_aio_complete_handle;
-  }
-#endif
-
-  kvs_container_handle cont_hd = dev_hd->meta_cont_hd;
-  if(io_op == IOCB_ASYNC_PUT_CMD) {
-    ret = (kvs_result)dev_hd->meta_cont_hd->dev->driver->store_tuple(
-      cont_hd, key, value, *((kvs_store_option*)io_option), (void*)&async_completed,
-      (void*)&async_result, syncio, cbfn);
-  }else if(io_op == IOCB_ASYNC_GET_CMD){
-    ret = (kvs_result)dev_hd->meta_cont_hd->dev->driver->retrieve_tuple(
-      cont_hd, key, value, *((kvs_retrieve_option*)io_option), (void*)&async_completed,
-      (void*)&async_result, syncio, cbfn);
-  }else if(io_op == IOCB_ASYNC_DEL_CMD){
-    ret = (kvs_result)dev_hd->meta_cont_hd->dev->driver->delete_tuple(
-      cont_hd, key, *((kvs_delete_option*)io_option), (void*)&async_completed,
-      (void*)&async_result, syncio, cbfn);
-  }else if(io_op == IOCB_ASYNC_CHECK_KEY_EXIST_CMD){
-    ret = (kvs_result)dev_hd->meta_cont_hd->dev->driver->exist_tuple(
-      cont_hd, 1, key, value->length, (uint8_t *)value->value, (void*)&async_completed,
-      (void*)&async_result, syncio, cbfn);
-  }else {
-    fprintf(stderr, "KVAPI internal error unsupported aio type:%d passed.\n",
-      io_op);
-    return KVS_ERR_UNCORRECTIBLE;
-  }
-  if(ret != KVS_SUCCESS) {
-    return ret;
-  }
-  if(!syncio) {//used async io interface
-    while(true) {//wait for complete
-      if(async_completed) {
-        break;
-      }
-    }
-    ret = async_result;
-  }
-
-  return ret;
-}
-  
-
-kvs_result _store_container_metadata(kvs_device_handle dev_hd,
-  const cont_metadata *cont, kvs_store_type st_type) {
-  kvs_result ret = KVS_SUCCESS;
-  //calculate key size and payload size
-  uint16_t payload_size = _get_container_payload_size();
-  kvs_key_t klen = strnlen(cont->name, MAX_CONT_PATH_LEN - 1) + 1; //contains end '\0'
-  char* key = (char*)kvs_zalloc(klen, PAGE_ALIGN);
-  char* payload_buff = (char*)kvs_zalloc(payload_size, PAGE_ALIGN);
-  if(!key || !payload_buff) {
-    if(key) kvs_free(key);
-    if(payload_buff) kvs_free(payload_buff);
-    return KVS_ERR_HEAP_ALLOC_FAILURE;
-  }
-  // construct the payload content
-  snprintf(key, klen, "%s", cont->name);
-  _construct_container_metadata_payload(cont, payload_buff);
-
-  //store to meta data keyspace
-  kvs_store_option option;
-  option.st_type = st_type;
-  option.kvs_store_compress = false;
-  const kvs_key  kvskey = {key, klen};
-  kvs_value kvsvalue = {payload_buff, payload_size, 0, 0};
-  ret = _sync_io_to_meta_keyspace(dev_hd, &kvskey, &kvsvalue, &option,
-    IOCB_ASYNC_PUT_CMD);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "store keyspace meta data failed with error 0x%x - %s\n",
-       ret, kvs_errstr(ret));
-  }
-
-  kvs_free(key);
-  kvs_free(payload_buff);
-  return ret;
-}
-
-kvs_result _retrieve_container_metadata(kvs_device_handle dev_hd,
-  cont_metadata *cont) {
-  kvs_result ret = KVS_SUCCESS;
-
-  //malloc resources
-  uint16_t vlen = _get_container_payload_size();
-  vlen = ((vlen - 1)/DMA_ALIGN + 1) * DMA_ALIGN;
-  kvs_key_t klen = strlen(cont->name) + 1; //contains end '\0'
-  char *key   = (char*)kvs_malloc(klen, PAGE_ALIGN);
-  char *value = (char*)kvs_malloc(vlen, PAGE_ALIGN);
-  if(key == NULL || value == NULL) {
-    fprintf(stderr, "failed to allocate\n");
-    if(key) kvs_free(key);
-    if(value) kvs_free(value);
-    return KVS_ERR_HEAP_ALLOC_FAILURE;
-  }
-  snprintf(key, klen, "%s", cont->name);
-
-  kvs_retrieve_option option;
-  memset(&option, 0, sizeof(kvs_retrieve_option));
-  option.kvs_retrieve_decompress = false;
-  option.kvs_retrieve_delete = false;
-  const kvs_key  kvskey = {key, klen };
-  kvs_value kvsvalue = {value, vlen , 0, 0 /*offset */};
-  ret = _sync_io_to_meta_keyspace(dev_hd, &kvskey, &kvsvalue, &option,
-    IOCB_ASYNC_GET_CMD);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "retrieve container %s metatdata failed error 0x%x - %s\n",
-      key, ret, kvs_errstr(ret));
-    kvs_free(key);
-    kvs_free(value);
-    return ret;
-  }
-
-  //parse continers information
-  _parse_container_metadata_payload(cont, value);
-
-  kvs_free(key);
-  kvs_free(value);
-  return ret;
-}
-
-kvs_result _open_container(kvs_container_handle cont_hd) {
-  kvs_result ret = KVS_SUCCESS;
-  cont_metadata cont = {0, 0, 0, 0, 0, 0, cont_hd->name};
-  ret = _retrieve_container_metadata(cont_hd->dev, &cont);
-  if(ret != KVS_SUCCESS) {
-    return ret;
-  }
-  cont.opened = 1;
-  cont_hd->keyspace_id = cont.keyspace_id;
-  ret = _store_container_metadata(cont_hd->dev, &cont, KVS_STORE_POST);
-  return ret;
-}
-
-kvs_result _close_container(kvs_container_handle cont_hd) {
-  kvs_result ret = KVS_SUCCESS;
-  cont_metadata cont = {0, 0, 0, 0, 0, 0, cont_hd->name};
-  ret = _retrieve_container_metadata(cont_hd->dev, &cont);
-  if(ret != KVS_SUCCESS) {
-    return ret;
-  }
-  cont.opened = 0;
-  ret = _store_container_metadata(cont_hd->dev, &cont, KVS_STORE_POST);
-  return ret;
-}
-
-kvs_result _create_container_entry(kvs_device_handle dev_hd,
-  const char *name, uint8_t keyspace_id, uint64_t cap_size,
-  const kvs_container_context *ctx) {
-  int ret = KVS_SUCCESS;
-  cont_metadata cont = {
-    keyspace_id, 0, ctx->option.ordering, cap_size, 0, 0, name};
-  ret = _store_container_metadata(dev_hd, &cont, KVS_STORE_NOOVERWRITE);
-  return (kvs_result)ret;
-}
-
-kvs_result _delete_container_entry(kvs_device_handle dev_hd,
-  const char *name) {
-  kvs_result ret = KVS_SUCCESS;
-  kvs_key_t klen = strlen(name) + 1; //contains end '\0'
-  char* key = (char*)kvs_zalloc(klen, PAGE_ALIGN);
-  if(!key) {
-    return KVS_ERR_HEAP_ALLOC_FAILURE;
-  }
-  snprintf(key, klen, "%s", name);
-
-  const kvs_key  kvskey = {key, klen};
-  kvs_delete_option option = {true};
-  ret = _sync_io_to_meta_keyspace(dev_hd, &kvskey, NULL, &option,
-    IOCB_ASYNC_DEL_CMD);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "delete container failed with error 0x%x - %s\n", ret,
-      kvs_errstr(ret));
-  }
-
-  kvs_free(key);
-  return ret;
-}
-
-kvs_result _exist_container_entry(kvs_device_handle dev_hd,
-  const char *name, uint8_t* exist_buff) {
-  kvs_result ret = KVS_SUCCESS;
-  kvs_key_t klen = strlen(name) + 1; //contains end '\0'
-  char* key = (char*)kvs_zalloc(klen, PAGE_ALIGN);
-  if(!key) {
-    return KVS_ERR_HEAP_ALLOC_FAILURE;
-  }
-
-  snprintf(key, klen, "%s", name);
-  const kvs_key  kvskey = {key, klen};
-  *exist_buff = 0;
-  kvs_value kvsvalue = {exist_buff, 1 , 0, 0};
-  /*exist io no option need*/
-  ret = _sync_io_to_meta_keyspace(dev_hd, &kvskey, &kvsvalue, NULL,
-    IOCB_ASYNC_CHECK_KEY_EXIST_CMD);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Check container exist failed with error 0x%x - %s\n", ret,
-      kvs_errstr(ret));
-  }
-
-  return ret;
-}
-
-
-void _construct_container_list_payload(const cont_list* conts,
-  char* payload_buff, uint32_t *data_len) {
-  uint16_t curr_position = 0;
-  //total container number
-  memcpy(payload_buff + curr_position, &conts->cont_num,
-         sizeof(conts->cont_num));
-  curr_position += sizeof(conts->cont_num);
-  //name of every container, 
-  //every container name has fixed size(MAX_CONT_PATH_LEN) in ssd
-  for(uint8_t idx = 0; idx < conts->cont_num; idx++){
-    memcpy(payload_buff+curr_position, &conts->entries[idx].keyspace_id,
-           sizeof(conts->entries[idx].keyspace_id));
-    curr_position += sizeof(conts->entries[idx].keyspace_id);
-
-    memcpy(payload_buff+curr_position, conts->entries[idx].names_buffer,
-           MAX_CONT_PATH_LEN);
-    curr_position += MAX_CONT_PATH_LEN;
-  }
-  *data_len = curr_position;
-}
-
-void _parse_container_list_payload(const char *payload, uint32_t data_len,
-  cont_list* conts) {
-  //parse continers information, format as following:
-  //first four bytes is the number of containers, then keyspace id and name  of every container
-  //is stored, the keyspace id and name size of every container is 4 and MAX_CONT_PATH_LEN(256)
-  if(data_len < sizeof(conts->cont_num)) { //means number of container is 0
-    conts->cont_num = 0;
-    return;
-  }
-  uint32_t curr_posi = 0;
-  conts->cont_num = *((container_id_t *)payload);
-  curr_posi += sizeof(conts->cont_num);
-  for(uint8_t idx = 0; idx < conts->cont_num; idx++){
-    conts->entries[idx].keyspace_id = *((keyspace_id_t *)(payload + curr_posi));
-    curr_posi += sizeof(conts->entries[idx].keyspace_id);
-
-    memcpy(conts->entries[idx].names_buffer, payload + curr_posi,
-      MAX_CONT_PATH_LEN);
-    curr_posi += MAX_CONT_PATH_LEN;
-  }
-}
-
-kvs_result _retreive_container_list(kvs_device_handle dev_hd,
-  cont_list* conts) {
-  kvs_result ret = KVS_SUCCESS;
-  kvs_key_t klen = strlen(KEY_SPACE_LIST_KEY_NAME) + 1;
-  uint32_t act_len = sizeof(*conts);
-  uint32_t vlen = ((act_len + 3) / 4) * 4;
-
-  char *key   = (char*)kvs_malloc(klen, PAGE_ALIGN);
-  char *value = (char*)kvs_malloc(vlen, PAGE_ALIGN);
-  if(key == NULL || value == NULL) {
-    fprintf(stderr, "failed to allocate\n");
-    if(key) kvs_free(key);
-    if(value) kvs_free(value);
-    return KVS_ERR_HEAP_ALLOC_FAILURE;
-  }
-
-  //retrieve container list data from KVSSD metadata keyspace
-  snprintf(key, klen, "%s", KEY_SPACE_LIST_KEY_NAME);
-  kvs_retrieve_option option;
-  memset(&option, 0, sizeof(kvs_retrieve_option));
-  option.kvs_retrieve_decompress = false;
-  option.kvs_retrieve_delete = false;
-  const kvs_key  kvskey = {key, klen };
-  kvs_value kvsvalue = {value, vlen , 0, 0 /*offset */};
-  ret = _sync_io_to_meta_keyspace(dev_hd, &kvskey, &kvsvalue, &option,
-    IOCB_ASYNC_GET_CMD);
-  if(ret != KVS_SUCCESS) {
-    if(ret == KVS_ERR_KEY_NOT_EXIST) {//before create first container, key isn't exist
-      ret = KVS_SUCCESS;
-    } else {
-      fprintf(stderr, "Input value buffer len:%d. actual data length:%d.\n", kvsvalue.length, 
-        kvsvalue.actual_value_size);
-      fprintf(stderr, "retrieve container list failed error 0x%x - %s\n", ret,
-        kvs_errstr(ret));
-    }
-    conts->cont_num = 0;
-    kvs_free(key);
-    kvs_free(value);
-    return ret;
-  }
-  _parse_container_list_payload((const char*)kvsvalue.value, kvsvalue.length,
-    conts);
-
-  kvs_free(key);
-  kvs_free(value);
-  return ret;
-}
-
-kvs_result _store_container_list(kvs_device_handle dev_hd,
-  const cont_list* conts) {
-  kvs_result ret = KVS_SUCCESS;
-  // construct the payload content
-  kvs_key_t klen = strlen(KEY_SPACE_LIST_KEY_NAME) + 1; 
-  char* key = (char*)kvs_zalloc(klen, PAGE_ALIGN);
-  char* payload_buff = (char*)kvs_zalloc(sizeof(*conts), PAGE_ALIGN);
-  if(!key || !payload_buff) {
-    if(key) kvs_free(key);
-    if(payload_buff) kvs_free(payload_buff);
-    return KVS_ERR_HEAP_ALLOC_FAILURE;
-  }
-  snprintf(key, klen, "%s", KEY_SPACE_LIST_KEY_NAME);
-  
-  uint32_t vlen = 0;
-  _construct_container_list_payload(conts, payload_buff, &vlen);
-
-  //store to meta data keyspace
-  kvs_store_option option;
-  option.st_type = KVS_STORE_POST;
-  option.kvs_store_compress = false;
-  const kvs_key  kvskey = {key, klen};
-  kvs_value kvsvalue = {payload_buff, vlen, 0, 0};  
-  ret = _sync_io_to_meta_keyspace(dev_hd, &kvskey, &kvsvalue, &option,
-    IOCB_ASYNC_PUT_CMD);
-  if(ret != KVS_SUCCESS ) {
-    fprintf(stderr, "store container list failed with error 0x%x - %s\n", ret,
-      kvs_errstr(ret));
-  }
-
-  kvs_free(key);
-  kvs_free(payload_buff);
-  return ret;
-}
-
-kvs_result _remove_from_container_list(kvs_device_handle dev_hd,
-  const char *name, keyspace_id_t *keyspace_id) {
-  kvs_result ret = KVS_SUCCESS;
-  //check whether key space is exist
-  cont_list* conts = (cont_list*)calloc(1, sizeof(cont_list));
-  if(!conts)
-    return KVS_ERR_HEAP_ALLOC_FAILURE;
-  conts->cont_num = 0;
-  ret = _retreive_container_list(dev_hd, conts);
-  if(ret != KVS_SUCCESS) {
-    free(conts);
-    return ret;
-  }
-  uint8_t idx = 0;
-  for(idx = 0; idx < conts->cont_num; idx++){
-    if(!strcmp(name, conts->entries[idx].names_buffer))
-      break;
-  }
-
-  //remove the found contain from list
-  if(idx >= conts->cont_num) { //not find the input container
-    free(conts);
-    return KVS_ERR_CONT_NOT_EXIST;
-  }else {
-    *keyspace_id = conts->entries[idx].keyspace_id;
-    // move forward valid contains
-    for(; idx < conts->cont_num - 1; idx++){
-      memcpy(conts->entries + idx, conts->entries + idx + 1,
-        sizeof(cont_list_entry));
-    }
-    conts->cont_num--;
-  }
-
-  // store contain list to ssd
-  ret = _store_container_list(dev_hd, conts);
-  if(ret != KVS_SUCCESS)
-    *keyspace_id = _INVALID_USER_KEYSPACE_ID;
-
-  free(conts);
-  return ret;
-}
-
-keyspace_id_t _search_an_avaliable_keyspace_id(const cont_list* conts) {
-  keyspace_id_t keyspace_id = USER_DATA_KEYSPACE_START_ID;
-  /* TODO when KVSSD support more than two containers, add code to find a keyspace id, that 
-     *  not used, that is to say not in the inputted containers list(#conts) .
-     */ 
-  return keyspace_id;
-}
-bool _contain_keyspace(const cont_list* conts, keyspace_id_t ks_id) {
-  for(uint8_t idx = 0; idx < conts->cont_num; idx++){
-    if(ks_id == conts->entries[idx].keyspace_id) {
-      return true;
-    }
-  }
-  return false;
-}
-
-kvs_result _add_to_container_list(kvs_device_handle dev_hd,
-  const char *name, keyspace_id_t *keyspace_id/*in and out param*/) {
-  kvs_result ret = KVS_SUCCESS;
-  //check whether key space has exist or has reach max number supported
-  cont_list* conts = (cont_list*)calloc(1, sizeof(cont_list));
-  if(!conts)
-    return KVS_ERR_HEAP_ALLOC_FAILURE;
-
-  conts->cont_num = 0;
-  ret = _retreive_container_list(dev_hd, conts);
-  if(ret != KVS_SUCCESS) {
-    free(conts);
-    return ret;
-  }
-  if(conts->cont_num >= NR_MAX_CONT) {
-    free(conts);
-    fprintf(stderr, "Max container number %d has reached", NR_MAX_CONT);
-    fprintf(stderr, " add container %s failed.\n", name);
-    return KVS_ERR_CONT_MAX;
-  }
-  for(uint8_t idx = 0; idx < conts->cont_num; idx++) {
-    if(!strcmp(name, conts->entries[idx].names_buffer)) {
-      free(conts);
-      return KVS_ERR_CONT_EXIST;
-    }
+  if (!_container_opened(cont_hd->dev, cont_hd->name)) {
+    return KVS_ERR_CONT_CLOSE;
   }
-  //get the keyspace id
-  keyspace_id_t ks_id = *keyspace_id;
-  if(ks_id == _INVALID_USER_KEYSPACE_ID) {
-    ks_id = _search_an_avaliable_keyspace_id(conts);
-  } else if(_contain_keyspace(conts, ks_id)){
-    free(conts);
-    return KVS_ERR_UNRECOVERED_ERROR; //this keyspace has been used
-  }
-  //add new continainer to container list
-  snprintf(conts->entries[conts->cont_num].names_buffer,
-    MAX_CONT_PATH_LEN, "%s", name);
-  conts->entries[conts->cont_num].keyspace_id = ks_id;
-  conts->cont_num++;
-
-  // store contain list to ssd
-  ret = _store_container_list(dev_hd, conts);
-  if(ret == KVS_SUCCESS) {
-    *keyspace_id = ks_id;
-  }
-  free(conts);
-  return ret;
+  return KVS_SUCCESS;
 }
 
 kvs_result kvs_open_device(const char *dev_path, kvs_device_handle *dev_hd) {
@@ -1068,15 +460,12 @@ kvs_result kvs_open_device(const char *dev_path, kvs_device_handle *dev_hd) {
 		  "the library is not properly configured: please run kvs_init_env() first\n");
     return KVS_ERR_ENV_NOT_INITIALIZED;
   }
-  kvs_device_handle user_dev = new _kvs_device_handle();
-  if(user_dev == NULL){
-    WRITE_ERR("Memory is not enough,malloc failed!\n");
-    return KVS_ERR_MEMORY_MALLOC_FAIL;
-  }
-  kv_device_priv *dev  = _find_local_device_from_path(dev_path, &(g_env.list_devices));
-  if (dev == NULL) {
+	
+  kvs_device_handle user_dev = (kvs_device_handle)malloc(sizeof(struct _kvs_device_handle));
+  
+  kv_device_priv *dev  = _find_local_device_from_path(dev_path, g_env.list_devices);
+  if (dev == 0) {
     WRITE_ERR("can't find the device: %s\n", dev_path);
-    delete user_dev;
     return KVS_ERR_DEV_NOT_EXIST;
   }
   
@@ -1097,38 +486,13 @@ kvs_result kvs_open_device(const char *dev_path, kvs_device_handle *dev_hd) {
   }
 #else
   if(dev->isemul || dev->iskerneldev)
-    ret = user_dev->driver->init(dev_path, g_env.configfile, g_env.queuedepth,
-      g_env.is_polling);
-  if(ret != KVS_SUCCESS) {
-    delete user_dev;
-    return (kvs_result)ret;
-  }
-
+    ret = user_dev->driver->init(dev_path, g_env.configfile, g_env.queuedepth, g_env.is_polling);
 #endif
   user_dev->dev_path = (char*)malloc(strlen(dev_path)+1);
-  if(user_dev->dev_path == NULL){
-    delete user_dev;
-    return KVS_ERR_MEMORY_MALLOC_FAIL;
-  }
-  snprintf(user_dev->dev_path, (strlen(dev_path)+1), "%s", dev_path);
+  strcpy(user_dev->dev_path, dev_path);
   g_env.open_devices.push_back(user_dev);
-
-  //create meta data key space
-  kvs_container_handle cont_handle = 
-    (kvs_container_handle)malloc(sizeof(struct _kvs_container_handle));
-  if(!cont_handle) {
-    user_dev->meta_cont_hd = NULL;
-    g_env.open_devices.remove(user_dev);
-    kvs_close_device(user_dev);
-    *dev_hd = NULL;
-    return KVS_ERR_MEMORY_MALLOC_FAIL;
-  }
-  user_dev->meta_cont_hd = cont_handle;
-  cont_handle->keyspace_id = META_DATA_KEYSPACE_ID;
-  cont_handle->dev = user_dev;
-  snprintf(cont_handle->name, sizeof(cont_handle->name), "%s", "meta_data_keyspace");
-
   *dev_hd = user_dev;
+
   return (kvs_result)ret;
 }
 
@@ -1139,86 +503,43 @@ kvs_result kvs_close_device(kvs_device_handle user_dev) {
   if (!_device_opened(user_dev)) {
     return KVS_ERR_DEV_NOT_OPENED;
   }
-
-  //free all opened container handle
-  if(user_dev->meta_cont_hd)
-    free(user_dev->meta_cont_hd);
-
-  for (auto it = g_env.list_open_container.begin(); it != g_env.list_open_container.end();) {
-    it = g_env.list_open_container.erase(it);
-  }
-  for (const auto &t : user_dev->open_cont_hds) {
-    free(t);
-  }
-  
+    
   delete user_dev->driver;
   delete user_dev->dev;
   g_env.open_devices.remove(user_dev);
   free(user_dev->dev_path);
-  delete user_dev;
+  free(user_dev);
   return KVS_SUCCESS;
 }
 
-kvs_result kvs_create_container(kvs_device_handle dev_hd, const char *name,
-  uint64_t size, const kvs_container_context *ctx) {
-  if((dev_hd == NULL) || (name == NULL)) {
-    return KVS_ERR_PARAM_INVALID;
-  }
-  if(*name == '\0'){
-    return KVS_ERR_CONT_NAME;
-  }
-  uint32_t name_len = strnlen(name, MAX_CONT_PATH_LEN) + 1;
-  if(name_len > MAX_CONT_PATH_LEN) {
-    return KVS_ERR_CONT_PATH_TOO_LONG;
-  }
-  if (!_device_opened(dev_hd)) {
-    return KVS_ERR_DEV_NOT_EXIST;
-  }
-  // Fix SS-563: If the kvs_container_option is not equal to KVS_KEY_ORDER_NONE,
-  // return KVS_ERR_OPTION_INVALID
-  if (ctx->option.ordering != KVS_KEY_ORDER_NONE) {
-    fprintf(stderr, "Do not support key order %d!\n",ctx->option.ordering);
-    return KVS_ERR_OPTION_INVALID;
-  }
-  keyspace_id_t keyspace_id = _INVALID_USER_KEYSPACE_ID;
-  kvs_result ret = _add_to_container_list(dev_hd, name, &keyspace_id);
-  if(ret != KVS_SUCCESS) {
-    return ret;
-  }
-  ret = _create_container_entry(dev_hd, name, keyspace_id, size, ctx);
-  if(ret != KVS_SUCCESS) {
-    _remove_from_container_list(dev_hd, name, &keyspace_id);
-    return ret;
-  }
-  return ret;
-}
+kvs_result kvs_create_container (kvs_device_handle dev_hd, const char *name, uint64_t size, const kvs_container_context *ctx) {
+  /*
+  kvs_container *container = (kvs_container *)malloc(sizeof(kvs_container));
 
-kvs_result kvs_delete_container(kvs_device_handle dev_hd,
-  const char *cont_name) {
-  if((dev_hd == NULL) || (cont_name == NULL)) {
-    return KVS_ERR_PARAM_INVALID;
-  }
-  if (!_device_opened(dev_hd)) {
-    return KVS_ERR_DEV_NOT_EXIST;
-  }
-  //before delete container, container should in close state
-  if (_container_opened(dev_hd, cont_name)) {
-    return KVS_ERR_CONT_OPEN;
-  }
+  container->name = (kvs_container_name *)malloc(sizeof(kvs_container_name));
+  container->name->name = (char*)malloc(strlen(name));
+  container->name->name_len = strlen(name);
+  strcpy(container->name->name, name);
 
-  kvs_result ret = KVS_SUCCESS;
-  //remove container from container list
-  keyspace_id_t keyspace_id_removed = _INVALID_USER_KEYSPACE_ID;
-  ret = _remove_from_container_list(dev_hd, cont_name, &keyspace_id_removed);
-  if(ret != KVS_SUCCESS) {
-    return ret;
-  }
-  //delete container entry, if delete failed, should recover original state
-  ret = _delete_container_entry(dev_hd, cont_name);
-  if(ret != KVS_SUCCESS) {
-    _add_to_container_list(dev_hd, cont_name, &keyspace_id_removed);
-    return ret;
+  dev_hd->list_containers.push_back(container);
+  */
+  return KVS_SUCCESS;
+  
+}
+
+kvs_result kvs_delete_container (kvs_device_handle dev_hd, const char *cont_name) {
+  /*
+  for (const auto &t: dev_hd->list_containers) {
+    if(strcmp(t->name->name, cont_name) == 0) {
+      fprintf(stdout, "KVSSD: Container %s is deleted\n", cont_name);
+      if(t->name->name)
+	free (t->name->name);
+      if(t->name)
+	free(t->name);
+      free(t);
+    }
   }
+  */
   return KVS_SUCCESS;
 }
 
@@ -1226,146 +547,49 @@ kvs_result kvs_open_container(kvs_device_handle dev_hd, const char* name, kvs_co
   if((dev_hd == NULL) || (name == NULL) || (cont_hd == NULL)) {
     return KVS_ERR_PARAM_INVALID;
   }
-  if(*name == '\0') {
+  if(*name == '\0'){
     return KVS_ERR_CONT_NAME;
   }
   if (!_device_opened(dev_hd)) {
-    return KVS_ERR_DEV_NOT_EXIST;
+    return KVS_ERR_DEV_NOT_OPENED;
   } 
-  if (_container_opened(dev_hd, name)) {
+  if (_container_opened(dev_hd,name)) {
     return KVS_ERR_CONT_OPEN;
   }
-
-  //check container exist
-  uint8_t exist = 0;
-  kvs_result ret = _exist_container_entry(dev_hd, name, &exist);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Check container exist failed. error code:0x%x.\n", ret);
-    return ret;
-  }
-  if(!exist) {
-    return KVS_ERR_CONT_NOT_EXIST;
-  }
-  //open container
-  kvs_container_handle cont_handle = (kvs_container_handle)malloc(
-    sizeof(struct _kvs_container_handle));
-  if(!cont_handle) {
-    return KVS_ERR_MEMORY_MALLOC_FAIL;
-  }
+  kvs_container_handle cont_handle = (kvs_container_handle)malloc(sizeof(struct _kvs_container_handle));
   cont_handle->dev = dev_hd;
-  snprintf(cont_handle->name, sizeof(cont_handle->name), "%s", name);
+  strcpy(cont_handle->name, name);
 
-  ret = _open_container(cont_handle);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Update container state failed. error code:0x%x.\n", ret);
-    free(cont_handle);
-    return ret;
-  }
+  dev_hd->driver->open_containers.push_back(cont_handle);
 
-  //dev_hd->driver->open_containers.push_back(cont_handle);
-  dev_hd->open_cont_hds.push_back(cont_handle);
-  g_env.list_open_container.push_back(cont_handle);
   *cont_hd = cont_handle;
 
   return KVS_SUCCESS;
 }
 
-kvs_result kvs_close_container(kvs_container_handle cont_hd){
+kvs_result kvs_close_container (kvs_container_handle cont_hd){
   kvs_result ret = _check_container_handle(cont_hd);
   if (ret!=KVS_SUCCESS) {
     return ret;
   }
 
-  ret = _close_container(cont_hd);
-  if(ret != KVS_SUCCESS) {
-    fprintf(stderr, "Close container failed. error code:0x%x-%s.\n", ret,
-      kvs_errstr(ret));
-    return ret;
-  }
-  kvs_device_handle dev_hd = cont_hd->dev;
-  dev_hd->open_cont_hds.remove(cont_hd);
-  g_env.list_open_container.remove(cont_hd);
-
-  free(cont_hd);
-  return ret;
+  cont_hd->dev->driver->open_containers.remove(cont_hd);
+  if(cont_hd)
+    free(cont_hd);
+  
+  return KVS_SUCCESS;
 }
 
-kvs_result kvs_get_container_info(kvs_container_handle cont_hd,
-  kvs_container *cont) {
-  if(cont == NULL) {
-    return KVS_ERR_PARAM_INVALID;
-  }
-  kvs_result ret = _check_container_handle(cont_hd);
-  if (ret != KVS_SUCCESS) {
-    return ret;
-  }
+kvs_result kvs_get_container_info (kvs_container_handle cont_hd, kvs_container *cont) {
 
-  cont_metadata cont_meta = {0, 0, 0, 0, 0, 0, cont_hd->name};
-  ret = _retrieve_container_metadata(cont_hd->dev, &cont_meta);
-  if(ret != KVS_SUCCESS)
-    return ret;
-  cont->opened = cont_meta.opened;
-  cont->capacity = cont_meta.capacity;
-  cont->free_size = cont_meta.free_size;
-  cont->count = cont_meta.kv_count;
-  cont->scale = (cont_meta.free_size * 0.1)/(cont_meta.capacity * 0.1) * 100;
-  cont->name->name_len = strnlen(cont_hd->name,MAX_CONT_PATH_LEN);
-  snprintf(cont->name->name, cont->name->name_len + 1, "%s", cont_hd->name);
-  return ret;
+  fprintf(stdout, "WARN: not implemented yet\n");
+  return KVS_SUCCESS;
 }
 
-kvs_result kvs_list_containers(kvs_device_handle dev_hd, uint32_t index,
-  uint32_t buffer_size, kvs_container_name *names, uint32_t *cont_cnt) {
-  kvs_result ret = KVS_SUCCESS;
-  if((dev_hd == NULL) || (names == NULL) || (cont_cnt == NULL)) {
-    return KVS_ERR_PARAM_INVALID;
-  }
-  if(index < 1 ||  index > NR_MAX_CONT) {
-    WRITE_ERR("Index of container/keyspace should be start form 1 to less or equal MAX container number!\n");
-    return KVS_ERR_CONT_INDEX;
-  }
-  if (!_device_opened(dev_hd)) {
-    return KVS_ERR_DEV_NOT_EXIST;
-  }
-
-  *cont_cnt = 0;
-  cont_list* conts = (cont_list*)calloc(1, sizeof(cont_list));
-  if(!conts)
-    return KVS_ERR_HEAP_ALLOC_FAILURE;
-  conts->cont_num = 0;
-  ret = _retreive_container_list(dev_hd, conts);
-  if(ret != KVS_SUCCESS) {
-    free(conts);
-    return ret;
-  }
-  if(index > conts->cont_num && conts->cont_num != 0) {
-    WRITE_ERR("Index of container/keyspace inputted is too bigger.\n");
-    return KVS_ERR_CONT_INDEX;
-  }
-
-  uint32_t items_buff_cnt = buffer_size/sizeof(kvs_container_name);
-   *cont_cnt = 0;
-  for(uint8_t idx = index - 1; idx < conts->cont_num; idx++) {//index start from 1
-    if(*cont_cnt >= items_buff_cnt) {
-      if(!items_buff_cnt) {
-        WRITE_ERR("At least one container to read, buffer inputted is empty\n");
-        ret = KVS_ERR_BUFFER_SMALL;
-      }
-      break; //buffer exhausted 
-    }
-    uint32_t len = strlen(conts->entries[idx].names_buffer) + 1;
-    if(len > names[idx].name_len) {
-      WRITE_ERR("The buffer that used to store name is too small.\n");
-      return KVS_ERR_BUFFER_SMALL;
-    }
-    snprintf(names[idx].name, names[idx].name_len, "%s",
-      conts->entries[idx].names_buffer);
-    names[idx].name_len = len; //include end '\0'
-    *cont_cnt += 1;
-  }
-
-  free(conts);
-  return ret;
+static kvs_result _check_key_length(kvs_key* key) {
+  if (key->length > KVS_MAX_KEY_LENGTH)
+    return KVS_ERR_KEY_LENGTH_INVALID;
+  return KVS_SUCCESS;
 }
 
 kvs_result kvs_get_tuple_info (kvs_container_handle cont_hd, const kvs_key *key, kvs_tuple_info *info) {
@@ -1422,9 +646,9 @@ kvs_result kvs_store_tuple(kvs_container_handle cont_hd, const kvs_key *key,
   ret = validate_request(key, value);
   if(ret)
     return (kvs_result)ret;
-
-  ret = cont_hd->dev->driver->store_tuple(cont_hd, key, value,
-    ctx->option, ctx->private1, ctx->private2, 1, 0);
+  
+  ret =  cont_hd->dev->driver->store_tuple(0, key, value, ctx->option,
+					   ctx->private1, ctx->private2, 1, 0);
   return (kvs_result)ret;
 }
 
@@ -1442,9 +666,9 @@ kvs_result kvs_store_tuple_async(kvs_container_handle cont_hd, const kvs_key *ke
   ret = validate_request(key, value);
   if(ret)
     return (kvs_result)ret;
-
-  ret = cont_hd->dev->driver->store_tuple(cont_hd, key, value,
-    ctx->option, ctx->private1, ctx->private2, 0, cbfn);
+  
+  ret = cont_hd->dev->driver->store_tuple(0, key, value, ctx->option,
+					   ctx->private1, ctx->private2, 0, cbfn);
   return (kvs_result)ret;
 }
 
@@ -1461,11 +685,11 @@ kvs_result kvs_retrieve_tuple(kvs_container_handle cont_hd, const kvs_key *key,
   ret = validate_request(key, value);
   if(ret)
     return (kvs_result)ret;
-  if (value->length & (KVS_VALUE_LENGTH_ALIGNMENT_UNIT - 1))
+  if (value->length % KVS_ALIGNMENT_UNIT)
       return KVS_ERR_VALUE_LENGTH_MISALIGNED;
 
-  ret = cont_hd->dev->driver->retrieve_tuple(cont_hd, key, value,
-    ctx->option, ctx->private1, ctx->private2, 1, 0);
+  ret = cont_hd->dev->driver->retrieve_tuple(0, key, value, ctx->option,
+					     ctx->private1, ctx->private2, 1, 0);
   return (kvs_result)ret;
 }
 
@@ -1481,14 +705,123 @@ kvs_result kvs_retrieve_tuple_async(kvs_container_handle cont_hd, const kvs_key
   ret = validate_request(key, value);
   if(ret)
     return (kvs_result)ret;
-  if (value->length & (KVS_VALUE_LENGTH_ALIGNMENT_UNIT - 1))
+  if (value->length % KVS_ALIGNMENT_UNIT)
+      return KVS_ERR_VALUE_LENGTH_MISALIGNED;
+
+  ret = cont_hd->dev->driver->retrieve_tuple(0, key, value, ctx->option,
+					      ctx->private1, ctx->private2, 0, cbfn);
+  return (kvs_result)ret;
+}
+
+#ifdef KVS_REMOTE
+kvs_result kvs_list_tuple(kvs_container_handle cont_hd, const kvs_key *prefix_key, const kvs_key *start_key,
+                              uint16_t max_keys_to_list, kvs_value *value, const kvs_list_context *ctx) {
+  int ret = _check_container_handle(cont_hd);
+  int32_t max_key_length = 0;
+
+  if (ret!=KVS_SUCCESS) {
+    return (kvs_result)ret;
+  }
+
+  if(value == NULL) {
+    return KVS_ERR_PARAM_INVALID;
+  }
+
+  if (prefix_key && _check_key_length(prefix_key) != KVS_SUCCESS)
+    return KVS_ERR_KEY_LENGTH_INVALID;
+
+  if (start_key && _check_key_length(start_key) != KVS_SUCCESS)
+    return KVS_ERR_KEY_LENGTH_INVALID;
+
+  //kvs_get_max_key_length (kvs_device_handle dev_hd, &max_key_length)
+  max_key_length = (prefix_key ? prefix_key->length : 0) + (start_key ? start_key->length : 0);
+  if (max_key_length > KVS_MAX_KEY_LENGTH)
+    return KVS_ERR_KEY_LENGTH_INVALID;
+
+  ret = validate_request(NULL, value);
+  if(ret)
+    return (kvs_result)ret;
+
+  if (value->length % KVS_ALIGNMENT_UNIT)
       return KVS_ERR_VALUE_LENGTH_MISALIGNED;
 
-  ret = cont_hd->dev->driver->retrieve_tuple(cont_hd, key, value,
-    ctx->option, ctx->private1, ctx->private2, 0, cbfn);
+  ret = cont_hd->dev->driver->list_tuple(0, prefix_key, start_key, max_keys_to_list, value,
+                                             ctx->private1, ctx->private2, 1, 0);
   return (kvs_result)ret;
 }
 
+kvs_result kvs_list_tuple_async(kvs_container_handle cont_hd, const kvs_key *prefix_key, const kvs_key *start_key,
+                                    uint16_t max_keys_to_list,
+                                    kvs_value *value, const kvs_list_context *ctx,
+                                    kvs_callback_function cbfn) {
+  int ret = _check_container_handle(cont_hd);
+  if (ret!=KVS_SUCCESS) {
+    return (kvs_result)ret;
+  }
+
+  if(value == NULL) {
+    return KVS_ERR_PARAM_INVALID;
+  }
+
+  ret = validate_request(prefix_key, value);
+  if(ret)
+    return (kvs_result)ret;
+
+  ret = validate_request(start_key, NULL);
+  if(ret)
+    return (kvs_result)ret;
+
+  if (value->length % KVS_ALIGNMENT_UNIT)
+      return KVS_ERR_VALUE_LENGTH_MISALIGNED;
+
+  ret = cont_hd->dev->driver->list_tuple(0, prefix_key, start_key, max_keys_to_list, value,
+                                         ctx->private1, ctx->private2, 0, cbfn);
+  return (kvs_result)ret;
+}
+
+kvs_result kvs_retrieve_tuple_direct(kvs_container_handle cont_hd, 
+                                     const kvs_key *key, const kvs_value *value, 
+                                     uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kvs_retrieve_context *ctx) {
+
+  int ret = _check_container_handle(cont_hd);
+  if (ret!=KVS_SUCCESS) {
+    return (kvs_result)ret;
+  }
+  if((key == NULL) || (ctx == NULL)) {
+    return KVS_ERR_PARAM_INVALID;
+  }
+  ret = validate_request(key, NULL);
+  if(ret)
+    return (kvs_result)ret;
+
+  ret = cont_hd->dev->driver->retrieve_tuple_direct(0, key, value, client_rdma_key, client_rdma_qhandle, ctx->option,
+                                             ctx->private1, ctx->private2, 1, 0);
+  return (kvs_result)ret;
+}
+
+kvs_result kvs_store_tuple_direct(kvs_container_handle cont_hd,
+                                     const kvs_key *key, const kvs_value *value,
+                                     uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kvs_store_context *ctx) {
+
+  int ret = _check_container_handle(cont_hd);
+  if (ret!=KVS_SUCCESS) {
+    return (kvs_result)ret;
+  }
+  if((key == NULL) || (ctx == NULL)) {
+    return KVS_ERR_PARAM_INVALID;
+  }
+  ret = validate_request(key, NULL);
+  if(ret)
+    return (kvs_result)ret;
+
+  ret = cont_hd->dev->driver->store_tuple_direct(0, key, value, client_rdma_key, client_rdma_qhandle, ctx->option,
+                                             ctx->private1, ctx->private2, 1, 0);
+  return (kvs_result)ret;
+}
+
+
+#endif
+
 kvs_result kvs_delete_tuple(kvs_container_handle cont_hd, const kvs_key *key,
 		const kvs_delete_context *ctx) {
   int ret = _check_container_handle(cont_hd);
@@ -1502,8 +835,8 @@ kvs_result kvs_delete_tuple(kvs_container_handle cont_hd, const kvs_key *key,
   if(ret)
     return (kvs_result)ret;
 
-  ret = cont_hd->dev->driver->delete_tuple(cont_hd, key,
-    ctx->option, ctx->private1, ctx->private2, 1, 0);
+  ret = cont_hd->dev->driver->delete_tuple(0, key, ctx->option, ctx->private1,
+					    ctx->private2, 1, 0);
   return (kvs_result)ret;
 }
 
@@ -1519,8 +852,8 @@ kvs_result kvs_delete_tuple_async(kvs_container_handle cont_hd, const kvs_key* k
   ret = validate_request(key, 0);
   if(ret) return (kvs_result)ret;
   
-  ret = cont_hd->dev->driver->delete_tuple(cont_hd, key,
-    ctx->option, ctx->private1, ctx->private2, 0, cbfn);
+  ret = cont_hd->dev->driver->delete_tuple(0, key, ctx->option, ctx->private1,
+					    ctx->private2, 0, cbfn);
   return (kvs_result)ret;
 }
 
@@ -1538,14 +871,12 @@ kvs_result kvs_exist_tuples(kvs_container_handle cont_hd, uint32_t key_cnt, cons
   ret = validate_request(keys, 0);
   if(ret) return (kvs_result)ret;
   
-  ret = cont_hd->dev->driver->exist_tuple(cont_hd, key_cnt, keys,
-    buffer_size, result_buffer, ctx->private1, ctx->private2, 1, 0); 
+  ret = cont_hd->dev->driver->exist_tuple(0, key_cnt, keys, buffer_size, result_buffer, ctx->private1, ctx->private2, 1, 0); 
   return (kvs_result)ret;
 
 }
 
-kvs_result kvs_exist_tuples_async(kvs_container_handle cont_hd, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, 
-                                  uint8_t *result_buffer, const kvs_exist_context *ctx, kvs_callback_function cbfn) {
+kvs_result kvs_exist_tuples_async(kvs_container_handle cont_hd, uint32_t key_cnt, const kvs_key *keys, uint32_t buffer_size, uint8_t *result_buffer, const kvs_exist_context *ctx, kvs_callback_function cbfn) {
   int ret = _check_container_handle(cont_hd);
   if (ret!=KVS_SUCCESS) {
     return (kvs_result)ret;
@@ -1558,9 +889,8 @@ kvs_result kvs_exist_tuples_async(kvs_container_handle cont_hd, uint32_t key_cnt
   ret = validate_request(keys, 0);
   if(ret) return (kvs_result)ret;
   
-  ret = cont_hd->dev->driver->exist_tuple(cont_hd, key_cnt, keys,
-    buffer_size, result_buffer, ctx->private1, ctx->private2, 0, cbfn);
-
+  ret = cont_hd->dev->driver->exist_tuple(0, key_cnt, keys, buffer_size, result_buffer, ctx->private1, ctx->private2, 0, cbfn);
+  
   return (kvs_result)ret;
 }
 
@@ -1576,8 +906,8 @@ kvs_result kvs_open_iterator(kvs_container_handle cont_hd, const kvs_iterator_co
   if(!_is_valid_bitmask(ctx->bitmask))
     return KVS_ERR_ITERATOR_COND_INVALID;
 
-  ret = cont_hd->dev->driver->open_iterator(cont_hd, ctx->option,
-    ctx->bitmask, ctx->bit_pattern, iter_hd);
+  ret = cont_hd->dev->driver->open_iterator(0, ctx->option, ctx->bitmask,
+					     ctx->bit_pattern, iter_hd);
   return (kvs_result)ret;
 }
 
@@ -1591,7 +921,7 @@ kvs_result kvs_close_iterator(kvs_container_handle cont_hd, kvs_iterator_handle
     return KVS_ERR_PARAM_INVALID;
   }
 
-  ret =  cont_hd->dev->driver->close_iterator(cont_hd, hiter);
+  ret =  cont_hd->dev->driver->close_iterator(0, hiter);
   return (kvs_result)ret;
 }
 
@@ -1602,7 +932,7 @@ kvs_result kvs_close_iterator_all(kvs_container_handle cont_hd) {
     return (kvs_result)ret;
   }
 
-  ret = cont_hd->dev->driver->close_iterator_all(cont_hd);
+  ret = cont_hd->dev->driver->close_iterator_all(0);
   return (kvs_result)ret;
 }
 
@@ -1616,8 +946,7 @@ kvs_result kvs_list_iterators(kvs_container_handle cont_hd, kvs_iterator_info *k
   if(count < 1 || count > KVS_MAX_ITERATE_HANDLE)
     return KVS_ERR_ITERATOR_NUM_OUT_RANGE;
 
-  ret = cont_hd->dev->driver->list_iterators(cont_hd, kvs_iters,
-    count);
+  ret = cont_hd->dev->driver->list_iterators(0, kvs_iters, count);
   return (kvs_result)ret;
 }
 
@@ -1633,7 +962,7 @@ kvs_result kvs_iterator_next(kvs_container_handle cont_hd, kvs_iterator_handle h
     return KVS_ERR_ITERATOR_BUFFER_SIZE;
   }
 
-  ret = cont_hd->dev->driver->iterator_next(cont_hd, hiter, iter_list, ctx->private1, ctx->private2, 1, 0);
+  ret = cont_hd->dev->driver->iterator_next(hiter, iter_list, ctx->private1, ctx->private2, 1, 0);
   return (kvs_result)ret;
 }
 
@@ -1650,7 +979,7 @@ kvs_result kvs_iterator_next_async(kvs_container_handle cont_hd, kvs_iterator_ha
     return KVS_ERR_ITERATOR_BUFFER_SIZE;
   }
 
-  ret = cont_hd->dev->driver->iterator_next(cont_hd, hiter, iter_list, ctx->private1, ctx->private2, 0, cbfn);
+  ret = cont_hd->dev->driver->iterator_next(hiter, iter_list, ctx->private1, ctx->private2, 0, cbfn);
   return (kvs_result)ret;
 }
 
@@ -1691,9 +1020,9 @@ kvs_result kvs_get_device_utilization(kvs_device_handle dev_hd, int32_t *dev_uti
   }
   if (!_device_opened(dev_hd)) {
     ret = KVS_ERR_DEV_NOT_OPENED;
-  } else {
+  } else
     ret = dev_hd->driver->get_used_size(dev_util);
-  }
+  
   return (kvs_result)ret;
 }
 
@@ -1704,9 +1033,8 @@ kvs_result kvs_get_device_capacity(kvs_device_handle dev_hd, int64_t *dev_capa)
   }
   if (!_device_opened(dev_hd)) {
     ret = KVS_ERR_DEV_NOT_OPENED;
-  } else {
+  } else
     ret = dev_hd->driver->get_total_size(dev_capa);
-  }
   return (kvs_result)ret;
 }
 
@@ -1765,6 +1093,44 @@ kvs_result kvs_get_optimal_value_length (kvs_device_handle dev_hd, int32_t *opt_
   return KVS_SUCCESS;
 }
 
+#ifdef KVS_REMOTE
+
+  kvs_result kvs_lock_tuple(kvs_container_handle cont_hd, const kvs_key *key, uint64_t instance_uuid, const kvs_lock_context *ctx) {
+    int ret = _check_container_handle(cont_hd);
+    if (ret!=KVS_SUCCESS) {
+      return (kvs_result)ret;
+    }
+    if((key == NULL) || (ctx == NULL))
+      return KVS_ERR_PARAM_INVALID;
+
+    ret = validate_request(key, 0);
+    if(ret)
+      return (kvs_result)ret;
+
+    ret = cont_hd->dev->driver->lock_tuple(0, key, instance_uuid, ctx->option, ctx->private1, ctx->private2, true, NULL);
+    return (kvs_result)ret;  
+
+  }
+
+  kvs_result kvs_unlock_tuple(kvs_container_handle cont_hd, const kvs_key *key, uint64_t instance_uuid, const kvs_lock_context *ctx) {
+
+    int ret = _check_container_handle(cont_hd);
+    if (ret!=KVS_SUCCESS) {
+      return (kvs_result)ret;
+    }
+    if((key == NULL) || (ctx == NULL))
+      return KVS_ERR_PARAM_INVALID;
+
+    ret = validate_request(key, 0);
+    if(ret)
+      return (kvs_result)ret;
+
+    ret = cont_hd->dev->driver->unlock_tuple(0, key, instance_uuid, ctx->option, ctx->private1, ctx->private2, true, NULL);
+    return (kvs_result)ret; 
+  }
+
+#endif
+
 
 void *_kvs_zalloc(size_t size_bytes, size_t alignment, const char *file) {
   WRITE_LOG("kvs_zalloc size: %ld, align: %ld, from %s\n", size_bytes, alignment, file);
@@ -1798,5 +1164,4 @@ void _kvs_free(void * buf, const char *file) {
 int32_t kvs_get_ioevents(kvs_container_handle cont_hd, int maxevents) {
   return cont_hd->dev->driver->process_completions(maxevents);
 }
-}// namespace api_private
-
+}
diff --git a/PDK/core/src/device_abstract_layer/emulator/sample_code/sample_interrupt.cpp b/PDK/core/src/device_abstract_layer/emulator/sample_code/sample_interrupt.cpp
new file mode 100644
index 0000000..c977dfb
--- /dev/null
+++ b/PDK/core/src/device_abstract_layer/emulator/sample_code/sample_interrupt.cpp
@@ -0,0 +1,433 @@
+#include "assert.h"
+#include <string>
+#include <iostream>
+#include <sstream>
+#include <thread>
+#include <vector>
+
+#include <stdio.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sys/time.h>
+
+#include <mutex>
+#include <chrono>
+#include <thread>
+#include <set>
+#include <unordered_set>
+#include <map>
+#include <unordered_map>
+#include <iomanip>
+#include <ctime>
+#include <condition_variable>
+#include <atomic>
+#include "kvs_adi.h"
+
+/* total kv to insert */
+int TOTAL_KV_COUNT = 10000;
+
+typedef struct {
+    kv_key *key;
+    kv_value *value;
+    kv_result retcode;
+
+    std::condition_variable done_cond;
+    std::mutex mutex;
+    std::atomic<int> done;
+} iohdl_t;
+
+
+void free_kv(kv_key *key, kv_value *value) {
+    // free key
+    if (key != NULL) {
+        if (key->key != NULL) {
+            free(key->key);
+        }
+        free(key);
+    }
+
+    // free value
+    if (value != NULL) {
+        if (value->value != NULL) {
+            free(value->value);
+        }
+        free(value);
+    }
+}
+
+// add check queue shutdown call
+// add SQ --> CQ can be done through a middle queue
+//
+// entry point for polling thread
+void poll_CQ(kv_queue_handle cq_hdl, uint32_t timeout) {
+    // set up polling
+    kv_result res = KV_SUCCESS;
+    uint32_t count = 100;
+    while (res != KV_ERR_QUEUE_IN_SHUTDOWN) {
+        count = 100;
+        res = kv_poll_completion(cq_hdl, timeout, &count);
+    }
+
+    printf("polling done\n");
+}
+
+// this function will be called after completion of a command
+void interrupt_func(void *data, int number) {
+    (void) data;
+    (void) number;
+
+    //printf("inside interrupt\n");
+}
+
+int create_sq(kv_device_handle dev_hdl, uint16_t sqid, uint16_t qsize, uint16_t cqid, kv_queue_handle *quehdl) {
+
+    kv_queue qinfo;
+    qinfo.queue_id = sqid;
+    qinfo.queue_size = qsize;
+    qinfo.completion_queue_id = cqid;
+    qinfo.queue_type = SUBMISSION_Q_TYPE;
+    qinfo.extended_info = NULL;
+
+    if (KV_SUCCESS != kv_create_queue(dev_hdl, &qinfo, quehdl)) {
+        printf("submission queue creation failed\n");
+        exit(1);
+    }
+    return 0;
+}
+
+int create_cq(kv_device_handle dev_hdl, uint16_t cqid, uint16_t qsize, kv_queue_handle *quehdl) {
+
+    kv_queue qinfo;
+    qinfo.queue_id = cqid;
+    qinfo.queue_size = qsize;
+    qinfo.completion_queue_id = 0;
+    qinfo.queue_type = COMPLETION_Q_TYPE;
+    qinfo.extended_info = NULL;
+
+    if (KV_SUCCESS != kv_create_queue(dev_hdl, &qinfo, quehdl)) {
+        printf("completion queue creation failed\n");
+        exit(1);
+    }
+    return 0;
+}
+
+uint64_t current_timestamp() {
+    std::chrono::system_clock::time_point timepoint = std::chrono::system_clock::now();
+    uint64_t nano_from_epoch = std::chrono::duration_cast<std::chrono::nanoseconds>(timepoint.time_since_epoch()).count();
+    return nano_from_epoch;
+}
+
+// post processing function for a command
+// op->private_data is the data caller passed in
+void on_IO_complete_func(kv_io_context *ctx) {
+    // depends if you allocated space for each command, you can manage
+    // allocated spaces here. This should in sync with your IO cycle
+    // management strategy.
+    //
+    // get completed async command results, free any allocated spaces if necessary.
+    //
+    // printf("finished one command\n");
+    iohdl_t *iohdl = (iohdl_t *) ctx->private_data;
+
+    std::unique_lock<std::mutex> lock(iohdl->mutex);
+    iohdl->key = const_cast<kv_key*> (ctx->key);
+    iohdl->value = ctx->value;
+    iohdl->retcode = ctx->retcode;
+
+    printf("command finished with result 0x%X\n", iohdl->retcode);
+    iohdl->done = 1;
+    iohdl->done_cond.notify_one();
+
+}
+
+// fill key value with random data
+void populate_key_value_startwith(uint32_t keystart, kv_key *key, kv_value *value) {
+
+    char *buffer = (char *)key->key;
+    uint32_t blen = key->length;
+
+    uint8_t *base = (uint8_t *)&keystart;
+    uint8_t prefix[4] = {
+            *(base + 3),
+            *(base + 2),
+            *(base + 1),
+            *(base + 0) };
+
+    // if prefix is 0, skip prefix setup in key value
+    if (keystart != 0) {
+        assert(blen > 4);
+        // copy 4 bytes of prefix
+        memcpy(buffer, (char *)prefix, 4);
+        buffer += 4;
+        blen -= 4;
+    }
+
+    int rand_num = std::rand();
+    unsigned long long current_time = current_timestamp() + rand_num;
+    char timestr[16];
+    snprintf(timestr, 16, "%llu", current_time);
+    memcpy(buffer, timestr, std::min(blen, (uint32_t)sizeof(timestr)));
+    // done with key
+
+    // generate value
+    buffer = (char *) value->value;
+    blen = value->length;
+    // ending with null
+    buffer[blen - 1] = 0;
+
+    // create random value
+    for (unsigned int i = 0; i < blen - 1; i++) {
+        unsigned int j = 'a';//(char)(i%128); //std::rand() % 128;
+        buffer[i] = j;
+    }
+
+    printf("generate a key %s, value legngth = %d\n", (char *) (key->key), value->length);
+}
+
+// how many keys to insert
+void insert_kv_store(kv_device_handle dev_hdl, kv_queue_handle sq_hdl, const int klen, const int vlen, int count) {
+    // set up IO handle and postprocessing
+    iohdl_t *iohdl = (iohdl_t *) malloc(sizeof(iohdl_t));
+    memset(iohdl, 0, sizeof(iohdl_t));
+    kv_postprocess_function *post_fn_data = (kv_postprocess_function *)
+                                malloc(sizeof(kv_postprocess_function));
+    post_fn_data->post_fn = on_IO_complete_func;
+    post_fn_data->private_data = (void *) iohdl;
+
+    // allocate key value for reuse
+    kv_key *key = (kv_key *) malloc(sizeof(kv_key));
+    key->length = klen;
+    key->key = malloc(key->length);
+    memset(key->key, 0, key->length);
+
+    // value for insert
+    kv_value *val = (kv_value *) malloc(sizeof(kv_value));
+    val->length = vlen;
+
+    val->actual_value_size = vlen;
+    val->value = malloc(val->length);
+    memset(val->value, 0, val->length);
+    val->offset = 0;
+
+    // value for read back validation
+    kv_value *valread = (kv_value *) malloc(sizeof(kv_value));
+    valread->length = vlen;
+
+    valread->actual_value_size = vlen;
+    valread->value = malloc(val->length);
+    memset(valread->value, 0, valread->length);
+    valread->offset = 0;
+
+    // save key value to the IO handle
+    iohdl->key = key;
+    iohdl->value = val;
+
+    // set up namespace
+    kv_namespace_handle ns_hdl = NULL;
+    get_namespace_default(dev_hdl, &ns_hdl);
+
+    // insert key value
+    for (int i = 0; i < count; i++) {
+
+        // populate key value with random content
+        populate_key_value_startwith(0, iohdl->key, iohdl->value);
+
+        iohdl->done  = 0;
+
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_result res = kv_store(sq_hdl, ns_hdl, key, val, KV_STORE_OPT_DEFAULT, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_store failed with error: 0x%X\n", res);
+            res = kv_store(sq_hdl, ns_hdl, key, val, KV_STORE_OPT_DEFAULT, post_fn_data);
+        }
+
+        // check result asynchronously
+        std::unique_lock<std::mutex> lock(iohdl->mutex);
+        while (iohdl->done == 0) {
+            iohdl->done_cond.wait(lock);
+        }
+        // expected result
+        if (iohdl->retcode != KV_SUCCESS) {
+            printf("kv_store a new key failed: 0x%X\n", iohdl->retcode);
+            exit(1);
+        } else {
+            printf("kv_store succeeded: value length = %d\n", val->length);
+        }
+        iohdl->done = 0;
+        iohdl->retcode = KV_ERR_COMMAND_SUBMITTED;
+        lock.unlock();
+
+
+        // save original
+        // read it back
+        kv_value *valwritten = iohdl->value;
+        iohdl->value = valread;
+        memset(valread->value, 0, valread->length);
+        
+        printf("value read = %p\n", valread);
+        // read value
+        res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, valread, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_retrieve failed with error: 0x%X\n", res);
+            res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, valread, post_fn_data);
+        }
+
+        // check result asynchronously
+        lock.lock();
+        while (iohdl->done == 0) {
+            iohdl->done_cond.wait(lock);
+        }
+        lock.unlock();
+
+        // expected result
+        if (iohdl->retcode != KV_SUCCESS) {
+            printf("kv_retrieve failed: 0x%X, done = %d\n", iohdl->retcode, iohdl->done.load());
+            exit(1);
+        }
+
+        // compare values
+        if (valread->length != valwritten->length) {
+            printf("value size is different: %u, %u\n", valread->length, valwritten->length);
+            exit(1);
+        }
+
+        if (memcmp(valread->value, valwritten->value, valread->length) != 0) {
+            printf("value is different: %u, %u, vlen = %d\n", valread->length, valwritten->length, vlen);
+            printf("value is different: %p, %s, %s\n", valread, (char*)valread->value, (char*)valwritten->value);
+            exit(1);
+        }
+
+        printf("kv_retrieve succeeded\n");
+
+        // restore IO handle to original
+        valread = iohdl->value;
+        iohdl->value = valwritten;
+    }
+
+    // done, free memories
+    free(valread->value);
+    free(valread);
+    free_kv(iohdl->key, iohdl->value);
+    free(post_fn_data);
+    free(iohdl);
+}
+
+void create_qpair(kv_device_handle dev_hdl,
+    std::map<kv_queue_handle, kv_queue_handle>& qpairs,
+    kv_queue_handle *sq_hdl,
+    kv_queue_handle *cq_hdl,
+    uint16_t sqid,
+    uint16_t cqid,
+    uint16_t q_depth,
+    kv_interrupt_handler int_handler) {
+    
+    // create a CQ
+    // ONLY DO THIS, if you have NOT created the CQ earlier
+    create_cq(dev_hdl, cqid, q_depth, cq_hdl);
+
+    // NOT doing polling
+    // see sample_poll.cpp for reference
+    // completion q poll thread
+    // std::thread th = std::thread(poll_CQ, *cq_hdl, 0);
+    // th.detach();
+
+    // this is only for completion Q
+    // this will kick off completion Q processing
+    kv_set_interrupt_handler(*cq_hdl, int_handler); 
+
+    // create a SQ
+    create_sq(dev_hdl, sqid, q_depth, cqid, sq_hdl);
+
+    qpairs.insert(std::make_pair(*sq_hdl, *cq_hdl));
+
+}
+
+
+int main(int argc, char**argv) {
+    
+    if (argc <  2) {
+        printf("Please run\n  %s <number of keys>\n", argv[0]);
+        printf("Default: %d\n", TOTAL_KV_COUNT);
+    } else {
+        TOTAL_KV_COUNT = std::stoi(argv[1]);
+        printf("Insert keys: %d\n", TOTAL_KV_COUNT);
+    }
+
+    int klen = 16;
+    int vlen = 4096;
+
+    kv_device_init_t dev_init;
+
+    // you can supply your own configuration file
+    dev_init.configfile = "./kvssd_emul.conf";
+
+    // the emulator device path is fixed.
+    dev_init.devpath = "/dev/kvemul";
+    dev_init.need_persistency = FALSE;
+    dev_init.is_polling = FALSE;
+
+    kv_device_handle dev_hdl = NULL;
+
+    // initialize the device
+    kv_result ret = kv_initialize_device(&dev_init, &dev_hdl);
+    if (ret != KV_SUCCESS) {
+        printf("device creation error\n");
+        exit(1);
+    }
+
+    // set up interrupt handler
+    _kv_interrupt_handler int_func = {interrupt_func, (void *)4, 4};
+    kv_interrupt_handler int_handler = &int_func;
+
+    // to keep track all opened queue pairs
+    std::map<kv_queue_handle, kv_queue_handle> qpairs;
+    kv_queue_handle sq_hdl;
+    kv_queue_handle cq_hdl;
+
+    // convenient wrapper function
+    // create a SQ/CQ pair, and start a thread to insert key value pairs
+    create_qpair(dev_hdl, qpairs,  &sq_hdl, &cq_hdl,
+            1,  // sqid
+            2,  // cqid
+            64, // q depth
+            int_handler);
+
+
+    // start a thread to insert key values through submission Q
+    printf("starting thread to insert key values\n");
+    std::thread th = std::thread(insert_kv_store, dev_hdl, sq_hdl, klen, vlen, TOTAL_KV_COUNT);
+    if (th.joinable()) {
+        th.join();
+    }
+
+    // graceful shutdown
+    // watch if all Qs are done
+    for (auto& qpair : qpairs) {
+        kv_queue_handle sqhdl = qpair.first;
+        kv_queue_handle cqhdl = qpair.second;
+        while (get_queued_commands_count(cqhdl) > 0 || get_queued_commands_count(sqhdl) > 0) {
+            // wait for CQ to complete before shutdown
+            std::this_thread::sleep_for(std::chrono::milliseconds(10));
+        }
+    }
+    
+    /*
+    // delete queues
+    for (auto& p : qpairs) {
+        if (kv_delete_queue(dev_hdl, p.first) != KV_SUCCESS) {
+            printf("kv_delete_queue failed\n");
+            exit(1);
+        }
+        if (kv_delete_queue(dev_hdl, p.second) != KV_SUCCESS) {
+            printf("kv_delete_queue failed\n");
+            exit(1);
+        }
+    }
+    */
+    // shutdown
+    kv_cleanup_device(dev_hdl);
+    fprintf(stderr, "done\n");
+    return 0;
+}
diff --git a/PDK/core/src/device_abstract_layer/emulator/sample_code/sample_poll.cpp b/PDK/core/src/device_abstract_layer/emulator/sample_code/sample_poll.cpp
new file mode 100644
index 0000000..4a61b98
--- /dev/null
+++ b/PDK/core/src/device_abstract_layer/emulator/sample_code/sample_poll.cpp
@@ -0,0 +1,573 @@
+#include "assert.h"
+#include <string>
+#include <iostream>
+#include <sstream>
+#include <thread>
+#include <vector>
+
+#include <stdio.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sys/time.h>
+
+#include <mutex>
+#include <chrono>
+#include <thread>
+#include <set>
+#include <unordered_set>
+#include <map>
+#include <unordered_map>
+#include <iomanip>
+#include <ctime>
+#include <condition_variable>
+#include <atomic>
+#include "kvs_adi.h"
+
+// default settings
+int TOTAL_KV_COUNT = 5000;
+
+// device run time structure
+struct device_handle_t {
+    kv_device_handle dev_hdl;
+    kv_namespace_handle ns_hdl;
+    kv_queue_handle sq_hdl;
+    kv_queue_handle cq_hdl;
+    int qdepth;
+    // char *configfile;
+};
+
+
+typedef struct {
+    kv_key *key;
+    kv_value *value;
+    kv_result retcode;
+    kv_postprocess_function post_fn_data;
+
+    // value read back
+    kv_value *value_save;
+
+    // for kv_exist
+    char *buffer;
+    int buffer_size;
+    int buffer_count;
+
+    // for iterator
+    kv_iterator_handle hiter;
+
+    // depends if individual IO sync control is needed
+    std::condition_variable done_cond;
+    std::mutex mutex;
+    std::atomic<int> done;
+} iohdl_t;
+
+
+void free_value(kv_value *value) {
+    // free value
+    if (value != NULL) {
+        if (value->value != NULL) {
+            free(value->value);
+        }
+        free(value);
+    }
+}
+
+void free_key(kv_key *key) {
+    // free key
+    if (key != NULL) {
+        if (key->key != NULL) {
+            free(key->key);
+        }
+        free(key);
+    }
+}
+
+
+// this function will be called after completion of a command
+void interrupt_func(void *data, int number) {
+    (void) data;
+    (void) number;
+    // printf("inside interrupt\n");
+}
+
+int create_sq(kv_device_handle dev_hdl, uint16_t sqid, uint16_t qsize, uint16_t cqid, kv_queue_handle *quehdl) {
+
+    kv_queue qinfo;
+    qinfo.queue_id = sqid;
+    qinfo.queue_size = qsize;
+    qinfo.completion_queue_id = cqid;
+    qinfo.queue_type = SUBMISSION_Q_TYPE;
+    qinfo.extended_info = NULL;
+
+    if (KV_SUCCESS != kv_create_queue(dev_hdl, &qinfo, quehdl)) {
+        printf("submission queue creation failed\n");
+    }
+    return 0;
+}
+
+int create_cq(kv_device_handle dev_hdl, uint16_t cqid, uint16_t qsize, kv_queue_handle *quehdl) {
+
+    kv_queue qinfo;
+    qinfo.queue_id = cqid;
+    qinfo.queue_size = qsize;
+    qinfo.completion_queue_id = 0;
+    qinfo.queue_type = COMPLETION_Q_TYPE;
+    qinfo.extended_info = NULL;
+
+    if (KV_SUCCESS != kv_create_queue(dev_hdl, &qinfo, quehdl)) {
+        printf("completion queue creation failed\n");
+    }
+    return 0;
+}
+
+uint64_t current_timestamp() {
+    std::chrono::system_clock::time_point timepoint = std::chrono::system_clock::now();
+    uint64_t nano_from_epoch = std::chrono::duration_cast<std::chrono::nanoseconds>(timepoint.time_since_epoch()).count();
+    return nano_from_epoch;
+}
+
+// post processing function for a command
+// op->private_data is the data caller passed in
+// Please note ctx is internal data for read only.
+void on_IO_complete_func(kv_io_context *ctx) {
+    // depends if you allocated space for each command, you can manage
+    // allocated spaces here. This should in sync with your IO cycle
+    // management strategy.
+    //
+    // get completed async command results, free any allocated spaces if necessary.
+    //
+    // printf("finished one command\n");
+    iohdl_t *iohdl = (iohdl_t *) ctx->private_data;
+    iohdl->retcode = ctx->retcode;
+
+    // this buffer was supplied by ASYNC call, just put it into IO handle
+    // for easier processing
+    if (ctx->opcode == KV_OPC_CHECK_KEY_EXIST) {
+        iohdl->buffer_size = ctx->result.buffer_size;
+        iohdl->buffer_count = ctx->result.buffer_count;
+    }
+
+    // this iterator is returned in response to ASYNC iterator open call
+    // must be retrieved by caller for iterator next calls
+    // then closed after use.
+    if (ctx->opcode == KV_OPC_OPEN_ITERATOR) {
+        iohdl->hiter = ctx->result.hiter;
+        iohdl->buffer_size = ctx->result.buffer_size;
+        iohdl->buffer_count = ctx->result.buffer_count;
+    }
+
+
+    // printf("command finished with result 0x%X\n", iohdl->retcode);
+    // std::unique_lock<std::mutex> lock(iohdl->mutex);
+    // iohdl->done = 1;
+    // iohdl->done_cond.notify_one();
+}
+
+// fill key value with random data
+void populate_key_value_startwith(uint32_t keystart, kv_key *key, kv_value *value) {
+
+    char *buffer = (char *)key->key;
+    uint32_t blen = key->length;
+
+    uint8_t *base = (uint8_t *)&keystart;
+
+    /*
+    uint8_t prefix[4] = {
+            *(base + 0),
+            *(base + 1),
+            *(base + 2),
+            *(base + 3) };
+    */
+
+    // if prefix is 0, skip prefix setup in key value
+    if (keystart != 0) {
+        assert(blen > 4);
+        // copy 4 bytes of prefix
+        memcpy(buffer, (char *)base, 4);
+        buffer += 4;
+        blen -= 4;
+    }
+
+    // printf("got key 4MSB 0x%X\n", keystart);
+    // printf("key 4MSB 0x%X\n", *(uint32_t *)(buffer - 4));
+
+    // use this to see deterministic sequence of keys
+    static unsigned long long current_time = 0;
+    current_time++;
+
+    char timestr[128];
+    snprintf(timestr, 128, "%llu", current_time);
+    memcpy(buffer, timestr, std::min(blen, (uint32_t)sizeof(timestr)));
+    // done with key
+
+    // generate value
+    buffer = (char *) value->value;
+    blen = value->length;
+    // ending with null
+    buffer[blen - 1] = 0;
+
+    // copy key, then add random value
+    memcpy(buffer, key->key, key->length);
+    blen -= key->length - 1;
+    buffer += key->length;
+    for (unsigned int i = 0; i < blen - 1; i++) {
+        unsigned int j = '1' + std::rand() % 30;
+        buffer[i] = j;
+    }
+
+    // printf("genrate a key %s, value %s\n", (char *) (key->key), (char *)value->value);
+    // printf("genrate a key %s\n", (char *) (key->key));
+}
+
+// set random values for those iohdls
+void populate_iohdls(iohdl_t *iohdls, uint32_t prefix, int klen, int vlen, uint32_t count) {
+    // set up IO handle and postprocessing
+
+    for (uint32_t i = 0; i < count; i++) {
+        // populate random value
+        iohdl_t *iohdl = iohdls + i;
+        populate_key_value_startwith(prefix, iohdl->key, iohdl->value);
+
+        kv_value *value = iohdl->value;
+        kv_value *value_save = iohdl->value_save;
+
+        memcpy(value_save->value, value->value, value->length);
+    }
+}
+
+// create array of io handles that hold keys and values
+void create_iohdls(iohdl_t **iohdls, uint32_t prefix, int klen, int vlen, uint32_t count) {
+    *iohdls = (iohdl_t *) calloc(count, sizeof(iohdl_t));
+
+    for (uint32_t i = 0; i < count; i++) {
+        iohdl_t *iohdl = (*iohdls) + i;
+
+        iohdl->post_fn_data.post_fn = on_IO_complete_func;
+        iohdl->post_fn_data.private_data = (void *) iohdl;
+
+        // allocate key value for reuse
+        kv_key *key = (kv_key *) malloc(sizeof(kv_key));
+        key->length = klen;
+        key->key = malloc(key->length);
+        memset(key->key, 0, key->length);
+
+        // value for insert
+        kv_value *val = (kv_value *) malloc(sizeof(kv_value));
+        val->length = vlen;
+
+        val->actual_value_size = vlen;
+        val->value = malloc(vlen);
+        memset(val->value, 0, val->length);
+        val->offset = 0;
+
+        // value for read back validation
+        kv_value *value_save = (kv_value *) malloc(sizeof(kv_value));
+        value_save->length = vlen;
+
+        value_save->actual_value_size = vlen;
+        value_save->value = malloc(vlen);
+        memset(value_save->value, 0, vlen);
+        value_save->offset = 0;
+
+        // save key value to the IO handle
+        iohdl->key = key;
+        iohdl->value = val;
+        iohdl->value_save = value_save;
+    }
+}
+
+void free_iohdls(iohdl_t *iohdls, uint32_t count) {
+    for (uint32_t i = 0; i < count; i++) {
+        iohdl_t *iohdl = iohdls + i;
+        free_key(iohdl->key);
+        free_value(iohdl->value);
+        free_value(iohdl->value_save);
+    }
+    free(iohdls);
+}
+
+// read back and compare
+uint64_t process_one_round_read(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        // printf("reading key: %s\n", key->key);
+        // prefix of keys
+        // printf("reading key 4MSB: 0x%X\n", *(uint32_t *)&key->key);
+
+        kv_value *value = (iohdls + i)->value;
+        memset(value->value, '0', value->length);
+
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_retrieve failed with error: 0x%X\n", res);
+            res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+    uint64_t end = current_timestamp();
+
+    // compare read value with write value
+    for (uint32_t i = 0; i < count; i++) {
+        // printf("validating key: %s\n", (iohdls + i)->key->key);
+        if (memcmp((iohdls + i)->value_save->value, (iohdls + i)->value->value, (iohdls + i)->value->length)) {
+            // printf("failed: value read not the same as value written, key %s\n value + 16 %s\n value %s\n", (iohdls + i)->key->key, (iohdls + i)->value->value + 16, (iohdls + i)->value_save->value);
+            // XXX please check if any duplicate keys are used, only unique keys should
+            // be used, otherwise you may get false positive.
+            printf("failed: value read not the same as value written\n");
+            // printf("Note: please only use unique keys for this validation\n");
+            exit(1);
+        }
+    }
+
+    // printf("polling total done: %d, inserted %d\n", total_done, count);
+    // double iotime  = double(total) / count;
+    // printf("average time per IO: %.1f (ns)\n", iotime);
+    return (end - start);
+}
+
+// insert enough keys but avoid excessive queue contention
+uint64_t process_one_round_write(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        kv_value *value = (iohdls + i)->value;
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_store failed with error: 0x%X\n", res);
+            res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+    uint64_t end = current_timestamp();
+
+    // printf("polling total done: %d, inserted %d\n", total_done, count);
+    // double iotime  = double(total) / count;
+    // printf("average time per IO: %.1f (ns)\n", iotime);
+    return (end - start);
+}
+
+// test cycle of read and write in rounds
+void sample_main(device_handle_t *device_handle, int klen, int vlen, int tcount, int qdepth) {
+
+    //kv_device_handle dev_hdl = device_handle->dev_hdl;
+    kv_namespace_handle ns_hdl = device_handle->ns_hdl;
+    kv_queue_handle sq_hdl = device_handle->sq_hdl;
+    kv_queue_handle cq_hdl = device_handle->cq_hdl;
+
+    // uint32_t prefix = 0xFFFF1234;
+    uint32_t prefix = 0;
+    iohdl_t *iohdls = NULL;
+
+    // break it into many rounds to minimize q contention
+    uint32_t count = qdepth;
+    create_iohdls(&iohdls, prefix, klen, vlen, count);
+
+    uint32_t left = tcount;
+    // total IO time in ns
+    uint64_t total_w = 0;
+    uint64_t total_r = 0;
+
+    while (left > 0) {
+        if (left < count) {
+            count = left;
+        }
+        left -= count;
+
+        // set up new random key values
+        populate_iohdls(iohdls, prefix, klen, vlen, count);
+
+        // test write and read
+        uint64_t time_w = process_one_round_write(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+        total_w += time_w;
+
+        uint64_t time_r = process_one_round_read(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+        total_r += time_r;
+    }
+
+    // done, free memories
+    free_iohdls(iohdls, qdepth);
+}
+
+void create_qpair(kv_device_handle dev_hdl,
+    kv_queue_handle *sq_hdl,
+    kv_queue_handle *cq_hdl,
+    uint16_t sqid,
+    uint16_t cqid,
+    uint16_t q_depth)
+{
+    
+    // create a CQ
+    // ONLY DO THIS, if you have NOT created the CQ earlier
+    create_cq(dev_hdl, cqid, q_depth, cq_hdl);
+
+    // this is only for completion Q
+    // this will kick off completion Q processing
+    // kv_set_interrupt_handler(cq_hdl, int_handler); 
+
+    // create a SQ
+    create_sq(dev_hdl, sqid, q_depth, cqid, sq_hdl);
+}
+
+
+void create_device(device_handle_t *device_handle, int qdepth) {
+    // int klen = 16;
+    // int vlen = 4096;
+    // int qdepth = 64;
+    kv_device_init_t dev_init;
+
+    // you can supply your own configuration file
+    dev_init.configfile = "./kvssd_emul.conf";
+    // the emulator device path is fixed and virtual
+    // you don't need to create it.
+    dev_init.devpath = "/dev/kvemul";
+    dev_init.need_persistency = FALSE;
+    dev_init.is_polling = TRUE;
+
+    kv_device_handle dev_hdl = NULL;
+
+    device_handle->qdepth = qdepth;
+
+    // initialize the device
+    kv_result ret = kv_initialize_device(&dev_init, &dev_hdl);
+    if (ret != KV_SUCCESS) {
+        printf("device creation error\n");
+        exit(1);
+    }
+
+    kv_namespace_handle ns_hdl = NULL;
+    get_namespace_default(dev_hdl, &ns_hdl);
+
+    // set up interrupt handler
+    // _kv_interrupt_handler int_func = {interrupt_func, (void *)4, 4};
+    // kv_interrupt_handler int_handler = &int_func;
+    kv_queue_handle sq_hdl;
+    kv_queue_handle cq_hdl;
+
+    // convenient wrapper function
+    // create a SQ/CQ pair
+    create_qpair(dev_hdl, &sq_hdl, &cq_hdl,
+            1,  // sqid
+            2,  // cqid
+            qdepth); // q depth
+
+    device_handle->dev_hdl = dev_hdl;
+    device_handle->ns_hdl = ns_hdl;
+    device_handle->sq_hdl = sq_hdl;
+    device_handle->cq_hdl = cq_hdl;
+    device_handle->qdepth = qdepth;
+}
+
+
+void shutdown_device(device_handle_t *device_handle) {
+    if (device_handle == NULL) {
+        return;
+    }
+
+    kv_device_handle dev_hdl = device_handle->dev_hdl;
+    kv_namespace_handle ns_hdl = device_handle->ns_hdl;
+    kv_queue_handle sq_hdl = device_handle->sq_hdl;
+    kv_queue_handle cq_hdl = device_handle->cq_hdl;
+
+    while (get_queued_commands_count(cq_hdl) > 0 || get_queued_commands_count(sq_hdl) > 0) {
+        // wait for CQ to complete before shutdown
+        std::this_thread::sleep_for(std::chrono::milliseconds(10));
+    }
+    
+    // delete queues
+    if (kv_delete_queue(dev_hdl, sq_hdl) != KV_SUCCESS) {
+        printf("kv_delete_queue failed\n");
+        exit(1);
+    }
+    if (kv_delete_queue(dev_hdl, cq_hdl) != KV_SUCCESS) {
+        printf("kv_delete_queue failed\n");
+        exit(1);
+    }
+
+    // shutdown
+    kv_delete_namespace(dev_hdl, ns_hdl);
+    kv_cleanup_device(dev_hdl);
+}
+
+
+int main(int argc, char**argv) {
+    
+    if (argc <  2) {
+        printf("Please run\n  %s <number of keys>\n", argv[0]);
+        printf("Default: %d\n", TOTAL_KV_COUNT);
+    } else {
+        TOTAL_KV_COUNT = std::stoi(argv[1]);
+        printf("Insert keys: %d\n", TOTAL_KV_COUNT);
+    }
+
+    int klen = 16;
+    int vlen = 4096;
+    int qdepth = 64;
+
+    device_handle_t *device1 = (device_handle_t *) malloc(sizeof (device_handle_t));
+    device_handle_t *device2 = (device_handle_t *) malloc(sizeof (device_handle_t));
+
+    printf("creating device 1\n");
+    create_device(device1, qdepth);
+    printf("creating device 2\n");
+    create_device(device2, qdepth);
+
+    // start a thread to insert key values through submission Q
+    // then read them back to validate correctness
+    printf("starting operation on device 1\n");
+    std::thread th = std::thread(sample_main, device1, klen, vlen, TOTAL_KV_COUNT, qdepth);
+
+    printf("starting operation on device 2\n");
+    std::thread th1 = std::thread(sample_main, device2, klen, vlen, TOTAL_KV_COUNT, qdepth);
+
+    if (th.joinable()) {
+        th.join();
+    }
+    printf("stop operation on device 1\n");
+
+    if (th1.joinable()) {
+        th1.join();
+    }
+    printf("stop operation on device 2\n");
+
+    printf("shutdown all devices\n");
+    shutdown_device(device1);
+    shutdown_device(device2);
+    free(device1);
+    free(device2);
+
+    printf("all done\n");
+    return 0;
+}
diff --git a/PDK/core/src/device_abstract_layer/emulator/src/io_cmd.cpp b/PDK/core/src/device_abstract_layer/emulator/src/io_cmd.cpp
index bdb62ed..dde6589 100644
--- a/PDK/core/src/device_abstract_layer/emulator/src/io_cmd.cpp
+++ b/PDK/core/src/device_abstract_layer/emulator/src/io_cmd.cpp
@@ -114,14 +114,14 @@ kv_result io_cmd::execute_cmd() {
         case KV_OPC_GET: {
                 // set result into value
                 op_get_struct_t info = ioctx.command.get_info;
-                ioctx.retcode = ns->kv_retrieve(ioctx.ks_id, ioctx.key, info.option, ioctx.value, (void *) this);
+                ioctx.retcode = ns->kv_retrieve(ioctx.key, info.option, ioctx.value, (void *) this);
                 break;
             }
 
         case KV_OPC_STORE: {
                 uint32_t consumed_bytes = 0;
                 op_store_struct_t info = ioctx.command.store_info;
-                ioctx.retcode = ns->kv_store(ioctx.ks_id, ioctx.key, ioctx.value, info.option, &consumed_bytes, (void *) this);
+                ioctx.retcode = ns->kv_store(ioctx.key, ioctx.value, info.option, &consumed_bytes, (void *) this);
 
                 // kv_namespace_stat ns_st;
                 // ns->kv_get_namespace_stat(&ns_st);
@@ -133,7 +133,7 @@ kv_result io_cmd::execute_cmd() {
         case KV_OPC_DELETE: {
                 uint32_t reclaimed_bytes = 0;
                 op_delete_struct_t info = ioctx.command.delete_info;
-                ioctx.retcode = ns->kv_delete(ioctx.ks_id, ioctx.key, info.option, &reclaimed_bytes, (void *) this);
+                ioctx.retcode = ns->kv_delete(ioctx.key, info.option, &reclaimed_bytes, (void *) this);
                 // fprintf(stderr, "reclaimed: %llu\n", reclaimed_bytes);
                 break;
             }
@@ -141,19 +141,19 @@ kv_result io_cmd::execute_cmd() {
         case KV_OPC_DELETE_GROUP: {
                 uint64_t reclaimed_bytes = 0;
                 op_delete_group_struct_t info = ioctx.command.delete_group_info;
-                ioctx.retcode = ns->kv_delete_group(ioctx.ks_id, info.grp_cond, &reclaimed_bytes, (void *) this);
+                ioctx.retcode = ns->kv_delete_group(info.grp_cond, &reclaimed_bytes, (void *) this);
                 break;
             }
 
         case KV_OPC_PURGE: {
                 op_purge_struct_t info = ioctx.command.purge_info;
-                ioctx.retcode = ns->kv_purge(ioctx.ks_id, info.option, (void *) this);
+                ioctx.retcode = ns->kv_purge(info.option, (void *) this);
                 break;
             }
 
         case KV_OPC_CHECK_KEY_EXIST: {
                 op_key_exist_struct_t &info  = ioctx.command.key_exist_info;
-                ioctx.retcode = ns->kv_exist(ioctx.ks_id, ioctx.key, info.keycount, info.result, info.result_size, (void *) this);
+                ioctx.retcode = ns->kv_exist(ioctx.key, info.keycount, info.result, info.result_size, (void *) this);
                 ioctx.result.buffer_size = info.result_size;
                 ioctx.result.buffer_count = info.keycount;
                 break;
@@ -161,7 +161,7 @@ kv_result io_cmd::execute_cmd() {
 
         case KV_OPC_OPEN_ITERATOR: {
                 op_iterator_open_struct_t info = ioctx.command.iterator_open_info;
-                ioctx.retcode = ns->kv_open_iterator(ioctx.ks_id, info.it_op, &info.it_cond, &ioctx.result.hiter, (void *) this);
+                ioctx.retcode = ns->kv_open_iterator(info.it_op, &info.it_cond, &ioctx.result.hiter, (void *) this);
                 break;
             }
 
@@ -187,7 +187,7 @@ kv_result io_cmd::execute_cmd() {
         case KV_OPC_SANITIZE_DEVICE: {
                 //op_sanitize_struct_t info = ioctx.command.sanitize_info;
                 // XXX can't do much, just call purge function
-                ioctx.retcode = ns->kv_purge(ioctx.ks_id, KV_PURGE_OPT_DEFAULT, (void *) this);
+                ioctx.retcode = ns->kv_purge(KV_PURGE_OPT_DEFAULT, (void *) this);
                 break;
             }
 
@@ -196,6 +196,7 @@ kv_result io_cmd::execute_cmd() {
                 ioctx.retcode = ns->kv_list_iterators(info.kv_iters, info.iter_cnt, (void *) this);
                 break;
             }
+
         default:
             WRITE_WARN("OPCODE %d not recognized", ioctx.opcode);
             ioctx.retcode = KV_ERR_SYS_IO;
diff --git a/PDK/core/src/device_abstract_layer/emulator/src/kv_device.cpp b/PDK/core/src/device_abstract_layer/emulator/src/kv_device.cpp
index c917a59..dc596cd 100644
--- a/PDK/core/src/device_abstract_layer/emulator/src/kv_device.cpp
+++ b/PDK/core/src/device_abstract_layer/emulator/src/kv_device.cpp
@@ -813,7 +813,7 @@ kv_result kv_device_internal::kv_get_namespace_stat(const kv_device_handle dev_h
 // more important IO APIs below
 // operate on a device object, can access kv_device_internal members
 // ASYNC IO in a device context
-kv_result kv_device_internal::kv_purge(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, kv_purge_option option, kv_postprocess_function *post_fn) {
+kv_result kv_device_internal::kv_purge(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_purge_option option, kv_postprocess_function *post_fn) {
 
     if (que_hdl == NULL || ns_hdl == NULL) {
         return KV_ERR_PARAM_INVALID;
@@ -823,10 +823,6 @@ kv_result kv_device_internal::kv_purge(kv_queue_handle que_hdl, kv_namespace_han
         return KV_ERR_OPTION_INVALID;
     }
 
-    if(ks_id < SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-          return KV_ERR_KEYSPACE_INVALID;
-    }
-
     kv_device_internal *dev = (kv_device_internal *) que_hdl->dev;
     if (dev == NULL) {
         return KV_ERR_DEV_NOT_EXIST;
@@ -857,7 +853,6 @@ kv_result kv_device_internal::kv_purge(kv_queue_handle que_hdl, kv_namespace_han
     cmd->ioctx.command.purge_info = info;
     cmd->ioctx.key = NULL;
     cmd->ioctx.value = NULL;
-    cmd->ioctx.ks_id = ks_id;
 
     // pass ioctx by value, as there is no ownership of those memories in the
     // structure
@@ -866,7 +861,7 @@ kv_result kv_device_internal::kv_purge(kv_queue_handle que_hdl, kv_namespace_han
 }
 
 // async IO
-kv_result kv_device_internal::kv_open_iterator(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_iterator_option it_op, const kv_group_condition *it_cond, kv_postprocess_function *post_fn) {
+kv_result kv_device_internal::kv_open_iterator(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_iterator_option it_op, const kv_group_condition *it_cond, kv_postprocess_function *post_fn) {
 
     if (que_hdl == NULL || ns_hdl == NULL || it_cond == NULL) {
         return KV_ERR_PARAM_INVALID;
@@ -874,9 +869,6 @@ kv_result kv_device_internal::kv_open_iterator(kv_queue_handle que_hdl, kv_names
     if (it_op != KV_ITERATOR_OPT_KEY && it_op != KV_ITERATOR_OPT_KV && it_op != KV_ITERATOR_OPT_KV_WITH_DELETE) {
         return KV_ERR_OPTION_INVALID;
     }
-    if(ks_id < SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-          return KV_ERR_KEYSPACE_INVALID;
-    }
 
     kv_namespace_internal *ns = (kv_namespace_internal *) ns_hdl->ns;
     if (ns == NULL) {
@@ -908,7 +900,7 @@ kv_result kv_device_internal::kv_open_iterator(kv_queue_handle que_hdl, kv_names
     cmd->ioctx.command.iterator_open_info.it_cond.bit_pattern = it_cond->bit_pattern;
     cmd->ioctx.key = NULL;
     cmd->ioctx.value = NULL;
-    cmd->ioctx.ks_id = ks_id;
+
     return dev->submit_io(que_hdl, cmd);
 }
 
@@ -1092,7 +1084,7 @@ kv_result kv_device_internal::kv_list_iterators(kv_queue_handle que_hdl, kv_name
 }
 
 // async IO
-kv_result kv_device_internal::kv_delete(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *key, kv_delete_option option, kv_postprocess_function *post_fn) {
+kv_result kv_device_internal::kv_delete(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_delete_option option, kv_postprocess_function *post_fn) {
     if (que_hdl == NULL || ns_hdl == NULL || key == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
@@ -1101,10 +1093,6 @@ kv_result kv_device_internal::kv_delete(kv_queue_handle que_hdl, kv_namespace_ha
     if (res != KV_SUCCESS) {
         return res;
     }
- 
-    if(ks_id < SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-          return KV_ERR_KEYSPACE_INVALID;
-    }
 
     /*
     if (option != KV_DELETE_OPT_DEFAULT) {
@@ -1142,13 +1130,12 @@ kv_result kv_device_internal::kv_delete(kv_queue_handle que_hdl, kv_namespace_ha
     }
     cmd->ioctx.opcode = KV_OPC_DELETE;
     cmd->ioctx.command.delete_info = info;
-    cmd->ioctx.ks_id = ks_id;
 
 
     return dev->submit_io(que_hdl, cmd);
 }
 
-kv_result kv_device_internal::kv_delete_group(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, kv_group_condition *grp_cond, kv_postprocess_function *post_fn) {
+kv_result kv_device_internal::kv_delete_group(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_group_condition *grp_cond, kv_postprocess_function *post_fn) {
     if (que_hdl == NULL || ns_hdl == NULL || grp_cond == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
@@ -1158,10 +1145,6 @@ kv_result kv_device_internal::kv_delete_group(kv_queue_handle que_hdl, kv_namesp
         return KV_ERR_DEV_NOT_EXIST;
     }
 
-    if(ks_id <SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-          return KV_ERR_KEYSPACE_INVALID;
-    }
-
     emul_ioqueue *que = (emul_ioqueue *)(que_hdl->queue);
     if (que == NULL) {
         return KV_ERR_QUEUE_QID_INVALID;
@@ -1187,13 +1170,12 @@ kv_result kv_device_internal::kv_delete_group(kv_queue_handle que_hdl, kv_namesp
     }
     cmd->ioctx.opcode = KV_OPC_DELETE_GROUP;
     cmd->ioctx.command.delete_group_info= info;
-    cmd->ioctx.ks_id = ks_id;
 
 
     return dev->submit_io(que_hdl, cmd);
 }
 
-kv_result kv_device_internal::kv_exist(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *keys, uint32_t key_cnt, kv_postprocess_function *post_fn, uint32_t buffer_size, uint8_t *buffer) {
+kv_result kv_device_internal::kv_exist(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *keys, uint32_t key_cnt, kv_postprocess_function *post_fn, uint32_t buffer_size, uint8_t *buffer) {
 
     if (que_hdl == NULL || ns_hdl == NULL || keys == NULL || buffer == NULL) {
         return KV_ERR_PARAM_INVALID;
@@ -1207,10 +1189,6 @@ kv_result kv_device_internal::kv_exist(kv_queue_handle que_hdl, kv_namespace_han
         }
     }
 
-    if(ks_id <SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-        return KV_ERR_KEYSPACE_INVALID;
-    }
-
     ioqueue *queue = (ioqueue *)(que_hdl->queue);
     if (queue == NULL) {
         return KV_ERR_QUEUE_QID_INVALID;
@@ -1242,26 +1220,21 @@ kv_result kv_device_internal::kv_exist(kv_queue_handle que_hdl, kv_namespace_han
     cmd->ioctx.command.key_exist_info.result = buffer;
     cmd->ioctx.command.key_exist_info.result_size = buffer_size;
     cmd->ioctx.command.key_exist_info.keycount = key_cnt;
-    cmd->ioctx.ks_id = ks_id;
 
     return dev->submit_io(que_hdl, cmd);
 }
 
 // ASYNC IO in a device context
-kv_result kv_device_internal::kv_retrieve(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *key, kv_retrieve_option option, const kv_postprocess_function *post_fn, kv_value *value) {
+kv_result kv_device_internal::kv_retrieve(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_retrieve_option option, const kv_postprocess_function *post_fn, kv_value *value) {
     if (que_hdl == NULL || ns_hdl == NULL || key == NULL || value == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
 
-    kv_result res = validate_key_value(key, value);
+    kv_result res = validate_key_value(key, NULL);
     if (res != KV_SUCCESS) {
         return res;
     }
 
-    if(ks_id < SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-          return KV_ERR_KEYSPACE_INVALID;
-    }
-
     /*
     if (option != KV_RETRIEVE_OPT_DEFAULT && option != KV_RETRIEVE_OPT_DECOMPRESS) {
         return KV_ERR_OPTION_INVALID;
@@ -1299,7 +1272,6 @@ kv_result kv_device_internal::kv_retrieve(kv_queue_handle que_hdl, kv_namespace_
     }
     cmd->ioctx.opcode = KV_OPC_GET;
     cmd->ioctx.command.get_info = info;
-    cmd->ioctx.ks_id = ks_id;
    
 
     return dev->submit_io(que_hdl, cmd);
@@ -1307,7 +1279,7 @@ kv_result kv_device_internal::kv_retrieve(kv_queue_handle que_hdl, kv_namespace_
 
 
 // Async IO
-kv_result kv_device_internal::kv_store(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *key, const kv_value *value, kv_store_option option, const kv_postprocess_function *post_fn) {
+kv_result kv_device_internal::kv_store(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, const kv_value *value, kv_store_option option, const kv_postprocess_function *post_fn) {
     if (que_hdl == NULL || ns_hdl == NULL || key == NULL || value == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
@@ -1317,10 +1289,6 @@ kv_result kv_device_internal::kv_store(kv_queue_handle que_hdl, kv_namespace_han
         return res;
     }
 
-    if(ks_id < SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-          return KV_ERR_KEYSPACE_INVALID;
-    }
-
     /* disable checking
     if (option != KV_STORE_OPT_DEFAULT && option != KV_STORE_OPT_IDEMPOTENT && option != KV_STORE_OPT_COMPRESS) {
         return KV_ERR_OPTION_INVALID;
@@ -1358,7 +1326,7 @@ kv_result kv_device_internal::kv_store(kv_queue_handle que_hdl, kv_namespace_han
     }
     cmd->ioctx.opcode = KV_OPC_STORE;
     cmd->ioctx.command.store_info = info;
-    cmd->ioctx.ks_id = ks_id;
+
 
     return dev->submit_io(que_hdl, cmd);
 }
@@ -1414,12 +1382,9 @@ kv_interrupt_handler kv_device_internal::kv_get_interrupt_handler(kv_queue_handl
 
 // for emulator, XXX only use queue with id 1
 kv_result kv_device_internal::submit_io(kv_queue_handle que_hdl, io_cmd *cmd) {
-    kv_result res = KV_SUCCESS;
-    ioqueue *queue = (ioqueue *)(que_hdl->queue);
-    emul_ioqueue* eque = (emul_ioqueue*)queue;
+
     if (que_hdl == NULL || cmd == NULL) {
-        res = KV_ERR_PARAM_INVALID;
-        goto free_io_cmd;
+        return KV_ERR_PARAM_INVALID;
     }
 
     // skip this, as higher layer should have checked it.
@@ -1427,26 +1392,19 @@ kv_result kv_device_internal::submit_io(kv_queue_handle que_hdl, io_cmd *cmd) {
     // if (res != KV_SUCCESS) {
     //     return res;
     // }
+
+    ioqueue *queue = (ioqueue *)(que_hdl->queue);
     if (queue == NULL) {
-        res = KV_ERR_QUEUE_QID_INVALID;
-        goto free_io_cmd;
+        return KV_ERR_QUEUE_QID_INVALID;
     }
 
     if (queue->get_type() != SUBMISSION_Q_TYPE) {
-        res = KV_ERR_QUEUE_QID_INVALID;
-        goto free_io_cmd;
-    }
-    res = eque->enqueue(cmd, true);
-    if(res != KV_SUCCESS){
-        goto free_io_cmd;
-    }
-    return res;
-    
-free_io_cmd:
-    if(cmd){
-        delete cmd;
-        cmd = NULL;
+        return KV_ERR_QUEUE_QID_INVALID;
     }
+
+    emul_ioqueue* eque = (emul_ioqueue*)queue;
+    auto res = eque->enqueue(cmd, false);
+
     return res;
 }
 
diff --git a/PDK/core/src/device_abstract_layer/emulator/src/kv_emulator.cpp b/PDK/core/src/device_abstract_layer/emulator/src/kv_emulator.cpp
index eb1268b..a976157 100644
--- a/PDK/core/src/device_abstract_layer/emulator/src/kv_emulator.cpp
+++ b/PDK/core/src/device_abstract_layer/emulator/src/kv_emulator.cpp
@@ -39,6 +39,7 @@
 #include <time.h>
 #include "io_cmd.hpp"
 #include "kv_emulator.hpp"
+#include "private_result.h"
 
 uint64_t _kv_emul_queue_latency;
 
@@ -54,17 +55,15 @@ kv_emulator::kv_emulator(uint64_t capacity, std::vector<double> iops_model_coeff
 kv_emulator::~kv_emulator() {
     std::unique_lock<std::mutex> lock(m_map_mutex);
     emulator_map_t::iterator it_tmp;
-    for(uint32_t i = 0 ; i < SAMSUNG_MAX_KEYSPACE_CNT ; i++){	
-      auto it = m_map[i].begin();
-      while (it != m_map[i].end()) {
-          kv_key *key = it->first;
-          free(key->key);
-          delete key;
-
-          it_tmp = it;
-          it++;
-          m_map[i].erase(it_tmp);
-      }
+    auto it = m_map.begin();
+    while (it != m_map.end()) {
+        kv_key *key = it->first;
+        free(key->key);
+        delete key;
+
+        it_tmp = it;
+        it++;
+        m_map.erase(it_tmp);
     }
 }
 
@@ -80,10 +79,10 @@ inline kv_key *new_kv_key(const kv_key *key) {
 uint64_t counter = 0;
 // basic operations
 
-kv_result kv_emulator::kv_store(uint8_t ks_id, const kv_key *key, const kv_value *value, uint8_t option, uint32_t *consumed_bytes, void *ioctx) {
+kv_result kv_emulator::kv_store(const kv_key *key, const kv_value *value, uint8_t option, uint32_t *consumed_bytes, void *ioctx) {
     (void) ioctx;
     // track consumed spaced
-    if (m_capacity <= 0 && m_available < (value->length + key->length)) {
+    if (m_capacity != 0 && m_available < (value->length + key->length)) {
         // fprintf(stderr, "No more device space left\n");
         return KV_ERR_DEV_CAPACITY;
     }
@@ -102,8 +101,8 @@ kv_result kv_emulator::kv_store(uint8_t ks_id, const kv_key *key, const kv_value
         std::unique_lock<std::mutex> lock(m_map_mutex);
 
 
-        auto it = m_map[ks_id].find((kv_key *)key);
-        if (it != m_map[ks_id].end()) {
+        auto it = m_map.find((kv_key *)key);
+        if (it != m_map.end()) {
             if (option == KV_STORE_OPT_IDEMPOTENT) return KV_ERR_KEY_EXIST;
 
             // update space
@@ -119,7 +118,7 @@ kv_result kv_emulator::kv_store(uint8_t ks_id, const kv_key *key, const kv_value
         }
         else {
             kv_key *new_key = new_kv_key(key);
-            m_map[ks_id].emplace(std::make_pair(new_key, std::move(valstr)));
+            m_map.emplace(std::make_pair(new_key, std::move(valstr)));
 
             m_available -= key->length + value->length;
 
@@ -140,7 +139,7 @@ kv_result kv_emulator::kv_store(uint8_t ks_id, const kv_key *key, const kv_value
     return KV_SUCCESS;
 }
 
-kv_result kv_emulator::kv_retrieve(uint8_t ks_id, const kv_key *key, uint8_t option, kv_value *value, void *ioctx) {
+kv_result kv_emulator::kv_retrieve(const kv_key *key, uint8_t option, kv_value *value, void *ioctx) {
     (void) ioctx;
 
     kv_result ret = KV_ERR_KEY_NOT_EXIST;
@@ -158,10 +157,10 @@ kv_result kv_emulator::kv_retrieve(uint8_t ks_id, const kv_key *key, uint8_t opt
     {
 
         std::unique_lock<std::mutex> lock(m_map_mutex);
-        auto it = m_map[ks_id].find((kv_key*)key);
-        if (it != m_map[ks_id].end()) {
+        auto it = m_map.find((kv_key*)key);
+        if (it != m_map.end()) {
             uint32_t dlen = it->second.length();
-            if(value->offset != 0 && (value->offset >= dlen)){
+            if (value->offset != 0 && value->offset >= dlen) {
                 return KV_ERR_VALUE_OFFSET_INVALID;
             }
             uint32_t copylen = std::min(dlen - value->offset, value->length);
@@ -191,7 +190,7 @@ kv_result kv_emulator::kv_retrieve(uint8_t ks_id, const kv_key *key, uint8_t opt
 }
 
 
-kv_result kv_emulator::kv_exist(uint8_t ks_id, const kv_key *key, uint32_t keycount, uint8_t *buffers, uint32_t &buffer_size, void *ioctx) {
+kv_result kv_emulator::kv_exist(const kv_key *key, uint32_t keycount, uint8_t *buffers, uint32_t &buffer_size, void *ioctx) {
     (void) ioctx;
 
     int bitpos = 0;
@@ -214,8 +213,8 @@ kv_result kv_emulator::kv_exist(uint8_t ks_id, const kv_key *key, uint32_t keyco
         const int setidx     = (bitpos / 8);
         const int bitoffset  =  bitpos - setidx * 8;
 
-        auto it = m_map[ks_id].find((kv_key*)&key[i]);
-        if (it != m_map[ks_id].end()) {
+        auto it = m_map.find((kv_key*)&key[i]);
+        if (it != m_map.end()) {
             buffers[setidx] |= (1 << bitoffset);
         }
     }
@@ -225,30 +224,27 @@ kv_result kv_emulator::kv_exist(uint8_t ks_id, const kv_key *key, uint32_t keyco
     return KV_SUCCESS;
 }
 
-kv_result kv_emulator::kv_purge(uint8_t ks_id, kv_purge_option option, void *ioctx) {
+kv_result kv_emulator::kv_purge(kv_purge_option option, void *ioctx) {
     (void) ioctx;
-    emulator_map_t::iterator it_tmp;
+
     if (option != KV_PURGE_OPT_DEFAULT) {
         WRITE_WARN("only default purge option is supported");
         return KV_ERR_OPTION_INVALID;
     }
 
     std::unique_lock<std::mutex> lock(m_map_mutex);
-    for (auto it = m_map[ks_id].begin(); it != m_map[ks_id].end(); ) {
+    for (auto it = m_map.begin(); it != m_map.end(); it++) {
         kv_key *key = it->first;
+        m_map.erase(it);
         free(key->key);
         delete key;
-
-        it_tmp = it;
-        it++;
-        m_map[ks_id].erase(it_tmp);
     }
 
     m_available = m_capacity;
     return KV_SUCCESS;
 }
 
-kv_result kv_emulator::kv_delete(uint8_t ks_id, const kv_key *key, uint8_t option, uint32_t *recovered_bytes, void *ioctx) {
+kv_result kv_emulator::kv_delete(const kv_key *key, uint8_t option, uint32_t *recovered_bytes, void *ioctx) {
     (void) ioctx;
 
     if (key == NULL || key->key == NULL) {
@@ -260,8 +256,8 @@ kv_result kv_emulator::kv_delete(uint8_t ks_id, const kv_key *key, uint8_t optio
     }
 
     std::unique_lock<std::mutex> lock(m_map_mutex);
-    auto it = m_map[ks_id].find((kv_key*)key);
-    if (it != m_map[ks_id].end()) {
+    auto it = m_map.find((kv_key*)key);
+    if (it != m_map.end()) {
         kv_key *key = it->first;
 
         uint32_t len = key->length + it->second.length();
@@ -270,7 +266,7 @@ kv_result kv_emulator::kv_delete(uint8_t ks_id, const kv_key *key, uint8_t optio
             *recovered_bytes = len;
         }
 
-        m_map[ks_id].erase(it);
+        m_map.erase(it);
         free(key->key);
         delete key;
     } else {
@@ -283,7 +279,7 @@ kv_result kv_emulator::kv_delete(uint8_t ks_id, const kv_key *key, uint8_t optio
 }
 
 // iterator
-kv_result kv_emulator::kv_open_iterator(uint8_t ks_id, const kv_iterator_option opt, const kv_group_condition *cond, bool_t keylen_fixed, kv_iterator_handle *iter_hdl, void *ioctx) {
+kv_result kv_emulator::kv_open_iterator(const kv_iterator_option opt, const kv_group_condition *cond, bool_t keylen_fixed, kv_iterator_handle *iter_hdl, void *ioctx) {
     (void) ioctx;
 
     if (cond == NULL || iter_hdl == NULL || cond == NULL) {
@@ -303,13 +299,12 @@ kv_result kv_emulator::kv_open_iterator(uint8_t ks_id, const kv_iterator_option
             && m_iterator_list[i].bitmask == cond->bitmask
             && m_iterator_list[i].status == 1) {
             *iter_hdl = i+1;
-            return KV_ERR_ITERATOR_ALREADY_OPEN;
+            return api_private::KVS_ERR_ITERATOR_OPEN;
         }
     }
 
     _kv_iterator_handle *iH = new _kv_iterator_handle();
     iH->it_op = opt;
-    iH->ksid = ks_id;
     iH->it_cond.bitmask = cond->bitmask;
     iH->it_cond.bit_pattern = cond->bit_pattern;
     iH->has_fixed_keylen = keylen_fixed;
@@ -318,7 +313,6 @@ kv_result kv_emulator::kv_open_iterator(uint8_t ks_id, const kv_iterator_option
     memcpy((void*)iH->current_key, (char *)&prefix, 4);
     iH->keylength = 4;
     iH->end = FALSE;
-    iH->ksid = ks_id;
 
     //std::bitset<32> set0 (*(uint32_t*)iH->current_key);
     //std::cerr << "minkey = " << set0 << std::endl;
@@ -337,7 +331,7 @@ kv_result kv_emulator::kv_open_iterator(uint8_t ks_id, const kv_iterator_option
     m_iterator_list[itid - 1].handle_id = itid;
     m_iterator_list[itid - 1].status = 1;
     m_iterator_list[itid - 1].type = opt;
-    m_iterator_list[itid - 1].keyspace_id = ks_id;
+    m_iterator_list[itid - 1].keyspace_id = m_nsid;
     m_iterator_list[itid - 1].prefix = cond->bit_pattern;
     m_iterator_list[itid - 1].bitmask = cond->bitmask;
     m_iterator_list[itid - 1].is_eof = 0;
@@ -383,9 +377,8 @@ kv_result kv_emulator::kv_iterator_next_set(kv_iterator_handle iter_handle_id, k
 
 
     uint32_t prefix = 0;
-    int8_t ks_id = iter_hdl->ksid;
-    auto it = m_map[ks_id].lower_bound(&key);
-    while (it != m_map[ks_id].end()) {
+    auto it = m_map.lower_bound(&key);
+    while (it != m_map.end()) {
         const int klength = it->first->length;
         const int vlength = it->second.length();
 
@@ -395,14 +388,14 @@ kv_result kv_emulator::kv_iterator_next_set(kv_iterator_handle iter_handle_id, k
             memcpy(&prefix, it->first->key, 4);
 
             // if no more match, which means we reached the end of matching list
-            if ((prefix & iter_hdl->it_cond.bitmask) != 
-                (iter_hdl->it_cond.bit_pattern & iter_hdl->it_cond.bitmask)) {
+            if ((prefix & iter_hdl->it_cond.bitmask) != iter_hdl->it_cond.bit_pattern) {
                 iter_list->end = TRUE;
                 end = TRUE;
                 break;
             }
         }
 
+        // printf("matched 0x%X, current key prefix 0x%X, -- %d\n", to_match, prefix, i);
         // found a key
         size_t datasize = klength;
         if (!iter_hdl->has_fixed_keylen) {
@@ -440,12 +433,12 @@ kv_result kv_emulator::kv_iterator_next_set(kv_iterator_handle iter_handle_id, k
         counter++;
 
         if (delete_value) {
-            it = m_map[ks_id].erase(it);
+            it = m_map.erase(it);
         } else {
             it++;
         }
     }
-    //printf("Emulator internal iterator: XXX got entries %d\n", counter);
+    // printf("emulator internal iterator: XXX got entries %d\n", counter);
     iter_list->num_entries = counter;
     iter_list->size = buffer_pos;
     if (end != TRUE) {
@@ -488,11 +481,10 @@ kv_result kv_emulator::kv_iterator_next(kv_iterator_handle iter_handle_id, kv_ke
     }
 
     uint32_t prefix = 0;
-    int8_t ks_id = iter_hdl->ksid;
-    auto it = m_map[ks_id].lower_bound(&key1);
+    auto it = m_map.lower_bound(&key1);
 
     // the end
-    if (it == m_map[ks_id].end()) {
+    if (it == m_map.end()) {
         iter_hdl->end = TRUE;
         return KV_SUCCESS;
     }
@@ -543,12 +535,12 @@ kv_result kv_emulator::kv_iterator_next(kv_iterator_handle iter_handle_id, kv_ke
 
     // delete the identified key, it points to next element
     if (delete_value) {
-        it = m_map[ks_id].erase(it);
+        it = m_map.erase(it);
     } else {
         it++;
     }
     // save next key for next iteration
-    if (it != m_map[ks_id].end()) {
+    if (it != m_map.end()) {
         key->length = klength;
         iter_hdl->keylength = klength;
         memcpy(iter_hdl->current_key, it->first->key, klength);
@@ -598,11 +590,6 @@ kv_result kv_emulator::kv_list_iterators(kv_iterator *iter_list, uint32_t *count
         iter_list[i].prefix = m_iterator_list[i].prefix;
         iter_list[i].bitmask = m_iterator_list[i].bitmask;
         iter_list[i].is_eof = m_iterator_list[i].is_eof;
-
-        /*The bitpattern of the KV API is of big-endian mode. If the CPU is of little-endian mode,
-              the bit pattern and bit mask should be transformed.*/
-        iter_list[i].prefix = htobe32(iter_list[i].prefix);
-        iter_list[i].bitmask = htobe32(iter_list[i].bitmask);
     }
     *count = min;
 
@@ -615,7 +602,7 @@ kv_result kv_emulator::kv_list_iterators(kv_iterator *iter_list, uint32_t *count
     return KV_SUCCESS;
 }
 
-kv_result kv_emulator::kv_delete_group(uint8_t ks_id, kv_group_condition *grp_cond, uint64_t *recovered_bytes, void *ioctx) {
+kv_result kv_emulator::kv_delete_group(kv_group_condition *grp_cond, uint64_t *recovered_bytes, void *ioctx) {
     (void) ioctx;
 
     uint32_t minkey = grp_cond->bitmask & grp_cond->bit_pattern;
@@ -630,8 +617,8 @@ kv_result kv_emulator::kv_delete_group(uint8_t ks_id, kv_group_condition *grp_co
 
     std::unique_lock<std::mutex> lock(m_map_mutex);
 
-    auto it = m_map[ks_id].lower_bound(&key);
-    while (it != m_map[ks_id].end()) {
+    auto it = m_map.lower_bound(&key);
+    while (it != m_map.end()) {
         uint32_t prefix = 0;
         memcpy(&prefix, it->first->key, 4);
 
@@ -648,7 +635,7 @@ kv_result kv_emulator::kv_delete_group(uint8_t ks_id, kv_group_condition *grp_co
 
         it_tmp = it;
         it++;
-        m_map[ks_id].erase(it_tmp);
+        m_map.erase(it_tmp);
     }
 
     return KV_SUCCESS;
diff --git a/PDK/core/src/device_abstract_layer/emulator/src/kv_namespace.cpp b/PDK/core/src/device_abstract_layer/emulator/src/kv_namespace.cpp
index cc56e76..b80126e 100644
--- a/PDK/core/src/device_abstract_layer/emulator/src/kv_namespace.cpp
+++ b/PDK/core/src/device_abstract_layer/emulator/src/kv_namespace.cpp
@@ -145,14 +145,9 @@ kv_result kv_namespace_internal::kv_get_namespace_stat(kv_namespace_stat *ns_st)
 
 // direct interact with kv storage
 // this is processed off submission Q
-kv_result kv_namespace_internal::kv_purge(uint8_t ks_id,kv_purge_option option, void *ioctx) {
+kv_result kv_namespace_internal::kv_purge(kv_purge_option option, void *ioctx) {
     set_purge_in_progress(TRUE); 
-
-    if(ks_id < SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-        return KV_ERR_KEYSPACE_INVALID;
-    }
-
-    kv_result res = m_kvstore->kv_purge(ks_id, option, ioctx);
+    kv_result res = m_kvstore->kv_purge(option, ioctx);
 
     // restore capacity
     if (res == KV_SUCCESS) {
@@ -165,16 +160,12 @@ kv_result kv_namespace_internal::kv_purge(uint8_t ks_id,kv_purge_option option,
 
 
 // directly work with kvstore
-kv_result kv_namespace_internal::kv_store(uint8_t ks_id, const kv_key *key, const kv_value *value, uint8_t option, uint32_t *consumed_bytes, void *ioctx) {
+kv_result kv_namespace_internal::kv_store(const kv_key *key, const kv_value *value, uint8_t option, uint32_t *consumed_bytes, void *ioctx) {
     if (key == NULL || value == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
 
-    if(ks_id < SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-        return KV_ERR_KEYSPACE_INVALID;
-    }
-
-    kv_result res = m_kvstore->kv_store(ks_id, key, value, option, consumed_bytes, ioctx);
+    kv_result res = m_kvstore->kv_store(key, value, option, consumed_bytes, ioctx);
 
     if (res == KV_SUCCESS && consumed_bytes != NULL) {
         // update capacity
@@ -184,16 +175,12 @@ kv_result kv_namespace_internal::kv_store(uint8_t ks_id, const kv_key *key, cons
     return res;
 }
 
-kv_result kv_namespace_internal::kv_delete(uint8_t ks_id, const kv_key *key, uint8_t option, uint32_t *recovered_bytes, void *ioctx) {
+kv_result kv_namespace_internal::kv_delete(const kv_key *key, uint8_t option, uint32_t *recovered_bytes, void *ioctx) {
     if (key == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
 
-    if(ks_id <SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-        return KV_ERR_KEYSPACE_INVALID;
-    }
-
-    kv_result res = m_kvstore->kv_delete(ks_id, key, option, recovered_bytes, ioctx);
+    kv_result res = m_kvstore->kv_delete(key, option, recovered_bytes, ioctx);
 
     if (res == KV_SUCCESS && recovered_bytes != NULL) {
         // update capacity
@@ -203,40 +190,29 @@ kv_result kv_namespace_internal::kv_delete(uint8_t ks_id, const kv_key *key, uin
 }
 
 
-kv_result kv_namespace_internal::kv_exist(uint8_t ks_id, const kv_key *key, uint32_t keycount, uint8_t *value, uint32_t &valuesize, void *ioctx) {
+kv_result kv_namespace_internal::kv_exist(const kv_key *key, uint32_t keycount, uint8_t *value, uint32_t &valuesize, void *ioctx) {
     if (key == NULL || value== NULL) {
         return KV_ERR_PARAM_INVALID;
     }
-
-    if(ks_id <SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-        return KV_ERR_KEYSPACE_INVALID;
-    }
-    return m_kvstore->kv_exist(ks_id, key, keycount, value, valuesize, ioctx);
+    return m_kvstore->kv_exist(key, keycount, value, valuesize, ioctx);
 }
 
-kv_result kv_namespace_internal::kv_retrieve(uint8_t ks_id, const kv_key *key, uint8_t option, kv_value *value, void *ioctx) {
+kv_result kv_namespace_internal::kv_retrieve(const kv_key *key, uint8_t option, kv_value *value, void *ioctx) {
     if (key == NULL || value == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
 
-    if(ks_id <SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-        return KV_ERR_KEYSPACE_INVALID;
-    }
-
-    return m_kvstore->kv_retrieve(ks_id, key, option, value, ioctx);
+    return m_kvstore->kv_retrieve(key, option, value, ioctx);
 }
 
+
 // at this stage to interact with kvstore, these APIs are all synchronous
-kv_result kv_namespace_internal::kv_open_iterator(uint8_t ks_id, const kv_iterator_option it_op, const kv_group_condition *it_cond, kv_iterator_handle *iter_hdl, void *ioctx) {
+kv_result kv_namespace_internal::kv_open_iterator(const kv_iterator_option it_op, const kv_group_condition *it_cond, kv_iterator_handle *iter_hdl, void *ioctx) {
     if (iter_hdl == 0 || it_cond == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
 
-    if(ks_id <SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-        return KV_ERR_KEYSPACE_INVALID;
-    }
-
-    return m_kvstore->kv_open_iterator(ks_id, it_op, it_cond, m_dev->is_keylen_fixed(), iter_hdl, ioctx);
+    return m_kvstore->kv_open_iterator(it_op, it_cond, m_dev->is_keylen_fixed(), iter_hdl, ioctx);
 }
 
 // return after done, no IO command
@@ -270,15 +246,12 @@ kv_result kv_namespace_internal::kv_list_iterators(kv_iterator *kv_iters, uint32
     return m_kvstore->kv_list_iterators(kv_iters, iter_cnt, ioctx);
 }
 
-kv_result kv_namespace_internal::kv_delete_group(uint8_t ks_id, kv_group_condition *grp_cond, uint64_t *recovered_bytes, void *ioctx) {
+kv_result kv_namespace_internal::kv_delete_group(kv_group_condition *grp_cond, uint64_t *recovered_bytes, void *ioctx) {
     if (grp_cond == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
-    if(ks_id <SAMSUNG_MIN_KEYSPACE_ID || ks_id >= SAMSUNG_MAX_KEYSPACE_CNT){
-        return KV_ERR_KEYSPACE_INVALID;
-    }
 
-    return m_kvstore->kv_delete_group(ks_id, grp_cond, recovered_bytes, ioctx);
+    return m_kvstore->kv_delete_group(grp_cond, recovered_bytes, ioctx);
 }
 
 uint64_t kv_namespace_internal::get_total_capacity() {
diff --git a/PDK/core/src/device_abstract_layer/emulator/src/kvs_adi.cpp b/PDK/core/src/device_abstract_layer/emulator/src/kvs_adi.cpp
index 336a937..fdf97f1 100644
--- a/PDK/core/src/device_abstract_layer/emulator/src/kvs_adi.cpp
+++ b/PDK/core/src/device_abstract_layer/emulator/src/kvs_adi.cpp
@@ -133,28 +133,24 @@ kv_result _kv_bypass_namespace(const kv_device_handle dev_hdl, const kv_namespac
 }
 
 // IO APIs
-kv_result kv_purge(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, kv_purge_option option, kv_postprocess_function *post_fn) {
+kv_result kv_purge(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_purge_option option, kv_postprocess_function *post_fn) {
 
     if (que_hdl == NULL || ns_hdl == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
 
     kv_device_internal *dev = (kv_device_internal *) que_hdl->dev;
-    return (dev->kv_purge(que_hdl, ns_hdl, ks_id, option, post_fn));
+    return (dev->kv_purge(que_hdl, ns_hdl, option, post_fn));
 }
 
-kv_result kv_open_iterator(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_iterator_option it_op, const kv_group_condition *it_cond, kv_postprocess_function *post_fn) {
+kv_result kv_open_iterator(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_iterator_option it_op, const kv_group_condition *it_cond, kv_postprocess_function *post_fn) {
 
     if (que_hdl == NULL || ns_hdl == NULL || it_cond == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
-    // Change to big end When current cpu is little end
-    kv_group_condition it_cond_new;
-    it_cond_new.bitmask = htobe32(it_cond->bitmask); 
-    it_cond_new.bit_pattern = htobe32(it_cond->bit_pattern);
 
     kv_device_internal *dev = (kv_device_internal *) que_hdl->dev;
-    return (dev->kv_open_iterator(que_hdl, ns_hdl, ks_id, it_op, &it_cond_new, post_fn));
+    return (dev->kv_open_iterator(que_hdl, ns_hdl, it_op, it_cond, post_fn));
 }
 
 kv_result kv_close_iterator(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_iterator_handle iter_hdl, kv_postprocess_function *post_fn) {
@@ -186,58 +182,58 @@ kv_result kv_list_iterators(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
     return (dev->kv_list_iterators(que_hdl, ns_hdl, post_fn, kv_iters, iter_cnt));
 }
 
-kv_result kv_delete(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *key, kv_delete_option option, kv_postprocess_function *post_fn) {
+kv_result kv_delete(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_delete_option option, kv_postprocess_function *post_fn) {
 
     if (que_hdl == NULL || ns_hdl == NULL || key == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
 
     kv_device_internal *dev = (kv_device_internal *) que_hdl->dev;
-    return (dev->kv_delete(que_hdl, ns_hdl, ks_id, key, option, post_fn));
+    return (dev->kv_delete(que_hdl, ns_hdl, key, option, post_fn));
 }
 
-kv_result kv_delete_group(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, kv_group_condition *grp_cond, kv_postprocess_function *post_fn) {
+kv_result kv_delete_group(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_group_condition *grp_cond, kv_postprocess_function *post_fn) {
 
     if (que_hdl == NULL || ns_hdl == NULL || grp_cond == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
 
     kv_device_internal *dev = (kv_device_internal *) que_hdl->dev;
-    return (dev->kv_delete_group(que_hdl, ns_hdl, ks_id, grp_cond, post_fn));
+    return (dev->kv_delete_group(que_hdl, ns_hdl, grp_cond, post_fn));
 
 }
 
-kv_result kv_exist(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *key, uint32_t key_cnt, uint32_t buffer_size, uint8_t *buffer, kv_postprocess_function *post_fn) {
+kv_result kv_exist(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, uint32_t key_cnt, uint32_t buffer_size, uint8_t *buffer, kv_postprocess_function *post_fn) {
 
     if (que_hdl == NULL || ns_hdl == NULL || key == NULL || buffer == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
 
     kv_device_internal *dev = (kv_device_internal *) que_hdl->dev;
-    return (dev->kv_exist(que_hdl, ns_hdl, ks_id, key, key_cnt, post_fn, buffer_size, buffer));
+    return (dev->kv_exist(que_hdl, ns_hdl, key, key_cnt, post_fn, buffer_size, buffer));
 }
 
-kv_result kv_retrieve(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *key, kv_retrieve_option option, kv_value *value, const kv_postprocess_function *post_fn) {
+kv_result kv_retrieve(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_retrieve_option option, kv_value *value, const kv_postprocess_function *post_fn) {
     if (que_hdl == NULL || ns_hdl == NULL || key == NULL || value == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
 
     kv_device_internal *dev = (kv_device_internal *) que_hdl->dev;
-    return (dev->kv_retrieve(que_hdl, ns_hdl, ks_id, key, option, post_fn, value));
+    return (dev->kv_retrieve(que_hdl, ns_hdl, key, option, post_fn, value));
 
 }
 
-kv_result kv_store(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-  uint8_t ks_id, const kv_key *key, const kv_value *value, kv_store_option option,
-  const kv_postprocess_function *post_fn) {
+kv_result kv_store(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, const kv_value *value, kv_store_option option, const kv_postprocess_function *post_fn) {
+
     if (que_hdl == NULL || ns_hdl == NULL || key == NULL || value == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
 
     kv_device_internal *dev = (kv_device_internal *) que_hdl->dev;
-    return (dev->kv_store(que_hdl, ns_hdl, ks_id, key, value, option, post_fn));
+    return (dev->kv_store(que_hdl, ns_hdl, key, value, option, post_fn));
 }
 
+
 kv_result kv_poll_completion(kv_queue_handle que_hdl, uint32_t timeout_usec, uint32_t *num_events) {
     if (que_hdl == NULL || num_events == NULL) {
         return KV_ERR_PARAM_INVALID;
diff --git a/PDK/core/src/device_abstract_layer/emulator/test/test_suite.cpp b/PDK/core/src/device_abstract_layer/emulator/test/test_suite.cpp
new file mode 100644
index 0000000..bf0ce40
--- /dev/null
+++ b/PDK/core/src/device_abstract_layer/emulator/test/test_suite.cpp
@@ -0,0 +1,1869 @@
+#include "assert.h"
+#include <string>
+#include <iostream>
+#include <sstream>
+#include <thread>
+#include <vector>
+
+#include <stdio.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sys/time.h>
+
+#include <mutex>
+#include <chrono>
+#include <thread>
+#include <set>
+#include <unordered_set>
+#include <map>
+#include <unordered_map>
+#include <iomanip>
+#include <ctime>
+#include <condition_variable>
+#include <atomic>
+#include "kvs_adi.h"
+
+// default settings
+int TOTAL_KV_COUNT = 10000;
+int g_KEYLEN_FIXED = 0;
+int g_KEYLEN = 16;
+
+typedef struct {
+    kv_key *key;
+    kv_value *value;
+    kv_result retcode;
+    kv_postprocess_function post_fn_data;
+
+    // value read back
+    kv_value *value_save;
+
+    // for kv_exist
+    char *buffer;
+    int buffer_size;
+    int buffer_count;
+
+    // for iterator
+    kv_iterator_handle hiter;
+
+    // depends if individual IO sync control is needed
+    std::condition_variable done_cond;
+    std::mutex mutex;
+    std::atomic<int> done;
+} iohdl_t;
+
+
+void free_value(kv_value *value) {
+    // free value
+    if (value != NULL) {
+        if (value->value != NULL) {
+            free(value->value);
+        }
+        free(value);
+    }
+}
+
+void free_key(kv_key *key) {
+    // free key
+    if (key != NULL) {
+        if (key->key != NULL) {
+            free(key->key);
+        }
+        free(key);
+    }
+}
+
+
+// this function will be called after completion of a command
+void interrupt_func(void *data, int number) {
+    (void) data;
+    (void) number;
+    // printf("inside interrupt\n");
+}
+
+int create_sq(kv_device_handle dev_hdl, uint16_t sqid, uint16_t qsize, uint16_t cqid, kv_queue_handle *quehdl) {
+
+    kv_queue qinfo;
+    qinfo.queue_id = sqid;
+    qinfo.queue_size = qsize;
+    qinfo.completion_queue_id = cqid;
+    qinfo.queue_type = SUBMISSION_Q_TYPE;
+    qinfo.extended_info = NULL;
+
+    if (KV_SUCCESS != kv_create_queue(dev_hdl, &qinfo, quehdl)) {
+        printf("submission queue creation failed\n");
+        exit(1);
+    }
+    return 0;
+}
+
+int create_cq(kv_device_handle dev_hdl, uint16_t cqid, uint16_t qsize, kv_queue_handle *quehdl) {
+
+    kv_queue qinfo;
+    qinfo.queue_id = cqid;
+    qinfo.queue_size = qsize;
+    qinfo.completion_queue_id = 0;
+    qinfo.queue_type = COMPLETION_Q_TYPE;
+    qinfo.extended_info = NULL;
+
+    if (KV_SUCCESS != kv_create_queue(dev_hdl, &qinfo, quehdl)) {
+        printf("completion queue creation failed\n");
+        exit(1);
+    }
+    return 0;
+}
+
+uint64_t current_timestamp() {
+    std::chrono::system_clock::time_point timepoint = std::chrono::system_clock::now();
+    uint64_t nano_from_epoch = std::chrono::duration_cast<std::chrono::nanoseconds>(timepoint.time_since_epoch()).count();
+    return nano_from_epoch;
+}
+
+// post processing function for a command
+// op->private_data is the data caller passed in
+// Please note ctx is internal data for read only.
+void on_IO_complete_func(kv_io_context *ctx) {
+    // depends if you allocated space for each command, you can manage
+    // allocated spaces here. This should in sync with your IO cycle
+    // management strategy.
+    //
+    // get completed async command results, free any allocated spaces if necessary.
+    //
+    // printf("finished one command\n");
+    iohdl_t *iohdl = (iohdl_t *) ctx->private_data;
+    iohdl->retcode = ctx->retcode;
+
+    if (ctx->opcode == KV_OPC_CHECK_KEY_EXIST) {
+        iohdl->buffer_size = ctx->result.buffer_size;
+        iohdl->buffer_count = ctx->result.buffer_count;
+    }
+
+    if (ctx->opcode == KV_OPC_OPEN_ITERATOR) {
+        iohdl->hiter = ctx->result.hiter;
+        iohdl->buffer_size = ctx->result.buffer_size;
+        iohdl->buffer_count = ctx->result.buffer_count;
+    }
+
+
+    // printf("command finished with result 0x%X\n", iohdl->retcode);
+    // std::unique_lock<std::mutex> lock(iohdl->mutex);
+    // iohdl->done = 1;
+    // iohdl->done_cond.notify_one();
+}
+
+// fill key value with random data
+void populate_key_value_startwith(uint32_t keystart, kv_key *key, kv_value *value) {
+
+    char *buffer = (char *)key->key;
+    uint32_t blen = key->length;
+
+    uint8_t *base = (uint8_t *)&keystart;
+
+    /*
+    uint8_t prefix[4] = {
+            *(base + 0),
+            *(base + 1),
+            *(base + 2),
+            *(base + 3) };
+    */
+
+    // if prefix is 0, skip prefix setup in key value
+    if (keystart != 0) {
+        assert(blen > 4);
+        // copy 4 bytes of prefix
+        memcpy(buffer, (char *)base, 4);
+        buffer += 4;
+        blen -= 4;
+    }
+
+    // printf("got key 4MSB 0x%X\n", keystart);
+    // printf("key 4MSB 0x%X\n", *(uint32_t *)(buffer - 4));
+
+    // use this to see deterministic sequence of keys
+    static unsigned long long current_time = 0;
+    current_time++;
+
+    char timestr[128];
+    snprintf(timestr, 128, "%llu", current_time);
+    memcpy(buffer, timestr, std::min(blen, (uint32_t)sizeof(timestr)));
+    // done with key
+
+    // generate value
+    buffer = (char *) value->value;
+    blen = value->length;
+    // ending with null
+    buffer[blen - 1] = 0;
+
+    // copy key, then add random value
+    memcpy(buffer, key->key, key->length);
+    blen -= key->length - 1;
+    buffer += key->length;
+    for (unsigned int i = 0; i < blen - 1; i++) {
+        unsigned int j = '0' + std::rand() % 30;
+        buffer[i] = j;
+    }
+
+    // printf("genrate a key %s, value %s\n", (char *) (key->key), (char *)value->value);
+    // printf("genrate a key %s\n", (char *) (key->key));
+}
+
+// set random values for those iohdls
+void populate_iohdls(iohdl_t *iohdls, uint32_t prefix, int klen, int vlen, uint32_t count) {
+    // set up IO handle and postprocessing
+
+    for (uint32_t i = 0; i < count; i++) {
+        // populate random value
+        iohdl_t *iohdl = iohdls + i;
+        populate_key_value_startwith(prefix, iohdl->key, iohdl->value);
+
+        kv_value *value = iohdl->value;
+        kv_value *value_save = iohdl->value_save;
+
+        memcpy(value_save->value, value->value, value->length);
+    }
+}
+
+// create array of io handles that hold keys and values
+void create_iohdls(iohdl_t **iohdls, uint32_t prefix, int klen, int vlen, uint32_t count) {
+    *iohdls = (iohdl_t *) calloc(count, sizeof(iohdl_t));
+
+    for (uint32_t i = 0; i < count; i++) {
+        iohdl_t *iohdl = (*iohdls) + i;
+
+        iohdl->post_fn_data.post_fn = on_IO_complete_func;
+        iohdl->post_fn_data.private_data = (void *) iohdl;
+
+        // allocate key value for reuse
+        kv_key *key = (kv_key *) malloc(sizeof(kv_key));
+        key->length = klen;
+        key->key = malloc(key->length);
+        memset(key->key, 0, key->length);
+
+        // value for insert
+        kv_value *val = (kv_value *) malloc(sizeof(kv_value));
+        val->length = vlen;
+
+        val->actual_value_size = vlen;
+        val->value = malloc(vlen);
+        memset(val->value, 0, val->length);
+        val->offset = 0;
+
+        // value for read back validation
+        kv_value *value_save = (kv_value *) malloc(sizeof(kv_value));
+        value_save->length = vlen;
+
+        value_save->actual_value_size = vlen;
+        value_save->value = malloc(vlen);
+        memset(value_save->value, 0, vlen);
+        value_save->offset = 0;
+
+        // save key value to the IO handle
+        iohdl->key = key;
+        iohdl->value = val;
+        iohdl->value_save = value_save;
+    }
+}
+
+void free_iohdls(iohdl_t *iohdls, uint32_t count) {
+    for (uint32_t i = 0; i < count; i++) {
+        iohdl_t *iohdl = iohdls + i;
+        free_key(iohdl->key);
+        free_value(iohdl->value);
+        free_value(iohdl->value_save);
+    }
+    free(iohdls);
+}
+
+// read back and compare
+uint64_t process_one_round_read(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        // printf("reading key: %s\n", key->key);
+        // prefix of keys
+        // printf("reading key 4MSB: 0x%X\n", *(uint32_t *)&key->key);
+
+        kv_value *value = (iohdls + i)->value;
+        memset(value->value, 0, value->length);
+
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_retrieve failed with error: 0x%X\n", res);
+            res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+    uint64_t end = current_timestamp();
+
+    // compare read value with write value
+    for (uint32_t i = 0; i < count; i++) {
+        // printf("validating key: %s\n", (iohdls + i)->key->key);
+        if (memcmp((iohdls + i)->value_save->value, (iohdls + i)->value->value, (iohdls + i)->value->length)) {
+            // printf("failed: value read not the same as value written, key %s.\n", (iohdls + i)->key->key);
+            // XXX please check if any duplicate keys are used, only unique keys should
+            // be used, otherwise you may get false positive.
+            printf("failed: value read not the same as value written\n");
+            // printf("Note: please only use unique keys for this validation\n");
+            exit(1);
+        }
+    }
+
+    // printf("polling total done: %d, inserted %d\n", total_done, count);
+    // double iotime  = double(total) / count;
+    // printf("average time per IO: %.1f (ns)\n", iotime);
+    return (end - start);
+}
+
+
+/*
+uint64_t process_one_round_write(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count);
+    process_one_round_write(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+*/
+uint64_t process_one_round_read_partial(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+
+        // use another buffer to read back
+        kv_value *value = (iohdls + i)->value;
+        kv_value *value_save = (iohdls + i)->value_save;
+        memcpy(value_save->value, value->value, value->length);
+        value_save->actual_value_size = value->actual_value_size;
+
+        // printf("original value XXX\n%s\n", value->value);
+        // printf("copied value YYY key: %s\n%s\n", key->key, value_save->value);
+        memset(value->value, 0, value->length);
+
+        // reset full size to be filled
+        (iohdls + i)->value->actual_value_size = 0;
+        // set offset for retrieve
+        (iohdls + i)->value->offset = std::rand() % (iohdls + i)->value->length;
+
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_retrieve failed with error: 0x%X\n", res);
+            res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+    uint64_t end = current_timestamp();
+
+    // compare read value with write value
+    for (uint32_t i = 0; i < count; i++) {
+        // printf("validating key: %s\n", (iohdls + i)->key->key);
+        uint32_t retrieved_len = (iohdls + i)->value->length;
+        if (retrieved_len != ((iohdls + i)->value_save->length - (iohdls + i)->value->offset)) {
+            printf("retrieved partial value length not matched\n");
+            exit(1);
+        }
+
+        if (memcmp((char *)(iohdls + i)->value_save->value + (iohdls + i)->value->offset, (char *)(iohdls + i)->value->value, retrieved_len)) {
+            printf("partial value read at offset %d, len %u not the same as value written, key %s.\n", (iohdls + i)->value->offset, retrieved_len, (char *)(iohdls + i)->key->key);
+            // printf("partial value read is: XXX\n%s\n", (char *)(iohdls + i)->value->value);
+            // printf("full value read is: XXX\n%s\n", (char *)(iohdls + i)->value_save->value);
+            exit(1);
+        }
+
+        if ((iohdls + i)->value_save->actual_value_size != (iohdls + i)->value->actual_value_size) {
+            printf("returned full value size doesn't match from partial read\n");
+        }
+    }
+
+    // reset
+    for (uint32_t i = 0; i < count; i++) {
+        // reset offset after testing
+        kv_value *value = (iohdls + i)->value;
+        kv_value *value_save = (iohdls + i)->value_save;
+        value->offset = 0;
+        memcpy(value->value, value_save->value, value_save->length);
+        value->length = value_save->length;
+    }
+
+    // printf("polling total done: %d, inserted %d\n", total_done, count);
+    // double iotime  = double(total) / count;
+    // printf("average time per IO: %.1f (ns)\n", iotime);
+    return (end - start);
+}
+
+
+uint64_t process_one_round_read_offset_invalid(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+
+        // set wrong offset for retrieve
+        kv_value *value = (iohdls + i)->value;
+        value->offset = std::rand() + 1 + (iohdls + i)->value->length;
+
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_retrieve failed with error: 0x%X\n", res);
+            res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+    uint64_t end = current_timestamp();
+
+    // compare read value with write value
+    for (uint32_t i = 0; i < count; i++) {
+        // printf("validating key: %s\n", (iohdls + i)->key->key);
+        if ((iohdls + i)->retcode != KV_ERR_VALUE_OFFSET_INVALID) {
+            printf("test invalid offset read failed\n");
+            exit(1);
+        }
+    }
+
+    for (uint32_t i = 0; i < count; i++) {
+        // reset offset after testing
+        kv_value *value = (iohdls + i)->value;
+        value->offset = 0;
+    }
+
+    // printf("polling total done: %d, inserted %d\n", total_done, count);
+    // double iotime  = double(total) / count;
+    // printf("average time per IO: %.1f (ns)\n", iotime);
+    return (end - start);
+}
+
+
+// insert enough keys but avoid excessive queue contention
+uint64_t process_one_round_write(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        kv_value *value = (iohdls + i)->value;
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_store failed with error: 0x%X\n", res);
+            res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+    uint64_t end = current_timestamp();
+
+
+    // this allows overwrite, so all should succeed
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        if ((iohdls + i)->retcode != KV_SUCCESS) {
+            printf("kv_store key test failed: item %u: return code 0x%03X\n", i, (iohdls + i)->retcode);
+            exit(1);
+        }
+    }
+
+    // printf("polling total done: %d, inserted %d\n", total_done, count);
+    // double iotime  = double(total) / count;
+    // printf("average time per IO: %.1f (ns)\n", iotime);
+    return (end - start);
+}
+
+// insert a key
+uint64_t insert_key(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, kv_key *key, kv_value *value, uint32_t count) {
+
+    iohdl_t iohdl;
+    iohdl.key = key;
+    iohdl.value = value;
+    kv_postprocess_function post_fn_data = { on_IO_complete_func, &iohdl};
+
+    kv_result res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, &post_fn_data);
+    while (res != KV_SUCCESS) {
+        printf("insert_key failed with error: 0x%03X\n", res);
+        res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, &post_fn_data);
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < 1) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+
+    if (iohdl.retcode != KV_SUCCESS) {
+        printf("insert one key failed: return code 0x%03X\n", iohdl.retcode);
+        exit(1);
+    }
+
+    return 0;
+}
+
+
+// delete a given key
+uint64_t delete_key(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, kv_key *key, uint32_t count) {
+
+    iohdl_t iohdl;
+    memset(&iohdl, 0, sizeof (iohdl));
+    iohdl.key = key;
+    kv_postprocess_function post_fn_data = { on_IO_complete_func, &iohdl};
+    kv_result res = kv_delete(sq_hdl, ns_hdl, key, KV_DELETE_OPT_DEFAULT, &post_fn_data);
+    while (res != KV_SUCCESS) {
+        printf("kv_store failed with error: 0x%X\n", res);
+        res = kv_delete(sq_hdl, ns_hdl, key, KV_DELETE_OPT_DEFAULT, &post_fn_data);
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < 1) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+
+    if (iohdl.retcode != KV_SUCCESS) {
+        printf("delete one key failed: return code 0x%03X\n", iohdl.retcode);
+        exit(1);
+    }
+
+    return 0;
+}
+
+
+uint64_t process_one_round_kv_exist(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        kv_value *value = (iohdls + i)->value;
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_exit test kv_store failed with error: 0x%03X\n", res);
+            res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+    uint64_t end = current_timestamp();
+
+    // io completion function got return code in iohdl
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        if ((iohdls + i)->retcode != KV_SUCCESS) {
+            printf("kv_exist write key test failed: item %u: return code 0x%03X\n", i, (iohdls + i)->retcode);
+            exit(1);
+        }
+    }
+
+    // set up first
+    uint32_t blen = (count -1) / 8 + 1;
+
+    uint8_t *buffer = (uint8_t *) calloc(blen, 1);
+    uint8_t *buffer_expected = (uint8_t *) calloc(blen, 1);
+
+    kv_key *keys = (kv_key *) calloc(count, sizeof(kv_key));
+    // set half of keys that shouldn't exist
+    int bitpos = 0;
+    for (uint32_t i = 0; i < count; i++, bitpos++) {
+        kv_key *key_i = keys + i;
+        key_i->key = calloc((iohdls + i)->key->length, 1);
+        memcpy(key_i->key, (iohdls + i)->key->key, (iohdls + i)->key->length);
+        key_i->length = (iohdls + i)->key->length;
+
+        // fprintf(stderr, "key len %d\n", key_i->length);
+        if (i % 2 == 0) {
+            // char *buffer = (char *) key_i->key;
+            // delete these keys, so they are truly gone
+            delete_key(ns_hdl, sq_hdl, cq_hdl, key_i, 1);
+        } else {
+            // insert a key to make sure it's there
+            // insert_key(ns_hdl, sq_hdl, cq_hdl, (iohdls + i)->key, (iohdls + i)->value, 1);
+            // set up expected bit results
+            const int setidx = (bitpos / 8);
+            const int bitoffset  =  bitpos - setidx * 8;
+            buffer_expected[setidx] |= (1 << bitoffset);
+
+            // printf("XXX byteoffset %d, bitoffset %d\n", setidx, bitoffset);
+        }
+    }
+
+    kv_postprocess_function *post_fn_data = &((iohdls + 0)->post_fn_data);
+    kv_result res = kv_exist(sq_hdl, ns_hdl, keys, count, blen, buffer, post_fn_data);
+    while (res != KV_SUCCESS) {
+        printf("kv_exist failed with error: 0x%03X\n", res);
+        res = kv_exist(sq_hdl, ns_hdl, keys, count, blen, buffer, post_fn_data);
+    }
+
+    // poll for completion, just one operation
+    total_done = 0;
+    while (total_done < 1) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+
+    // compare results
+    blen = (iohdls + 0)->buffer_size;
+    // printf("XXX blen is %u\n", blen);
+    if (memcmp(buffer, buffer_expected, blen)) {
+        printf("kv_exist test failed\n");
+        exit(1);
+    }
+
+    free(buffer);
+    free(buffer_expected);
+    for (uint32_t i = 0; i < count; i++, bitpos++) {
+        free((keys + i)->key);
+    }
+    free(keys);
+
+    return (end - start);
+}
+
+
+void change_value(kv_value *value) {
+    // generate value
+    char *buffer = (char *) value->value;
+    uint32_t blen = value->length;
+    // ending with null
+    buffer[blen - 1] = 0;
+    for (unsigned int i = 0; i < blen - 1; i++) {
+        unsigned int j = '0' + std::rand() % 30;
+        buffer[i] = j;
+    }
+}
+
+// insert enough keys but avoid excessive queue contention
+uint64_t process_one_round_write_duplicatekeys_notallowed(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        kv_value *value = (iohdls + i)->value;
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_IDEMPOTENT, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_store failed with error: 0x%X\n", res);
+            res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_IDEMPOTENT, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+    uint64_t end = current_timestamp();
+
+    // io completion function got return code in iohdl
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        if ((iohdls + i)->retcode != KV_ERR_KEY_EXIST) {
+            printf("duplicate key not allowed test failed\n");
+            exit(1);
+        }
+    }
+
+    // printf("polling total done: %d, inserted %d\n", total_done, count);
+    // double iotime  = double(total) / count;
+    // printf("average time per IO: %.1f (ns)\n", iotime);
+    return (end - start);
+}
+
+
+// duplicate key to overwrite
+uint64_t process_one_round_write_duplicatekeys_allowed(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        kv_value *value = (iohdls + i)->value;
+
+        // set a new random value for overwrite test for read back test
+        change_value(value);
+        memcpy((iohdls + i)->value_save->value, value->value, value->length);
+
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_store failed with error: 0x%X\n", res);
+            res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+    uint64_t end = current_timestamp();
+
+    // io completion function got return code in iohdl
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        if ((iohdls + i)->retcode != KV_SUCCESS) {
+            printf("duplicate key allowed test failed: item %u: return code 0x%03X\n", i, (iohdls + i)->retcode);
+            exit(1);
+        }
+    }
+
+    // printf("polling total done: %d, inserted %d\n", total_done, count);
+    // double iotime  = double(total) / count;
+    // printf("average time per IO: %.1f (ns)\n", iotime);
+    return (end - start);
+}
+
+
+// duplicate key to overwrite
+uint64_t process_one_round_delete(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        kv_value *value = (iohdls + i)->value;
+
+        // set a new random value for overwrite test for read back test
+        change_value(value);
+        memcpy((iohdls + i)->value_save->value, value->value, value->length);
+
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_delete(sq_hdl, ns_hdl, key, KV_DELETE_OPT_DEFAULT, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_store failed with error: 0x%X\n", res);
+            res = kv_delete(sq_hdl, ns_hdl, key, KV_DELETE_OPT_DEFAULT, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+    uint64_t end = current_timestamp();
+
+    // io completion function got return code in iohdl
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        if ((iohdls + i)->retcode != KV_SUCCESS) {
+            printf("delete key test failed: item %u: return code 0x%03X\n", i, (iohdls + i)->retcode);
+            exit(1);
+        }
+    }
+
+    // test read back, all should fail
+    for (uint32_t i = 0; i < count; i++) {
+        kv_key *key = (iohdls + i)->key;
+        kv_value *value = (iohdls + i)->value;
+        memset(value->value, 0, value->length);
+
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_retrieve failed with error: 0x%X\n", res);
+            res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    total_done = 0;
+    while (total_done < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+
+    // compare read value with write value
+    for (uint32_t i = 0; i < count; i++) {
+        if ((iohdls + i)->retcode != KV_ERR_KEY_NOT_EXIST) {
+            printf("delete key test failed, key shouldn't be found: item %u: return code 0x%03X\n", i, (iohdls + i)->retcode);
+            exit(1);
+        }
+    }
+
+    return (end - start);
+}
+
+// delete a given key
+uint64_t delete_group(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, kv_group_condition *grp_cond) {
+
+    iohdl_t iohdl;
+    memset(&iohdl, 0, sizeof (iohdl));
+    kv_postprocess_function post_fn_data = { on_IO_complete_func, &iohdl};
+    kv_result res = kv_delete_group(sq_hdl, ns_hdl, grp_cond, &post_fn_data);
+
+    while (res != KV_SUCCESS) {
+        printf("kv_delete_group failed with error: 0x%X\n", res);
+        res = kv_delete_group(sq_hdl, ns_hdl, grp_cond, &post_fn_data);
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < 1) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+
+    if (iohdl.retcode != KV_SUCCESS) {
+        printf("delete group failed: return code 0x%03X\n", iohdl.retcode);
+        exit(1);
+    }
+
+    return 0;
+}
+
+
+
+// duplicate key to overwrite
+uint64_t process_one_round_delete_group(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint32_t prefix = 0x87654321;
+
+    kv_group_condition grp_cond;
+    grp_cond.bitmask = 0xFFFFFF00;
+    grp_cond.bit_pattern = prefix;
+
+    // set new value
+    for (uint32_t i = 0; i < count; i++) {
+        // populate random value
+        iohdl_t *iohdl = iohdls + i;
+        populate_key_value_startwith(prefix, iohdl->key, iohdl->value);
+
+        kv_value *value = iohdl->value;
+        kv_value *value_save = iohdl->value_save;
+
+        memcpy(value_save->value, value->value, value->length);
+    }
+
+    // save the data
+    process_one_round_write(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+
+    delete_group(ns_hdl, sq_hdl, cq_hdl, &grp_cond);
+
+    // test read back, all should fail
+    for (uint32_t i = 0; i < count; i++) {
+        kv_key *key = (iohdls + i)->key;
+        kv_value *value = (iohdls + i)->value;
+        memset(value->value, 0, value->length);
+        (iohdls + i)->retcode = KV_ERR_COMMAND_INITIALIZED;
+
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_retrieve failed with error: 0x%X\n", res);
+            res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+
+    // compare read value with write value
+    for (uint32_t i = 0; i < count; i++) {
+        if ((iohdls + i)->retcode != KV_ERR_KEY_NOT_EXIST) {
+            printf("delete group test failed, key shouldn't be found: item %u: return code 0x%03X\n", i, (iohdls + i)->retcode);
+
+            if ((iohdls + i)->key && (iohdls + i)->key->key) {
+                uint32_t *addr = (uint32_t *) (iohdls + i)->key->key;
+                uint32_t *addr1 = addr + 1;
+                uint32_t *addr2 = addr + 2;
+                printf("leading command key bytes: 0x%X %X %X\n", *addr, *addr1, *addr2);
+            }
+
+            exit(1);
+        }
+    }
+
+    return 0;
+}
+
+typedef struct iterator_entries_t {
+    kv_group_condition grp_cond;
+    kv_iterator_option iter_op;
+    uint32_t prefix;
+
+    // expected total # for iteration
+    uint32_t expected_total;
+
+    // key values should be matched these
+    std::set<std::string> keys;
+    std::set<std::string> values;
+} iterator_entries_t;
+
+
+// generate a set of data, and insert them, record them for iterator use
+void generate_prefixed_dataset(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count, iterator_entries_t& entries, uint32_t prefix, kv_group_condition *grp_cond, kv_iterator_option iter_op, uint32_t expected_total) {
+    //////////////////////////////
+    // generate a set
+    entries.grp_cond = *grp_cond;
+    entries.iter_op = iter_op;
+    entries.prefix = prefix;
+    entries.expected_total = expected_total;
+
+    for (uint32_t i = 0; i < count; i++) {
+        // populate random value
+        iohdl_t *iohdl = iohdls + i;
+        populate_key_value_startwith(prefix, iohdl->key, iohdl->value);
+
+        kv_value *value = iohdl->value;
+        kv_value *value_save = iohdl->value_save;
+
+        memcpy(value_save->value, value->value, value->length);
+
+        std::string kstr = std::string((char *) iohdl->key->key, iohdl->key->length);
+        if (entries.keys.find(kstr) != entries.keys.end()) {
+            printf("INFO: repeated keys\n");
+        }
+        entries.keys.insert(kstr);
+
+        std::string vstr = std::string((char *) iohdl->value->value, iohdl->value->length);
+        if (entries.values.find(vstr) != entries.values.end()) {
+            printf("INFO: repeated values\n");
+        }
+        entries.values.insert(vstr);
+        // printf("saved klen %d, vlen %d\n", iohdl->key->length, iohdl->value->length);
+    }
+    // save the data for iteration
+    process_one_round_write(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+}
+
+void prepare_test_iterators_one_bitmask(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count, iterator_entries_t& iterator_entries, kv_iterator_option iter_op) {
+    uint32_t expected_total = count;
+    uint32_t prefix = 0x87654321;
+    kv_group_condition grp_cond;
+
+    //////////////////////////////
+    grp_cond.bitmask = 0xFF000000;
+    grp_cond.bit_pattern = prefix;
+    // delete all existing entries
+    delete_group(ns_hdl, sq_hdl, cq_hdl, &grp_cond);
+
+
+    //////////////////////////////
+    // generate a set
+    grp_cond.bitmask = 0xFFFFFFFF;
+    grp_cond.bit_pattern = prefix;
+    generate_prefixed_dataset(ns_hdl, sq_hdl, cq_hdl, iohdls, count, iterator_entries, prefix, &grp_cond, iter_op, expected_total);
+}
+
+
+// insert entries, and record results for validation
+// gradually increase matching entries for each dataset
+void prepare_test_iterators_multiple_bitmask(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count, iterator_entries_t& iterator_entries, kv_iterator_option iter_op) {
+    // these will match all 0xFF...
+    uint32_t expected_total = 0;
+    uint32_t prefix = 0x87654321;
+    kv_group_condition grp_cond;
+
+    //////////////////////////////
+    grp_cond.bitmask = 0xFF000000;
+    grp_cond.bit_pattern = prefix;
+    // delete all existing entries
+    delete_group(ns_hdl, sq_hdl, cq_hdl, &grp_cond);
+
+
+    //////////////////////////////
+    // generate a another set
+    grp_cond.bitmask = 0xFFFFFFFF;
+    grp_cond.bit_pattern = prefix;
+    expected_total = count; 
+    generate_prefixed_dataset(ns_hdl, sq_hdl, cq_hdl, iohdls, count, iterator_entries, prefix, &grp_cond, iter_op, expected_total);
+
+
+    //////////////////////////////
+    // generate a another set
+    grp_cond.bitmask = 0xFFFFFF00;
+    grp_cond.bit_pattern = prefix;
+    expected_total += count; 
+    generate_prefixed_dataset(ns_hdl, sq_hdl, cq_hdl, iohdls, count, iterator_entries, prefix, &grp_cond, iter_op, expected_total);
+
+
+    //////////////////////////////
+    // generate a another set
+    grp_cond.bitmask = 0xFFFF0000;
+    grp_cond.bit_pattern = prefix;
+    expected_total += count; 
+
+    generate_prefixed_dataset(ns_hdl, sq_hdl, cq_hdl, iohdls, count, iterator_entries, prefix, &grp_cond, iter_op, expected_total);
+
+    // printf("XXX total inserted size %lu\n", expected_total);
+    // printf("XXX key inserted size %lu\n", iterator_entries.keys.size());
+    // printf("XXX value inserted size %lu\n", iterator_entries.values.size());
+    // set final matched total for validation
+    iterator_entries.expected_total = expected_total;
+    iterator_entries.grp_cond.bitmask = 0xFF000000;
+    iterator_entries.grp_cond.bit_pattern = 0x87654321;
+}
+
+//////////////////////////////////////////
+// process iterator returned buffer
+// use global g_KEYLEN to decide the fixed length of keys
+void processing_iterator_returned_keys_fixed(kv_iterator_list *iter_list, std::vector<std::string>& keys) {
+
+    uint8_t *buffer = (uint8_t *) iter_list->it_list;
+    // uint32_t blen = iter_list->size;
+    uint32_t num_entries = iter_list->num_entries;
+
+    uint32_t klen = g_KEYLEN;
+    // uint32_t vlen = sizeof(kv_value_t);
+
+    while (num_entries > 0) {
+        keys.push_back(std::string((char *) buffer, klen));
+        buffer += klen;
+        num_entries--;
+    }
+}
+
+void processing_iterator_returned_keys_variable(kv_iterator_list *iter_list, std::vector<std::string>& keys) {
+    uint8_t *buffer = (uint8_t *) iter_list->it_list;
+    // uint32_t blen = iter_list->size;
+    uint32_t num_entries = iter_list->num_entries;
+
+    uint32_t klen = sizeof(uint32_t);
+    uint32_t klen_value = 0;
+    // uint32_t vlen = sizeof(kv_value_t);
+    // uint32_t vlen_value = 0;
+
+    while (num_entries > 0) {
+        // first get klen
+        uint8_t* addr = (uint8_t *) &klen_value;
+        for (unsigned int i = 0; i < klen; i++) {
+            *(addr + i) = *(buffer + i);
+        }
+        buffer += klen;
+
+        keys.push_back(std::string((char *) buffer, klen_value));
+        buffer += klen_value;
+
+        num_entries--;
+    }
+}
+
+void processing_iterator_returned_keyvals_fixed(kv_iterator_list *iter_list, std::vector<std::string>& keys, std::vector<std::string>& values) {
+
+    uint8_t *buffer = (uint8_t *) iter_list->it_list;
+    // uint32_t blen = iter_list->size;
+    uint32_t num_entries = iter_list->num_entries;
+
+    uint32_t klen = g_KEYLEN;
+    // uint32_t klen_value = 0;
+    uint32_t vlen = sizeof(kv_value_t);
+    uint32_t vlen_value = 0;
+
+    while (num_entries > 0) {
+        // get fixed key
+        keys.push_back(std::string((char *)buffer, klen));
+        buffer += klen;
+
+        // get vlen
+        uint8_t *addr = (uint8_t *)&vlen_value;
+        for (unsigned int i = 0; i < vlen; i++) {
+            *(addr + i) = *(buffer + i);
+        }
+        buffer += vlen;
+
+        values.push_back(std::string((char *)buffer, vlen_value));
+        buffer += vlen_value;
+
+        num_entries--;
+    }
+}
+
+void processing_iterator_returned_keyvals_variable(kv_iterator_list *iter_list, std::vector<std::string>& keys, std::vector<std::string>& values) {
+
+    uint8_t *buffer = (uint8_t *) iter_list->it_list;
+    // uint32_t blen = iter_list->size;
+    uint32_t num_entries = iter_list->num_entries;
+
+    uint32_t klen = sizeof(uint32_t);
+    uint32_t klen_value = 0;
+    uint32_t vlen = sizeof(kv_value_t);
+    uint32_t vlen_value = 0;
+
+    while (num_entries > 0) {
+        // first get klen
+        uint8_t *addr = (uint8_t *) &klen_value;
+        for (unsigned int i = 0; i < klen; i++) {
+            *(addr + i) = *(buffer + i);
+        }
+        buffer += klen;
+
+        // printf("XXX got klen %u\n", klen_value);
+
+        keys.push_back(std::string((char *) buffer, klen_value));
+        buffer += klen_value;
+
+        // get vlen
+        addr = (uint8_t *) &vlen_value;
+        for (unsigned int i = 0; i < vlen; i++) {
+            *(addr + i) = *(buffer + i);
+        }
+        buffer += vlen;
+
+        // printf("XXX got vlen %u\n", vlen_value);
+
+        values.push_back(std::string((char *) buffer, vlen_value));
+        buffer += vlen_value;
+
+        num_entries--;
+    }
+}
+ 
+
+// XXX use a global to check if key is fixed size or not
+// default is fixed size
+void processing_iterator_returned_keyvals(kv_iterator_list *iter_list, kv_iterator_option iter_op, std::vector<std::string>& keys, std::vector<std::string>& values) {
+    // now check returned key/values
+    if (iter_op == KV_ITERATOR_OPT_KEY) {
+        if (g_KEYLEN_FIXED) {
+            processing_iterator_returned_keys_fixed(iter_list, keys);
+        } else {
+            processing_iterator_returned_keys_variable(iter_list, keys);
+        }
+    }
+
+    if (iter_op == KV_ITERATOR_OPT_KV) {
+        if (g_KEYLEN_FIXED) {
+            processing_iterator_returned_keyvals_fixed(iter_list, keys, values);
+        } else {
+            processing_iterator_returned_keyvals_variable(iter_list, keys, values);
+        }
+    }
+}
+
+
+void get_iterator_results(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, 
+    iohdl_t *iohdls, uint32_t count,
+    kv_group_condition *grp_cond, kv_iterator_option iter_op, 
+    std::vector<std::string>& keys, std::vector<std::string>& values) {
+
+    //////////////////////////////////
+    // open iterator
+    // iohdl.hiter (a handle)
+    iohdl_t iohdl;
+    // kv_iterator_option iter_op = KV_ITERATOR_OPT_KEY;
+    // kv_iterator_option iter_op = KV_ITERATOR_OPT_KV;
+    
+    kv_postprocess_function post_fn_data = { on_IO_complete_func, &iohdl};
+    kv_result res = kv_open_iterator(sq_hdl, ns_hdl, iter_op, grp_cond, &post_fn_data);
+
+    while (res != KV_SUCCESS) {
+        printf("kv_open_iterator failed with error: 0x%X\n", res);
+        res = kv_open_iterator(sq_hdl, ns_hdl, iter_op, grp_cond, &post_fn_data);
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < 1) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+
+    if (iohdl.retcode != KV_SUCCESS) {
+        printf("kv_open_iterator failed failed: return code 0x%03X\n", iohdl.retcode);
+        exit(1);
+    }
+
+    //////////////////////////////////
+    // iterator next
+    kv_iterator_handle hiter = iohdl.hiter;
+    kv_iterator_list iter_list;
+
+    uint32_t buffer_size = 6 * 4096 + 1234;
+    uint8_t buffer[buffer_size];
+    iter_list.size = 6 * 4096 + 1234;
+    iter_list.it_list = (uint8_t *) buffer;
+    iter_list.num_entries = 0;
+    iter_list.end = FALSE;
+    uint32_t total = 0;
+
+    while (1) {
+        res = kv_iterator_next(sq_hdl, ns_hdl, hiter, &iter_list, &post_fn_data);
+
+        // poll for completion
+        uint32_t total_done = 0;
+        while (total_done < 1) {
+            uint32_t finished = 0;
+            kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+            if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+                printf("kv_poll_completion error! exit\n");
+                exit(1);
+            }
+            total_done += finished;
+        }
+
+        if (iohdl.retcode != KV_SUCCESS && iohdl.retcode != KV_WRN_MORE) {
+            printf("kv_iterator_next failed: return code 0x%03X\n", iohdl.retcode);
+            exit(1);
+        }
+
+        // now check returned key/values
+        processing_iterator_returned_keyvals(&iter_list, iter_op, keys, values);
+        // printf("client got entries:%d\n", iter_list.num_entries);
+        total += iter_list.num_entries;
+
+        if (iter_list.end) {
+            break;
+        }
+    }
+    // printf("client got total entries:%d\n", total);
+
+
+    // close iterator
+    //////////////////////////////////
+    // iohdl_t iohdl;
+    // iohdl.hiter (a handle)
+    // kv_postprocess_function post_fn_data = { on_IO_complete_func, &iohdl};
+    res = kv_close_iterator(sq_hdl, ns_hdl, iohdl.hiter, &post_fn_data);
+
+    while (res != KV_SUCCESS) {
+        printf("kv_close_iterator failed with error: 0x%X\n", res);
+        res = kv_close_iterator(sq_hdl, ns_hdl, iohdl.hiter, &post_fn_data);
+    }
+
+    // poll for completion
+    total_done = 0;
+    while (total_done < 1) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+
+    if (iohdl.retcode != KV_SUCCESS) {
+        printf("kv_close_iterator failed: return code 0x%03X\n", iohdl.retcode);
+        exit(1);
+    }
+}
+
+////////////////////////////////////////////////
+// the test assumes all keys and values are unique
+// when data are generated and inserted 
+void validate_iterator_results(iterator_entries_t& iterator_entries, std::vector<std::string>& keys, std::vector<std::string>& values) {
+
+    uint32_t inserted_n = iterator_entries.keys.size();
+
+    uint32_t keyn = keys.size();
+    uint32_t valn = values.size();
+
+    if (keyn != inserted_n) {
+        printf("iterator returned different key counts from what's inserted\n");
+        printf("inserted %d keys, iterator returned %d keys\n", inserted_n, keyn);
+        exit(1);
+    }
+
+    // check keys
+    for (uint32_t i = 0; i < keyn; i++) {
+        std::string kstr = keys[i];
+
+        auto it = iterator_entries.keys.find(kstr);
+        auto ite = iterator_entries.keys.end();
+        if (it == ite) {
+            printf("original key size %lu: klen: %lu\n", iterator_entries.keys.size(), it->size());
+            // printf("iterator returned key: %s\n", kstr.c_str());
+            printf("iterator returned keys not found in original inserted set\n");
+            exit(1);
+        }
+    }
+
+    // check values 
+    if (valn == 0) {
+        return;
+    }
+
+    for (uint32_t i = 0; i < valn; i++) {
+        std::string vstr = values[i];
+        auto it = iterator_entries.values.find(vstr);
+        if (it == iterator_entries.values.end()) {
+            printf("iterator returned values not found in original inserted set\n");
+            exit(1);
+        }
+    }
+}
+
+// test iterators
+uint64_t test_iterators_one_bitmask(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count, kv_iterator_option iter_op) {
+
+    std::vector<std::string> keys;
+    std::vector<std::string> values;
+
+    iterator_entries_t iterator_entries;
+    prepare_test_iterators_one_bitmask(ns_hdl, sq_hdl, cq_hdl, iohdls, count, iterator_entries, iter_op);
+
+    get_iterator_results(ns_hdl, sq_hdl, cq_hdl, iohdls, count, &iterator_entries.grp_cond, iter_op, keys, values);
+
+    validate_iterator_results(iterator_entries, keys, values);
+
+    return 0;
+}
+
+// test iterators
+uint64_t test_iterators_multiple_bitmask(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count, kv_iterator_option iter_op) {
+
+    std::vector<std::string> keys;
+    std::vector<std::string> values;
+
+    iterator_entries_t iterator_entries;
+    prepare_test_iterators_multiple_bitmask(ns_hdl, sq_hdl, cq_hdl, iohdls, count, iterator_entries, iter_op);
+
+    get_iterator_results(ns_hdl, sq_hdl, cq_hdl, iohdls, count, &iterator_entries.grp_cond, iter_op, keys, values);
+
+    validate_iterator_results(iterator_entries, keys, values);
+
+    return 0;
+}
+
+
+void test_close_iterator(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, std::vector<kv_iterator_handle>& iters) {
+
+    for (kv_iterator_handle hiter : iters) {
+        // close iterator
+        iohdl_t iohdl;
+        iohdl.hiter = hiter;
+        kv_postprocess_function post_fn_data = { on_IO_complete_func, &iohdl};
+
+        kv_result res = kv_close_iterator(sq_hdl, ns_hdl, iohdl.hiter, &post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_close_iterator failed with error: 0x%X\n", res);
+            res = kv_close_iterator(sq_hdl, ns_hdl, iohdl.hiter, &post_fn_data);
+        }
+
+        // poll for completion
+        int total_done = 0;
+        while (total_done < 1) {
+            uint32_t finished = 0;
+            kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+            if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+                printf("kv_poll_completion error! exit\n");
+                exit(1);
+            }
+            total_done += finished;
+        }
+
+        if (iohdl.retcode != KV_SUCCESS) {
+            printf("kv_close_iterator failed: return code 0x%03X\n", iohdl.retcode);
+            exit(1);
+        }
+    }
+}
+
+
+kv_result test_open_iterator(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, std::vector<kv_iterator_handle>& iters) {
+
+    //////////////////////////////////
+    // open iterator
+    // iohdl.hiter (a handle)
+    iohdl_t iohdl;
+    // kv_iterator_option iter_op = KV_ITERATOR_OPT_KV;
+    kv_iterator_option iter_op = KV_ITERATOR_OPT_KEY;
+
+    static int i = 0;
+    i++;
+
+    // only 1 iterator with the same prefix can be opened
+    uint32_t prefix = 0x87654321 + i;
+    kv_group_condition grp_cond;
+    //////////////////////////////
+    grp_cond.bitmask = 0xFF000000;
+    grp_cond.bit_pattern = prefix;
+
+
+    //////////////////////////////////
+    // open iterator
+    // iohdl.hiter (a handle)
+    kv_postprocess_function post_fn_data = { on_IO_complete_func, &iohdl};
+    kv_result res = kv_open_iterator(sq_hdl, ns_hdl, iter_op, &grp_cond, &post_fn_data);
+
+    while (res != KV_SUCCESS) {
+        printf("kv_open_iterator failed with error: 0x%X\n", res);
+        res = kv_open_iterator(sq_hdl, ns_hdl, iter_op, &grp_cond, &post_fn_data);
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < 1) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+
+    /*
+    if (iohdl.retcode != KV_SUCCESS) {
+        printf("kv_open_iterator failed failed: return code 0x%03X\n", iohdl.retcode);
+        exit(1);
+    }*/
+
+    if (iohdl.retcode == KV_SUCCESS) {
+        iters.push_back(iohdl.hiter);
+    }
+    return iohdl.retcode;
+}
+
+// test list iterators, return the count of open iterators
+int test_list_iterators(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl) {
+
+    kv_iterator kv_iters[SAMSUNG_MAX_ITERATORS];
+    memset(kv_iters, 0, sizeof(kv_iters));
+    uint32_t count = SAMSUNG_MAX_ITERATORS;
+
+    iohdl_t iohdl;
+    kv_postprocess_function post_fn_data = { on_IO_complete_func, &iohdl};
+
+    // this sync admin call
+    kv_result res = kv_list_iterators(sq_hdl, ns_hdl, kv_iters, &count, &post_fn_data);
+
+    if (count != SAMSUNG_MAX_ITERATORS) {
+        printf("kv_list_iterators doesn't return all iterators (open or closed), returned only %u\n", count);
+        exit(1);
+    }
+
+    if (res) {
+        printf("kv_list_iterators with error: 0x%X\n", res);
+	exit(1);
+    }
+
+    int valid_count = 0;
+    // simply print for any return iterators
+    for (uint32_t i = 0; i < count; i++) {
+        if (kv_iters[i].handle_id > 0 && kv_iters[i].status) {
+            // fprintf(stderr, "found iterator id %d\n", kv_iters[i].handle_id);
+            valid_count++;
+        }
+    }
+
+    // printf("valid count %d\n", valid_count);
+    return valid_count;
+}
+
+
+// test enforcement of max iterators allowed
+void test_max_allowed_iterators(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl) {
+    std::vector<kv_iterator_handle> iters;
+
+    // test count open iterators
+    int count = test_list_iterators(ns_hdl, sq_hdl, cq_hdl);
+    if (count != 0) {
+    	printf("kv_list_iterators doesn't return 0 count of open iterators at fresh start\n");
+	exit(1);
+    }
+
+    for (int i = 0; i < SAMSUNG_MAX_ITERATORS; i++) {
+        kv_result ret = test_open_iterator(ns_hdl, sq_hdl, cq_hdl, iters);
+
+        if (ret != KV_SUCCESS) {
+            printf("open iterators within limit failed, return code 0x%03X\n", ret);
+            exit(1);
+        }
+    }
+
+    // test count open iterators
+    count = test_list_iterators(ns_hdl, sq_hdl, cq_hdl);
+    if (count != SAMSUNG_MAX_ITERATORS) {
+    	printf("kv_list_iterators doesn't return correct count of open iterators\n");
+	exit(1);
+    }
+
+    for (int i = 0; i < SAMSUNG_MAX_ITERATORS; i++) {
+        kv_result ret = test_open_iterator(ns_hdl, sq_hdl, cq_hdl, iters);
+
+        if (ret != KV_ERR_TOO_MANY_ITERATORS_OPEN) {
+            printf("open iterators exceeding limit test failed, return code 0x%03X\n", ret);
+            exit(1);
+        }
+    }
+
+    // close opened iterators
+    test_close_iterator(ns_hdl, sq_hdl, cq_hdl, iters);
+
+    count = test_list_iterators(ns_hdl, sq_hdl, cq_hdl);
+    if (count != 0) {
+    	printf("kv_list_iterators doesn't return correct 0 count of after closing all iterators, instead returned %u\n", count);
+	exit(1);
+    }
+}
+
+
+// test iterators main routine
+uint64_t test_iterators(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count, kv_iterator_option iter_op) {
+    test_iterators_one_bitmask(ns_hdl, sq_hdl, cq_hdl, iohdls, count, iter_op);
+    test_iterators_multiple_bitmask(ns_hdl, sq_hdl, cq_hdl, iohdls, count, iter_op);
+    return 0;
+}
+
+// test cycle of read and write in rounds
+void API_test_main(kv_device_handle dev_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, int klen, int vlen, int tcount, int qdepth) {
+
+    // set up namespace
+    kv_namespace_handle ns_hdl = NULL;
+    get_namespace_default(dev_hdl, &ns_hdl);
+
+    // uint32_t prefix = 0xFFFF1234;
+    // XXX
+    uint32_t prefix = 0x12345678;
+    iohdl_t *iohdls = NULL;
+
+    // break it into many rounds to minimize q contention
+    uint32_t count = qdepth;
+    create_iohdls(&iohdls, prefix, klen, vlen, count);
+
+    uint32_t left = tcount;
+    // total IO time in ns
+    uint64_t total_w = 0;
+    uint64_t total_r = 0;
+
+    while (left > 0) {
+        if (left < count) {
+            count = left;
+        }
+        left -= count;
+
+        // set up new random key values
+        populate_iohdls(iohdls, prefix, klen, vlen, count);
+
+        // test write and read
+        uint64_t time_w = process_one_round_write(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+        total_w += time_w;
+
+        uint64_t time_r = process_one_round_read(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+        total_r += time_r;
+
+        // test no overwrite
+        process_one_round_write_duplicatekeys_notallowed(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+
+        // test overwrite
+        process_one_round_write_duplicatekeys_allowed(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+        process_one_round_read(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+
+        // test partial read
+        process_one_round_read_partial(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+
+        // test partial invalid read
+        process_one_round_read_offset_invalid(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+
+        // test kv exist
+        process_one_round_kv_exist(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+
+        // test delete then read
+        process_one_round_delete(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+
+        // test delete group
+        process_one_round_delete_group(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+
+        // test iterators
+        test_iterators(ns_hdl, sq_hdl, cq_hdl, iohdls, count, KV_ITERATOR_OPT_KEY);
+        test_iterators(ns_hdl, sq_hdl, cq_hdl, iohdls, count, KV_ITERATOR_OPT_KV);
+
+        // validate max # of open iterators is enforced
+        test_max_allowed_iterators(ns_hdl, sq_hdl, cq_hdl);
+    }
+
+    // done, free memories
+    free_iohdls(iohdls, qdepth);
+
+    printf("all tests succeeded\n");
+
+    /* only use kvperf to measure performance
+     *
+    // IOPS = IO count * 1000 * 1000 / time_ns
+    double iops_w = tcount * 1000.0 * 1000 / total_w;
+    double mean_iotime_w = total_w/1000.0/tcount;
+    printf("IOPS_w: %.1f K/s\n", iops_w);
+    printf("mean IO duration(write): %.1f us\n", mean_iotime_w);
+
+    // bytes * 10**9 / ( time * 1024 * 1024)
+    double throughput_w = tcount * (klen + vlen) / 1024.0 * 1000 * 1000 / total_w * 1000 / 1024;
+    printf("device throughput (write): %.1f MB/s\n", throughput_w);
+
+    double iops_r = tcount * 1000.0 * 1000 / total_r;
+    double mean_iotime_r = total_r/1000.0/tcount;
+    printf("IOPS_r: %.1f K/s\n", iops_r);
+    printf("mean IO duration(read): %.1f us\n", mean_iotime_r);
+
+    // bytes * 10**9 / ( time * 1024 * 1024)
+    double throughput_r = tcount * (klen + vlen) / 1024.0 * 1000 * 1000 / total_r * 1000 / 1024;
+    printf("device throughput (read): %.1f MB/s\n", throughput_r);
+
+    printf("IOs %u, IO time %llu (ns), app time %llu (ns)\n", tcount, (long long unsigned) total_w, (long long unsigned) (end - start));
+
+    */
+}
+
+void create_qpair(kv_device_handle dev_hdl,
+    std::map<kv_queue_handle, kv_queue_handle>& qpairs,
+    kv_queue_handle *sq_hdl,
+    kv_queue_handle *cq_hdl,
+    uint16_t sqid,
+    uint16_t cqid,
+    uint16_t q_depth,
+    kv_interrupt_handler *int_handler) {
+    
+    // create a CQ
+    // ONLY DO THIS, if you have NOT created the CQ earlier
+    create_cq(dev_hdl, cqid, q_depth, cq_hdl);
+
+    // this is only for completion Q
+    // this will kick off completion Q processing
+    // kv_set_interrupt_handler(cq_hdl, int_handler); 
+
+    // create a SQ
+    create_sq(dev_hdl, sqid, q_depth, cqid, sq_hdl);
+
+    qpairs.insert(std::make_pair(*sq_hdl, *cq_hdl));
+
+}
+
+
+int main(int argc, char**argv) {
+    
+    if (argc <  2) {
+        printf("This program will excerise all critical APIs.\n"
+               "If it runs into any errors during the test,\n"
+               "it will stop without reporting success.\n");
+
+        printf("    Please run\n  %s <number of keys>\n", argv[0]);
+        printf("    Default: %d\n", TOTAL_KV_COUNT);
+    } else {
+        TOTAL_KV_COUNT = std::stoi(argv[1]);
+        printf("    Keys to insert: %d\n", TOTAL_KV_COUNT);
+    }
+
+    int klen = 16;
+    int vlen = 4096;
+    int qdepth = 64;
+
+    // initialize globals
+    g_KEYLEN_FIXED = 0;
+    g_KEYLEN = klen;
+
+    kv_device_init_t dev_init;
+
+    // you can supply your own configuration file
+    dev_init.configfile = "kvssd_emul.conf";
+
+    // the emulator device path is fixed.
+    // it's virtual, you don't need to create it
+    dev_init.devpath = "/dev/kvemul";
+    dev_init.need_persistency = FALSE;
+    dev_init.is_polling = TRUE;
+
+    kv_device_handle dev_hdl = NULL;
+
+    // initialize the device
+    kv_result ret = kv_initialize_device(&dev_init, &dev_hdl);
+    if (ret != KV_SUCCESS) {
+        printf("device creation error\n");
+        exit(1);
+    }
+
+    // print device stats
+    kv_device devinfo;
+    kv_device_stat devst;
+    kv_get_device_info(dev_hdl, &devinfo);
+    kv_get_device_stat(dev_hdl, &devst);
+    printf("capacity is %luB\n", devinfo.capacity);
+    printf("device stats utilization %u\n", devst.utilization);
+    // printf("device stats ns count %u\n", devst.namespace_count);
+    // printf("device stats queue count %u\n", devst.queue_count);
+    // printf("device waf %u\n", devst.waf);
+ 
+    // set up interrupt handler
+    _kv_interrupt_handler int_func = {interrupt_func, (void *)4, 4};
+    kv_interrupt_handler int_handler = &int_func;
+
+    // to keep track all opened queue pairs
+    std::map<kv_queue_handle, kv_queue_handle> qpairs;
+    kv_queue_handle sq_hdl;
+    kv_queue_handle cq_hdl;
+
+    // convenient wrapper function
+    // create a SQ/CQ pair
+    create_qpair(dev_hdl, qpairs,  &sq_hdl, &cq_hdl,
+            1,  // sqid
+            2,  // cqid
+            qdepth, // q depth
+            &int_handler);
+
+    // start a thread to insert key values through submission Q
+    printf("starting thread for testing\n");
+    std::thread th = std::thread(API_test_main, dev_hdl, sq_hdl, cq_hdl, klen, vlen, TOTAL_KV_COUNT, qdepth);
+    if (th.joinable()) {
+        th.join();
+    }
+
+    // graceful shutdown
+    // watch if all Qs are done
+    for (auto& qpair : qpairs) {
+        kv_queue_handle sqhdl = qpair.first;
+        kv_queue_handle cqhdl = qpair.second;
+        while (get_queued_commands_count(cqhdl) > 0 || get_queued_commands_count(sqhdl) > 0) {
+            // wait for CQ to complete before shutdown
+            std::this_thread::sleep_for(std::chrono::milliseconds(10));
+        }
+    }
+    
+    // delete queues
+    for (auto& p : qpairs) {
+        if (kv_delete_queue(dev_hdl, p.first) != KV_SUCCESS) {
+            printf("kv_delete_queue failed\n");
+            exit(1);
+        }
+        if (kv_delete_queue(dev_hdl, p.second) != KV_SUCCESS) {
+            printf("kv_delete_queue failed\n");
+            exit(1);
+        }
+    }
+
+    // print device stats
+    kv_get_device_info(dev_hdl, &devinfo);
+    kv_get_device_stat(dev_hdl, &devst);
+    printf("capacity is %luB\n", devinfo.capacity);
+    printf("device stats utilization %u\n", devst.utilization);
+    // printf("device stats ns count %u\n", devst.namespace_count);
+    // printf("device stats queue count %u\n", devst.queue_count);
+    // printf("device waf %u\n", devst.waf);
+ 
+    // shutdown
+    kv_cleanup_device(dev_hdl);
+    
+    return 0;
+}
diff --git a/PDK/core/src/device_abstract_layer/include/kvs_adi.h b/PDK/core/src/device_abstract_layer/include/kvs_adi.h
index 6adc3c0..0b0ebd5 100644
--- a/PDK/core/src/device_abstract_layer/include/kvs_adi.h
+++ b/PDK/core/src/device_abstract_layer/include/kvs_adi.h
@@ -34,6 +34,7 @@
 #define SAMSUNG_KVS_ADI_H
 
 #include "stdint.h"
+#include "kvs_result.h"
 
 #ifdef __cplusplus
 extern "C" {
@@ -49,21 +50,16 @@ extern "C" {
 
 #define SAMSUNG_MAX_ITERATORS 16
 
-#define SAMSUNG_MAX_KEYSPACE_CNT 2
-#define SAMSUNG_MIN_KEYSPACE_ID 0
-#define KV_ALIGNMENT_UNIT 512
-
-
 /**
  * return value from all interfaces
  */
 typedef int32_t kv_result;
 
     // Generic command status                
-#define    KV_SUCCESS                            0x0        ///< success
+#define    KV_SUCCESS                            0x0      ///< success
 
 // warnings
-#define    KV_WRN_MORE                          0xF000        ///< more data is available, but buffer is not enough
+#define    KV_WRN_MORE                          0xF000    ///< more data is available, but buffer is not enough
 
 // errors                  
 #define    KV_ERR_DEV_CAPACITY                  0x004     ///< device does not have enough space
@@ -73,7 +69,6 @@ typedef int32_t kv_result;
 #define    KV_ERR_DEV_SANITIZE_FAILED           0x008     ///< the previous sanitize operation failed
 
 #define    KV_ERR_ITERATOR_NOT_EXIST            0x00C     ///< no iterator exists
-#define    KV_ERR_ITERATOR_ALREADY_OPEN         0x00D     ///< terator is already open
 #define    KV_ERR_KEY_INVALID                   0x00F     ///< key invalid (value of key is NULL)
 #define    KV_ERR_KEY_LENGTH_INVALID            0x010     ///< key length is out of range (unsupported key length)
 #define    KV_ERR_KEY_NOT_EXIST                 0x011     ///< given key doesn't exist
@@ -88,8 +83,6 @@ typedef int32_t kv_result;
 #define    KV_ERR_VALUE_OFFSET_INVALID          0x023     ///< value offset is invalid meaning that offset is out of bound.
 #define    KV_ERR_VENDOR                        0x025     ///< vendor-specific error is returned, check the system log for more details
 #define    KV_ERR_PERMISSION                    0x026     ///< unable to open device due to permission error
-#define    KV_ERR_MISALIGNED_VALUE_OFFSET       0x20C     ///< misaligned value offset
-
 
 // command specific status(errors)               
 #define    KV_ERR_BUFFER_SMALL                  0x001     ///< provided buffer size too small for iterator_next operation
@@ -137,10 +130,7 @@ typedef int32_t kv_result;
 ///< device driver does not support.
 #define    KV_ERR_DD_UNSUPPORTED       0x02C 
 
-//device does not support the specified keyspace
-#define KV_ERR_KEYSPACE_INVALID        0x031
-
-/**
+/** 
  * \mainpage A libary for Samsung Key-Value Storage ADI
  */
 
@@ -179,7 +169,7 @@ typedef enum cmd_opcode_t {
  * This type is used to represent a key length. Currently uint8_t is
  * used to represent a key which can grow up to KV_MAX_KEY_LEN bytes.
  */
-typedef uint8_t kv_key_t;
+typedef uint16_t kv_key_t;
 
 /** 
  * This type is used to represent a value length. Currently uint32_t is
@@ -502,6 +492,8 @@ typedef struct {
   void *pattern;            ///< buffer address for sanize overwrite pattern 
 } kv_sanitize_pattern; 
 
+
+
 /**
  * \defgroup device_interfaces
  */
@@ -657,6 +649,7 @@ typedef struct {
     kv_group_condition *grp_cond;
 } op_delete_group_struct_t;
 
+
 ////////////////////////////////
 // this part must be the same as the public portion of 
 // io_ctx_t
@@ -703,19 +696,12 @@ typedef struct {
         kv_iterator_handle hiter;
     } result;
 
-    struct {
-        int id;
-        bool end;
-        void *buf;
-        int buflength;
-    } hiter;
 
 //private
     void (*post_fn)(kv_io_context *op);   ///< asynchronous notification callback (valid only for async I/O)
     uint32_t timeout_usec;
     uint16_t qid;
     uint32_t nsid;
-    int8_t ks_id;
 
     // command specific structure
     // see structures defined above
@@ -1127,7 +1113,11 @@ kv_result kv_get_namespace_stat(const kv_device_handle dev_hdl, const kv_namespa
   KV_ERR_SYS_PERMISSION 		this caller does not have a permission to call this interface
   KV_ERR_VENDOR			vendor-specific error is returned, check the system log for more details 
   */
-kv_result kv_purge(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, kv_purge_option option, kv_postprocess_function *post_fn);
+kv_result kv_purge(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_purge_option option, kv_postprocess_function *post_fn);
+
+  kv_result kv_lock(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, uint8_t read_lock, uint8_t is_blocking, uint8_t lock_prio, 
+                         uint32_t lock_duration, uint64_t instance_uuid, const kv_postprocess_function *cb);
+  kv_result kv_unlock(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, uint8_t read_unlock, uint64_t instance_uuid, const kv_postprocess_function *cb);
 
 /**
   \ingroup Iterator_Interfaces
@@ -1171,9 +1161,9 @@ kv_result kv_purge(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t
   KV_ERR_VENDOR			vendor-specific error is returned, check the system log for more details
  
   */
-kv_result kv_open_iterator(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_iterator_option it_op, const kv_group_condition *it_cond, kv_postprocess_function *post_fn);
+kv_result kv_open_iterator(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_iterator_option it_op, const kv_group_condition *it_cond, kv_postprocess_function *post_fn);
 
-kv_result kv_open_iterator_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_iterator_option it_op, const kv_group_condition *it_cond, uint8_t *iterhandle);
+kv_result kv_open_iterator_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_iterator_option it_op, const kv_group_condition *it_cond, uint8_t *iterhandle);
 kv_result kv_close_iterator_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_iterator_handle iter_hdl);
 kv_result kv_list_iterators_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_iterator *kv_iters, uint32_t *iter_cnt);
 kv_result kv_iterator_next_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_iterator_handle iter_hdl, kv_iterator_list *iter_list);
@@ -1319,7 +1309,7 @@ kv_result kv_list_iterators(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
   KV_ERR_SYS_PERMISSION 		this caller does not have a permission to call this interface
   KV_ERR_VENDOR			vendor-specific error is returned, check the system log for more details
   */
-kv_result kv_delete(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *key, kv_delete_option option, kv_postprocess_function *post_fn);
+kv_result kv_delete(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_delete_option option, kv_postprocess_function *post_fn);
 
 
 /**
@@ -1351,7 +1341,7 @@ kv_result kv_delete(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t
   KV_ERR_PARAM_INVALID       grp_cond or op_hdl cannot be NULL, both shall have been already allocated
   KV_ERR_VENDOR           vendor-specific error is returned, check the system log for more details
  */
-kv_result kv_delete_group(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, kv_group_condition *grp_cond, kv_postprocess_function *post_fn);
+kv_result kv_delete_group(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_group_condition *grp_cond, kv_postprocess_function *post_fn);
 
 /**
   kv_exist
@@ -1392,7 +1382,7 @@ kv_result kv_delete_group(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, u
   KV_ERR_SYS_PERMISSION 		this caller does not have a permission to call this interface
   KV_ERR_VENDOR			vendor-specific error is returned, check the system log for more details
   */
-kv_result kv_exist(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *keys, uint32_t keycount, uint32_t buffer_size, uint8_t *buffer, kv_postprocess_function *post_fn);
+kv_result kv_exist(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *keys, uint32_t keycount, uint32_t buffer_size, uint8_t *buffer, kv_postprocess_function *post_fn);
 
 /**
   kv_retrieve 
@@ -1441,7 +1431,14 @@ kv_result kv_exist(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t
   KV_ERR_VALUE_LENGTH_INVALID	the value length is out of range of kv_device.min_value_len and kv_device.max_value_len
   KV_ERR_VENDOR			vendor-specific error is returned, check the system log for more details
   */
-kv_result kv_retrieve(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *key, kv_retrieve_option option, kv_value *value,  const kv_postprocess_function *post_fn);
+kv_result kv_retrieve(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_retrieve_option option, kv_value *value,  const kv_postprocess_function *post_fn);
+
+
+kv_result kv_retrieve_direct(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_retrieve_option option, const kv_value *value, 
+                             uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kv_postprocess_function *post_fn);
+
+kv_result kv_store_direct(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_store_option option, const kv_value *value,
+                             uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kv_postprocess_function *post_fn);
 
 /**
   kv_store
@@ -1494,7 +1491,56 @@ kv_result kv_retrieve(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8
   KV_ERR_VENDOR			vendor-specific error is returned, check the system log for more details
 
   */
-kv_result kv_store(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *key, const kv_value *value, kv_store_option option, const kv_postprocess_function *post_fn);
+kv_result kv_store(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, const kv_value *value, kv_store_option option, const kv_postprocess_function *post_fn);
+
+
+/**
+  kv_list
+
+  This interface shall post an operation to a submission queue of device to store a key-value pair. This routine works asynchronously and returns immediately regardless of whether the pair is actually stored to a device or not. Since one of the benefits of using key value interface is to avoid unnecessary read-modify-write, partial update is not allowed, and the kv_value.offset parameter is ignored.
+
+  The precise behavior of this interface is determined by the options set by users.
+  * KVS_LIST_START_ROOT: no prefix, list everything from root
+  * KVS_LIST_START_KEY: no prefix, list from given key
+  * KVS_LIST_PREFIX_START_ROOT: with key as prefix, list from root
+  * KVS_LIST_PRFEIX_START_KEY: with key as prefix and start key, list from given key
+
+  If a user defines an interrupt handler (i.e., kv_set_interrupt_handler()) and a postprocess function (i.e., post_fn), the interrupt handler will call the postprocess function when the device triggers an interrupt to notify the completion of the operation. If no postprocess function is defined, the interrupt handler just finishes its operation. If no interrupt handler is defined but a postprocess function is defined, the function is ignored.
+
+  If a postprocess function (i.e., post_fn) is defined, the interrupt handler (an interrupt handler defined by kv_set_interrupt_handler()) or the poller (kv_poll_completion()) will call the postprocess function when the device triggers an interrupt to notify the completion of the operation or when the poller detects that the device finished the specified operation.
+
+
+  [SAMSUNG]
+  The valid key size for Samsung PM983 is between 16 and 255 bytes, and the valid value size is between 64 bytes and 2MB.
+
+  PARAMETERS
+  IN que_hdl	queue handle
+  IN ns_hdl		namespace handle, or KV_NAMESPACE_DEFAULT
+  IN key		key
+  IN value		value
+  IN post_fn	a postprocess function which is called when the operation completes
+
+  RETURNS
+  KV_SUCCESS
+
+  KV_WRN_OVERWRITTEN		this store operation overwrote a mutable object created by kv_append()
+
+  ERROR CODE
+  KV_ERR_DEV_NOT_EXIST 		no device exists
+  KV_ERR_KEY_INVALID		key format is invalid, or kv_key->key buffer is null
+  KV_ERR_KEY_LENGTH_INVALID	the key length is out of range of kv_device.min_key_len and kv_device.max_key_len
+  KV_ERR_NS_NOT_EXIST		the namespace does not exist
+  KV_ERR_OPTION_INVALID		the device does not support the specified options
+  KV_ERR_PARAM_INVALID 		key or value cannot be NULL, both shall have been already allocated
+  KV_ERR_QUEUE_QID_INVALID	submission queue identifier is invalid
+  KV_ERR_SYS_IO 			the host failed to communicate with the device
+  KV_ERR_SYS_PERMISSION 		this caller does not have a permission to call this interface
+  KV_ERR_VALUE_LENGTH_INVALID	the value length is out of range of kv_device.min_value_len and kv_device.max_value_len
+  KV_ERR_VENDOR			vendor-specific error is returned, check the system log for more details
+
+  */
+kv_result kv_list(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, uint8_t key_offset, uint16_t max_keys_to_list, kv_value *value, const kv_postprocess_function *post_fn);
+
 
 /**
  \ingroup Completion Interfaces
diff --git a/PDK/core/src/device_abstract_layer/include/private/history.hpp b/PDK/core/src/device_abstract_layer/include/private/history.hpp
index 42efa9a..25d1d67 100644
--- a/PDK/core/src/device_abstract_layer/include/private/history.hpp
+++ b/PDK/core/src/device_abstract_layer/include/private/history.hpp
@@ -39,7 +39,6 @@
 #include "math.h"
 #include "stdint.h"
 #include <vector>
-#include <cmath>
 
 enum op_type {
     STAT_FIRST  =0,
diff --git a/PDK/core/src/device_abstract_layer/include/private/kv_device.hpp b/PDK/core/src/device_abstract_layer/include/private/kv_device.hpp
index ae489e9..fd7f350 100644
--- a/PDK/core/src/device_abstract_layer/include/private/kv_device.hpp
+++ b/PDK/core/src/device_abstract_layer/include/private/kv_device.hpp
@@ -154,9 +154,9 @@ public:
     static kv_result _kv_bypass_namespace(const kv_device_handle dev_hdl, const kv_namespace_handle ns_hdl, bool_t bypass);
 
     // async IO APIs are below
-    kv_result kv_purge(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, kv_purge_option option, kv_postprocess_function *post_fn);
+    kv_result kv_purge(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_purge_option option, kv_postprocess_function *post_fn);
 
-    kv_result kv_open_iterator(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_iterator_option it_op, const kv_group_condition *it_cond, kv_postprocess_function *post_fn);
+    kv_result kv_open_iterator(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_iterator_option it_op, const kv_group_condition *it_cond, kv_postprocess_function *post_fn);
 
     kv_result kv_close_iterator(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_postprocess_function *post_fn, kv_iterator_handle iter_hdl);
 
@@ -167,14 +167,18 @@ public:
     kv_result kv_list_iterators(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_postprocess_function  *post_fn, kv_iterator *kv_iters, uint32_t *iter_cnt);
 
     /*** Key value APIs ***/
-    kv_result kv_delete(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *key, kv_delete_option option, kv_postprocess_function *post_fn);
+    kv_result kv_delete(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_delete_option option, kv_postprocess_function *post_fn);
     
-    kv_result kv_delete_group(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, kv_group_condition *grp_cond, kv_postprocess_function *post_fn);
+    kv_result kv_delete_group(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_group_condition *grp_cond, kv_postprocess_function *post_fn);
 
-    kv_result kv_exist(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *key, uint32_t key_cnt, kv_postprocess_function *post_fn, uint32_t buffer_size, uint8_t *buffer);
+    kv_result kv_exist(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, uint32_t key_cnt, kv_postprocess_function *post_fn, uint32_t buffer_size, uint8_t *buffer);
 
-    kv_result kv_retrieve(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *key, kv_retrieve_option option, const kv_postprocess_function *post_fn, kv_value *value);
-    kv_result kv_store(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, uint8_t ks_id, const kv_key *key, const kv_value *value, kv_store_option option, const kv_postprocess_function *post_fn);
+    kv_result kv_retrieve(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_retrieve_option option, const kv_postprocess_function *post_fn, kv_value *value);
+    kv_result kv_store(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, const kv_value *value, kv_store_option option, const kv_postprocess_function *post_fn);
+    kv_result kv_list(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, uint8_t key_offset, uint16_t max_keys_to_list, kv_value *value, const kv_postprocess_function *post_fn);
+
+    kv_result kv_lock(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, uint8_t read_lock, uint8_t is_blocking, uint8_t lock_prio, uint32_t lock_duration, uint64_t instance_uuid, const kv_postprocess_function *post_fn);
+    kv_result kv_unlock(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, uint8_t read_unlock, uint64_t instance_uuid, const kv_postprocess_function *post_fn);
     /*** poll and interrupt handler APIs***/
     // poll will check completion queue, and find corresponding submission
     // queue
diff --git a/PDK/core/src/device_abstract_layer/include/private/kv_emulator.hpp b/PDK/core/src/device_abstract_layer/include/private/kv_emulator.hpp
index 8b499e4..5a2826e 100644
--- a/PDK/core/src/device_abstract_layer/include/private/kv_emulator.hpp
+++ b/PDK/core/src/device_abstract_layer/include/private/kv_emulator.hpp
@@ -88,18 +88,19 @@ public:
     virtual ~kv_noop_emulator() {}
 
     // basic operations
-    kv_result kv_store(uint8_t ks_id, const kv_key *key, const kv_value *value, uint8_t option, uint32_t *consumed_bytes, void *ioctx) { return KV_SUCCESS; }
-    kv_result kv_retrieve(uint8_t ks_id, const kv_key *key, uint8_t option, kv_value *value, void *ioctx) { return KV_SUCCESS; }
-    kv_result kv_exist(uint8_t ks_id, const kv_key *key, uint32_t keycount, uint8_t *value, uint32_t &valuesize, void *ioctx) { return KV_SUCCESS; }
-    kv_result kv_purge( uint8_t ks_id, kv_purge_option option, void *ioctx) { return KV_SUCCESS; }
-    kv_result kv_delete(uint8_t ks_id, const kv_key *key, uint8_t option, uint32_t *recovered_bytes, void *ioctx) { return KV_SUCCESS; }
+    kv_result kv_store(const kv_key *key, const kv_value *value, uint8_t option, uint32_t *consumed_bytes, void *ioctx) { return KV_SUCCESS; }
+    kv_result kv_retrieve(const kv_key *key, uint8_t option, kv_value *value, void *ioctx) { return KV_SUCCESS; }
+    kv_result kv_exist(const kv_key *key, uint32_t keycount, uint8_t *value, uint32_t &valuesize, void *ioctx) { return KV_SUCCESS; }
+    kv_result kv_purge(kv_purge_option option, void *ioctx) { return KV_SUCCESS; }
+    kv_result kv_delete(const kv_key *key, uint8_t option, uint32_t *recovered_bytes, void *ioctx) { return KV_SUCCESS; }
+
     // iterator
-    kv_result kv_open_iterator(uint8_t ks_id, const kv_iterator_option opt, const kv_group_condition *cond, bool_t keylen_fixed, kv_iterator_handle *iter_hdl, void *ioctx) { return KV_SUCCESS; }
+    kv_result kv_open_iterator(const kv_iterator_option opt, const kv_group_condition *cond, bool_t keylen_fixed, kv_iterator_handle *iter_hdl, void *ioctx) { return KV_SUCCESS; }
     kv_result kv_close_iterator(kv_iterator_handle iter_hdl, void *ioctx) { return KV_SUCCESS; }
     kv_result kv_iterator_next(kv_iterator_handle iter_hdl, kv_key *key, kv_value *value, void *ioctx) { return KV_SUCCESS; }
     kv_result kv_iterator_next_set(kv_iterator_handle iter_hdl, kv_iterator_list *iter_list, void *ioctx) { return KV_SUCCESS; }
     kv_result kv_list_iterators(kv_iterator *iter_list, uint32_t *count, void *ioctx) { return KV_SUCCESS; }
-    kv_result kv_delete_group( uint8_t ks_id, kv_group_condition *grp_cond, uint64_t *recovered_bytes, void *ioctx) { return KV_SUCCESS; }
+    kv_result kv_delete_group(kv_group_condition *grp_cond, uint64_t *recovered_bytes, void *ioctx) { return KV_SUCCESS; }
 
     kv_result set_interrupt_handler(const kv_interrupt_handler int_hdl) { return KV_SUCCESS; }
     kv_interrupt_handler get_interrupt_handler() { return NULL; }
@@ -115,18 +116,19 @@ public:
     virtual ~kv_emulator();
 
     // basic operations
-    kv_result kv_store(uint8_t ks_id, const kv_key *key, const kv_value *value, uint8_t option, uint32_t *consumed_bytes, void *ioctx);
-    kv_result kv_retrieve(uint8_t ks_id, const kv_key *key, uint8_t option, kv_value *value, void *ioctx);
-    kv_result kv_exist(uint8_t ks_id, const kv_key *key, uint32_t keycount, uint8_t *value, uint32_t &valuesize, void *ioctx);
-    kv_result kv_purge( uint8_t ks_id, kv_purge_option option, void *ioctx);
-    kv_result kv_delete(uint8_t ks_id, const kv_key *key, uint8_t option, uint32_t *recovered_bytes, void *ioctx);
+    kv_result kv_store(const kv_key *key, const kv_value *value, uint8_t option, uint32_t *consumed_bytes, void *ioctx);
+    kv_result kv_retrieve(const kv_key *key, uint8_t option, kv_value *value, void *ioctx);
+    kv_result kv_exist(const kv_key *key, uint32_t keycount, uint8_t *value, uint32_t &valuesize, void *ioctx);
+    kv_result kv_purge(kv_purge_option option, void *ioctx);
+    kv_result kv_delete(const kv_key *key, uint8_t option, uint32_t *recovered_bytes, void *ioctx);
+
     // iterator
-    kv_result kv_open_iterator(uint8_t ks_id, const kv_iterator_option opt, const kv_group_condition *cond, bool_t keylen_fixed, kv_iterator_handle *iter_hdl, void *ioctx);
+    kv_result kv_open_iterator(const kv_iterator_option opt, const kv_group_condition *cond, bool_t keylen_fixed, kv_iterator_handle *iter_hdl, void *ioctx);
     kv_result kv_close_iterator(kv_iterator_handle iter_hdl, void *ioctx);
     kv_result kv_iterator_next_set(kv_iterator_handle iter_hdl, kv_iterator_list *iter_list, void *ioctx);
     kv_result kv_iterator_next(kv_iterator_handle iter_hdl, kv_key *key, kv_value *value, void *ioctx);
     kv_result kv_list_iterators(kv_iterator *iter_list, uint32_t *count, void *ioctx);
-    kv_result kv_delete_group( uint8_t ks_id, kv_group_condition *grp_cond, uint64_t *recovered_bytes, void *ioctx);
+    kv_result kv_delete_group(kv_group_condition *grp_cond, uint64_t *recovered_bytes, void *ioctx);
 
     uint64_t get_total_capacity();
     uint64_t get_available();
@@ -149,7 +151,7 @@ private:
 
     typedef std::map<kv_key*, std::string, CmpEmulPrefix> emulator_map_t;
     //std::map<uint32_t, std::unordered_map<kv_key*, std::string> > m_map;
-    std::map<kv_key*, std::string, CmpEmulPrefix> m_map[SAMSUNG_MAX_KEYSPACE_CNT];
+    std::map<kv_key*, std::string, CmpEmulPrefix> m_map;
     std::mutex m_map_mutex;
 
     std::map<int32_t, _kv_iterator_handle *> m_it_map;
diff --git a/PDK/core/src/device_abstract_layer/include/private/kv_namespace.hpp b/PDK/core/src/device_abstract_layer/include/private/kv_namespace.hpp
index 14acd80..b146ea1 100644
--- a/PDK/core/src/device_abstract_layer/include/private/kv_namespace.hpp
+++ b/PDK/core/src/device_abstract_layer/include/private/kv_namespace.hpp
@@ -63,17 +63,18 @@ public:
     kv_result kv_get_namespace_stat(kv_namespace_stat *ns_st);
 
     // all these are sync IO, directly working with kvstore
-    kv_result kv_purge( uint8_t ks_id, kv_purge_option option, void *ioctx);
-    kv_result kv_delete(uint8_t ks_id, const kv_key *key, uint8_t option, uint32_t *recovered_bytes, void *ioctx);
-    kv_result kv_exist(uint8_t ks_id, const kv_key *key, uint32_t keycount, uint8_t *value, uint32_t &valuesize, void *ioctx);
-    kv_result kv_retrieve(uint8_t ks_id, const kv_key *key, uint8_t option, kv_value *value, void *ioctx);
-    kv_result kv_store(uint8_t ks_id, const kv_key *key, const kv_value *value, uint8_t option, uint32_t *consumed_bytes, void *ioctx);
-    kv_result kv_open_iterator(uint8_t ks_id, const kv_iterator_option it_op, const kv_group_condition *it_cond, kv_iterator_handle *iter_hdl, void *ioctx);
+    kv_result kv_purge(kv_purge_option option, void *ioctx);
+    kv_result kv_delete(const kv_key *key, uint8_t option, uint32_t *recovered_bytes, void *ioctx);
+    kv_result kv_exist(const kv_key *key, uint32_t keycount, uint8_t *value, uint32_t &valuesize, void *ioctx);
+    kv_result kv_retrieve(const kv_key *key, uint8_t option, kv_value *value, void *ioctx);
+    kv_result kv_store(const kv_key *key, const kv_value *value, uint8_t option, uint32_t *consumed_bytes, void *ioctx);
+
+    kv_result kv_open_iterator(const kv_iterator_option it_op, const kv_group_condition *it_cond, kv_iterator_handle *iter_hdl, void *ioctx);
     kv_result kv_close_iterator(kv_iterator_handle iter_hdl, void *ioctx);
     kv_result kv_iterator_next(kv_iterator_handle iter_hdl, kv_key *key, kv_value *value, void *ioctx);
     kv_result kv_iterator_next_set(kv_iterator_handle iter_hdl, kv_iterator_list *iter_list, void *ioctx);
     kv_result kv_list_iterators(kv_iterator *kv_iters, uint32_t *iter_cnt, void *ioctx);
-    kv_result kv_delete_group( uint8_t ks_id, kv_group_condition *grp_cond, uint64_t *recovered_bytes, void *ioctx);
+    kv_result kv_delete_group(kv_group_condition *grp_cond, uint64_t *recovered_bytes, void *ioctx);
 
     kv_result set_interrupt_handler(const kv_interrupt_handler int_hdl);
     kv_interrupt_handler get_interrupt_handler();
diff --git a/PDK/core/src/device_abstract_layer/include/private/kvs_adi_internal.h b/PDK/core/src/device_abstract_layer/include/private/kvs_adi_internal.h
index 70d2c17..916cdf2 100644
--- a/PDK/core/src/device_abstract_layer/include/private/kvs_adi_internal.h
+++ b/PDK/core/src/device_abstract_layer/include/private/kvs_adi_internal.h
@@ -48,19 +48,19 @@ class kv_device_api {
 public:
     virtual ~kv_device_api() {}
     //basic operations
-    virtual kv_result kv_store(uint8_t ks_id, const kv_key *key, const kv_value *value, uint8_t option, uint32_t *consumed_bytes, void *ioctx) =0;
-    virtual kv_result kv_retrieve(uint8_t ks_id, const kv_key *key, uint8_t option, kv_value *value, void *ioctx) =0;
-    virtual kv_result kv_exist(uint8_t ks_id, const kv_key *key, uint32_t keycount, uint8_t *value, uint32_t &valuesize, void *ioctx) =0;
-    virtual kv_result kv_purge(uint8_t ks_id, kv_purge_option option, void *ioctx) =0;
-    virtual kv_result kv_delete(uint8_t ks_id, const kv_key *key, uint8_t option, uint32_t *recovered_bytes, void *ioctx) =0;
+    virtual kv_result kv_store(const kv_key *key, const kv_value *value, uint8_t option, uint32_t *consumed_bytes, void *ioctx) =0;
+    virtual kv_result kv_retrieve(const kv_key *key, uint8_t option, kv_value *value, void *ioctx) =0;
+    virtual kv_result kv_exist(const kv_key *key, uint32_t keycount, uint8_t *value, uint32_t &valuesize, void *ioctx) =0;
+    virtual kv_result kv_purge(kv_purge_option option, void *ioctx) =0;
+    virtual kv_result kv_delete(const kv_key *key, uint8_t option, uint32_t *recovered_bytes, void *ioctx) =0;
 
     // iterator
-    virtual kv_result kv_open_iterator(uint8_t ks_id, const kv_iterator_option opt, const kv_group_condition *cond, bool_t keylen_fixed, kv_iterator_handle *iter_hdl, void *ioctx) =0;
+    virtual kv_result kv_open_iterator(const kv_iterator_option opt, const kv_group_condition *cond, bool_t keylen_fixed, kv_iterator_handle *iter_hdl, void *ioctx) =0;
     virtual kv_result kv_close_iterator(kv_iterator_handle iter_hdl, void *ioctx) =0;
     virtual kv_result kv_iterator_next_set(kv_iterator_handle iter_hdl, kv_iterator_list *iter_list, void *ioctx) =0;
     virtual kv_result kv_iterator_next(kv_iterator_handle iter_hdl, kv_key *key, kv_value *value, void *ioctx) =0;
     virtual kv_result kv_list_iterators(kv_iterator *iter_list, uint32_t *count, void *ioctx) =0;
-    virtual kv_result kv_delete_group(uint8_t ks_id, kv_group_condition *grp_cond, uint64_t *recovered_bytes, void *ioctx) =0;
+    virtual kv_result kv_delete_group(kv_group_condition *grp_cond, uint64_t *recovered_bytes, void *ioctx) =0;
 
     // device setup related
     // Only good for physical devices.
@@ -117,7 +117,6 @@ struct _kv_iterator_handle {
     int keylength;
 
     uint32_t nsid;
-    int8_t ksid;
 
     kv_iterator_option it_op;
     kv_group_condition it_cond;
diff --git a/PDK/core/src/device_abstract_layer/include/private/kvs_utils.h b/PDK/core/src/device_abstract_layer/include/private/kvs_utils.h
index 45da697..9206381 100644
--- a/PDK/core/src/device_abstract_layer/include/private/kvs_utils.h
+++ b/PDK/core/src/device_abstract_layer/include/private/kvs_utils.h
@@ -116,12 +116,12 @@ inline void write_info(FILE * out, const char* format, ... ) {
 
 
 #ifdef ENABLE_LOGGING
-	#define WRITE_ERR(...) do { write_err(__VA_ARGS__);} while (0)
+	#define WRITE_ERR(...) do { write_err(__VA_ARGS__); exit(1); } while (0)
 	#define WRITE_WARN(...) write_warn(stderr, __VA_ARGS__)
 	#define WRITE_INFO(...) write_info(stdout, __VA_ARGS__)
 	#define WRITE_LOG(...) logprintf(__VA_ARGS__)
 #else
-	#define WRITE_ERR(...) do { write_err(__VA_ARGS__);} while (0)
+	#define WRITE_ERR(...) do { write_err(__VA_ARGS__); exit(1); } while (0)
 	#define WRITE_WARN(...) write_warn(stderr, __VA_ARGS__)
 	#define WRITE_INFO(...) write_info(stdout, __VA_ARGS__)
 	#define WRITE_LOG(...)
@@ -228,9 +228,6 @@ static inline kv_result validate_key_value(const kv_key *key, const kv_value *va
         if (value->length < SAMSUNG_KV_MIN_VALUE_LEN || value->length > SAMSUNG_KV_MAX_VALUE_LEN) {
             return KV_ERR_VALUE_LENGTH_INVALID;
         }
-        if(value->offset & (KV_ALIGNMENT_UNIT - 1)) {
-          return KV_ERR_MISALIGNED_VALUE_OFFSET;
-        }
     }
 
     return KV_SUCCESS;
diff --git a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/README b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/README
index 92fba96..0429156 100644
--- a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/README
+++ b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/README
@@ -31,9 +31,9 @@ library and test binaries are at:
 How to test with KVSSD using kernel module (tenative)
     1. load the correct version of kernel module for KVSSD
     2. run test
-        sudo LD_LIBRARY_PATH=. ./test_suite 12356 0xffffffff 0x12345678 /dev/nvme2n1 0
-        sudo LD_LIBRARY_PATH=. ./sample_interrupt 2000 /dev/nvme2n1 0 
-        sudo LD_LIBRARY_PATH=. ./sample_poll 2000 /dev/nvme2n1 0    
+        sudo LD_LIBRARY_PATH=. ./test_suite 12356 0xffffffff 0x12345678 /dev/nvme2n1
+        sudo LD_LIBRARY_PATH=. ./sample_interrupt 2000 /dev/nvme2n1    
+        sudo LD_LIBRARY_PATH=. ./sample_poll 2000 /dev/nvme2n1    
 
 6). 
 ----
diff --git a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kadi.cpp b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kadi.cpp
index e7af33f..c9416e2 100644
--- a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kadi.cpp
+++ b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kadi.cpp
@@ -25,6 +25,7 @@
 #include "kadi.h"
 #include "linux_nvme_ioctl.h"
 #include "kadi_debug.h"
+#include <errno.h>
 
 //#define DUMP_ISSUE_CMD 1
 
@@ -38,10 +39,10 @@ struct epoll_event list_of_events[1];
 
 const int identify_ret_data_size = 4096;
 
-kv_result KADI::iter_readall(uint8_t ks_id, kv_iter_context *iter_ctx, 
+kv_result KADI::iter_readall(kv_iter_context *iter_ctx, 
     nvme_kv_iter_req_option option, std::list<std::pair<void *, int>> &buflist)
 {
-    kv_result r = iter_open(ks_id, iter_ctx, option);
+    kv_result r = iter_open(iter_ctx, option);
     if (r != 0)
         return r;
     while (!iter_ctx->end)
@@ -84,9 +85,7 @@ int KADI::open(std::string &devpath)
     for (int i = 0; i < qdepth; i++)
     {
         aio_cmd_ctx *ctx = (aio_cmd_ctx *)calloc(1, sizeof(aio_cmd_ctx));
-        if(ctx){
-            ctx->index = i;
-        }
+        ctx->index = i;
         free_cmdctxs.push_back(ctx);
     }
 #ifdef EPOLL_DEV
@@ -162,10 +161,7 @@ int KADI::close()
             free((void*)p);
         }
 
-        if(ioctl(fd, NVME_IOCTL_DEL_AIOCTX, &aioctx) < 0){
-            std::cerr << "KV device is closed error!" << std::endl;
-            return KADI_ERR_IO;
-        }
+        ioctl(fd, NVME_IOCTL_DEL_AIOCTX, &aioctx);
         ::close((int)aioctx.eventfd);
         ::close(fd);
         std::cerr << "KV device is closed: fd " << fd << std::endl;
@@ -181,6 +177,7 @@ int KADI::close()
 KADI::aio_cmd_ctx *KADI::get_cmd_ctx(const kv_postprocess_function *cb)
 {
     std::unique_lock<std::mutex> lock(cmdctx_lock);
+
     while (free_cmdctxs.empty())
     {
         if (cmdctx_cond.wait_for(lock, std::chrono::seconds(5)) == std::cv_status::timeout)
@@ -191,13 +188,9 @@ KADI::aio_cmd_ctx *KADI::get_cmd_ctx(const kv_postprocess_function *cb)
 
     aio_cmd_ctx *p = free_cmdctxs.back();
     free_cmdctxs.pop_back();
-    if(cb) {
-      p->post_fn = cb->post_fn;
-      p->post_data = cb->private_data;
-    }else {
-      p->post_fn = NULL;
-      p->post_data = NULL;
-    }
+
+    p->post_fn = cb->post_fn;
+    p->post_data = cb->private_data;
     pending_cmdctxs.insert(std::make_pair(p->index, p));
     return p;
 }
@@ -211,14 +204,13 @@ void KADI::release_cmd_ctx(aio_cmd_ctx *p)
     cmdctx_cond.notify_one();
 }
 
-kv_result KADI::iter_open(uint8_t ks_id, kv_iter_context *iter_handle,
-  nvme_kv_iter_req_option option)
+kv_result KADI::iter_open(kv_iter_context *iter_handle, nvme_kv_iter_req_option option)
 {
     struct nvme_passthru_kv_cmd cmd;
     memset(&cmd, 0, sizeof(struct nvme_passthru_kv_cmd));
 
     cmd.opcode = nvme_cmd_kv_iter_req;
-    cmd.cdw3 = ks_id;
+    cmd.cdw3 = space_id;
     cmd.nsid = nsid;
     cmd.cdw4 = (ITER_OPTION_OPEN | option);
     cmd.cdw12 = iter_handle->prefix;
@@ -243,7 +235,7 @@ kv_result KADI::iter_close(kv_iter_context *iter_handle)
     struct nvme_passthru_kv_cmd cmd;
     memset(&cmd, 0, sizeof(struct nvme_passthru_kv_cmd));
     cmd.opcode = nvme_cmd_kv_iter_req;
-    //cmd.cdw3 = space_id;  //iterator close and read don't need keyspace id
+    cmd.cdw3 = space_id;
     cmd.nsid = nsid;
     cmd.cdw4 = ITER_OPTION_CLOSE;
     cmd.cdw5 = iter_handle->handle;
@@ -265,7 +257,7 @@ kv_result KADI::iter_read_async(kv_iter_context *iter_handle, const kv_postproce
     ioctx->buf = iter_handle->buf;
     ioctx->cmd.opcode = nvme_cmd_kv_iter_read;
     ioctx->cmd.nsid = nsid;
-    //ioctx->cmd.cdw3 = space_id; //iterator close and read don't need keyspace id
+    ioctx->cmd.cdw3 = space_id;
     ioctx->cmd.cdw5 = iter_handle->handle;
     ioctx->cmd.data_addr = (__u64)iter_handle->buf;
     ioctx->cmd.data_length = iter_handle->buflen;
@@ -292,7 +284,7 @@ kv_result KADI::iter_read(kv_iter_context *iter_handle)
 
     cmd.opcode = nvme_cmd_kv_iter_read;
     cmd.nsid = nsid;
-    //cmd.cdw3 = space_id; //iterator close and read don't need keyspace id
+    cmd.cdw3 = space_id;
     cmd.cdw5 = iter_handle->handle;
     cmd.data_addr = (__u64)iter_handle->buf;
     cmd.data_length = iter_handle->buflen;
@@ -374,11 +366,6 @@ kv_result KADI::iter_list(kv_iterator *iter_list, uint32_t *count)
         iter_list[open_count].reserved[2] = (*(uint8_t *)(logbuf + offset + 15));
         // fprintf(stderr, "handle_id=%d status=%d type=%d prefix=%08x bitmask=%08x is_eof=%d\n",
         //             iter_list[open_count].handle_id, iter_list[open_count].status, iter_list[open_count].type, iter_list[open_count].prefix, iter_list[open_count].bitmask, iter_list[open_count].is_eof);
-        /*The bitpattern of the KV API is of big-endian mode. If the CPU is of little-endian mode,
-                  the bit pattern and bit mask should be transformed.*/
-        iter_list[open_count].prefix = htobe32(iter_list[open_count].prefix);
-        iter_list[open_count].bitmask = htobe32(iter_list[open_count].bitmask);
-
         open_count++;
     }
     *count = open_count;
@@ -414,11 +401,8 @@ redo:
     afterKeygap = (((*length + 3) >> 2) << 2);
     bufoffset += afterKeygap;
 
-    if (!db){
-        return false;
-    }
-    if (db && !db->exist(0, *key, *length, 0))
-    {//currently, class iterbuf_reader is not used.
+    if (db && !db->exist(*key, *length))
+    {
         goto redo;
     }
 
@@ -449,13 +433,12 @@ uint32_t KADI::get_dev_waf()
         return KV_ERR_SYS_IO;
     }
 
-    uint32_t waf = 0;
-    memcpy(&waf, logbuf + 256, sizeof(waf));
+    uint32_t waf = *((uint32_t *)&logbuf[256]);
 
     return waf;
+
 }
-kv_result KADI::kv_store(uint8_t ks_id, kv_key *key, kv_value *value,
-  nvme_kv_store_option option, const kv_postprocess_function *cb)
+kv_result KADI::kv_store(kv_key *key, kv_value *value, nvme_kv_store_option option, const kv_postprocess_function *cb)
 {
   if (!key || !key->key || !value)
    {
@@ -469,27 +452,32 @@ kv_result KADI::kv_store(uint8_t ks_id, kv_key *key, kv_value *value,
 
     ioctx->cmd.opcode = nvme_cmd_kv_store;
     ioctx->cmd.nsid = nsid;
-    ioctx->cmd.cdw3 = ks_id;
+
     if (key->length > KVCMD_INLINE_KEY_MAX)
     {
         ioctx->cmd.key_addr = (__u64)key->key;
+        ioctx->cmd.key_length = key->length;
     }
     else
     {
         memcpy((void *)ioctx->cmd.key, (void *)key->key, key->length);
     }
     ioctx->cmd.cdw5 = value->offset;
-    ioctx->cmd.key_length = key->length;
+    if(key->length > 255) {
+      ioctx->cmd.key_length = 0xff;
+    } else {
+      ioctx->cmd.key_length = key->length;
+    }
     ioctx->cmd.cdw4 = option;
-    ioctx->cmd.cdw11 = key->length - 1;
+    //ioctx->cmd.cdw11 = key->length - 1;
     ioctx->cmd.data_addr = (__u64)value->value;
     ioctx->cmd.data_length = value->length;
     ioctx->cmd.cdw10 = (value->length >> 2);
     ioctx->cmd.ctxid = aioctx.ctxid;
     ioctx->cmd.reqid = ioctx->index;
- 
+    
 #ifdef DUMP_ISSUE_CMD
-    //dump_cmd(&ioctx->cmd);
+    dump_cmd(&ioctx->cmd);
     std::cerr << "IO:kv_store: key = " << print_key((const char *)key->key, key->length) << ", len = " << (int)key->length << std::endl;
 #endif
 
@@ -503,7 +491,62 @@ kv_result KADI::kv_store(uint8_t ks_id, kv_key *key, kv_value *value,
     return 0;
 }
 
-kv_result KADI::kv_retrieve(uint8_t ks_id, kv_key *key, kv_value *value, const kv_postprocess_function *cb)
+kv_result KADI::kv_list(kv_key *key, uint8_t key_offset, uint16_t max_keys_to_list, kv_value *value, const kv_postprocess_function *cb)
+{
+   if (!key || !key->key || !value)
+   {
+       return KADI_ERR_NULL_INPUT;
+   }
+    aio_cmd_ctx *ioctx = get_cmd_ctx(cb);
+    memset((void *)&ioctx->cmd, 0, sizeof(struct nvme_passthru_kv_cmd));
+
+    ioctx->key = key;
+    ioctx->value = value;
+
+    ioctx->cmd.opcode = nvme_cmd_kv_list;
+    ioctx->cmd.nsid = nsid;
+     ioctx->cmd.cdw3 = space_id;
+    ioctx->cmd.cdw5 = value->offset;
+    ioctx->cmd.data_addr = (__u64)value->value;
+    ioctx->cmd.data_length = value->length;
+    if (key->length <= KVCMD_INLINE_KEY_MAX)
+    {
+        memcpy((void *)ioctx->cmd.key, (void *)key->key, key->length);
+    }
+    else
+    {
+        ioctx->cmd.key_addr = (__u64)key->key;
+        ioctx->cmd.key_length = key->length;
+    }
+
+    if(key->length > 255) {
+      ioctx->cmd.key_length = 0xff;
+    } else {
+      ioctx->cmd.key_length = key->length;
+    }
+
+    ioctx->cmd.list_key_offset = key_offset;
+    ioctx->cmd.list_max_keys = max_keys_to_list;
+    ioctx->cmd.reqid = ioctx->index;
+    ioctx->cmd.ctxid = aioctx.ctxid;
+
+
+#ifdef DUMP_ISSUE_CMD
+    dump_list_cmd(&ioctx->cmd);
+    std::cerr << "IO:kv_list: key = " << print_key((const char *)key->key, key->length) << ", len = " << (int)key->length << std::endl;
+#endif
+    int ret = ioctl(fd, NVME_IOCTL_AIO_CMD, &ioctx->cmd);
+    if (ret < 0)
+    {
+        //std::cerr << "kv_list I/O failed: cmd = " << (unsigned int)NVME_IOCTL_AIO_CMD << ", fd = " << fd << ", cmd = " << (unsigned int)ioctx->cmd.opcode << ", ret = " << ret <<std::endl;
+
+        release_cmd_ctx(ioctx);
+        return KV_ERR_SYS_IO;
+    }
+    return 0;
+}
+
+kv_result KADI::kv_retrieve(kv_key *key, kv_value *value, const kv_postprocess_function *cb)
 {
    if (!key || !key->key || !value)
    {
@@ -517,7 +560,7 @@ kv_result KADI::kv_retrieve(uint8_t ks_id, kv_key *key, kv_value *value, const k
 
     ioctx->cmd.opcode = nvme_cmd_kv_retrieve;
     ioctx->cmd.nsid = nsid;
-    ioctx->cmd.cdw3 = ks_id;
+    ioctx->cmd.cdw3 = space_id;
     ioctx->cmd.cdw4 = 0;
     ioctx->cmd.cdw5 = value->offset;
     ioctx->cmd.data_addr = (__u64)value->value;
@@ -529,8 +572,15 @@ kv_result KADI::kv_retrieve(uint8_t ks_id, kv_key *key, kv_value *value, const k
     else
     {
         ioctx->cmd.key_addr = (__u64)key->key;
+        ioctx->cmd.key_length = key->length;
+    }
+
+    if(key->length > 255) {
+      ioctx->cmd.key_length = 0xff;
+    } else {
+      ioctx->cmd.key_length = key->length;
     }
-    ioctx->cmd.key_length = key->length;
+
     ioctx->cmd.reqid = ioctx->index;
     ioctx->cmd.ctxid = aioctx.ctxid;
 
@@ -549,7 +599,233 @@ kv_result KADI::kv_retrieve(uint8_t ks_id, kv_key *key, kv_value *value, const k
     return 0;
 }
 
-kv_result KADI::kv_retrieve_sync(uint8_t ks_id, kv_key *key, kv_value *value)
+
+kv_result KADI::kv_retrieve_direct(kv_key *key, kv_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kv_postprocess_function *cb)
+{
+    if (!key || !key->key )
+    {
+        return KADI_ERR_NULL_INPUT;
+    }
+
+    aio_cmd_ctx *ioctx = get_cmd_ctx(cb);
+    memset((void *)&ioctx->cmd, 0, sizeof(struct nvme_passthru_kv_cmd));
+
+    ioctx->key = key;
+    ioctx->value = value;
+
+    ioctx->cmd.opcode = nvme_cmd_kv_retrieve;
+    ioctx->cmd.nsid = nsid;
+    ioctx->cmd.cdw3 = space_id;
+    ioctx->cmd.cdw4 = 0;
+    ioctx->cmd.cdw5 = 0;
+    ioctx->cmd.data_addr = (__u64)value->value;
+    ioctx->cmd.cdw10 = value->length;
+
+    ioctx->cmd.rkey[0] = (__u8)client_rdma_key;
+    ioctx->cmd.rkey[1] = (__u8)(client_rdma_key >> 8);
+    ioctx->cmd.rkey[2] = (__u8)(client_rdma_key >> 16);
+    ioctx->cmd.rkey[3] = (__u8)(client_rdma_key >> 24);
+
+    ioctx->cmd.type = 0xF;
+
+    if (key->length <= KVCMD_INLINE_KEY_MAX)
+    {
+        
+    }
+    else
+    {
+        ioctx->cmd.key_addr = (__u64)key->key;
+        ioctx->cmd.key_length = key->length;
+    }
+
+    if(key->length > 256) {
+      ioctx->cmd.key_length = 0xff;
+    } else {
+      ioctx->cmd.key_length = key->length;
+    }
+
+    uint32_t cd11_val = 0;
+    cd11_val = (client_rdma_qhandle << 16);
+    ioctx->cmd.cdw11 = cd11_val;
+
+    ioctx->cmd.reqid = ioctx->index;
+    ioctx->cmd.ctxid = aioctx.ctxid;
+
+#ifdef DUMP_ISSUE_CMD
+    dump_retrieve_cmd(&ioctx->cmd);
+    std::cerr << "IO:kv_retrieve direct sync: key = " << print_key((const char *)key->key, key->length) << ", len = " << (int)key->length << std::endl;
+#endif
+
+    int ret = ioctl(fd, NVME_IOCTL_AIO_CMD, &ioctx->cmd);
+    if (ret < 0)
+    {
+        release_cmd_ctx(ioctx);
+        return KV_ERR_SYS_IO;
+    }
+
+    return 0;
+}
+
+kv_result KADI::kv_store_direct(kv_key *key, kv_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kv_postprocess_function *cb)
+{
+    if (!key || !key->key )
+    {
+        return KADI_ERR_NULL_INPUT;
+    }
+
+    aio_cmd_ctx *ioctx = get_cmd_ctx(cb);
+    memset((void *)&ioctx->cmd, 0, sizeof(struct nvme_passthru_kv_cmd));
+
+    ioctx->key = key;
+    ioctx->value = value;
+
+    ioctx->cmd.opcode = nvme_cmd_kv_store;
+    ioctx->cmd.nsid = nsid;
+    ioctx->cmd.cdw3 = space_id;
+    ioctx->cmd.cdw4 = 0;
+    ioctx->cmd.cdw5 = 0;
+    ioctx->cmd.data_addr = (__u64)value->value;
+    ioctx->cmd.cdw10 = value->length;
+
+    ioctx->cmd.rkey[0] = (__u8)client_rdma_key;
+    ioctx->cmd.rkey[1] = (__u8)(client_rdma_key >> 8);
+    ioctx->cmd.rkey[2] = (__u8)(client_rdma_key >> 16);
+    ioctx->cmd.rkey[3] = (__u8)(client_rdma_key >> 24);
+
+    ioctx->cmd.type = 0xF;
+
+    if (key->length <= KVCMD_INLINE_KEY_MAX)
+    {
+
+    }
+    else
+    {
+        ioctx->cmd.key_addr = (__u64)key->key;
+        ioctx->cmd.key_length = key->length;
+    }
+
+    if(key->length > 256) {
+      ioctx->cmd.key_length = 0xff;
+    } else {
+      ioctx->cmd.key_length = key->length;
+    }
+
+    uint32_t cd11_val = 0;
+    cd11_val = (client_rdma_qhandle << 16);
+    ioctx->cmd.cdw11 = cd11_val;
+
+    ioctx->cmd.reqid = ioctx->index;
+    ioctx->cmd.ctxid = aioctx.ctxid;
+#ifdef DUMP_ISSUE_CMD
+    dump_retrieve_cmd(&ioctx->cmd);
+    std::cerr << "IO:kv_store_direct sync: key = " << print_key((const char *)key->key, key->length) << ", len = " << (int)key->length << std::endl;
+#endif
+
+    int ret = ioctl(fd, NVME_IOCTL_AIO_CMD, &ioctx->cmd);
+    if (ret < 0)
+    {
+        release_cmd_ctx(ioctx);
+        return KV_ERR_SYS_IO;
+    }
+
+    return 0;
+}
+
+
+    kv_result KADI::kv_lock(kv_key *key, nvme_kv_lock_req_option option, uint8_t priority, uint64_t ins_uuid, uint32_t lock_duration, const kv_postprocess_function *cb) {
+
+      if (!key || !key->key)
+      {
+        return KADI_ERR_NULL_INPUT;
+      }
+
+      aio_cmd_ctx *ioctx = get_cmd_ctx(cb);
+      memset((void *)&ioctx->cmd, 0, sizeof(struct nvme_passthru_kv_cmd));
+      ioctx->key = key;
+      ioctx->value = 0;
+
+      ioctx->cmd.opcode = nvme_cmd_kv_lock;
+      ioctx->cmd.nsid = nsid;
+      ioctx->cmd.cdw2 = (ins_uuid & 0xFFFFFFFF);
+      ioctx->cmd.cdw3 = ((ins_uuid >> 32) & 0xFFFFFFFF);
+      ioctx->cmd.cdw10 = lock_duration;
+      uint32_t cd11_val = 0;
+      cd11_val = (cd11_val | (uint8_t)key->length);
+      cd11_val |= ((uint16_t)(priority & 0x3) << 8);
+      cd11_val |= option; 
+      ioctx->cmd.cdw11 = cd11_val;
+      if (key->length <= KVCMD_INLINE_KEY_MAX) {
+        memcpy((void *)ioctx->cmd.key, (void *)key->key, key->length);
+
+      } else {
+
+        return KADI_ERR_KEY;
+      }
+      ioctx->cmd.key_length = key->length;
+
+      ioctx->cmd.reqid = ioctx->index;
+      ioctx->cmd.ctxid = aioctx.ctxid;
+
+  #ifdef DUMP_ISSUE_CMD
+      dump_cmd(&ioctx->cmd);
+  #endif
+
+      if (ioctl(fd, NVME_IOCTL_AIO_CMD, &ioctx->cmd) < 0)
+      {
+          release_cmd_ctx(ioctx);
+          return KV_ERR_SYS_IO;
+      }
+  
+      return 0;
+    }
+
+    kv_result KADI::kv_unlock(kv_key *key, nvme_kv_lock_req_option option, uint64_t ins_uuid, const kv_postprocess_function *cb) {
+      if (!key || !key->key)
+      {
+        return KADI_ERR_NULL_INPUT;
+      }
+
+      aio_cmd_ctx *ioctx = get_cmd_ctx(cb);
+      memset((void *)&ioctx->cmd, 0, sizeof(struct nvme_passthru_kv_cmd));
+      ioctx->key = key;
+      ioctx->value = 0;
+
+      ioctx->cmd.opcode = nvme_cmd_kv_unlock;
+      ioctx->cmd.nsid = nsid;
+      ioctx->cmd.cdw2 = (ins_uuid & 0xFFFFFFFF);
+      ioctx->cmd.cdw3 = ((ins_uuid >> 32) & 0xFFFFFFFF);
+      uint32_t cd11_val = 0;
+      cd11_val = (cd11_val | (uint8_t)key->length);
+      cd11_val |= option;
+      ioctx->cmd.cdw11 = cd11_val;
+      if (key->length <= KVCMD_INLINE_KEY_MAX) {
+        memcpy((void *)ioctx->cmd.key, (void *)key->key, key->length);
+
+      } else {
+
+        return KADI_ERR_KEY;
+      }
+      ioctx->cmd.key_length = key->length;
+
+      ioctx->cmd.reqid = ioctx->index;
+      ioctx->cmd.ctxid = aioctx.ctxid;
+
+  #ifdef DUMP_ISSUE_CMD
+      dump_cmd(&ioctx->cmd);
+  #endif
+
+      if (ioctl(fd, NVME_IOCTL_AIO_CMD, &ioctx->cmd) < 0)
+      {
+          release_cmd_ctx(ioctx);
+          return KV_ERR_SYS_IO;
+      }
+  
+      return 0;
+    }
+
+
+
+kv_result KADI::kv_retrieve_sync(kv_key *key, kv_value *value)
 {
     if (!key || !key->key || !value || !value->value)
     {
@@ -560,7 +836,7 @@ kv_result KADI::kv_retrieve_sync(uint8_t ks_id, kv_key *key, kv_value *value)
     memset((void *)&cmd, 0, sizeof(struct nvme_passthru_kv_cmd));
     cmd.opcode = nvme_cmd_kv_retrieve;
     cmd.nsid = nsid;
-    cmd.cdw3 = ks_id;
+    cmd.cdw3 = space_id;
     cmd.cdw4 = 0;
     cmd.cdw5 = value->offset;
     cmd.data_addr = (__u64)value->value;
@@ -572,8 +848,14 @@ kv_result KADI::kv_retrieve_sync(uint8_t ks_id, kv_key *key, kv_value *value)
     else
     {
         cmd.key_addr = (__u64)key->key;
+        cmd.key_length = key->length;
+    }
+
+    if(key->length > 256) {
+      cmd.key_length = 0xff;
+    } else {
+      cmd.key_length = key->length;
     }
-    cmd.key_length = key->length;
 
 #ifdef DUMP_ISSUE_CMD
     dump_retrieve_cmd(&cmd);
@@ -593,6 +875,7 @@ kv_result KADI::kv_retrieve_sync(uint8_t ks_id, kv_key *key, kv_value *value)
     return ret;
 }
 
+
 kv_result KADI::update_capacity()
 {
     uint64_t bytesused, capacity;
@@ -609,9 +892,6 @@ kv_result KADI::update_capacity()
 kv_result KADI::get_freespace(uint64_t &bytesused, uint64_t &capacity, double &utilization)
 {
     char *data = (char *)calloc(1, identify_ret_data_size);
-    if(data == NULL){
-       return KADI_ERR_MEMORY;
-    }
     struct nvme_passthru_cmd cmd;
     memset(&cmd, 0, sizeof(struct nvme_passthru_cmd));
     cmd.opcode = nvme_cmd_admin_identify;
@@ -622,10 +902,8 @@ kv_result KADI::get_freespace(uint64_t &bytesused, uint64_t &capacity, double &u
 
     if (ioctl(fd, NVME_IOCTL_ADMIN_CMD, &cmd) < 0)
     {
-        if (data){
+        if (data)
             free(data);
-            data = NULL;
-        }
         return KV_ERR_SYS_IO;
     }
 
@@ -634,25 +912,29 @@ kv_result KADI::get_freespace(uint64_t &bytesused, uint64_t &capacity, double &u
     capacity = namespace_size * BLOCK_SIZE;
     bytesused = namespace_utilization * BLOCK_SIZE;
     utilization = (1.0 * namespace_utilization) / namespace_size;
-    if (data){
+    if (data)
         free(data);
-        data = NULL;
-    }
     return 0;
 }
 
-bool KADI::exist(uint8_t ks_id, void *key, int length, const kv_postprocess_function *cb)
+bool KADI::exist(void *key, int length, const kv_postprocess_function *cb)
 {
     int ret = 0;
     aio_cmd_ctx *ioctx = get_cmd_ctx(cb);
     memset((void *)&ioctx->cmd, 0, sizeof(struct nvme_passthru_kv_cmd));
     ioctx->cmd.opcode = nvme_cmd_kv_exist;
     ioctx->cmd.nsid = nsid;
-    ioctx->cmd.cdw3 = ks_id;
-    ioctx->cmd.key_length = length;
+    ioctx->cmd.cdw3 = space_id;
+    if (((kv_key*)key)->length > 255) {
+      ioctx->cmd.key_length = 0xff;
+    } else {
+      ioctx->cmd.key_length = length;
+    }
+
     if (length > KVCMD_INLINE_KEY_MAX)
     {
         ioctx->cmd.key_addr = (__u64)key;
+        ioctx->cmd.key_length = ((kv_key*)key)->length;
     }
     else
     {
@@ -679,13 +961,12 @@ bool KADI::exist(uint8_t ks_id, void *key, int length, const kv_postprocess_func
     return (ret == 0) ? true : false;
 }
 
-bool KADI::exist(uint8_t ks_id, kv_key *key, const kv_postprocess_function *cb)
+bool KADI::exist(kv_key *key, const kv_postprocess_function *cb)
 {
-    return exist(ks_id, (void *)key->key, key->length, cb);
+    return exist((void *)key->key, key->length, cb);
 }
 
-kv_result KADI::kv_delete(uint8_t ks_id, kv_key *key,
-  const kv_postprocess_function *cb, int check_exist)
+kv_result KADI::kv_delete(kv_key *key, const kv_postprocess_function *cb, int check_exist)
 {
     if (!key || !key->key)
     {
@@ -699,7 +980,7 @@ kv_result KADI::kv_delete(uint8_t ks_id, kv_key *key,
 
     ioctx->cmd.opcode = nvme_cmd_kv_delete;
     ioctx->cmd.nsid = nsid;
-    ioctx->cmd.cdw3 = ks_id;
+    ioctx->cmd.cdw3 = space_id;
     ioctx->cmd.cdw4 = check_exist;
     if (key->length <= KVCMD_INLINE_KEY_MAX)
     {
@@ -708,8 +989,15 @@ kv_result KADI::kv_delete(uint8_t ks_id, kv_key *key,
     else
     {
         ioctx->cmd.key_addr = (__u64)key->key;
+        ioctx->cmd.key_length = key->length;
     }
-    ioctx->cmd.key_length = key->length;
+
+    if(key->length > 255) {
+      ioctx->cmd.key_length = 0xff;
+    } else {
+      ioctx->cmd.key_length = key->length;
+    }
+
     ioctx->cmd.reqid = ioctx->index;
     ioctx->cmd.ctxid = aioctx.ctxid;
 
@@ -839,9 +1127,8 @@ kv_result KADI::fill_ioresult(const aio_cmd_ctx &ioctx, const struct nvme_aioeve
         break;
     }
 
-    if (ioresult.retcode != 0) {
+    if (ioresult.retcode != 0)
         return 0;
-    }
 
     switch (ioresult.opcode)
     {
@@ -850,12 +1137,18 @@ kv_result KADI::fill_ioresult(const aio_cmd_ctx &ioctx, const struct nvme_aioeve
 
         if (ioctx.value)
         {
-            ioresult.value->actual_value_size = event.result - ioctx.value->offset;
-            //event.result is the total value length that returned from ssd, 
-            //remain value length = event.result - offset, user inputted buffer length may
-            //big or little than remain_val_len
-            ioresult.value->length = std::min(ioresult.value->actual_value_size,
-              ioctx.value->length);
+            ioresult.value->actual_value_size = event.result;
+            ioresult.value->length = std::min(event.result, ioctx.value->length);
+            //std::cerr << "length = " << ioresult.value->length << ", actual = " << ioresult.value->actual_value_size <<std::endl;
+        }
+        break;
+
+    case nvme_cmd_kv_list:
+
+        if (ioctx.value)
+        {
+            ioresult.value->actual_value_size = event.result;
+            ioresult.value->length = std::min(event.result, ioctx.value->length);
             //std::cerr << "length = " << ioresult.value->length << ", actual = " << ioresult.value->actual_value_size <<std::endl;
         }
         break;
diff --git a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kadi.h b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kadi.h
index 6bdd8f0..7526319 100644
--- a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kadi.h
+++ b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kadi.h
@@ -33,11 +33,12 @@
 
 #define ITER_BUFSIZE 32768
 
+
 class KvsTransContext;
 class KvsReadContext;
 class KvsSyncWriteContext;
 class CephContext;
-typedef uint8_t kv_key_t;
+typedef uint16_t kv_key_t;
 typedef uint32_t kv_value_t;
 typedef int kv_result;
 
@@ -69,6 +70,13 @@ enum nvme_kv_iter_req_option {
     ITER_OPTION_DEL_KEY_VALUE = 0x10,
 };
 
+enum nvme_kv_lock_req_option {
+    LOCK_OPTION_READER_NON_BLOCKING = 0x0,
+    LOCK_OPTION_READER_BLOCKING = 0x800,
+    LOCK_OPTION_WRITER_NON_BLOCKING = 0x400,
+    LOCK_OPTION_WRITER_BLOCKING = 0xC00,
+};
+
 enum nvme_kv_opcode {
     nvme_cmd_admin_get_log_page	= 0x02,
     nvme_cmd_admin_identify	= 0x06,
@@ -79,10 +87,12 @@ enum nvme_kv_opcode {
     nvme_cmd_kv_iter_req	= 0xB1,
     nvme_cmd_kv_iter_read	= 0xB2,
     nvme_cmd_kv_exist	= 0xB3,
-    nvme_cmd_kv_capacity = 0x06
+    nvme_cmd_kv_capacity = 0x06,
+    nvme_cmd_kv_lock = 0xA0,
+    nvme_cmd_kv_unlock = 0xA4,
+    nvme_cmd_kv_list = 0xD2
 };
 
-
 /*
 typedef struct {
     int opcode;
@@ -197,7 +207,7 @@ public:
     uint64_t capacity;
     typedef std::list<std::pair<kv_key *, kv_value *> >::iterator aio_iter;
     typedef struct {
-        unsigned int index;
+        int index;
         kv_key* key = 0;
         kv_value *value = 0;
         void *buf = 0;
@@ -232,7 +242,7 @@ private:
     std::condition_variable cmdctx_cond;
 
     std::vector<aio_cmd_ctx *>   free_cmdctxs;
-    std::map<unsigned, aio_cmd_ctx *> pending_cmdctxs;
+    std::map<int, aio_cmd_ctx *> pending_cmdctxs;
 
     int qdepth;
     
@@ -253,20 +263,23 @@ private:
 public:
 
     uint32_t  get_dev_waf();
-    kv_result kv_store(uint8_t ks_id, kv_key *key, kv_value *value, nvme_kv_store_option option, const kv_postprocess_function* cb);
-    kv_result kv_retrieve(uint8_t ks_id, kv_key *key, kv_value *value, const kv_postprocess_function* cb);
-    kv_result kv_retrieve_sync(uint8_t ks_id, kv_key *key, kv_value *value);
-    kv_result kv_delete(uint8_t ks_id, kv_key *key, const kv_postprocess_function* cb, int check_exist = 0);
-    kv_result iter_open(uint8_t ks_id, kv_iter_context *iter_handle, nvme_kv_iter_req_option option);
+    kv_result kv_store(kv_key *key, kv_value *value, nvme_kv_store_option option, const kv_postprocess_function* cb);
+    kv_result kv_retrieve(kv_key *key, kv_value *value, const kv_postprocess_function* cb);
+    kv_result kv_retrieve_sync(kv_key *key, kv_value *value);
+    kv_result kv_retrieve_direct(kv_key *key, kv_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kv_postprocess_function* cb);
+    kv_result kv_store_direct(kv_key *key, kv_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kv_postprocess_function* cb);
+    kv_result kv_list(kv_key *key, uint8_t key_offset, uint16_t max_keys_to_list, kv_value *value, const kv_postprocess_function* cb);
+    kv_result kv_delete(kv_key *key, const kv_postprocess_function* cb, int check_exist = 0);
+    kv_result iter_open(kv_iter_context *iter_handle, nvme_kv_iter_req_option option);
     kv_result iter_close(kv_iter_context *iter_handle);
     kv_result iter_read(kv_iter_context *iter_handle);
     kv_result iter_read_async(kv_iter_context *iter_handle, const kv_postprocess_function *cb);
-    kv_result iter_readall(uint8_t ks_id, kv_iter_context *iter_ctx,
-      nvme_kv_iter_req_option option, std::list<std::pair<void*, int> > &buflist);
+    kv_result iter_readall(kv_iter_context *iter_ctx, nvme_kv_iter_req_option option,
+                             std::list<std::pair<void*, int> > &buflist);
     kv_result iter_list(kv_iterator *iter_list, uint32_t *count);
     kv_result poll_completion(uint32_t &num_events, uint32_t timeout_us);
-    bool exist(uint8_t ks_id, kv_key *key, const kv_postprocess_function *cb = 0);
-    bool exist(uint8_t ks_id, void *key, int length, const kv_postprocess_function *cb = 0);
+    bool exist(kv_key *key, const kv_postprocess_function *cb = 0);
+    bool exist(void *key, int length, const kv_postprocess_function *cb = 0);
     int open(std::string &devpath);
     int close();
     int fill_ioresult(const aio_cmd_ctx &ioctx, const struct nvme_aioevent &event, kv_io_context &result);
@@ -279,7 +292,8 @@ public:
     }
     bool is_opened() { return (fd != -1); }
     void dump_cmd(struct nvme_passthru_kv_cmd *cmd);
-
+    kv_result kv_lock(kv_key *key, nvme_kv_lock_req_option option, uint8_t priority, uint64_t ins_uuid, uint32_t lock_duration, const kv_postprocess_function *cb);
+    kv_result kv_unlock(kv_key *key, nvme_kv_lock_req_option option, uint64_t ins_uuid, const kv_postprocess_function *cb);
 };
 
 
@@ -308,6 +322,7 @@ public:
 #define KADI_ERR_TUPLE_NOT_EXIST	(-22)
 #define KADI_ERR_VALUE	(-23)
 
+
 #endif
 
 
diff --git a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kadi_debug.cpp b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kadi_debug.cpp
index 08878d7..cd2502a 100644
--- a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kadi_debug.cpp
+++ b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kadi_debug.cpp
@@ -1,68 +1,62 @@
 #include "kadi_debug.h"
 #include "linux_nvme_ioctl.h"
-#include <string.h>
+
 std::string print_key(const char *data, int length) {
     char key[512];
     int keylen = 0;
     for (int i =0 ; i < length; i++) {
-      keylen += snprintf(key, sizeof(key), "%02x", data[i]);
+      keylen += sprintf(key, "%02x", data[i]);
     }
     return std::string (key, keylen);
 }
 
 void dump_delete_cmd(struct nvme_passthru_kv_cmd *cmd) {
     char buf[2048];
-    memset(buf, '\0', sizeof(buf));
-    int offset = snprintf(buf, sizeof(cmd->opcode), "[dump delete cmd (%02x)]\n", cmd->opcode);
+    int offset = sprintf(buf, "[dump delete cmd (%02x)]\n", cmd->opcode);
 
-    offset += snprintf(buf+offset, sizeof(cmd->opcode), "\t opcode(%02x)\n", cmd->opcode);
-    offset += snprintf(buf+offset, sizeof(cmd->nsid), "\t nsid(%04x)\n", cmd->nsid);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw3), "\t cdw3(%04x)\n", cmd->cdw3);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw4), "\t cdw4(%04x)\n", cmd->cdw4);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw5), "\t cdw5(%04x)\n", cmd->cdw5);
+    offset += sprintf(buf+offset, "\t opcode(%02x)\n", cmd->opcode);
+    offset += sprintf(buf+offset, "\t nsid(%04x)\n", cmd->nsid);
+    offset += sprintf(buf+offset, "\t cdw3(%04x)\n", cmd->cdw3);
+    offset += sprintf(buf+offset, "\t cdw4(%04x)\n", cmd->cdw4);
+    offset += sprintf(buf+offset, "\t cdw5(%04x)\n", cmd->cdw5);
 
-    offset += snprintf(buf+offset, sizeof(cmd->key_length), "\t cmd.key_length(%02x)\n", cmd->key_length);
+    offset += sprintf(buf+offset, "\t cmd.key_length(%02x)\n", cmd->key_length);
 
     if (cmd->key_length <= 16) {
-        offset += snprintf(buf+offset, sizeof(print_key((char*)cmd->key, cmd->key_length).c_str()), 
-                            "\t cmd.key (%s)\n", print_key((char*)cmd->key, cmd->key_length).c_str());
+        offset += sprintf(buf+offset, "\t cmd.key (%s)\n", print_key((char*)cmd->key, cmd->key_length).c_str());
     }
     else {
-        offset += snprintf(buf+offset, sizeof(print_key((char*)cmd->key_addr, cmd->key_length).c_str()), 
-                            "\t cmd.key (%s)\n", print_key((char*)cmd->key_addr, cmd->key_length).c_str());
+        offset += sprintf(buf+offset, "\t cmd.key (%s)\n", print_key((char*)cmd->key_addr, cmd->key_length).c_str());
     }
-    offset += snprintf(buf+offset, sizeof(cmd->reqid), "\t reqid(%04llu)\n", cmd->reqid);
-    offset += snprintf(buf+offset, sizeof(cmd->ctxid), "\t ctxid(%04d)\n", cmd->ctxid);
+    offset += sprintf(buf+offset, "\t reqid(%04llu)\n", cmd->reqid);
+    offset += sprintf(buf+offset, "\t ctxid(%04d)\n", cmd->ctxid);
     std::cerr << buf <<std::endl;
 }
 
 
 void dump_retrieve_cmd(struct nvme_passthru_kv_cmd *cmd) {
     char buf[2048];
-    memset(buf, '\0', sizeof(buf));
-    int offset = snprintf(buf, sizeof(cmd->opcode), "[dump retrieve cmd (%02x)]\n", cmd->opcode);
+    int offset = sprintf(buf, "[dump retrieve cmd (%02x)]\n", cmd->opcode);
 
-    offset += snprintf(buf+offset, sizeof(cmd->opcode), "\t opcode(%02x)\n", cmd->opcode);
-    offset += snprintf(buf+offset, sizeof(cmd->nsid), "\t nsid(%04x)\n", cmd->nsid);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw3), "\t cdw3(%04x)\n", cmd->cdw3);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw4), "\t cdw4(%04x)\n", cmd->cdw4);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw5), "\t cdw5(%04x)\n", cmd->cdw5);
+    offset += sprintf(buf+offset, "\t opcode(%02x)\n", cmd->opcode);
+    offset += sprintf(buf+offset, "\t nsid(%04x)\n", cmd->nsid);
+    offset += sprintf(buf+offset, "\t cdw3(%04x)\n", cmd->cdw3);
+    offset += sprintf(buf+offset, "\t cdw4(%04x)\n", cmd->cdw4);
+    offset += sprintf(buf+offset, "\t cdw5(%04x)\n", cmd->cdw5);
 
-    offset += snprintf(buf+offset, sizeof(cmd->key_length), "\t cmd.key_length(%02x)\n", cmd->key_length);
+    offset += sprintf(buf+offset, "\t cmd.key_length(%02x)\n", cmd->key_length);
 
     if (cmd->key_length <= 16) {
-        offset += snprintf(buf+offset, sizeof(print_key((char*)cmd->key, cmd->key_length).c_str()),
-                            "\t cmd.key (%s)\n", print_key((char*)cmd->key, cmd->key_length).c_str());
+        offset += sprintf(buf+offset, "\t cmd.key (%s)\n", print_key((char*)cmd->key, cmd->key_length).c_str());
     }
     else {
-        offset += snprintf(buf+offset, sizeof(print_key((char*)cmd->key_addr, cmd->key_length).c_str()),
-                            "\t cmd.key (%s)\n", print_key((char*)cmd->key_addr, cmd->key_length).c_str());
+        offset += sprintf(buf+offset, "\t cmd.key (%s)\n", print_key((char*)cmd->key_addr, cmd->key_length).c_str());
     }
 
-    offset += snprintf(buf+offset, sizeof(cmd->data_length), "\t cmd.data_length(%02x)\n", cmd->data_length);
-    offset += snprintf(buf+offset, sizeof((void*)cmd->data_addr), "\t cmd.data(%p)\n", (void*)cmd->data_addr);
-    offset += snprintf(buf+offset, sizeof(cmd->reqid), "\t reqid(%04llu)\n", cmd->reqid);
-    offset += snprintf(buf+offset, sizeof(cmd->ctxid), "\t ctxid(%04d)\n", cmd->ctxid);
+    offset += sprintf(buf+offset, "\t cmd.data_length(%02x)\n", cmd->data_length);
+    offset += sprintf(buf+offset, "\t cmd.data(%p)\n", (void*)cmd->data_addr);
+    offset += sprintf(buf+offset, "\t reqid(%04llu)\n", cmd->reqid);
+    offset += sprintf(buf+offset, "\t ctxid(%04d)\n", cmd->ctxid);
     std::cerr << buf <<std::endl;
 }
 
@@ -70,26 +64,25 @@ void dump_retrieve_cmd(struct nvme_passthru_kv_cmd *cmd) {
 void dump_cmd(struct nvme_passthru_kv_cmd *cmd)
 {
     char buf[2048];
-    memset(buf, '\0', sizeof(buf));
-    int offset = snprintf(buf, sizeof(cmd->opcode), "[dump issued cmd opcode (%02x)]\n", cmd->opcode);
-    offset += snprintf(buf+offset, sizeof(cmd->opcode), "\t opcode(%02x)\n", cmd->opcode);
-    offset += snprintf(buf+offset, sizeof(cmd->flags), "\t flags(%02x)\n", cmd->flags);
-    offset += snprintf(buf+offset, sizeof(cmd->rsvd1), "\t rsvd1(%04d)\n", cmd->rsvd1);
-    offset += snprintf(buf+offset, sizeof(cmd->nsid), "\t nsid(%08x)\n", cmd->nsid);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw2), "\t cdw2(%08x)\n", cmd->cdw2);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw3), "\t cdw3(%08x)\n", cmd->cdw3);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw4), "\t rsvd2(%08x)\n", cmd->cdw4);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw5), "\t cdw5(%08x)\n", cmd->cdw5);
-    offset += snprintf(buf+offset, sizeof((void *)cmd->data_addr), "\t data_addr(%p)\n",(void *)cmd->data_addr);
-    offset += snprintf(buf+offset, sizeof(cmd->data_length), "\t data_length(%08x)\n", cmd->data_length);
-    offset += snprintf(buf+offset, sizeof(cmd->key_length), "\t key_length(%08x)\n", cmd->key_length);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw10), "\t cdw10(%08x)\n", cmd->cdw10);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw11), "\t cdw11(%08x)\n", cmd->cdw11);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw12), "\t cdw12(%08x)\n", cmd->cdw12);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw13), "\t cdw13(%08x)\n", cmd->cdw13);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw14), "\t cdw14(%08x)\n", cmd->cdw14);
-    offset += snprintf(buf+offset, sizeof(cmd->cdw15), "\t cdw15(%08x)\n", cmd->cdw15);
-    offset += snprintf(buf+offset, sizeof(cmd->timeout_ms), "\t timeout_ms(%08x)\n", cmd->timeout_ms);
-    offset += snprintf(buf+offset, sizeof(cmd->result), "\t result(%08x)\n", cmd->result);
+    int offset = sprintf(buf, "[dump issued cmd opcode (%02x)]\n", cmd->opcode);
+    offset += sprintf(buf+offset, "\t opcode(%02x)\n", cmd->opcode);
+    offset += sprintf(buf+offset, "\t flags(%02x)\n", cmd->flags);
+    offset += sprintf(buf+offset, "\t rsvd1(%04d)\n", cmd->rsvd1);
+    offset += sprintf(buf+offset, "\t nsid(%08x)\n", cmd->nsid);
+    offset += sprintf(buf+offset, "\t cdw2(%08x)\n", cmd->cdw2);
+    offset += sprintf(buf+offset, "\t cdw3(%08x)\n", cmd->cdw3);
+    offset += sprintf(buf+offset, "\t rsvd2(%08x)\n", cmd->cdw4);
+    offset += sprintf(buf+offset, "\t cdw5(%08x)\n", cmd->cdw5);
+    offset += sprintf(buf+offset, "\t data_addr(%p)\n",(void *)cmd->data_addr);
+    offset += sprintf(buf+offset, "\t data_length(%08x)\n", cmd->data_length);
+    offset += sprintf(buf+offset, "\t key_length(%08x)\n", cmd->key_length);
+    offset += sprintf(buf+offset, "\t cdw10(%08x)\n", cmd->cdw10);
+    offset += sprintf(buf+offset, "\t cdw11(%08x)\n", cmd->cdw11);
+    offset += sprintf(buf+offset, "\t cdw12(%08x)\n", cmd->cdw12);
+    offset += sprintf(buf+offset, "\t cdw13(%08x)\n", cmd->cdw13);
+    offset += sprintf(buf+offset, "\t cdw14(%08x)\n", cmd->cdw14);
+    offset += sprintf(buf+offset, "\t cdw15(%08x)\n", cmd->cdw15);
+    offset += sprintf(buf+offset, "\t timeout_ms(%08x)\n", cmd->timeout_ms);
+    offset += sprintf(buf+offset, "\t result(%08x)\n", cmd->result);
     std::cerr << buf <<std::endl;
 }
\ No newline at end of file
diff --git a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kvs_adi.cpp b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kvs_adi.cpp
index 78494c5..71d97b6 100644
--- a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kvs_adi.cpp
+++ b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/kvs_adi.cpp
@@ -115,9 +115,6 @@ kv_result kv_initialize_device(void *dev_init, kv_device_handle *dev_hdl) {
     }
     
     KADI *dev = get_kadi_from_hdl(hnd);
-    if(dev == NULL){
-        return KV_ERR_DEV_NOT_EXIST;
-    }
     int ret = dev->open(devpath);
     if (ret == 0) {
         dev->update_capacity();
@@ -141,7 +138,7 @@ kv_result kv_initialize_device(void *dev_init, kv_device_handle *dev_hdl) {
 
 kv_result kv_cleanup_device(kv_device_handle dev_hdl) {
     FTRACE
-    if (!dev_hdl) return KV_ERR_PARAM_INVALID;
+    if (!dev_hdl) return KVS_ERR_PARAM_INVALID;
     if (dev_hdl->dev) {
         g_devices.remove(dev_hdl->dev);        
         delete (KADI*)dev_hdl->dev;
@@ -203,7 +200,7 @@ kv_result kv_get_device_stat(const kv_device_handle dev_hdl, kv_device_stat *dev
 
     dev_st->namespace_count = 1;
     dev_st->queue_count = 1;
-    dev_st->utilization = (uint16_t)round(utilization * 10000);
+    dev_st->utilization = (uint16_t)(utilization * 10000);
     dev_st->waf = 0;
     dev_st->extended_info = 0;
 
@@ -360,8 +357,7 @@ kv_result _kv_bypass_namespace(const kv_device_handle dev_hdl, const kv_namespac
 }
 
 // IO APIs
-kv_result kv_purge(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-    uint8_t ks_id, kv_purge_option option, kv_postprocess_function *post_fn) {
+kv_result kv_purge(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_purge_option option, kv_postprocess_function *post_fn) {
     FTRACE
 
     if (que_hdl == NULL || ns_hdl == NULL) {
@@ -371,15 +367,56 @@ kv_result kv_purge(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
     return KV_ERR_DD_UNSUPPORTED_CMD;
 }
 
-kv_result kv_open_iterator_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-    uint8_t ks_id, const kv_iterator_option it_op, const kv_group_condition *it_cond,
-    uint8_t *iter_handle) {
+
+kv_result kv_lock(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, uint8_t read_lock, uint8_t is_blocking, uint8_t lock_prio, uint32_t lock_duration, uint64_t instance_uuid, const kv_postprocess_function *post_fn) {
+
+  if (que_hdl == NULL || ns_hdl == NULL || key == NULL) {
+    return KV_ERR_PARAM_INVALID;
+  }
+  KADI *dev = (KADI *) que_hdl->dev;
+  if (dev == NULL) { return KV_ERR_DEV_NOT_EXIST; }
+
+
+  nvme_kv_lock_req_option lock_option = LOCK_OPTION_READER_NON_BLOCKING;
+
+  if (read_lock && is_blocking) {
+    lock_option = LOCK_OPTION_READER_BLOCKING;
+
+  } else if (!read_lock && is_blocking) {
+    lock_option = LOCK_OPTION_WRITER_BLOCKING;
+  } else if (!read_lock && !is_blocking) {
+    lock_option = LOCK_OPTION_WRITER_NON_BLOCKING;
+  }
+  return dev->kv_lock((kv_key*)key, lock_option, lock_prio, instance_uuid, lock_duration, post_fn);
+}
+
+kv_result kv_unlock(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, uint8_t read_unlock, uint64_t instance_uuid, const kv_postprocess_function *post_fn) {
+
+  if (que_hdl == NULL || ns_hdl == NULL || key == NULL) {
+    return KV_ERR_PARAM_INVALID;
+  }
+  KADI *dev = (KADI *) que_hdl->dev;
+  if (dev == NULL) { return KV_ERR_DEV_NOT_EXIST; }
+
+
+  nvme_kv_lock_req_option lock_option = LOCK_OPTION_READER_NON_BLOCKING;
+
+  if (read_unlock) {
+    lock_option = LOCK_OPTION_READER_NON_BLOCKING;
+
+  } else {
+    lock_option = LOCK_OPTION_WRITER_NON_BLOCKING;
+  }
+ 
+  return dev->kv_unlock((kv_key*)key, lock_option, instance_uuid, post_fn);
+
+}
+
+
+kv_result kv_open_iterator_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_iterator_option it_op, const kv_group_condition *it_cond, uint8_t *iter_handle) {
     kv_iter_context iter_ctx;
-    /* The bitpattern of the KV API is of big-endian mode. If the CPU is of little-
-endian mode, the bit pattern and bit mask should be transformed.
- */
-    iter_ctx.prefix  = htobe32(it_cond->bit_pattern);  
-    iter_ctx.bitmask = htobe32(it_cond->bitmask);     
+    iter_ctx.prefix  = it_cond->bit_pattern;
+    iter_ctx.bitmask = it_cond->bitmask;
     iter_ctx.buflen  = ITER_BUFSIZE;
 
     KADI *dev = (KADI *) que_hdl->dev;
@@ -399,15 +436,14 @@ endian mode, the bit pattern and bit mask should be transformed.
       default:
         return KV_ERR_OPTION_INVALID;
     }
-    kv_result ret = dev->iter_open(ks_id, &iter_ctx, dev_option);
+    kv_result ret = dev->iter_open(&iter_ctx, dev_option);
     if (ret == KV_SUCCESS) {
         *iter_handle = (uint8_t)iter_ctx.handle;
     }
     return ret;
 }
 
-kv_result kv_close_iterator_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-  kv_iterator_handle iter_hdl) {
+kv_result kv_close_iterator_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_iterator_handle iter_hdl) {
     FTRACE
     if (que_hdl == NULL || ns_hdl == NULL || iter_hdl == 0) {
         return KV_ERR_PARAM_INVALID;
@@ -420,8 +456,7 @@ kv_result kv_close_iterator_sync(kv_queue_handle que_hdl, kv_namespace_handle ns
   
     return  dev->iter_close(&iter_ctx);
 }
-kv_result kv_iterator_next(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-  kv_iterator_handle iter_hdl, kv_iterator_list *iter_list, kv_postprocess_function *post_fn)
+kv_result kv_iterator_next(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_iterator_handle iter_hdl, kv_iterator_list *iter_list, kv_postprocess_function *post_fn)
 {
     if (que_hdl == NULL || ns_hdl == NULL || iter_hdl <= 0
         || iter_hdl > SAMSUNG_MAX_ITERATORS || iter_list == NULL) {
@@ -436,8 +471,7 @@ kv_result kv_iterator_next(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
     return dev->iter_read_async(&iter_ctx, post_fn);
 }
 
-kv_result kv_iterator_next_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-  kv_iterator_handle iter_hdl, kv_iterator_list *iter_list) {
+kv_result kv_iterator_next_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_iterator_handle iter_hdl, kv_iterator_list *iter_list) {
     FTRACE
     if (que_hdl == NULL || ns_hdl == NULL || iter_hdl <= 0
         || iter_hdl > SAMSUNG_MAX_ITERATORS || iter_list == NULL) {
@@ -470,13 +504,11 @@ kv_result kv_iterator_next_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_
     }
     return ret;
 }
-kv_result kv_list_iterators(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-  kv_iterator *kv_iters, uint32_t *iter_cnt, kv_postprocess_function  *post_fn) {
+kv_result kv_list_iterators(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_iterator *kv_iters, uint32_t *iter_cnt, kv_postprocess_function  *post_fn) {
     return KV_ERR_DD_UNSUPPORTED_CMD;
 }
 
-kv_result kv_list_iterators_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-  kv_iterator *kv_iters, uint32_t *iter_cnt) {
+kv_result kv_list_iterators_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_iterator *kv_iters, uint32_t *iter_cnt) {
     FTRACE
     if (que_hdl == NULL || ns_hdl == NULL || kv_iters == NULL || iter_cnt == NULL) {
         return KV_ERR_PARAM_INVALID;
@@ -486,8 +518,7 @@ kv_result kv_list_iterators_sync(kv_queue_handle que_hdl, kv_namespace_handle ns
     return dev->iter_list(kv_iters, iter_cnt);
 }
 
-kv_result kv_delete(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-  uint8_t ks_id, const kv_key *key, kv_delete_option option, kv_postprocess_function *post_fn) {
+kv_result kv_delete(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_delete_option option, kv_postprocess_function *post_fn) {
     FTRACE
 
     if (que_hdl == NULL || ns_hdl == NULL || key == NULL) {
@@ -495,11 +526,10 @@ kv_result kv_delete(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
     }
     KADI *dev = (KADI *) que_hdl->dev;
     
-    return dev->kv_delete(ks_id, (kv_key *)key, post_fn, (int)option);
+    return dev->kv_delete((kv_key *)key, post_fn, (int)option);
 }
 
-kv_result kv_delete_group(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-  uint8_t ks_id, kv_group_condition *grp_cond, kv_postprocess_function *post_fn) {
+kv_result kv_delete_group(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, kv_group_condition *grp_cond, kv_postprocess_function *post_fn) {
     FTRACE
     if (que_hdl == NULL || ns_hdl == NULL || grp_cond == NULL) {
         return KV_ERR_PARAM_INVALID;
@@ -508,10 +538,9 @@ kv_result kv_delete_group(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
     return KV_ERR_DD_UNSUPPORTED_CMD;
 }
 
-kv_result kv_exist(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-  uint8_t ks_id, const kv_key *keys, uint32_t key_cnt, uint32_t buffer_size,
-  uint8_t *buffer, kv_postprocess_function *post_fn) {
+kv_result kv_exist(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *keys, uint32_t key_cnt, uint32_t buffer_size, uint8_t *buffer, kv_postprocess_function *post_fn) {
     FTRACE
+
     if (que_hdl == NULL || ns_hdl == NULL || key_cnt < 1 ) {
         return KV_ERR_PARAM_INVALID;
     }
@@ -527,43 +556,74 @@ kv_result kv_exist(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
     }
 
     KADI *dev = (KADI *) que_hdl->dev;
-    bool e =  dev->exist(ks_id, (kv_key *)&keys[0], post_fn);
-    if (post_fn == NULL) {
+    bool e =  dev->exist((kv_key *)&keys[0], post_fn);
+    if (post_fn == 0) {
         // sync 
         if (buffer && buffer_size > 0) buffer[0] = (e)? 1:0;
     } else if (e == false){
         // async submit failed 
-        return KV_ERR_SYS_IO;
+        return KVS_ERR_SYS_IO;
     }
     
     return KV_SUCCESS;
 }
 
-kv_result kv_retrieve(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-  uint8_t ks_id, const kv_key *key, kv_retrieve_option option, kv_value *value,
-  const kv_postprocess_function *post_fn) {
+kv_result kv_retrieve(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_retrieve_option option, kv_value *value, const kv_postprocess_function *post_fn) {
+    FTRACE
+    if (que_hdl == NULL || ns_hdl == NULL || key == NULL || value == NULL) {
+        return KV_ERR_PARAM_INVALID;
+    }
+    KADI *dev = (KADI *) que_hdl->dev;
+    return dev->kv_retrieve((kv_key*)key, value, post_fn);
+}
+
+kv_result kv_retrieve_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_retrieve_option option, kv_value *value) {
     FTRACE
     if (que_hdl == NULL || ns_hdl == NULL || key == NULL || value == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
     KADI *dev = (KADI *) que_hdl->dev;
-    return dev->kv_retrieve(ks_id, (kv_key*)key, value, post_fn);
+    return dev->kv_retrieve_sync((kv_key*)key, value);
+}
+
+kv_result kv_retrieve_direct(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_retrieve_option option, 
+                             const kv_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kv_postprocess_function *post_fn) {
+    FTRACE
+    if (que_hdl == NULL || ns_hdl == NULL || key == NULL ) {
+        return KV_ERR_PARAM_INVALID;
+    }
+    KADI *dev = (KADI *) que_hdl->dev;
+    return dev->kv_retrieve_direct((kv_key*)key, (kv_value *)value, client_rdma_key, client_rdma_qhandle, post_fn);
+
+}
+
+kv_result kv_store_direct(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, kv_store_option option,
+                             const kv_value *value, uint32_t client_rdma_key, uint16_t client_rdma_qhandle, const kv_postprocess_function *post_fn) {
+    FTRACE
+    if (que_hdl == NULL || ns_hdl == NULL || key == NULL ) {
+        return KV_ERR_PARAM_INVALID;
+    }
+    KADI *dev = (KADI *) que_hdl->dev;
+    return dev->kv_store_direct((kv_key*)key, (kv_value *)value, client_rdma_key, client_rdma_qhandle, post_fn);
+
 }
 
-kv_result kv_retrieve_sync(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-  uint8_t ks_id, const kv_key *key, kv_retrieve_option option, kv_value *value) {
+
+
+kv_result kv_list(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, uint8_t key_offset, uint16_t max_keys_to_list, kv_value *value, const kv_postprocess_function *post_fn) {
     FTRACE
+
     if (que_hdl == NULL || ns_hdl == NULL || key == NULL || value == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
     KADI *dev = (KADI *) que_hdl->dev;
-    return dev->kv_retrieve_sync(ks_id, (kv_key*)key, value);
+
+    return dev->kv_list((kv_key*)key, key_offset, max_keys_to_list, (kv_value*)value, post_fn);
 }
 
-kv_result kv_store(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
-  uint8_t ks_id, const kv_key *key, const kv_value *value, kv_store_option option,
-  const kv_postprocess_function *post_fn) {
+kv_result kv_store(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl, const kv_key *key, const kv_value *value, kv_store_option option, const kv_postprocess_function *post_fn) {
     FTRACE
+
     if (que_hdl == NULL || ns_hdl == NULL || key == NULL || value == NULL) {
         return KV_ERR_PARAM_INVALID;
     }
@@ -585,9 +645,10 @@ kv_result kv_store(kv_queue_handle que_hdl, kv_namespace_handle ns_hdl,
       default:
         return KV_ERR_OPTION_INVALID;
     }
-    return dev->kv_store(ks_id, (kv_key*)key, (kv_value*)value, dev_option, post_fn);
+    return dev->kv_store((kv_key*)key, (kv_value*)value, dev_option, post_fn);
 }
 
+
 kv_result kv_poll_completion(kv_queue_handle que_hdl, uint32_t timeout_usec, uint32_t *num_events) {
     FTRACE
     if (que_hdl == NULL || num_events == NULL) {
diff --git a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/linux_nvme_ioctl.h b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/linux_nvme_ioctl.h
index 7fc9f2b..93eb880 100644
--- a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/linux_nvme_ioctl.h
+++ b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/linux_nvme_ioctl.h
@@ -62,15 +62,28 @@ struct nvme_passthru_kv_cmd {
 	__u32	cdw3;
 	__u32	cdw4;
 	__u32	cdw5;
-	__u64	data_addr;
-	__u32	data_length;
-	__u32	key_length;
+	struct {
+		__u64	data_addr;
+		union {
+			struct {
+				__u32	data_length;
+				__u8	rsvd2;//Not used
+				__u8	list_key_offset;//List key offset
+				__u16	list_max_keys;//List max keys
+			};
+			struct {
+				__u8    length[3];//Length in 24 bit
+				__u8    rkey[4];//RDMA remote key
+				__u8    type;//SGL Type
+			};//Keyed SGL second half
+		};
+	};//dptr
 	__u32	cdw10;
 	__u32	cdw11;
 	union {
 		struct {
 			__u64 key_addr;
-			__u32 rsvd5;
+			__u32 key_length;
 			__u32 rsvd6;
 		};
 		__u8 key[16];
@@ -108,7 +121,6 @@ struct nvme_aioevents {
 	struct nvme_aioevent events[MAX_AIO_EVENTS];
 };
 
-
 #define nvme_admin_cmd nvme_passthru_cmd
 
 #define NVME_IOCTL_ID		_IO('N', 0x40)
@@ -118,7 +130,6 @@ struct nvme_aioevents {
 #define NVME_IOCTL_RESET	_IO('N', 0x44)
 #define NVME_IOCTL_SUBSYS_RESET	_IO('N', 0x45)
 #define NVME_IOCTL_RESCAN	_IO('N', 0x46)
-
 #define NVME_IOCTL_AIO_CMD		_IOWR('N', 0x47, struct nvme_passthru_kv_cmd)
 #define NVME_IOCTL_SET_AIOCTX	_IOWR('N', 0x48, struct nvme_aioctx)
 #define NVME_IOCTL_DEL_AIOCTX	_IOWR('N', 0x49, struct nvme_aioctx)
diff --git a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/sample_code/sample_interrupt.cpp b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/sample_code/sample_interrupt.cpp
new file mode 100644
index 0000000..7719fab
--- /dev/null
+++ b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/sample_code/sample_interrupt.cpp
@@ -0,0 +1,420 @@
+#include "assert.h"
+#include <string>
+#include <iostream>
+#include <sstream>
+#include <thread>
+#include <vector>
+
+#include <stdio.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sys/time.h>
+
+#include <mutex>
+#include <chrono>
+#include <thread>
+#include <set>
+#include <unordered_set>
+#include <map>
+#include <unordered_map>
+#include <iomanip>
+#include <ctime>
+#include <condition_variable>
+#include <atomic>
+#include "kvs_adi.h"
+
+#include "kadi.h"
+#include "kvs_adi_internal.h"
+
+/* total kv to insert */
+int TOTAL_KV_COUNT = 10000;
+
+typedef struct {
+    kv_key *key;
+    kv_value *value;
+    kv_result retcode;
+
+    std::condition_variable done_cond;
+    std::mutex mutex;
+    std::atomic<int> done;
+} iohdl_t;
+
+
+void free_kv(kv_key *key, kv_value *value) {
+    // free key
+    if (key != NULL) {
+        if (key->key != NULL) {
+            free(key->key);
+        }
+        free(key);
+    }
+
+    // free value
+    if (value != NULL) {
+        if (value->value != NULL) {
+            free(value->value);
+        }
+        free(value);
+    }
+}
+
+// this function will be called after completion of a command
+void interrupt_func(void *data, int number) {
+    (void) data;
+    (void) number;
+
+    //printf("inside interrupt\n");
+}
+
+int create_sq(kv_device_handle dev_hdl, uint16_t sqid, uint16_t qsize, uint16_t cqid, kv_queue_handle *quehdl) {
+
+    kv_queue qinfo;
+    qinfo.queue_id = sqid;
+    qinfo.queue_size = qsize;
+    qinfo.completion_queue_id = cqid;
+    qinfo.queue_type = SUBMISSION_Q_TYPE;
+    qinfo.extended_info = NULL;
+
+    if (KV_SUCCESS != kv_create_queue(dev_hdl, &qinfo, quehdl)) {
+        printf("submission queue creation failed\n");
+        exit(1);
+    }
+    KADI* adi = (KADI*) (dev_hdl->dev);
+    adi->start_cbthread();
+    return 0;
+}
+
+int create_cq(kv_device_handle dev_hdl, uint16_t cqid, uint16_t qsize, kv_queue_handle *quehdl) {
+
+    kv_queue qinfo;
+    qinfo.queue_id = cqid;
+    qinfo.queue_size = qsize;
+    qinfo.completion_queue_id = 0;
+    qinfo.queue_type = COMPLETION_Q_TYPE;
+    qinfo.extended_info = NULL;
+
+    if (KV_SUCCESS != kv_create_queue(dev_hdl, &qinfo, quehdl)) {
+        printf("completion queue creation failed\n");
+        exit(1);
+    }
+    KADI* adi = (KADI*) (dev_hdl->dev);
+    adi->start_cbthread();
+    return 0;
+}
+
+uint64_t current_timestamp() {
+    std::chrono::system_clock::time_point timepoint = std::chrono::system_clock::now();
+    uint64_t nano_from_epoch = std::chrono::duration_cast<std::chrono::nanoseconds>(timepoint.time_since_epoch()).count();
+    return nano_from_epoch;
+}
+
+// post processing function for a command
+// op->private_data is the data caller passed in
+void on_IO_complete_func(kv_io_context *ctx) {
+    // depends if you allocated space for each command, you can manage
+    // allocated spaces here. This should in sync with your IO cycle
+    // management strategy.
+    //
+    // get completed async command results, free any allocated spaces if necessary.
+    //
+     //printf("finished one command\n");
+    iohdl_t *iohdl = (iohdl_t *) ctx->private_data;
+    iohdl->retcode = ctx->retcode;
+
+    std::unique_lock<std::mutex> lock(iohdl->mutex);
+    iohdl->key = const_cast<kv_key*> (ctx->key);
+    iohdl->value = ctx->value;
+    iohdl->retcode = ctx->retcode;
+
+    printf("command finished with result 0x%X\n", iohdl->retcode);
+    iohdl->done = 1;
+    iohdl->done_cond.notify_one();
+
+}
+
+// fill key value with random data
+void populate_key_value_startwith(uint32_t keystart, kv_key *key, kv_value *value) {
+
+    char *buffer = (char *)key->key;
+    uint32_t blen = key->length;
+
+    uint8_t *base = (uint8_t *)&keystart;
+    uint8_t prefix[4] = {
+            *(base + 3),
+            *(base + 2),
+            *(base + 1),
+            *(base + 0) };
+
+    // if prefix is 0, skip prefix setup in key value
+    if (keystart != 0) {
+        assert(blen > 4);
+        // copy 4 bytes of prefix
+        memcpy(buffer, (char *)prefix, 4);
+        buffer += 4;
+        blen -= 4;
+    }
+
+    int rand_num = std::rand();
+    unsigned long long current_time = current_timestamp() + rand_num;
+    char timestr[16];
+    snprintf(timestr, 16, "%llu", current_time);
+    memcpy(buffer, timestr, std::min(blen, (uint32_t)sizeof(timestr)));
+    // done with key
+
+    // generate value
+    buffer = (char *) value->value;
+    blen = value->length;
+    // ending with null
+    buffer[blen - 1] = 0;
+
+    // create random value
+    for (unsigned int i = 0; i < blen - 1; i++) {
+        unsigned int j = 'a';//(char)(i%128); //std::rand() % 128;
+        buffer[i] = j;
+    }
+
+    printf("generate a key %s, value legngth = %d\n", (char *) (key->key), value->length);
+}
+
+// how many keys to insert
+void insert_kv_store(kv_device_handle dev_hdl, kv_queue_handle sq_hdl, const int klen, const int vlen, int count) {
+    // set up IO handle and postprocessing
+    iohdl_t *iohdl = (iohdl_t *) malloc(sizeof(iohdl_t));
+    memset(iohdl, 0, sizeof(iohdl_t));
+    kv_postprocess_function *post_fn_data = (kv_postprocess_function *)
+                                malloc(sizeof(kv_postprocess_function));
+    post_fn_data->post_fn = on_IO_complete_func;
+    post_fn_data->private_data = (void *) iohdl;
+
+    // allocate key value for reuse
+    kv_key *key = (kv_key *) malloc(sizeof(kv_key));
+    key->length = klen;
+    key->key = malloc(key->length);
+    memset(key->key, 0, key->length);
+
+    // value for insert
+    kv_value *val = (kv_value *) malloc(sizeof(kv_value));
+    val->length = vlen;
+    val->actual_value_size = vlen;
+    val->value = malloc(val->length);
+    memset(val->value, 0, val->length);
+    val->offset = 0;
+
+    // value for read back validation
+    kv_value *valread = (kv_value *) malloc(sizeof(kv_value));
+    valread->length = vlen;
+    valread->actual_value_size = vlen;
+    valread->value = malloc(val->length);
+    memset(valread->value, 0, valread->length);
+    valread->offset = 0;
+
+    // save key value to the IO handle
+    iohdl->key = key;
+    iohdl->value = val;
+
+    // set up namespace
+    kv_namespace_handle ns_hdl = NULL;
+    get_namespace_default(dev_hdl, &ns_hdl);
+
+    // insert key value
+    for (int i = 0; i < count; i++) {
+
+        // populate key value with random content
+        populate_key_value_startwith(0, iohdl->key, iohdl->value);
+
+        iohdl->done  = 0;
+
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_result res = kv_store(sq_hdl, ns_hdl, key, val, KV_STORE_OPT_DEFAULT, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_store failed with error: 0x%X\n", res);
+            res = kv_store(sq_hdl, ns_hdl, key, val, KV_STORE_OPT_DEFAULT, post_fn_data);
+        } 
+        // check result asynchronously
+        std::unique_lock<std::mutex> lock(iohdl->mutex);
+        while (iohdl->done == 0) {
+            iohdl->done_cond.wait(lock);
+        }
+        // expected result
+        if (iohdl->retcode != KV_SUCCESS) {
+            printf("kv_store a new key failed: 0x%X\n", iohdl->retcode);
+            exit(1);
+        } else {
+            printf("kv_store succeeded: value length = %d\n", val->length);
+        }
+        iohdl->done = 0;
+        iohdl->retcode = KV_ERR_COMMAND_SUBMITTED;
+        lock.unlock();
+
+
+        // save original
+        // read it back
+        kv_value *valwritten = iohdl->value;
+        iohdl->value = valread;
+        memset(valread->value, 0, valread->length);
+        
+        printf("value read = %p\n", valread);
+        // read value
+        res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, valread, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_retrieve failed with error: 0x%X\n", res);
+            res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, valread, post_fn_data);
+        }
+
+        // check result asynchronously
+        lock.lock();
+        while (iohdl->done == 0) {
+            iohdl->done_cond.wait(lock);
+        }
+        lock.unlock();
+
+        // expected result
+        if (iohdl->retcode != KV_SUCCESS) {
+            printf("kv_retrieve failed: 0x%X, done = %d\n", iohdl->retcode, iohdl->done.load());
+            exit(1);
+        }
+
+        // compare values
+        if (valread->length != valwritten->length) {
+            printf("value size is different: %u, %u\n", valread->length, valwritten->length);
+            exit(1);
+        }
+
+        if (memcmp(valread->value, valwritten->value, valread->length) != 0) {
+            printf("value is different: %u, %u, vlen = %d\n", valread->length, valwritten->length, vlen);
+            printf("value is different: %p, %s, %s\n", valread, (char*)valread->value, (char*)valwritten->value);
+            exit(1);
+        }
+
+        printf("kv_retrieve succeeded\n");
+
+        // restore IO handle to original
+        valread = iohdl->value;
+        iohdl->value = valwritten;
+    }
+
+    // done, free memories
+    free(valread->value);
+    free(valread);
+    free_kv(iohdl->key, iohdl->value);
+    free(post_fn_data);
+    free(iohdl);
+}
+
+void create_qpair(kv_device_handle dev_hdl,
+    std::map<kv_queue_handle, kv_queue_handle>& qpairs,
+    kv_queue_handle *sq_hdl,
+    kv_queue_handle *cq_hdl,
+    uint16_t sqid,
+    uint16_t cqid,
+    uint16_t q_depth,
+    kv_interrupt_handler int_handler) {
+    
+    // create a CQ
+    // ONLY DO THIS, if you have NOT created the CQ earlier
+    create_cq(dev_hdl, cqid, q_depth, cq_hdl);
+
+    // this is only for completion Q
+    // this will kick off completion Q processing
+    kv_set_interrupt_handler(*cq_hdl, int_handler); 
+
+    // create a SQ
+    create_sq(dev_hdl, sqid, q_depth, cqid, sq_hdl);
+
+    qpairs.insert(std::make_pair(*sq_hdl, *cq_hdl));
+
+}
+
+
+int main(int argc, char**argv) {
+    
+    char *device = NULL;
+
+    if (argc <  3) {
+        printf("Please run\n  %s <number of keys> <device>\n", argv[0]);
+        exit(1);
+
+    } else {
+        TOTAL_KV_COUNT = std::stoi(argv[1]);
+        printf("insert keys: %d\n", TOTAL_KV_COUNT);
+
+        device = argv[2];
+    }
+
+    int klen = 16;
+    int vlen = 4096;
+    int qdepth = 64;
+
+    kv_device_init_t dev_init;
+
+    // you can supply your own configuration file
+    // this is only valid for emulator
+    dev_init.configfile = "";
+
+    // the real kvssd device path
+    dev_init.devpath = device;
+    dev_init.need_persistency = FALSE;
+    dev_init.is_polling = FALSE;
+    dev_init.queuedepth = qdepth;
+
+    kv_device_handle dev_hdl = NULL;
+
+    // initialize the device
+    kv_result ret = kv_initialize_device(&dev_init, &dev_hdl);
+    if (ret != KV_SUCCESS) {
+        printf("device creation error\n");
+        exit(1);
+    }
+
+    // set up interrupt handler
+    _kv_interrupt_handler int_func = {interrupt_func, (void *)4, 4};
+    kv_interrupt_handler int_handler = &int_func;
+
+    // to keep track all opened queue pairs
+    std::map<kv_queue_handle, kv_queue_handle> qpairs;
+    kv_queue_handle sq_hdl;
+    kv_queue_handle cq_hdl;
+
+    // convenient wrapper function
+    // create a SQ/CQ pair, and start a thread to insert key value pairs
+    create_qpair(dev_hdl, qpairs,  &sq_hdl, &cq_hdl,
+            1,  // sqid
+            2,  // cqid
+            64, // q depth
+            int_handler);
+
+
+    // start a thread to insert key values through submission Q
+    printf("starting thread to insert key values\n");
+    std::thread th = std::thread(insert_kv_store, dev_hdl, sq_hdl, klen, vlen, TOTAL_KV_COUNT);
+    if (th.joinable()) {
+        th.join();
+    }
+
+    // graceful shutdown
+    // watch if all Qs are done
+    for (auto& qpair : qpairs) {
+        kv_queue_handle sqhdl = qpair.first;
+        kv_queue_handle cqhdl = qpair.second;
+    }
+    
+    /*
+    // delete queues
+    for (auto& p : qpairs) {
+        if (kv_delete_queue(dev_hdl, p.first) != KV_SUCCESS) {
+            printf("kv_delete_queue failed\n");
+            exit(1);
+        }
+        if (kv_delete_queue(dev_hdl, p.second) != KV_SUCCESS) {
+            printf("kv_delete_queue failed\n");
+            exit(1);
+        }
+    }
+    */
+    // shutdown
+    kv_cleanup_device(dev_hdl);
+    fprintf(stderr, "done\n");
+    return 0;
+}
diff --git a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/sample_code/sample_poll.cpp b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/sample_code/sample_poll.cpp
new file mode 100644
index 0000000..7158790
--- /dev/null
+++ b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/sample_code/sample_poll.cpp
@@ -0,0 +1,586 @@
+#include "assert.h"
+#include <string>
+#include <iostream>
+#include <sstream>
+#include <thread>
+#include <vector>
+
+#include <stdio.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sys/time.h>
+
+#include <mutex>
+#include <chrono>
+#include <thread>
+#include <set>
+#include <unordered_set>
+#include <map>
+#include <unordered_map>
+#include <iomanip>
+#include <ctime>
+#include <condition_variable>
+#include <atomic>
+#include "kvs_adi.h"
+
+// default settings
+int TOTAL_KV_COUNT = 5000;
+
+// device run time structure
+struct device_handle_t {
+    kv_device_handle dev_hdl;
+    kv_namespace_handle ns_hdl;
+    kv_queue_handle sq_hdl;
+    kv_queue_handle cq_hdl;
+    int qdepth;
+    // char *configfile;
+};
+
+
+typedef struct {
+    kv_key *key;
+    kv_value *value;
+    kv_result retcode;
+    kv_postprocess_function post_fn_data;
+
+    // value read back
+    kv_value *value_save;
+
+    // for kv_exist
+    char *buffer;
+    int buffer_size;
+    int buffer_count;
+    bool complete;
+
+    // for iterator
+    kv_iterator_handle hiter;
+
+    // depends if individual IO sync control is needed
+    std::condition_variable done_cond;
+    std::mutex mutex;
+    std::atomic<int> done;
+} iohdl_t;
+
+
+void free_value(kv_value *value) {
+    // free value
+    if (value != NULL) {
+        if (value->value != NULL) {
+            free(value->value);
+        }
+        free(value);
+    }
+}
+
+void free_key(kv_key *key) {
+    // free key
+    if (key != NULL) {
+        if (key->key != NULL) {
+            free(key->key);
+        }
+        free(key);
+    }
+}
+
+
+// this function will be called after completion of a command
+void interrupt_func(void *data, int number) {
+    (void) data;
+    (void) number;
+    // printf("inside interrupt\n");
+}
+
+int create_sq(kv_device_handle dev_hdl, uint16_t sqid, uint16_t qsize, uint16_t cqid, kv_queue_handle *quehdl) {
+
+    kv_queue qinfo;
+    qinfo.queue_id = sqid;
+    qinfo.queue_size = qsize;
+    qinfo.completion_queue_id = cqid;
+    qinfo.queue_type = SUBMISSION_Q_TYPE;
+    qinfo.extended_info = NULL;
+
+    if (KV_SUCCESS != kv_create_queue(dev_hdl, &qinfo, quehdl)) {
+        printf("submission queue creation failed\n");
+    }
+    return 0;
+}
+
+int create_cq(kv_device_handle dev_hdl, uint16_t cqid, uint16_t qsize, kv_queue_handle *quehdl) {
+
+    kv_queue qinfo;
+    qinfo.queue_id = cqid;
+    qinfo.queue_size = qsize;
+    qinfo.completion_queue_id = 0;
+    qinfo.queue_type = COMPLETION_Q_TYPE;
+    qinfo.extended_info = NULL;
+
+    if (KV_SUCCESS != kv_create_queue(dev_hdl, &qinfo, quehdl)) {
+        printf("completion queue creation failed\n");
+    }
+    return 0;
+}
+
+uint64_t current_timestamp() {
+    std::chrono::system_clock::time_point timepoint = std::chrono::system_clock::now();
+    uint64_t nano_from_epoch = std::chrono::duration_cast<std::chrono::nanoseconds>(timepoint.time_since_epoch()).count();
+    return nano_from_epoch;
+}
+
+// post processing function for a command
+// op->private_data is the data caller passed in
+// Please note ctx is internal data for read only.
+void on_IO_complete_func(kv_io_context *ctx) {
+    // depends if you allocated space for each command, you can manage
+    // allocated spaces here. This should in sync with your IO cycle
+    // management strategy.
+    //
+    // get completed async command results, free any allocated spaces if necessary.
+    //
+    // printf("finished one command\n");
+    iohdl_t *iohdl = (iohdl_t *) ctx->private_data;
+    iohdl->retcode = ctx->retcode;
+    iohdl->complete = true;
+
+    // this buffer was supplied by ASYNC call, just put it into IO handle
+    // for easier processing
+    if (ctx->opcode == KV_OPC_CHECK_KEY_EXIST) {
+        iohdl->buffer_size = ctx->result.buffer_size;
+        iohdl->buffer_count = ctx->result.buffer_count;
+    }
+
+    // this iterator is returned in response to ASYNC iterator open call
+    // must be retrieved by caller for iterator next calls
+    // then closed after use.
+    if (ctx->opcode == KV_OPC_OPEN_ITERATOR) {
+        iohdl->hiter = ctx->result.hiter;
+        iohdl->buffer_size = ctx->result.buffer_size;
+        iohdl->buffer_count = ctx->result.buffer_count;
+    }
+
+    // printf("command finished with result 0x%X\n", iohdl->retcode);
+    // std::unique_lock<std::mutex> lock(iohdl->mutex);
+    // iohdl->done = 1;
+    // iohdl->done_cond.notify_one();
+}
+
+// fill key value with random data
+void populate_key_value_startwith(uint32_t keystart, kv_key *key, kv_value *value) {
+
+    char *buffer = (char *)key->key;
+    uint32_t blen = key->length;
+
+    uint8_t *base = (uint8_t *)&keystart;
+
+    /*
+    uint8_t prefix[4] = {
+            *(base + 0),
+            *(base + 1),
+            *(base + 2),
+            *(base + 3) };
+    */
+
+    // if prefix is 0, skip prefix setup in key value
+    if (keystart != 0) {
+        assert(blen > 4);
+        // copy 4 bytes of prefix
+        memcpy(buffer, (char *)base, 4);
+        buffer += 4;
+        blen -= 4;
+    }
+
+    // printf("got key 4MSB 0x%X\n", keystart);
+    // printf("key 4MSB 0x%X\n", *(uint32_t *)(buffer - 4));
+
+    // use this to see deterministic sequence of keys
+    static unsigned long long current_time = 0;
+    current_time++;
+
+    char timestr[128];
+    snprintf(timestr, 128, "%llu", current_time);
+    memcpy(buffer, timestr, std::min(blen, (uint32_t)sizeof(timestr)));
+    // done with key
+
+    // generate value
+    buffer = (char *) value->value;
+    blen = value->length;
+    // ending with null
+    buffer[blen - 1] = 0;
+
+    // copy key, then add random value
+    memcpy(buffer, key->key, key->length);
+    blen -= key->length - 1;
+    buffer += key->length;
+    for (unsigned int i = 0; i < blen - 1; i++) {
+        unsigned int j = '1' + std::rand() % 30;
+        buffer[i] = j;
+    }
+
+    // printf("genrate a key %s, value %s\n", (char *) (key->key), (char *)value->value);
+    // printf("genrate a key %s\n", (char *) (key->key));
+}
+
+// set random values for those iohdls
+void populate_iohdls(iohdl_t *iohdls, uint32_t prefix, int klen, int vlen, uint32_t count) {
+    // set up IO handle and postprocessing
+
+    for (uint32_t i = 0; i < count; i++) {
+        // populate random value
+        iohdl_t *iohdl = iohdls + i;
+        populate_key_value_startwith(prefix, iohdl->key, iohdl->value);
+
+        kv_value *value = iohdl->value;
+        kv_value *value_save = iohdl->value_save;
+
+        memcpy(value_save->value, value->value, value->length);
+    }
+}
+
+// create array of io handles that hold keys and values
+void create_iohdls(iohdl_t **iohdls, uint32_t prefix, int klen, int vlen, uint32_t count) {
+    *iohdls = (iohdl_t *) calloc(count, sizeof(iohdl_t));
+
+    for (uint32_t i = 0; i < count; i++) {
+        iohdl_t *iohdl = (*iohdls) + i;
+
+        iohdl->post_fn_data.post_fn = on_IO_complete_func;
+        iohdl->post_fn_data.private_data = (void *) iohdl;
+
+        // allocate key value for reuse
+        kv_key *key = (kv_key *) malloc(sizeof(kv_key));
+        key->length = klen;
+        key->key = malloc(key->length);
+        memset(key->key, 0, key->length);
+
+        // value for insert
+        kv_value *val = (kv_value *) malloc(sizeof(kv_value));
+        val->length = vlen;
+        val->actual_value_size = vlen;
+        val->value = malloc(vlen);
+        memset(val->value, 0, val->length);
+        val->offset = 0;
+
+        // value for read back validation
+        kv_value *value_save = (kv_value *) malloc(sizeof(kv_value));
+        value_save->length = vlen;
+        value_save->actual_value_size = vlen;
+        value_save->value = malloc(vlen);
+        memset(value_save->value, 0, vlen);
+        value_save->offset = 0;
+
+        // save key value to the IO handle
+        iohdl->key = key;
+        iohdl->value = val;
+        iohdl->value_save = value_save;
+    }
+}
+
+void free_iohdls(iohdl_t *iohdls, uint32_t count) {
+    for (uint32_t i = 0; i < count; i++) {
+        iohdl_t *iohdl = iohdls + i;
+        free_key(iohdl->key);
+        free_value(iohdl->value);
+        free_value(iohdl->value_save);
+    }
+    free(iohdls);
+}
+
+// read back and compare
+uint64_t process_one_round_read(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        // printf("reading key: %s\n", key->key);
+        // prefix of keys
+        // printf("reading key 4MSB: 0x%X\n", *(uint32_t *)&key->key);
+
+        kv_value *value = (iohdls + i)->value;
+        memset(value->value, '0', value->length);
+        value->actual_value_size = 0;
+        (iohdls + i)->complete = false;
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_retrieve failed with error: 0x%X\n", res);
+            res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    int i = 0;
+    while (total_done < count) {
+        if((iohdls + i)->complete){
+            total_done++;
+            i++;
+        }
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+    }
+    uint64_t end = current_timestamp();
+
+    // compare read value with write value
+    for (uint32_t i = 0; i < count; i++) {
+        // printf("validating key: %s\n", (iohdls + i)->key->key);
+        if (memcmp((iohdls + i)->value_save->value, (iohdls + i)->value->value, (iohdls + i)->value->length)) {
+            // printf("failed: value read not the same as value written, key %s\n value + 16 %s\n value %s\n", (iohdls + i)->key->key, (iohdls + i)->value->value + 16, (iohdls + i)->value_save->value);
+            // XXX please check if any duplicate keys are used, only unique keys should
+            // be used, otherwise you may get false positive.
+            printf("failed: value read not the same as value written\n");
+            // printf("Note: please only use unique keys for this validation\n");
+            exit(1);
+        }
+
+        if ((iohdls + i)->value_save->actual_value_size != (iohdls + i)->value->actual_value_size) {
+            printf("failed: value full size not the same as value full size written\n");
+            exit(1);
+        }
+    }
+
+    // printf("polling total done: %d, inserted %d\n", total_done, count);
+    // double iotime  = double(total) / count;
+    // printf("average time per IO: %.1f (ns)\n", iotime);
+    return (end - start);
+}
+
+// insert enough keys but avoid excessive queue contention
+uint64_t process_one_round_write(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        kv_value *value = (iohdls + i)->value;
+        (iohdls + i)->complete = false;
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_store failed with error: 0x%X\n", res);
+            res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    int i = 0;
+    while (total_done < count) {
+        if((iohdls + i)->complete){
+            total_done++;
+            i++;
+        }
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+    }
+    uint64_t end = current_timestamp();
+
+    // printf("polling total done: %d, inserted %d\n", total_done, count);
+    // double iotime  = double(total) / count;
+    // printf("average time per IO: %.1f (ns)\n", iotime);
+    return (end - start);
+}
+
+// test cycle of read and write in rounds
+void sample_main(device_handle_t *device_handle, int klen, int vlen, int tcount, int qdepth) {
+
+    //kv_device_handle dev_hdl = device_handle->dev_hdl;
+    kv_namespace_handle ns_hdl = device_handle->ns_hdl;
+    kv_queue_handle sq_hdl = device_handle->sq_hdl;
+    kv_queue_handle cq_hdl = device_handle->cq_hdl;
+
+    // uint32_t prefix = 0xFFFF1234;
+    uint32_t prefix = 0;
+    iohdl_t *iohdls = NULL;
+
+    // break it into many rounds to minimize q contention
+    uint32_t count = qdepth;
+    create_iohdls(&iohdls, prefix, klen, vlen, count);
+
+    uint32_t left = tcount;
+    // total IO time in ns
+    uint64_t total_w = 0;
+    uint64_t total_r = 0;
+
+    while (left > 0) {
+        if (left < count) {
+            count = left;
+        }
+        left -= count;
+
+        // set up new random key values
+        populate_iohdls(iohdls, prefix, klen, vlen, count);
+
+        // test write and read
+        uint64_t time_w = process_one_round_write(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+        total_w += time_w;
+
+        uint64_t time_r = process_one_round_read(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+        total_r += time_r;
+    }
+
+    // done, free memories
+    free_iohdls(iohdls, qdepth);
+}
+
+void create_qpair(kv_device_handle dev_hdl,
+    kv_queue_handle *sq_hdl,
+    kv_queue_handle *cq_hdl,
+    uint16_t sqid,
+    uint16_t cqid,
+    uint16_t q_depth)
+{
+    
+    // create a CQ
+    // ONLY DO THIS, if you have NOT created the CQ earlier
+    create_cq(dev_hdl, cqid, q_depth, cq_hdl);
+
+    // this is only for completion Q
+    // this will kick off completion Q processing
+    // kv_set_interrupt_handler(cq_hdl, int_handler); 
+
+    // create a SQ
+    create_sq(dev_hdl, sqid, q_depth, cqid, sq_hdl);
+}
+
+
+void create_device(device_handle_t *device_handle, const char *devpath, int qdepth) {
+    // int klen = 16;
+    // int vlen = 4096;
+    // int qdepth = 64;
+    kv_device_init_t dev_init;
+
+    // you can supply your own configuration file
+    dev_init.configfile = "";
+    dev_init.devpath = devpath;
+    dev_init.need_persistency = FALSE;
+    dev_init.is_polling = TRUE;
+    dev_init.queuedepth = qdepth;
+
+    kv_device_handle dev_hdl = NULL;
+
+    device_handle->qdepth = qdepth;
+
+    // initialize the device
+    kv_result ret = kv_initialize_device(&dev_init, &dev_hdl);
+    if (ret != KV_SUCCESS) {
+        printf("device creation error\n");
+        exit(1);
+    }
+
+    kv_namespace_handle ns_hdl = NULL;
+    get_namespace_default(dev_hdl, &ns_hdl);
+
+    // set up interrupt handler
+    // _kv_interrupt_handler int_func = {interrupt_func, (void *)4, 4};
+    // kv_interrupt_handler int_handler = &int_func;
+    kv_queue_handle sq_hdl;
+    kv_queue_handle cq_hdl;
+
+    // convenient wrapper function
+    // create a SQ/CQ pair
+    create_qpair(dev_hdl, &sq_hdl, &cq_hdl,
+            1,  // sqid
+            2,  // cqid
+            qdepth); // q depth
+
+    device_handle->dev_hdl = dev_hdl;
+    device_handle->ns_hdl = ns_hdl;
+    device_handle->sq_hdl = sq_hdl;
+    device_handle->cq_hdl = cq_hdl;
+    device_handle->qdepth = qdepth;
+}
+
+
+void shutdown_device(device_handle_t *device_handle) {
+    if (device_handle == NULL) {
+        return;
+    }
+
+    kv_device_handle dev_hdl = device_handle->dev_hdl;
+    kv_namespace_handle ns_hdl = device_handle->ns_hdl;
+    kv_queue_handle sq_hdl = device_handle->sq_hdl;
+    kv_queue_handle cq_hdl = device_handle->cq_hdl;
+    
+    // delete queues
+    if (kv_delete_queue(dev_hdl, sq_hdl) != KV_SUCCESS) {
+        printf("kv_delete_queue failed\n");
+        exit(1);
+    }
+    if (kv_delete_queue(dev_hdl, cq_hdl) != KV_SUCCESS) {
+        printf("kv_delete_queue failed\n");
+        exit(1);
+    }
+
+    // shutdown
+    kv_delete_namespace(dev_hdl, ns_hdl);
+    kv_cleanup_device(dev_hdl);
+}
+
+
+int main(int argc, char**argv) {
+
+    char *device = NULL;
+    
+    if (argc <  3) {
+        printf("Please run\n  %s <number of keys> <device>\n", argv[0]);
+        exit(1);
+
+    } else {
+        TOTAL_KV_COUNT = std::stoi(argv[1]);
+        printf("insert keys: %d\n", TOTAL_KV_COUNT);
+
+        device = argv[2];
+    }
+
+    int klen = 16;
+    int vlen = 4096;
+    int qdepth = 64;
+
+    device_handle_t *device1 = (device_handle_t *) malloc(sizeof (device_handle_t));
+    device_handle_t *device2 = (device_handle_t *) malloc(sizeof (device_handle_t));
+
+    printf("creating device 1\n");
+    create_device(device1, device, qdepth);
+    printf("creating device 2\n");
+    create_device(device2, device, qdepth);
+
+    // start a thread to insert key values through submission Q
+    // then read them back to validate correctness
+    printf("starting operation on device 1\n");
+    std::thread th = std::thread(sample_main, device1, klen, vlen, TOTAL_KV_COUNT, qdepth);
+
+    printf("starting operation on device 2\n");
+    std::thread th1 = std::thread(sample_main, device2, klen, vlen, TOTAL_KV_COUNT, qdepth);
+
+    if (th.joinable()) {
+        th.join();
+    }
+    printf("stop operation on device 1\n");
+
+    if (th1.joinable()) {
+        th1.join();
+    }
+    printf("stop operation on device 2\n");
+
+    printf("shutdown all devices\n");
+    shutdown_device(device1);
+    shutdown_device(device2);
+    free(device1);
+    free(device2);
+
+    printf("all done\n");
+    return 0;
+}
diff --git a/PDK/core/src/device_abstract_layer/kernel_driver_adapter/test/test_suite.cpp b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/test/test_suite.cpp
new file mode 100644
index 0000000..86a0be5
--- /dev/null
+++ b/PDK/core/src/device_abstract_layer/kernel_driver_adapter/test/test_suite.cpp
@@ -0,0 +1,1752 @@
+#include "assert.h"
+#include <string>
+#include <iostream>
+#include <sstream>
+#include <thread>
+#include <vector>
+
+#include <stdio.h>
+#include <stdint.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sys/time.h>
+
+#include <mutex>
+#include <chrono>
+#include <thread>
+#include <set>
+#include <unordered_set>
+#include <map>
+#include <unordered_map>
+#include <iomanip>
+#include <ctime>
+#include <condition_variable>
+#include <atomic>
+#include "kvs_adi.h"
+#include "kadi.h"
+
+// default settings
+int TOTAL_KV_COUNT = 256;
+int g_KEYLEN_FIXED = 0;
+int g_KEYLEN = 16;
+unsigned int PREFIX_KV = 0;
+unsigned int BITMASK_KV = 0;
+int g_complete_count = 0;
+
+#define KVS_ERR_KEY_NOT_EXIST 0x310
+
+// device run time structure
+struct device_handle_t {
+    kv_device_init_t dev_init;
+    kv_device_handle dev_hdl;
+    kv_namespace_handle ns_hdl;
+    kv_queue_handle sq_hdl;
+    kv_queue_handle cq_hdl;
+    int qdepth;
+    // char *configfile;
+};
+
+
+typedef struct {
+    kv_key *key;
+    kv_value *value;
+    kv_result retcode;
+    kv_postprocess_function post_fn_data;
+
+    // value read back
+    kv_value *value_save;
+
+    // for kv_exist
+    char *buffer;
+    int buffer_size;
+    int buffer_count;
+    kv_iterator_list* iter_list;
+
+    // for iterator
+    kv_iterator_handle hiter;
+
+    // depends if individual IO sync control is needed
+    std::condition_variable done_cond;
+    std::mutex mutex;
+    std::atomic<int> done;
+} iohdl_t;
+
+
+void free_value(kv_value *value) {
+    // free value
+    if (value != NULL) {
+        if (value->value != NULL) {
+            free(value->value);
+        }
+        free(value);
+    }
+}
+
+void free_key(kv_key *key) {
+    // free key
+    if (key != NULL) {
+        if (key->key != NULL) {
+            free(key->key);
+        }
+        free(key);
+    }
+}
+
+
+// this function will be called after completion of a command
+void interrupt_func(void *data, int number) {
+    (void) data;
+    (void) number;
+    // printf("inside interrupt\n");
+}
+
+int create_sq(kv_device_handle dev_hdl, uint16_t sqid, uint16_t qsize, uint16_t cqid, kv_queue_handle *quehdl) {
+
+    kv_queue qinfo;
+    qinfo.queue_id = sqid;
+    qinfo.queue_size = qsize;
+    qinfo.completion_queue_id = cqid;
+    qinfo.queue_type = SUBMISSION_Q_TYPE;
+    qinfo.extended_info = NULL;
+
+    if (KV_SUCCESS != kv_create_queue(dev_hdl, &qinfo, quehdl)) {
+        printf("submission queue creation failed\n");
+    }
+    return 0;
+}
+
+int create_cq(kv_device_handle dev_hdl, uint16_t cqid, uint16_t qsize, kv_queue_handle *quehdl) {
+
+    kv_queue qinfo;
+    qinfo.queue_id = cqid;
+    qinfo.queue_size = qsize;
+    qinfo.completion_queue_id = 0;
+    qinfo.queue_type = COMPLETION_Q_TYPE;
+    qinfo.extended_info = NULL;
+
+    if (KV_SUCCESS != kv_create_queue(dev_hdl, &qinfo, quehdl)) {
+        printf("completion queue creation failed\n");
+    }
+    return 0;
+}
+
+uint64_t current_timestamp() {
+    std::chrono::system_clock::time_point timepoint = std::chrono::system_clock::now();
+    uint64_t nano_from_epoch = std::chrono::duration_cast<std::chrono::nanoseconds>(timepoint.time_since_epoch()).count();
+    return nano_from_epoch;
+}
+
+int reformat_iterbuffer(kv_iterator_list *iter_list)
+{
+  static const int KEY_LEN_BYTES = 4;
+  int ret = 0;
+  unsigned int key_size = 0;
+  int keydata_len_with_padding = 0;
+  char *data_buff = (char *)iter_list->it_list;
+  unsigned int buffer_size = iter_list->size;
+  unsigned int key_count = iter_list->num_entries;
+  // according to firmware command output format, convert them to KVAPI expected format without any padding
+  // all data alreay in user provided buffer, but need to remove padding bytes to conform KVAPI format
+  char *current_ptr = data_buff;
+  unsigned int buffdata_len = buffer_size;
+
+  if (current_ptr == 0) return KV_ERR_PARAM_INVALID;
+
+  if (buffdata_len < KEY_LEN_BYTES) { return KV_ERR_SYS_IO;  }
+
+  buffdata_len -= KEY_LEN_BYTES;
+  data_buff += KEY_LEN_BYTES;
+  for (uint32_t i = 0; i < key_count && buffdata_len > 0; i++)
+  {
+    if (buffdata_len < KEY_LEN_BYTES)
+    {
+      ret = KV_ERR_SYS_IO;
+      break;
+    }
+
+    // move 4 byte key len
+    memmove(current_ptr, data_buff, KEY_LEN_BYTES);
+    current_ptr += KEY_LEN_BYTES;
+
+    // get key size
+    key_size = *((uint32_t *)data_buff);
+    buffdata_len -= KEY_LEN_BYTES;
+    data_buff += KEY_LEN_BYTES;
+
+    if ((key_size > buffdata_len) || (key_size >= KVCMD_MAX_KEY_SIZE))
+    {
+      ret = KV_ERR_SYS_IO;
+      break;
+    }
+
+    // move key data
+    memmove(current_ptr, data_buff, key_size);
+    current_ptr += key_size;
+
+    // calculate 4 byte aligned current key len including padding bytes
+    keydata_len_with_padding = (((key_size + 3) >> 2) << 2);
+
+    // skip to start position of next key
+    buffdata_len -= keydata_len_with_padding;
+    data_buff += keydata_len_with_padding;
+  }
+  return ret;
+}
+
+// post processing function for a command
+// op->private_data is the data caller passed in
+// Please note ctx is internal data for read only.
+void on_IO_complete_func(kv_io_context *ctx) {
+    // depends if you allocated space for each command, you can manage
+    // allocated spaces here. This should in sync with your IO cycle
+    // management strategy.
+    //
+    // get completed async command results, free any allocated spaces if necessary.
+    //
+    // printf("finished one command\n");
+    g_complete_count++;
+    iohdl_t *iohdl = (iohdl_t *) ctx->private_data;
+    iohdl->retcode = ctx->retcode;
+
+    // this buffer was supplied by ASYNC call, just put it into IO handle
+    // for easier processing
+    if (ctx->opcode == KV_OPC_CHECK_KEY_EXIST) {
+        iohdl->buffer_size = ctx->result.buffer_size;
+        iohdl->buffer_count = ctx->result.buffer_count;
+    }
+
+    // this iterator is returned in response to ASYNC iterator open call
+    // must be retrieved by caller for iterator next calls
+    // then closed after use.
+    if (ctx->opcode == KV_OPC_OPEN_ITERATOR) {
+        iohdl->hiter = ctx->result.hiter;
+        //iohdl->buffer_size = ctx->result.buffer_size;
+        //iohdl->buffer_count = ctx->result.buffer_count;
+    }
+    if (ctx->opcode == nvme_cmd_kv_iter_read) {
+        iohdl->iter_list->end = (bool_t)ctx->hiter.end;
+        iohdl->iter_list->size =  ctx->hiter.buflength;
+        if(ctx->hiter.buf){
+            iohdl->iter_list->num_entries = *((uint32_t *)ctx->hiter.buf);
+            reformat_iterbuffer(iohdl->iter_list);
+        }
+    }
+
+    // printf("command finished with result 0x%X\n", iohdl->retcode);
+    // std::unique_lock<std::mutex> lock(iohdl->mutex);
+    // iohdl->done = 1;
+    // iohdl->done_cond.notify_one();
+}
+
+// fill key value with random data
+void populate_key_value_startwith(uint32_t keystart, kv_key *key, kv_value *value) {
+
+    char *buffer = (char *)key->key;
+    uint32_t blen = key->length;
+
+    uint8_t *base = (uint8_t *)&keystart;
+
+    /*
+    uint8_t prefix[4] = {
+            *(base + 0),
+            *(base + 1),
+            *(base + 2),
+            *(base + 3) };
+    */
+
+    // if prefix is 0, skip prefix setup in key value
+    if (keystart != 0) {
+        assert(blen > 4);
+        // copy 4 bytes of prefix
+        memcpy(buffer, (char *)base, 4);
+        buffer += 4;
+        blen -= 4;
+    }
+
+    // printf("got key 4MSB 0x%X\n", keystart);
+    // printf("key 4MSB 0x%X\n", *(uint32_t *)(buffer - 4));
+
+    // use this to see deterministic sequence of keys
+    static unsigned long long current_time = 0;
+    current_time++;
+
+    char timestr[128];
+    snprintf(timestr, 128, "%llu", current_time);
+    memcpy(buffer, timestr, std::min(blen, (uint32_t)sizeof(timestr)));
+    // done with key
+
+    // generate value
+    buffer = (char *) value->value;
+    blen = value->length;
+    // ending with null
+    buffer[blen - 1] = 0;
+
+    // copy key, then add random value
+    memcpy(buffer, key->key, key->length);
+    blen -= key->length - 1;
+    buffer += key->length;
+    for (unsigned int i = 0; i < blen - 1; i++) {
+        unsigned int j = '1' + std::rand() % 30;
+        buffer[i] = j;
+    }
+
+    // printf("genrate a key %s, value %s\n", (char *) (key->key), (char *)value->value);
+    // printf("genrate a key %s\n", (char *) (key->key));
+}
+
+// set random values for those iohdls
+void populate_iohdls(iohdl_t *iohdls, uint32_t prefix, int klen, int vlen, uint32_t count) {
+    // set up IO handle and postprocessing
+
+    for (uint32_t i = 0; i < count; i++) {
+        // populate random value
+        iohdl_t *iohdl = iohdls + i;
+        populate_key_value_startwith(prefix, iohdl->key, iohdl->value);
+
+        kv_value *value = iohdl->value;
+        kv_value *value_save = iohdl->value_save;
+
+        memcpy(value_save->value, value->value, value->length);
+    }
+}
+
+// create array of io handles that hold keys and values
+void create_iohdls(iohdl_t **iohdls, uint32_t prefix, int klen, int vlen, uint32_t count) {
+    *iohdls = (iohdl_t *) calloc(count, sizeof(iohdl_t));
+
+    for (uint32_t i = 0; i < count; i++) {
+        iohdl_t *iohdl = (*iohdls) + i;
+
+        iohdl->post_fn_data.post_fn = on_IO_complete_func;
+        iohdl->post_fn_data.private_data = (void *) iohdl;
+
+        // allocate key value for reuse
+        kv_key *key = (kv_key *) malloc(sizeof(kv_key));
+        key->length = klen;
+        key->key = malloc(key->length);
+        memset(key->key, 0, key->length);
+
+        // value for insert
+        kv_value *val = (kv_value *) malloc(sizeof(kv_value));
+        val->length = vlen;
+
+        val->actual_value_size = vlen;
+        val->value = malloc(vlen);
+        memset(val->value, 0, val->length);
+        val->offset = 0;
+
+        // value for read back validation
+        kv_value *value_save = (kv_value *) malloc(sizeof(kv_value));
+        value_save->length = vlen;
+
+        value_save->actual_value_size = vlen;
+        value_save->value = malloc(vlen);
+        memset(value_save->value, 0, vlen);
+        value_save->offset = 0;
+
+        // save key value to the IO handle
+        iohdl->key = key;
+        iohdl->value = val;
+        iohdl->value_save = value_save;
+    }
+}
+
+void free_iohdls(iohdl_t *iohdls, uint32_t count) {
+    for (uint32_t i = 0; i < count; i++) {
+        iohdl_t *iohdl = iohdls + i;
+        free_key(iohdl->key);
+        free_value(iohdl->value);
+        free_value(iohdl->value_save);
+    }
+    free(iohdls);
+}
+
+void change_value(kv_value *value) {
+    // generate value
+    char *buffer = (char *) value->value;
+    uint32_t blen = value->length;
+    // ending with null
+    buffer[blen - 1] = 0;
+    for (unsigned int i = 0; i < blen - 1; i++) {
+        unsigned int j = '0' + std::rand() % 30;
+        buffer[i] = j;
+    }
+}
+
+// delete a given key
+uint64_t delete_key(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, kv_key *key, uint32_t count) {
+
+    iohdl_t iohdl;
+    memset(&iohdl, 0, sizeof (iohdl));
+    iohdl.key = key;
+    g_complete_count = 0;
+    kv_postprocess_function post_fn_data = { on_IO_complete_func, &iohdl};
+    kv_result res = kv_delete(sq_hdl, ns_hdl, key, KV_DELETE_OPT_DEFAULT, &post_fn_data);
+    while (res != KV_SUCCESS) {
+        printf("kv_store failed with error: 0x%X\n", res);
+        res = kv_delete(sq_hdl, ns_hdl, key, KV_DELETE_OPT_DEFAULT, &post_fn_data);
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (g_complete_count < 1) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+    }
+
+    if (iohdl.retcode != KV_SUCCESS) {
+        printf("delete one key failed: return code 0x%03X\n", iohdl.retcode);
+        exit(1);
+    }
+
+    return 0;
+}
+
+// check exist of a given key
+bool check_key_exist(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, kv_key *key) {
+
+    uint8_t status = 0;
+    g_complete_count = 0;
+    iohdl_t iohdl;
+    memset(&iohdl, 0, sizeof (iohdl));
+    iohdl.key = key;
+    kv_postprocess_function post_fn_data = { on_IO_complete_func, &iohdl};
+    kv_result res = kv_exist(sq_hdl, ns_hdl, key, 1, 1, &status, &post_fn_data);
+    while (res != KV_SUCCESS) {
+        printf("kv_exist failed with error: 0x%X\n", res);
+        res = kv_exist(sq_hdl, ns_hdl, key, 1, 1, &status, &post_fn_data);
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (g_complete_count < 1) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+    }
+
+    if (iohdl.retcode == KV_SUCCESS && status == 0) {
+        // printf("key found\n");
+        return 1;
+    } else if (iohdl.retcode == KVS_ERR_KEY_NOT_EXIST && status == 0) {
+        // printf("key not found\n");
+        return 0;
+    } else {
+        printf("kv_exist returned error! exit\n");
+        exit(1);
+    }
+}
+
+
+uint64_t process_one_round_read_partial(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+
+        // use another buffer to read back
+        kv_value *value = (iohdls + i)->value;
+        kv_value *value_save = (iohdls + i)->value_save;
+        memcpy(value_save->value, value->value, value->length);
+        value_save->actual_value_size = value->actual_value_size;
+
+        // printf("original value XXX\n%s\n", value->value);
+        // printf("copied value YYY key: %s\n%s\n", key->key, value_save->value);
+        memset(value->value, 0, value->length);
+
+        // reset full size to be filled
+        (iohdls + i)->value->actual_value_size = 0;
+        // set offset for retrieve
+        (iohdls + i)->value->offset = std::rand() % (iohdls + i)->value->length;
+
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_retrieve failed with error: 0x%X\n", res);
+            res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    uint32_t total_done = 0;
+    while (total_done < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+    uint64_t end = current_timestamp();
+    // compare read value with write value
+    for (uint32_t i = 0; i < count; i++) {
+        // printf("validating key: %s\n", (iohdls + i)->key->key);
+        uint32_t retrieved_len = (iohdls + i)->value->length;
+        if (retrieved_len != ((iohdls + i)->value_save->length - (iohdls + i)->value->offset)) {
+            printf("retrieved partial value length not matched\n");
+            exit(1);
+        }
+
+        if (memcmp((char *)(iohdls + i)->value_save->value + (iohdls + i)->value->offset, (char *)(iohdls + i)->value->value, retrieved_len)) {
+            printf("partial value read at offset %d, len %u not the same as value written, key %s.\n", (iohdls + i)->value->offset, retrieved_len, (char *)(iohdls + i)->key->key);
+            // printf("partial value read is: XXX\n%s\n", (char *)(iohdls + i)->value->value);
+            // printf("full value read is: XXX\n%s\n", (char *)(iohdls + i)->value_save->value);
+            exit(1);
+        }
+
+        if ((iohdls + i)->value_save->actual_value_size != (iohdls + i)->value->actual_value_size) {
+            printf("returned full value size doesn't match from partial read\n");
+        }
+    }
+
+    // reset
+    for (uint32_t i = 0; i < count; i++) {
+        // reset offset after testing
+        kv_value *value = (iohdls + i)->value;
+        kv_value *value_save = (iohdls + i)->value_save;
+        value->offset = 0;
+        memcpy(value->value, value_save->value, value_save->length);
+        value->length = value_save->length;
+    }
+
+    // printf("polling total done: %d, inserted %d\n", total_done, count);
+    // double iotime  = double(total) / count;
+    // printf("average time per IO: %.1f (ns)\n", iotime);
+    return (end - start);
+}
+
+// read back and compare
+uint64_t process_one_round_read(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    g_complete_count = 0;
+    uint64_t start = current_timestamp();
+    // insert key value
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        // printf("reading key: %s\n", key->key);
+        // prefix of keys
+        // printf("reading key 4MSB: 0x%X\n", *(uint32_t *)&key->key);
+
+        kv_value *value = (iohdls + i)->value;
+        memset(value->value, '0', value->length);
+
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_retrieve failed with error: 0x%X\n", res);
+            res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    while (g_complete_count < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+    }
+    uint64_t end = current_timestamp();
+
+    // compare read value with write value
+    for (uint32_t i = 0; i < count; i++) {
+        // printf("validating key: %s\n", (iohdls + i)->key->key);
+        if (memcmp((iohdls + i)->value_save->value, (iohdls + i)->value->value, (iohdls + i)->value->length)) {
+            // printf("failed: value read not the same as value written, key %s\n value + 16 %s\n value %s\n", (iohdls + i)->key->key, (iohdls + i)->value->value + 16, (iohdls + i)->value_save->value);
+            // XXX please check if any duplicate keys are used, only unique keys should
+            // be used, otherwise you may get false positive.
+            printf("failed: value read not the same as value written, original data len %d, device returned data len %d\n", (iohdls + i)->value_save->length, (iohdls + i)->value->length);
+            // printf("Note: please only use unique keys for this validation\n");
+            exit(1);
+        }
+    }
+
+    // printf("polling total done: %d, inserted %d\n", total_done, count);
+    // double iotime  = double(total) / count;
+    // printf("average time per IO: %.1f (ns)\n", iotime);
+    return (end - start);
+}
+
+// insert enough keys but avoid excessive queue contention
+uint64_t process_one_round_write(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    g_complete_count = 0;
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        kv_value *value = (iohdls + i)->value;
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_store failed with error: 0x%X\n", res);
+            res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+            exit(1);
+        }
+    }
+
+    // poll for completion
+    while (g_complete_count < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+    }
+    uint64_t end = current_timestamp();
+
+    // printf("polling total done: %d, inserted %d\n", total_done, count);
+    // double iotime  = double(total) / count;
+    // printf("average time per IO: %.1f (ns)\n", iotime);
+    return (end - start);
+}
+
+
+
+typedef struct iterator_entries_t {
+    kv_group_condition grp_cond;
+    kv_iterator_option iter_op;
+    uint32_t prefix;
+
+    // expected total # for iteration
+    uint32_t expected_total;
+
+    // key values should be matched these
+    std::set<std::string> keys;
+    std::set<std::string> values;
+} iterator_entries_t;
+
+
+
+
+//////////////////////////////////////////
+// process iterator returned buffer
+// use global g_KEYLEN to decide the fixed length of keys
+void processing_iterator_returned_keys_fixed(kv_iterator_list *iter_list, std::vector<std::string>& keys) {
+
+    uint8_t *buffer = (uint8_t *) iter_list->it_list;
+    // uint32_t blen = iter_list->size;
+    uint32_t num_entries = iter_list->num_entries;
+    uint32_t klen = g_KEYLEN;
+    // uint32_t vlen = sizeof(kv_value_t);
+    while (num_entries > 0) {
+        keys.push_back(std::string((char *) buffer, klen));
+        // Validating
+        //printf("[%d] key:%d\n",num_entries,keys[num_entries-1]);
+        buffer += klen;
+        num_entries--;
+    }
+}
+
+/////
+// print key
+/////
+void print_key(std::string& key) {
+    char *data = (char *)key.data();
+
+    int len = key.size();
+    int i = 0;
+
+    printf("key is: ");
+    while (i <= (len - 4)) {
+        uint32_t prefix = 0;
+        memcpy(&prefix, data + i, 4);
+
+        printf(" %08X", prefix);
+        i += 4;
+    }
+    printf("\n");
+}
+
+void print_key(kv_key *key) {
+    char *data = (char *)key->key;
+
+    int len = key->length;
+    int i = 0;
+
+    printf("key is: ");
+    while (i <= (len - 4)) {
+        uint32_t prefix = 0;
+        memcpy(&prefix, data + i, 4);
+
+        printf(" %08X", prefix);
+        i += 4;
+    }
+    printf("\n");
+}
+
+
+
+////////////////////////////////////////////////
+// the test assumes all keys and values are unique
+// when data are generated and inserted 
+void validate_iterator_results(iterator_entries_t& iterator_entries, std::vector<std::string>& keys, std::vector<std::string>& values) {
+
+    std::set<std::string> returned_keys;
+
+    //uint32_t inserted_n = iterator_entries.keys.size();
+    uint32_t inserted_n = iterator_entries.expected_total;
+    uint32_t keyn = keys.size();
+    uint32_t valn = values.size();
+    //printf("inserted_n:%d, keyn:%d\n",inserted_n,keyn);
+    if (keyn != inserted_n) {
+        printf("iterator returned different key counts from what's inserted\n");
+        printf("inserted %d keys, iterator returned %d keys\n", inserted_n, keyn);
+        exit(1);
+    }
+
+    // check iterator returned keys
+    for (uint32_t i = 0; i < keyn; i++) {
+        std::string kstr = keys[i];
+
+        std::set<std::string>::iterator it1 = returned_keys.find(kstr);
+        if (it1 == returned_keys.end()) {
+            returned_keys.insert(keys[i]);
+        } else {
+            // repeated keys from iterator result
+            printf("Info: iterator returned repeated key: ");
+            print_key(kstr);
+        }
+
+        auto it = iterator_entries.keys.find(kstr);
+        auto ite = iterator_entries.keys.end();
+        if (it == ite) {
+            printf("Error: iterator returned extra: ");
+            print_key(kstr);
+            // XXX
+            // printf("iterator returned key: %s\n", kstr.c_str());
+            // printf("iterator returned key not found in original inserted set\n");
+            // exit(1);
+        }
+    }
+
+    // check original inserted keys
+    // printf("original inserted keys\n");
+    for (std::string kstr : iterator_entries.keys) {
+        auto it = returned_keys.find(kstr);
+        auto ite = returned_keys.end();
+        if (it == ite) {
+            printf("Error: iterator missed key: ");
+            print_key(kstr);
+            // XXX
+            // printf("iterator returned key: %s\n", kstr.c_str());
+            // printf("iterator returned key not found in original inserted set\n");
+            // exit(1);
+        }
+    }
+
+    // check values 
+    if (valn == 0) {
+        return;
+    }
+
+    /*for (uint32_t i = 0; i < valn; i++) {
+        std::string vstr = values[i];
+        auto it = iterator_entries.values.find(vstr);
+        if (it == iterator_entries.values.end()) {
+            printf("iterator returned values not found in original inserted set\n");
+            exit(1);
+        }
+    }*/
+}
+
+void processing_iterator_returned_keys_variable(kv_iterator_list *iter_list, std::vector<std::string>& keys) {
+    uint8_t *buffer = (uint8_t *) iter_list->it_list;
+    // uint32_t blen = iter_list->size;
+    uint32_t num_entries = iter_list->num_entries;
+    uint32_t klen = sizeof(uint32_t);
+    uint32_t klen_value = 0;
+    // uint32_t vlen = sizeof(kv_value_t);
+    // uint32_t vlen_value = 0;
+
+    while (num_entries > 0) {
+        // first get klen
+        uint8_t* addr = (uint8_t *) &klen_value;
+        for (unsigned int i = 0; i < klen; i++) {
+            *(addr + i) = *(buffer + i);
+        }
+        buffer += klen;
+        keys.push_back(std::string((char *) buffer, klen_value));
+        buffer += klen_value;
+        num_entries--;
+    }
+}
+
+// XXX use a global to check if key is fixed size or not
+// default is fixed size
+// Currently it supports fixed size KEYS only.
+void processing_iterator_returned_keyvals(kv_iterator_list *iter_list, kv_iterator_option iter_op, std::vector<std::string>& keys, std::vector<std::string>& values) {
+    // now check returned key/values
+    if (iter_op == KV_ITERATOR_OPT_KEY) {
+        if (g_KEYLEN_FIXED) {
+            processing_iterator_returned_keys_fixed(iter_list, keys);
+        } else {
+            processing_iterator_returned_keys_variable(iter_list, keys);
+        }
+    }
+
+    /*if (iter_op == KV_ITERATOR_OPT_KV) {
+        if (g_KEYLEN_FIXED) {
+            processing_iterator_returned_keyvals_fixed(iter_list, keys, values);
+        } else {
+            processing_iterator_returned_keyvals_variable(iter_list, keys, values);
+        }
+    }*/
+}
+
+
+void get_iterator_results(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, 
+    iohdl_t *iohdls, uint32_t count,
+    kv_group_condition *grp_cond, kv_iterator_option iter_op, 
+    std::vector<std::string>& keys, std::vector<std::string>& values) {
+
+    //////////////////////////////////
+    // open iterator
+    // iohdl.hiter (a handle)
+    iohdl_t iohdl;
+    // kv_iterator_option iter_op = KV_ITERATOR_OPT_KEY;
+    // kv_iterator_option iter_op = KV_ITERATOR_OPT_KV;
+    
+    kv_postprocess_function post_fn_data = { on_IO_complete_func, &iohdl};
+    kv_result res = kv_open_iterator_sync(sq_hdl, ns_hdl, iter_op, grp_cond, &iohdl.hiter);
+    while (res != KV_SUCCESS) {
+        printf("kv_open_iterator failed with error: 0x%X\n", res);
+        exit(1);
+    }
+
+    // poll for completion
+    /*uint32_t total_done = 0;
+    while (total_done < 1) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion 1 error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+    
+    if (iohdl.retcode != KV_SUCCESS) {
+        printf("kv_open_iterator failed failed: return code 0x%03X\n", iohdl.retcode);
+        //res = kv_close_iterator(sq_hdl, ns_hdl, iohdl.hiter, &post_fn_data);
+        if (iohdl.retcode == 0x903){
+            // too many iterators open            
+            printf("Too Many Iterators opens with error: 0x%03x",iohdl.retcode);
+        }
+        printf("Exiting\n");
+        exit(1);
+    }*/
+ 
+    //////////////////////////////////
+    // iterator next
+    kv_iterator_handle hiter = iohdl.hiter;
+    kv_iterator_list iter_list;
+    iohdl.iter_list = &iter_list;
+    uint32_t buffer_size = 32*1024;
+    //uint8_t buffer[buffer_size];
+    uint8_t *buffer;
+    iter_list.size = 32*1024;
+    //4K aligning the 32K buffer
+    int ret = posix_memalign((void **)&buffer, 4096, buffer_size);
+    if (ret || !buffer) {
+        printf("fail to alloc buf size %d\n", buffer_size);
+        //res = -ENOMEM;
+        exit(1);
+
+    }
+    //printf("After posix\n");
+    memset(buffer, 0, buffer_size);
+    //printf("Before Seg\n");
+    iter_list.it_list = (uint8_t *) buffer;
+    //printf("After Seg\n");
+    iter_list.num_entries = 0;
+    iter_list.end = FALSE;
+    uint32_t total = 0;
+    uint32_t run_cnt = 0; 
+    
+    while (1) {
+        iter_list.size = buffer_size;
+        memset(iter_list.it_list, 0, buffer_size); 
+        iter_list.num_entries = 0;        
+        g_complete_count = 0;
+        //printf("Before kv_iterator_next\n");
+        res = kv_iterator_next(sq_hdl, ns_hdl, hiter, &iter_list, &post_fn_data);
+        //printf("After kv_iterator_next\n");
+        // poll for completion
+        while (g_complete_count < 1) {
+            uint32_t finished = 0;
+            kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+            if (res != KV_SUCCESS && !iter_list.end) {
+                printf("kv_poll_completion 2 error! exit\n");
+                exit(1);
+            }
+            if (iter_list.end){
+                // Scan finished, these are the last entries
+                g_complete_count = 1;
+            }
+        }
+ 
+        if (iohdl.retcode != KV_SUCCESS && !iter_list.end) {
+            printf("kv_iterator_next_set failed: return code 0x%03X\n", iohdl.retcode);
+            exit(1);
+        }
+
+        // now check returned key/values
+        //printf("Before processing_iterator_returned_keyvals");
+        processing_iterator_returned_keyvals(&iter_list, iter_op, keys, values);
+       
+        printf("client got entries:%d\n", (uint32_t *)(iter_list.num_entries)); 
+        total += iter_list.num_entries;
+        //printf("going to stick in the loop\n");
+        if (iter_list.end) {
+            printf("Total Entries:%d\n",total);
+            //printf("didnt get  stuck in the loop\n");
+            break;
+        }
+    }
+   
+   // printf("client got total entries:%d\n", total);
+
+
+    // close iterator
+    //////////////////////////////////
+    // iohdl_t iohdl;
+    // iohdl.hiter (a handle)
+    // kv_postprocess_function post_fn_data = { on_IO_complete_func, &iohdl};
+    res = kv_close_iterator_sync(sq_hdl, ns_hdl, iohdl.hiter);
+    
+    if(buffer){ free(buffer);}
+    
+    while (res != KV_SUCCESS) {
+        printf(" kv_close_iterator failed There with error: 0x%X\n", res);
+        //res = kv_close_iterator(sq_hdl, ns_hdl, iohdl.hiter, &post_fn_data);
+        exit(1);
+    }
+
+    // poll for completion
+    /*total_done = 0;
+    while (total_done < 1) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion Here error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }
+
+    if (iohdl.retcode != KV_SUCCESS) {
+        printf("In the end kv_close_iterator failed: return code 0x%03X\n", iohdl.retcode);
+        exit(1);
+    }*/
+}
+
+// generate a set of data, and insert them, record them for iterator use
+void generate_prefixed_dataset(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count, iterator_entries_t& entries, uint32_t prefix, kv_group_condition *grp_cond, kv_iterator_option iter_op, uint32_t expected_total) {
+    //////////////////////////////
+    // generate a set
+    entries.grp_cond = *grp_cond;
+    entries.iter_op = iter_op;
+    entries.prefix = prefix;
+    entries.expected_total = expected_total;
+
+    for (uint32_t i = 0; i < count; i++) {
+        // populate random value
+        iohdl_t *iohdl = iohdls + i;
+        populate_key_value_startwith(prefix, iohdl->key, iohdl->value);
+
+        kv_value *value = iohdl->value;
+        kv_value *value_save = iohdl->value_save;
+
+        memcpy(value_save->value, value->value, value->length);
+
+        std::string kstr = std::string((char *) iohdl->key->key, iohdl->key->length);
+        if (entries.keys.find(kstr) != entries.keys.end()) {
+            printf("INFO: repeated keys\n");
+        }
+        entries.keys.insert(kstr);
+
+        std::string vstr = std::string((char *) iohdl->value->value, iohdl->value->length);
+        if (entries.values.find(vstr) != entries.values.end()) {
+            printf("INFO: repeated values\n");
+        }
+        entries.values.insert(vstr);
+        // printf("saved klen %d, vlen %d\n", iohdl->key->length, iohdl->value->length);
+    }
+    // save the data for iteration
+    process_one_round_write(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+}
+
+void prepare_test_iterators_one_bitmask(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count, iterator_entries_t& iterator_entries, kv_iterator_option iter_op) {
+    //uint32_t expected_total = count;
+    uint32_t expected_total = 0;
+    uint32_t prefix = 0;
+    prefix = PREFIX_KV;
+    kv_group_condition grp_cond;
+
+    //////////////////////////////
+    //grp_cond.bitmask = 0xFF000000;
+    //grp_cond.bit_pattern = prefix;
+    // delete all existing entries -- not supported now 
+    //delete_group(ns_hdl, sq_hdl, cq_hdl, &grp_cond);
+
+
+    //////////////////////////////
+    // generate a set
+    grp_cond.bitmask = BITMASK_KV;
+    grp_cond.bit_pattern = prefix;
+    expected_total = count;
+    generate_prefixed_dataset(ns_hdl, sq_hdl, cq_hdl, iohdls, count, iterator_entries, prefix, &grp_cond, iter_op, expected_total);
+    
+    iterator_entries.expected_total = expected_total;
+    iterator_entries.grp_cond.bitmask = BITMASK_KV; // 0xFF000000;
+    iterator_entries.grp_cond.bit_pattern = PREFIX_KV;
+
+    }
+    
+
+// test iterators
+uint64_t test_iterators_one_bitmask(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count, kv_iterator_option iter_op) {
+
+    std::vector<std::string> keys;
+    std::vector<std::string> values;
+
+    iterator_entries_t iterator_entries;
+    //printf("Before prepare_test_iterators_one_bitmask\n");
+    prepare_test_iterators_one_bitmask(ns_hdl, sq_hdl, cq_hdl, iohdls, count, iterator_entries, iter_op);
+    
+    //printf("Before get_iterator_results\n");
+    get_iterator_results(ns_hdl, sq_hdl, cq_hdl, iohdls, count, &iterator_entries.grp_cond, iter_op, keys, values);
+    
+    //printf("Validating 1 bitmask\n");
+    validate_iterator_results(iterator_entries, keys, values);
+
+    
+    return 0;
+}
+
+
+// test iterators main routine
+uint64_t test_iterators(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count, kv_iterator_option iter_op) {
+    test_iterators_one_bitmask(ns_hdl, sq_hdl, cq_hdl, iohdls, count, iter_op);
+    //test_iterators_multiple_bitmask(ns_hdl, sq_hdl, cq_hdl, iohdls, count, iter_op);
+    return 0;
+}
+
+// test list iterators, return the count of open iterators
+int test_list_iterators(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, std::vector<kv_iterator_handle>& iters) {
+
+    kv_iterator kv_iters[SAMSUNG_MAX_ITERATORS];
+    memset(kv_iters, 0, sizeof(kv_iters));
+    uint32_t count = SAMSUNG_MAX_ITERATORS;
+
+    iohdl_t iohdl;
+    kv_postprocess_function post_fn_data = { on_IO_complete_func, &iohdl};
+
+    // this sync admin call
+    kv_result res = kv_list_iterators_sync(sq_hdl, ns_hdl, kv_iters, &count);
+
+    if (res) {
+        printf("kv_list_iterators with error: 0x%X\n", res);
+        exit(1);
+    }
+
+    // simply print for any return iterators
+    for (uint32_t i = 0; i < count; i++) {
+        if (kv_iters[i].status == 1) {
+            fprintf(stderr, "found open iterator id %d\n", kv_iters[i].handle_id);
+            iters.push_back(kv_iters[i].handle_id);
+        }
+    }
+
+    return iters.size();
+}
+
+kv_iterator_handle test_open_iterator(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, kv_iterator_handle iter_handle) {
+
+    //////////////////////////////////
+    // open iterator
+    // iohdl.hiter (a handle)
+    iohdl_t iohdl;
+    iohdl.hiter = iter_handle;
+    // kv_iterator_option iter_op = KV_ITERATOR_OPT_KV;
+    kv_iterator_option iter_op = KV_ITERATOR_OPT_KEY;
+
+    static int i = 0;
+    i++;
+    // prefix has to be different for successful open
+    uint32_t prefix = 0x87654321 + i;
+    kv_group_condition grp_cond;
+    //////////////////////////////
+    grp_cond.bitmask = 0xFF000000;
+    grp_cond.bit_pattern = prefix;
+
+
+    //////////////////////////////////
+    // open iterator
+    // iohdl.hiter (a handle)
+    kv_result res = kv_open_iterator_sync(sq_hdl, ns_hdl, iter_op, &grp_cond, &iohdl.hiter);
+    while (res != KV_SUCCESS) {
+        printf("kv_open_iterator failed with error: 0x%X\n", res);
+        res = kv_open_iterator_sync(sq_hdl, ns_hdl, iter_op, &grp_cond, &iohdl.hiter);
+    }
+
+    // poll for completion
+    /*uint32_t total_done = 0;
+    while (total_done < 1) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+        total_done += finished;
+    }*/
+
+    /*
+    if (iohdl.retcode != KV_SUCCESS) {
+        printf("kv_open_iterator failed failed: return code 0x%03X\n", iohdl.retcode);
+        exit(1);
+    }*/
+
+    // XXX looks like device just returns 0 in case of max iterator has reached
+    /*if (iohdl.retcode == KV_SUCCESS && iohdl.hiter) {
+        iters.push_back(iohdl.hiter);
+    }
+
+    if (iohdl.hiter == 0) {
+        fprintf(stderr, "kv_open_iterator error, got 0, return code: 0x%x\n", iohdl.retcode);
+    }*/
+    return iohdl.hiter;
+}
+
+void test_close_iterator(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, std::vector<kv_iterator_handle>& iters) {
+ 
+    for (kv_iterator_handle hiter : iters) {
+        // close iterator
+        if (hiter <= 0) {
+            continue;
+        }
+        kv_result res = kv_close_iterator_sync(sq_hdl, ns_hdl, hiter);
+        while (res != KV_SUCCESS) {
+            printf("kv_close_iterator submit failed with error: 0x%X, retry\n", res);
+            res = kv_close_iterator_sync(sq_hdl, ns_hdl, hiter );
+        }
+        // poll for completion
+        /*int total_done = 0;
+        while (g_complete_count < 1) {
+            uint32_t finished = 0;
+            kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+            if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+                printf("kv_poll_completion error! exit\n");
+                exit(1);
+            }
+            total_done += finished;
+        }
+
+        if (iohdl.retcode != KV_SUCCESS) {
+            printf("kv_close_iterator failed: return code 0x%03X\n", iohdl.retcode);
+            exit(1);
+        }*/
+    }
+}
+
+// test max iterators allowed and list iterators
+void test_max_allowed_iterators(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl) {
+    std::vector<kv_iterator_handle> iters;
+
+    // close all existing iterators
+    int count = test_list_iterators(ns_hdl, sq_hdl, cq_hdl, iters);
+    test_close_iterator(ns_hdl, sq_hdl, cq_hdl, iters);
+    iters.clear();
+ 
+    // open max iterators
+    kv_iterator_handle iter_handle;
+    for (int i = 0; i < SAMSUNG_MAX_ITERATORS; i++) {
+        kv_iterator_handle itid = test_open_iterator(ns_hdl, sq_hdl, cq_hdl, iter_handle);
+        if (itid <= 0) {
+            printf("open iterators within limit failed, return invalid iterator handle id %u\n", itid);
+            // exit(1);
+        }
+    }
+
+    std::vector<kv_iterator_handle> iters_returned;
+    // test count open iterators
+    count = test_list_iterators(ns_hdl, sq_hdl, cq_hdl, iters_returned);
+    if (count != SAMSUNG_MAX_ITERATORS) {
+        printf("kv_list_iterators doesn't return correct count of open iterators\n");
+        printf("kv_list_iterators returned %lu expect %u\n", iters_returned.size(), SAMSUNG_MAX_ITERATORS);
+        exit(1);
+    }
+
+    // close opened iterators by this test
+    test_close_iterator(ns_hdl, sq_hdl, cq_hdl, iters_returned);
+
+    iters_returned.clear();
+    count = test_list_iterators(ns_hdl, sq_hdl, cq_hdl, iters_returned);
+    if (count != 0) {
+        printf("kv_list_iterators doesn't return correct 0 count of after closing all iterators\n");
+        exit(1);
+    }
+
+}
+
+uint64_t process_one_round_delete(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // insert key value
+    g_complete_count = 0;
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        kv_value *value = (iohdls + i)->value;
+
+        // set a new random value for overwrite test for read back test
+        change_value(value);
+        memcpy((iohdls + i)->value_save->value, value->value, value->length);
+
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_delete(sq_hdl, ns_hdl, key, KV_DELETE_OPT_DEFAULT, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_store failed with error: 0x%X\n", res);
+            res = kv_delete(sq_hdl, ns_hdl, key, KV_DELETE_OPT_DEFAULT, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    while (g_complete_count < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+    }
+    uint64_t end = current_timestamp();
+
+    // io completion function got return code in iohdl
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        if ((iohdls + i)->retcode != KV_SUCCESS) {
+            printf("delete key test failed: item %u: return code 0x%03X\n", i, (iohdls + i)->retcode);
+            exit(1);
+        }
+    }
+
+    // test read back, all should fail
+    g_complete_count = 0;
+    for (uint32_t i = 0; i < count; i++) {
+        kv_key *key = (iohdls + i)->key;
+        kv_value *value = (iohdls + i)->value;
+        memset(value->value, 0, value->length);
+
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+        kv_result res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_retrieve failed with error: 0x%X\n", res);
+            res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+        }
+    }
+
+    // poll for completion
+    while (g_complete_count < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+    }
+
+    // compare read value with write value
+    for (uint32_t i = 0; i < count; i++) {
+        if ((iohdls + i)->retcode != KVS_ERR_KEY_NOT_EXIST) {
+            printf("delete key test failed, key shouldn't be found: item %u: return code 0x%03X\n", i, (iohdls + i)->retcode);
+            exit(1);
+        }
+    }
+
+    return (end - start);
+}
+
+
+// read i th key and validate if it's really there
+bool process_one_key_read(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t i, int skip_value_compare) {
+
+    kv_key *key = (iohdls + i)->key;
+    // printf("reading key: %s\n", key->key);
+    // prefix of keys
+    // printf("reading key 4MSB: 0x%X\n", *(uint32_t *)&key->key);
+
+    kv_value *value = (iohdls + i)->value;
+    memset(value->value, '0', value->length);
+    g_complete_count = 0;
+    kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+    kv_result res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+    while (res != KV_SUCCESS) {
+        printf("kv_retrieve failed with error: 0x%X\n", res);
+        res = kv_retrieve(sq_hdl, ns_hdl, key, KV_RETRIEVE_OPT_DEFAULT, value, post_fn_data);
+    }
+
+    // poll for completion
+    while (g_complete_count < 1) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+    }
+
+    if ((iohdls + i)->retcode == KVS_ERR_KEY_NOT_EXIST) {
+        return false;
+    }
+
+    // compare read value with write value
+    // printf("validating key: %s\n", (iohdls + i)->key->key);
+    if (!skip_value_compare && memcmp((iohdls + i)->value_save->value, (iohdls + i)->value->value, (iohdls + i)->value->length)) {
+        // printf("failed: value read not the same as value written, key %s\n value + 16 %s\n value %s\n", (iohdls + i)->key->key, (iohdls + i)->value->value + 16, (iohdls + i)->value_save->value);
+        // XXX please check if any duplicate keys are used, only unique keys should
+        // be used, otherwise you may get false positive.
+        printf("failed: value read not the same as value written, original data len %d, device returned data len %d\n", (iohdls + i)->value_save->length, (iohdls + i)->value->length);
+        // printf("Note: please only use unique keys for this validation\n");
+        exit(1);
+    }
+
+    return true;
+}
+
+
+
+uint64_t process_one_round_kv_exist(kv_namespace_handle ns_hdl, kv_queue_handle sq_hdl, kv_queue_handle cq_hdl, iohdl_t *iohdls, uint32_t count) {
+
+    uint64_t start = current_timestamp();
+    // delete keys first
+    for (uint32_t i = 0; i < count; i++) {
+        delete_key(ns_hdl, sq_hdl, cq_hdl, (iohdls + i)->key, 1);
+    }
+ 
+    g_complete_count = 0;
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        kv_key *key = (iohdls + i)->key;
+        kv_value *value = (iohdls + i)->value;
+        kv_postprocess_function *post_fn_data = &((iohdls + i)->post_fn_data);
+
+        // save for comparison later
+        memcpy((iohdls + i)->value_save->value, (iohdls + i)->value->value, (iohdls + i)->value->length);
+
+        kv_result res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+        while (res != KV_SUCCESS) {
+            printf("kv_exit test kv_store failed with error: 0x%03X\n", res);
+            res = kv_store(sq_hdl, ns_hdl, key, value, KV_STORE_OPT_DEFAULT, post_fn_data);
+        }
+
+    }
+
+    // poll for completion
+    while (g_complete_count < count) {
+        uint32_t finished = 0;
+        kv_result res = kv_poll_completion(cq_hdl, 0, &finished);
+        if (res != KV_SUCCESS && res != KV_WRN_MORE) {
+            printf("kv_poll_completion error! exit\n");
+            exit(1);
+        }
+    }
+    uint64_t end = current_timestamp();
+
+    // io completion function got return code in iohdl
+    for (uint32_t i = 0; i < count; i++) {
+        // insert a key value pair
+        // Please note key and value is part of IO handle inside post_fn_data
+        if ((iohdls + i)->retcode != KV_SUCCESS) {
+            printf("kv_exist write key test failed: item %u: return code 0x%03X\n", i, (iohdls + i)->retcode);
+            exit(1);
+        }
+    }
+
+    // double check all keys are there
+    for (uint32_t i = 0; i < count; i++) {
+        if (!process_one_key_read(ns_hdl, sq_hdl, cq_hdl, iohdls, i, 0)) {
+            printf("inserted keys are not there\n");
+            exit(1);
+        }
+    }
+
+    kv_key *keys = (kv_key *) calloc(count, sizeof(kv_key));
+    // set half of keys that shouldn't exist
+    for (uint32_t i = 0; i < count; i++) {
+        kv_key *key_i = keys + i;
+        key_i->key = calloc((iohdls + i)->key->length, 1);
+        memcpy(key_i->key, (iohdls + i)->key->key, (iohdls + i)->key->length);
+        key_i->length = (iohdls + i)->key->length;
+
+        if (i % 2 == 0) {
+            // char *buffer = (char *) key_i->key;
+            // delete these keys, so they are truly gone
+            delete_key(ns_hdl, sq_hdl, cq_hdl, key_i, 1);
+
+            // printf("\n\nXXX check a deleted key\n");
+            // print_key(key_i);
+ 
+            if (process_one_key_read(ns_hdl, sq_hdl, cq_hdl, iohdls, i, 1)) {
+                printf("Error: a deleted key is still retrieved.\n");
+                exit(1);
+            }
+
+            if (check_key_exist(ns_hdl, sq_hdl, cq_hdl, key_i)) {
+                printf("Error: kv_exist found a deleted key\n");
+                exit(1);
+            }
+        } else {
+            // printf("\n\nXXX check a valid key\n");
+            // print_key(key_i);
+
+            if (!process_one_key_read(ns_hdl, sq_hdl, cq_hdl, iohdls, i, 0)) {
+                printf("Error: an existent key is not retrieved.\n");
+                exit(1);
+            }
+
+            if (!check_key_exist(ns_hdl, sq_hdl, cq_hdl, key_i)) {
+                printf("Error: kv_exist didn't find an existent key\n");
+                exit(1);
+            }
+        }
+    }
+
+    for (uint32_t i = 0; i < count; i++) {
+        free((keys + i)->key);
+    }
+    free(keys);
+
+    return (end - start);
+}
+
+
+// test cycle of read and write in rounds
+void sample_main(device_handle_t *device_handle, int klen, int vlen, int tcount, int qdepth) {
+    
+
+    //kv_device_handle dev_hdl = device_handle->dev_hdl;
+    kv_namespace_handle ns_hdl = device_handle->ns_hdl;
+    kv_queue_handle sq_hdl = device_handle->sq_hdl;
+    kv_queue_handle cq_hdl = device_handle->cq_hdl;
+
+    // uint32_t prefix = 0xFFFF1234;
+    uint32_t prefix = 0;
+    iohdl_t *iohdls = NULL;
+    
+    // break it into many rounds to minimize q contention
+    uint32_t count = qdepth;
+    create_iohdls(&iohdls, prefix, klen, vlen, count);
+
+    uint32_t left = tcount;
+    // total IO time in ns
+    uint64_t total_w = 0;
+    uint64_t total_r = 0;
+
+    //int i = 0;
+
+    while (left > 0) {
+        if (left < count) {
+            count = left;
+        }
+        left -= count;
+
+        // printf("round %d\n", i++);
+        // set up new random key valueis
+ 
+        populate_iohdls(iohdls, prefix, klen, vlen, count);
+
+        // test write and read
+        uint64_t time_w = process_one_round_write(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+        total_w += time_w;
+
+        uint64_t time_r = process_one_round_read(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+        total_r += time_r;
+
+        process_one_round_delete(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+
+        process_one_round_kv_exist(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+
+        // test list iterators
+        test_max_allowed_iterators(ns_hdl, sq_hdl, cq_hdl);
+
+        // test partial read, not supported by device yet
+        // process_one_round_read_partial(ns_hdl, sq_hdl, cq_hdl, iohdls, count);
+    }
+
+    // test iterators
+    test_iterators(ns_hdl, sq_hdl, cq_hdl, iohdls, count, KV_ITERATOR_OPT_KEY);
+   
+    // IOPS = IO count * 1000 * 1000 / time_ns
+    double iops_w = tcount * 1000.0 * 1000 / total_w;
+    double mean_iotime_w = total_w/1000.0/tcount;
+    printf("IOPS_w: %.1f K/s\n", iops_w);
+    printf("mean IO duration(write): %.1f us\n", mean_iotime_w);
+
+    // bytes * 10**9 / ( time * 1024 * 1024)
+    double throughput_w = tcount * (klen + vlen) / 1024.0 * 1000 * 1000 / total_w * 1000 / 1024;
+    printf("device throughput (write): %.1f MB/s\n", throughput_w);
+
+    double iops_r = tcount * 1000.0 * 1000 / total_r;
+    double mean_iotime_r = total_r/1000.0/tcount;
+    printf("IOPS_r: %.1f K/s\n", iops_r);
+    printf("mean IO duration(read): %.1f us\n", mean_iotime_r);
+
+    // bytes * 10**9 / ( time * 1024 * 1024)
+    double throughput_r = tcount * (klen + vlen) / 1024.0 * 1000 * 1000 / total_r * 1000 / 1024;
+    printf("device throughput (read): %.1f MB/s\n", throughput_r);
+
+    printf("XXX, %.1f, %.1f\n", iops_w, iops_r);
+    // printf("IOs %u, IO time %llu (ns), app time %llu (ns)\n", tcount, (long long unsigned) total_w, (long long unsigned) (end - start));
+
+
+    // done, free memories
+    free_iohdls(iohdls, qdepth);
+}
+
+void create_qpair(kv_device_handle dev_hdl,
+    kv_queue_handle *sq_hdl,
+    kv_queue_handle *cq_hdl,
+    uint16_t sqid,
+    uint16_t cqid,
+    uint16_t q_depth)
+{
+    
+    // create a CQ
+    // ONLY DO THIS, if you have NOT created the CQ earlier
+    create_cq(dev_hdl, cqid, q_depth, cq_hdl);
+
+    // this is only for completion Q
+    // this will kick off completion Q processing
+    // kv_set_interrupt_handler(cq_hdl, int_handler); 
+
+    // create a SQ
+    create_sq(dev_hdl, sqid, q_depth, cqid, sq_hdl);
+}
+
+// for linux kernel based device
+void create_device(const char *devpath, bool_t is_polling, int qdepth, device_handle_t *device_handle) {
+    // int klen = 16;
+    // int vlen = 4096;
+    // int qdepth = 64;
+    kv_device_init_t dev_init;
+
+    // you can supply your own configuration file
+    // dev_init.configfile = "./kvssd_emul.conf";
+    // the emulator device path is fixed and virtual
+    // you don't need to create it.
+    dev_init.devpath = strdup(devpath);
+    dev_init.need_persistency = FALSE;
+    dev_init.is_polling = is_polling;
+    dev_init.queuedepth = qdepth;
+    
+    kv_device_handle dev_hdl = NULL;
+    device_handle->qdepth = qdepth;
+
+    // initialize the device
+    kv_result ret = kv_initialize_device(&dev_init, &dev_hdl);
+    if (ret != KV_SUCCESS) {
+        printf("device creation error 0x%x\n", ret);
+        exit(1);
+    }
+    // print device stats
+    kv_device devinfo;
+    kv_device_stat devst;
+    kv_get_device_info(dev_hdl, &devinfo);
+    kv_get_device_stat(dev_hdl, &devst);
+    printf("device version is %uB\n", devinfo.version);
+    printf("capacity is %lluB\n", (unsigned long long) devinfo.capacity);
+    printf("device stats utilization %u\n", devst.utilization);
+    printf("device stats ns count %u\n", devst.namespace_count);
+    printf("device stats queue count %u\n", devst.queue_count);
+    // printf("device waf %u\n", devst.waf);
+
+    kv_namespace_handle ns_hdl = NULL;
+    get_namespace_default(dev_hdl, &ns_hdl);
+
+    // set up interrupt handler
+    // _kv_interrupt_handler int_func = {interrupt_func, (void *)4, 4};
+    // kv_interrupt_handler int_handler = &int_func;
+    kv_queue_handle sq_hdl;
+    kv_queue_handle cq_hdl;
+
+    // convenient wrapper function
+    // create a SQ/CQ pair
+    create_qpair(dev_hdl, &sq_hdl, &cq_hdl,
+            1,  // sqid
+            2,  // cqid
+            qdepth); // q depth
+
+    device_handle->dev_init = dev_init;
+    device_handle->dev_hdl = dev_hdl;
+    device_handle->ns_hdl = ns_hdl;
+    device_handle->sq_hdl = sq_hdl;
+    device_handle->cq_hdl = cq_hdl;
+    device_handle->qdepth = qdepth;
+}
+
+
+void shutdown_device(device_handle_t *device_handle) {
+    if (device_handle == NULL) {
+        return;
+    }
+
+    kv_device_handle dev_hdl = device_handle->dev_hdl;
+    kv_namespace_handle ns_hdl = device_handle->ns_hdl;
+    kv_queue_handle sq_hdl = device_handle->sq_hdl;
+    kv_queue_handle cq_hdl = device_handle->cq_hdl;
+ 
+    std::this_thread::sleep_for(std::chrono::milliseconds(1000));
+    // delete queues
+    if (kv_delete_queue(dev_hdl, sq_hdl) != KV_SUCCESS) {
+        printf("kv_delete_queue failed\n");
+        exit(1);
+    }
+    if (kv_delete_queue(dev_hdl, cq_hdl) != KV_SUCCESS) {
+        printf("kv_delete_queue failed\n");
+        exit(1);
+    }
+
+    // shutdown
+    kv_delete_namespace(dev_hdl, ns_hdl);
+    kv_cleanup_device(dev_hdl);
+    free((char *) device_handle->dev_init.devpath);
+}
+
+
+int main(int argc, char**argv) {
+    char *device = NULL;
+    if (argc <  5) {
+        printf("Please run\n  %s <number of keys>   <bitmask> <prefix>  <device>\nPlease use hexdecimal or bitmask and prefix", argv[0]);
+        printf("Example: sudo LD_LIBRARY_PATH=. ./test_suite 12356 0xffffffff 0x12345678 /dev/nvme0n1\n");
+        printf("Exiting..\n");
+        exit(1);
+        //printf("Default: %d\n", TOTAL_KV_COUNT);
+    } else {
+        char *prefix_str = NULL;
+        char *bitmask_str = NULL;
+        TOTAL_KV_COUNT = std::stoi(argv[1]);
+        printf("Insert keys: %d\n", TOTAL_KV_COUNT);        
+
+        char *pend;
+        bitmask_str = argv[2]; 
+        BITMASK_KV = strtoull(bitmask_str, &pend, 16);
+        printf("Bitmask:0x%x\n",BITMASK_KV);        
+        
+        prefix_str = argv[3];
+        PREFIX_KV = strtoull(prefix_str, &pend, 16);
+        printf("Prefix: 0x%x\n", PREFIX_KV);
+        device = argv[4];
+    
+    }
+
+    int klen = 16;
+    int vlen = 4096;
+    int qdepth = TOTAL_KV_COUNT;
+
+    
+    // initialize globals
+    g_KEYLEN_FIXED = 0;
+    g_KEYLEN = klen;    
+    device_handle_t *device1 = (device_handle_t *) malloc(sizeof (device_handle_t));
+
+    /*
+    device_handle_t *device1 = (device_handle_t *) malloc(sizeof (device_handle_t));
+    device_handle_t *device2 = (device_handle_t *) malloc(sizeof (device_handle_t));
+    device_handle_t *device3 = (device_handle_t *) malloc(sizeof (device_handle_t));
+    device_handle_t *device4 = (device_handle_t *) malloc(sizeof (device_handle_t));
+    */
+
+    /*
+     * device to test
+    # /dev/nvme10n1
+    # /dev/nvme11n1
+    # /dev/nvme12n1
+    # /dev/nvme13n1
+    */
+    bool_t is_polling = TRUE;
+
+    printf("creating device 1\n");
+    create_device(device, is_polling, qdepth, device1);
+
+    /*
+    printf("creating device 2\n");
+    create_device("/dev/nvme11n1", is_polling, qdepth, device2);
+
+    printf("creating device 3\n");
+    create_device("/dev/nvme12n1", is_polling, qdepth, device3);
+
+    printf("creating device 4\n");
+    create_device("/dev/nvme13n1", is_polling, qdepth, device4);
+    */
+
+
+    printf("starting operation on device 1\n");
+    std::thread th = std::thread(sample_main, device1, klen, vlen, TOTAL_KV_COUNT, qdepth);
+
+    if (th.joinable()) {
+        th.join();
+    }
+
+    /*
+    // start a thread to insert key values through submission Q
+    // then read them back to validate correctness
+    printf("starting operation on device 1\n");
+    std::thread th = std::thread(sample_main, device1, klen, vlen, TOTAL_KV_COUNT, qdepth);
+
+    printf("starting operation on device 2\n");
+    std::thread th1 = std::thread(sample_main, device2, klen, vlen, TOTAL_KV_COUNT, qdepth);
+
+    if (th.joinable()) {
+        th.join();
+    }
+    printf("stop operation on device 1\n");
+
+    if (th1.joinable()) {
+        th1.join();
+    }
+    printf("stop operation on device 2\n");
+    */
+
+    printf("shutdown all devices\n");
+    shutdown_device(device1);
+
+    free(device1);
+
+    /*
+    shutdown_device(device2);
+    shutdown_device(device3);
+    shutdown_device(device4);
+
+    free(device1);
+    free(device2);
+    free(device3);
+    free(device4);
+    */
+
+    printf("all done\n");
+    return 0;
+}
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_unit_test_program/kv_nvme.h b/PDK/driver/PCIe/kernel_driver/kernel_unit_test_program/kv_nvme.h
index e7552a9..88fa21a 100644
--- a/PDK/driver/PCIe/kernel_driver/kernel_unit_test_program/kv_nvme.h
+++ b/PDK/driver/PCIe/kernel_driver/kernel_unit_test_program/kv_nvme.h
@@ -36,6 +36,9 @@ enum nvme_kv_iter_req_option {
 #endif
 };
 
+
+
+
 enum nvme_kv_opcode {
     nvme_cmd_kv_store	= 0x81,
 	nvme_cmd_kv_append	= 0x83,
@@ -51,6 +54,7 @@ enum nvme_kv_opcode {
 #define KVCMD_MAX_KEY_SIZE		(255)
 #define KVCMD_MIN_KEY_SIZE		(255)
 
+
 int nvme_kv_store(int space_id, int fd, unsigned int nsid,
 		const char *key, int key_len,
 		const char *value, int value_len,
@@ -81,8 +85,5 @@ int nvme_kv_iterate_read_one(int sapce_id, int fd, unsigned int nsid,
 int nvme_kv_exist(int sapce_id, int fd, unsigned int nsid,
 		const char *key, int key_len);
 
-int nvme_kv_get_log(int fd, unsigned int nsid, char page_num,
-                    char *buffer, int bufferlen);
-
-
+int nvme_kv_get_log(int fd, unsigned int nsid, char page_num, char *buffer, int bufferlen);
 #endif /* #ifndef _KV_NVME_H_ */
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_unit_test_program/test_256_write.sh b/PDK/driver/PCIe/kernel_driver/kernel_unit_test_program/test_256_write.sh
old mode 100644
new mode 100755
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/Makefile b/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/Makefile
deleted file mode 100644
index 687d2c1..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/Makefile
+++ /dev/null
@@ -1,7 +0,0 @@
-obj-m   += nvme.o
-nvme-y  := nvme-core.o nvme-scsi.o
-
-all:
-	make -C /lib/modules/`uname -r`/build M=`pwd` modules
-clean:
-	make -C /lib/modules/`uname -r`/build M=`pwd` clean
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/linux_nvme_ioctl.h b/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/linux_nvme_ioctl.h
deleted file mode 100644
index 799091b..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/linux_nvme_ioctl.h
+++ /dev/null
@@ -1,779 +0,0 @@
-/*
- * Definitions for the NVM Express interface
- * Copyright (c) 2011-2013, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- *
- * You should have received a copy of the GNU General Public License along with
- * this program; if not, write to the Free Software Foundation, Inc., 
- * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
- */
-
-#ifndef _UAPI_LINUX_NVME_H
-#define _UAPI_LINUX_NVME_H
-
-#include <linux/types.h>
-
-struct nvme_id_power_state {
-	__le16			max_power;	/* centiwatts */
-	__u8			rsvd2;
-	__u8			flags;
-	__le32			entry_lat;	/* microseconds */
-	__le32			exit_lat;	/* microseconds */
-	__u8			read_tput;
-	__u8			read_lat;
-	__u8			write_tput;
-	__u8			write_lat;
-	__u8			rsvd16[16];
-};
-
-enum {
-	NVME_PS_FLAGS_MAX_POWER_SCALE	= 1 << 0,
-	NVME_PS_FLAGS_NON_OP_STATE	= 1 << 1,
-};
-
-struct nvme_id_ctrl {
-	__le16			vid;
-	__le16			ssvid;
-	char			sn[20];
-	char			mn[40];
-	char			fr[8];
-	__u8			rab;
-	__u8			ieee[3];
-	__u8			mic;
-	__u8			mdts;
-	__u8			rsvd78[178];
-	__le16			oacs;
-	__u8			acl;
-	__u8			aerl;
-	__u8			frmw;
-	__u8			lpa;
-	__u8			elpe;
-	__u8			npss;
-	__u8			rsvd264[248];
-	__u8			sqes;
-	__u8			cqes;
-	__u8			rsvd514[2];
-	__le32			nn;
-	__le16			oncs;
-	__le16			fuses;
-	__u8			fna;
-	__u8			vwc;
-	__le16			awun;
-	__le16			awupf;
-	__u8			rsvd530[1518];
-	struct nvme_id_power_state	psd[32];
-	__u8			vs[1024];
-};
-
-enum {
-	NVME_CTRL_ONCS_COMPARE			= 1 << 0,
-	NVME_CTRL_ONCS_WRITE_UNCORRECTABLE	= 1 << 1,
-	NVME_CTRL_ONCS_DSM			= 1 << 2,
-};
-
-struct nvme_lbaf {
-	__le16			ms;
-	__u8			ds;
-	__u8			rp;
-};
-
-struct nvme_id_ns {
-	__le64			nsze;
-	__le64			ncap;
-	__le64			nuse;
-	__u8			nsfeat;
-	__u8			nlbaf;
-	__u8			flbas;
-	__u8			mc;
-	__u8			dpc;
-	__u8			dps;
-	__u8			rsvd30[98];
-	struct nvme_lbaf	lbaf[16];
-	__u8			rsvd192[192];
-	__u8			vs[3712];
-};
-
-enum {
-	NVME_NS_FEAT_THIN	= 1 << 0,
-	NVME_LBAF_RP_BEST	= 0,
-	NVME_LBAF_RP_BETTER	= 1,
-	NVME_LBAF_RP_GOOD	= 2,
-	NVME_LBAF_RP_DEGRADED	= 3,
-};
-
-struct nvme_smart_log {
-	__u8			critical_warning;
-	__u8			temperature[2];
-	__u8			avail_spare;
-	__u8			spare_thresh;
-	__u8			percent_used;
-	__u8			rsvd6[26];
-	__u8			data_units_read[16];
-	__u8			data_units_written[16];
-	__u8			host_reads[16];
-	__u8			host_writes[16];
-	__u8			ctrl_busy_time[16];
-	__u8			power_cycles[16];
-	__u8			power_on_hours[16];
-	__u8			unsafe_shutdowns[16];
-	__u8			media_errors[16];
-	__u8			num_err_log_entries[16];
-	__u8			rsvd192[320];
-};
-
-enum {
-	NVME_SMART_CRIT_SPARE		= 1 << 0,
-	NVME_SMART_CRIT_TEMPERATURE	= 1 << 1,
-	NVME_SMART_CRIT_RELIABILITY	= 1 << 2,
-	NVME_SMART_CRIT_MEDIA		= 1 << 3,
-	NVME_SMART_CRIT_VOLATILE_MEMORY	= 1 << 4,
-};
-
-struct nvme_lba_range_type {
-	__u8			type;
-	__u8			attributes;
-	__u8			rsvd2[14];
-	__u64			slba;
-	__u64			nlb;
-	__u8			guid[16];
-	__u8			rsvd48[16];
-};
-
-enum {
-	NVME_LBART_TYPE_FS	= 0x01,
-	NVME_LBART_TYPE_RAID	= 0x02,
-	NVME_LBART_TYPE_CACHE	= 0x03,
-	NVME_LBART_TYPE_SWAP	= 0x04,
-
-	NVME_LBART_ATTRIB_TEMP	= 1 << 0,
-	NVME_LBART_ATTRIB_HIDE	= 1 << 1,
-};
-
-/* I/O commands */
-
-enum nvme_opcode {
-	nvme_cmd_flush		= 0x00,
-	nvme_cmd_write		= 0x01,
-	nvme_cmd_read		= 0x02,
-	nvme_cmd_write_uncor	= 0x04,
-	nvme_cmd_compare	= 0x05,
-	nvme_cmd_dsm		= 0x09,
-#if 1
- 	nvme_cmd_kv_store	= 0x81,
-	nvme_cmd_kv_append	= 0x83,
-	nvme_cmd_kv_retrieve	= 0x90,
-	nvme_cmd_kv_delete	= 0xA1,
-	nvme_cmd_kv_iter_req	= 0xB1,
-	nvme_cmd_kv_iter_read	= 0xB2, 
-	nvme_cmd_kv_exist	= 0xB3,
-#endif
-};
-
-#if 1
-#define is_kv_append_cmd(opcode)	((opcode) == nvme_cmd_kv_append)
-#define is_kv_store_cmd(opcode)	((opcode) == nvme_cmd_kv_store)
-#define is_kv_retrieve_cmd(opcode)	((opcode) == nvme_cmd_kv_retrieve)
-#define is_kv_delete_cmd(opcode)	((opcode) == nvme_cmd_kv_delete)
-#define is_kv_iter_req_cmd(opcode)	((opcode) == nvme_cmd_kv_iter_req)
-#define is_kv_iter_read_cmd(opcode)	((opcode) == nvme_cmd_kv_iter_read)
-#define is_kv_exist_cmd(opcode)	((opcode) == nvme_cmd_kv_exist)
-#define is_kv_cmd(opcode)	(is_kv_store_cmd(opcode) || is_kv_append_cmd(opcode) ||\
-			is_kv_retrieve_cmd(opcode) || is_kv_delete_cmd(opcode) ||\
-			is_kv_iter_req_cmd(opcode) || is_kv_iter_read_cmd(opcode) ||\
-			is_kv_exist_cmd(opcode))
-#define kv_nvme_is_write(opcode) (is_kv_store_cmd(opcode) || is_kv_append_cmd(opcode))
-#endif
-
-struct nvme_common_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__le32			cdw2[2];
-	__le64			metadata;
-	__le64			prp1;
-	__le64			prp2;
-	__le32			cdw10[6];
-};
-
-struct nvme_rw_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2;
-	__le64			metadata;
-	__le64			prp1;
-	__le64			prp2;
-	__le64			slba;
-	__le16			length;
-	__le16			control;
-	__le32			dsmgmt;
-	__le32			reftag;
-	__le16			apptag;
-	__le16			appmask;
-};
-
-#if 1
-struct nvme_kv_store_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd;
-	__le32			offset;
-	__u32			rsvd2;
-	__le64			prp1;
-	__le64			prp2;/* value dptr prp1,2 */
-	__le32			value_len; /* size in word */
-	__u8			key_len; /* 0 ~ 255 (key len -1) */
-   	__u8			option;
-   	__u8			invalid_byte:2;
-   	__u8			rsvd3:6;
-   	__u8			rsvd4;
-	union {
-		struct {
-			char	key[16];
-		};
-		struct {
-			__le64	key_prp;
-			__le64	key_prp2;
-		};
-	};
-};
-
-struct nvme_kv_append_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd;
-	__le32			offset;
-	__u32			rsvd2;
-	__le64			prp1;
-	__le64			prp2;/* value dptr prp1,2 */
-	__le32			value_len; /* size in word */
-	__u8			key_len; /* 0 ~ 255 (key len -1) */
-	__u8			option;
-	__u8			invalid_byte:2;
-	__u8			rsvd3:6;
-	__u8			rsvd4;
-	union {
-		struct {
-			char	key[16];
-		};
-		struct {
-			__le64	key_prp;
-			__le64	key_prp2;
-		};
-	};
-};
-
-struct nvme_kv_retrieve_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd;
-	__le32			offset;
-	__u32			rsvd2;
-	__le64			prp1;
-	__le64			prp2;/* value dptr prp1,2 */
-	__le32			value_len; /* size in word */
-	__u8			key_len; /* 0 ~ 255 (key len -1) */
-	__u8			option;
-	__u16			rsvd3;
-	union {
-		struct {
-			char	key[16];
-		};
-		struct {
-			__le64	key_prp;
-			__le64	key_prp2;
-		};
-	};
-};
-
-struct nvme_kv_delete_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd;
-	__le32			offset;
-	__u32			rsvd2;
-	__u64			rsvd3[2];
-	__le32			value_len; /* should be zero*/
-	__u8			key_len; /* 0 ~ 255 (key len -1) */
-	__u8			option;
-	__u16			rsvd4;
-	union {
-		struct {
-			char	key[16];
-		};
-		struct {
-			__le64	key_prp;
-			__le64	key_prp2;
-		};
-	};
-};
-
-struct nvme_kv_exist_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd;
-	__le32			offset;
-	__u32			rsvd2;
-	__u64			rsvd3[2];
-	__le32			value_len; /* should be zero*/
-	__u8			key_len; /* 0 ~ 255 (key len -1) */
-	__u8			option;
-	__u16			rsvd4;
-	union {
-		struct {
-			char	key[16];
-		};
-		struct {
-			__le64	key_prp;
-			__le64	key_prp2;
-		};
-	};
-};
-
-struct nvme_kv_iter_req_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd[4];
-	__le32			zero; /* should be zero*/
-	__u8			iter_handle;
-	__u8			option;
-	__u16			rsvd2;
-	__le32			iter_val;
-	__le32			iter_bitmask;
-	__u64			rsvd3;
-};
-
-struct nvme_kv_iter_read_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd[2];
-	__le64			prp1;
-	__le64			prp2;/* value dptr prp1,2 */
-	__le32			value_len; /* size in word */
-	__u8			iter_handle; /* 0 ~ 255 (key len -1) */
-	__u8			option;
-	__u16			rsvd2;
-	__u64			rsvd3[2];
-};
-
-#endif
-
-enum {
-	NVME_RW_LR			= 1 << 15,
-	NVME_RW_FUA			= 1 << 14,
-	NVME_RW_DSM_FREQ_UNSPEC		= 0,
-	NVME_RW_DSM_FREQ_TYPICAL	= 1,
-	NVME_RW_DSM_FREQ_RARE		= 2,
-	NVME_RW_DSM_FREQ_READS		= 3,
-	NVME_RW_DSM_FREQ_WRITES		= 4,
-	NVME_RW_DSM_FREQ_RW		= 5,
-	NVME_RW_DSM_FREQ_ONCE		= 6,
-	NVME_RW_DSM_FREQ_PREFETCH	= 7,
-	NVME_RW_DSM_FREQ_TEMP		= 8,
-	NVME_RW_DSM_LATENCY_NONE	= 0 << 4,
-	NVME_RW_DSM_LATENCY_IDLE	= 1 << 4,
-	NVME_RW_DSM_LATENCY_NORM	= 2 << 4,
-	NVME_RW_DSM_LATENCY_LOW		= 3 << 4,
-	NVME_RW_DSM_SEQ_REQ		= 1 << 6,
-	NVME_RW_DSM_COMPRESSED		= 1 << 7,
-};
-
-struct nvme_dsm_cmd {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[2];
-	__le64			prp1;
-	__le64			prp2;
-	__le32			nr;
-	__le32			attributes;
-	__u32			rsvd12[4];
-};
-
-enum {
-	NVME_DSMGMT_IDR		= 1 << 0,
-	NVME_DSMGMT_IDW		= 1 << 1,
-	NVME_DSMGMT_AD		= 1 << 2,
-};
-
-struct nvme_dsm_range {
-	__le32			cattr;
-	__le32			nlb;
-	__le64			slba;
-};
-
-/* Admin commands */
-
-enum nvme_admin_opcode {
-	nvme_admin_delete_sq		= 0x00,
-	nvme_admin_create_sq		= 0x01,
-	nvme_admin_get_log_page		= 0x02,
-	nvme_admin_delete_cq		= 0x04,
-	nvme_admin_create_cq		= 0x05,
-	nvme_admin_identify		= 0x06,
-	nvme_admin_abort_cmd		= 0x08,
-	nvme_admin_set_features		= 0x09,
-	nvme_admin_get_features		= 0x0a,
-	nvme_admin_async_event		= 0x0c,
-	nvme_admin_activate_fw		= 0x10,
-	nvme_admin_download_fw		= 0x11,
-	nvme_admin_format_nvm		= 0x80,
-	nvme_admin_security_send	= 0x81,
-	nvme_admin_security_recv	= 0x82,
-};
-
-enum {
-	NVME_QUEUE_PHYS_CONTIG	= (1 << 0),
-	NVME_CQ_IRQ_ENABLED	= (1 << 1),
-	NVME_SQ_PRIO_URGENT	= (0 << 1),
-	NVME_SQ_PRIO_HIGH	= (1 << 1),
-	NVME_SQ_PRIO_MEDIUM	= (2 << 1),
-	NVME_SQ_PRIO_LOW	= (3 << 1),
-	NVME_FEAT_ARBITRATION	= 0x01,
-	NVME_FEAT_POWER_MGMT	= 0x02,
-	NVME_FEAT_LBA_RANGE	= 0x03,
-	NVME_FEAT_TEMP_THRESH	= 0x04,
-	NVME_FEAT_ERR_RECOVERY	= 0x05,
-	NVME_FEAT_VOLATILE_WC	= 0x06,
-	NVME_FEAT_NUM_QUEUES	= 0x07,
-	NVME_FEAT_IRQ_COALESCE	= 0x08,
-	NVME_FEAT_IRQ_CONFIG	= 0x09,
-	NVME_FEAT_WRITE_ATOMIC	= 0x0a,
-	NVME_FEAT_ASYNC_EVENT	= 0x0b,
-	NVME_FEAT_SW_PROGRESS	= 0x0c,
-	NVME_FWACT_REPL		= (0 << 3),
-	NVME_FWACT_REPL_ACTV	= (1 << 3),
-	NVME_FWACT_ACTV		= (2 << 3),
-};
-
-struct nvme_identify {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[2];
-	__le64			prp1;
-	__le64			prp2;
-	__le32			cns;
-	__u32			rsvd11[5];
-};
-
-struct nvme_features {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[2];
-	__le64			prp1;
-	__le64			prp2;
-	__le32			fid;
-	__le32			dword11;
-	__u32			rsvd12[4];
-};
-
-struct nvme_create_cq {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[5];
-	__le64			prp1;
-	__u64			rsvd8;
-	__le16			cqid;
-	__le16			qsize;
-	__le16			cq_flags;
-	__le16			irq_vector;
-	__u32			rsvd12[4];
-};
-
-struct nvme_create_sq {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[5];
-	__le64			prp1;
-	__u64			rsvd8;
-	__le16			sqid;
-	__le16			qsize;
-	__le16			sq_flags;
-	__le16			cqid;
-	__u32			rsvd12[4];
-};
-
-struct nvme_delete_queue {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[9];
-	__le16			qid;
-	__u16			rsvd10;
-	__u32			rsvd11[5];
-};
-
-struct nvme_abort_cmd {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[9];
-	__le16			sqid;
-	__u16			cid;
-	__u32			rsvd11[5];
-};
-
-struct nvme_download_firmware {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[5];
-	__le64			prp1;
-	__le64			prp2;
-	__le32			numd;
-	__le32			offset;
-	__u32			rsvd12[4];
-};
-
-struct nvme_format_cmd {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[4];
-	__le32			cdw10;
-	__u32			rsvd11[5];
-};
-
-struct nvme_command {
-	union {
-		struct nvme_common_command common;
-		struct nvme_rw_command rw;
-		struct nvme_identify identify;
-		struct nvme_features features;
-		struct nvme_create_cq create_cq;
-		struct nvme_create_sq create_sq;
-		struct nvme_delete_queue delete_queue;
-		struct nvme_download_firmware dlfw;
-		struct nvme_format_cmd format;
-		struct nvme_dsm_cmd dsm;
-		struct nvme_abort_cmd abort;
-#if 1
-		struct nvme_kv_store_command kv_store;
-		struct nvme_kv_append_command kv_append;
-		struct nvme_kv_retrieve_command kv_retrieve;
-		struct nvme_kv_delete_command kv_delete;
-		struct nvme_kv_iter_req_command kv_iter_req;
-		struct nvme_kv_iter_read_command kv_iter_read;
-		struct nvme_kv_exist_command kv_exist;
-#endif
-	};
-};
-
-enum {
-	NVME_SC_SUCCESS			= 0x0,
-	NVME_SC_INVALID_OPCODE		= 0x1,
-	NVME_SC_INVALID_FIELD		= 0x2,
-	NVME_SC_CMDID_CONFLICT		= 0x3,
-	NVME_SC_DATA_XFER_ERROR		= 0x4,
-	NVME_SC_POWER_LOSS		= 0x5,
-	NVME_SC_INTERNAL		= 0x6,
-	NVME_SC_ABORT_REQ		= 0x7,
-	NVME_SC_ABORT_QUEUE		= 0x8,
-	NVME_SC_FUSED_FAIL		= 0x9,
-	NVME_SC_FUSED_MISSING		= 0xa,
-	NVME_SC_INVALID_NS		= 0xb,
-	NVME_SC_CMD_SEQ_ERROR		= 0xc,
-	NVME_SC_LBA_RANGE		= 0x80,
-	NVME_SC_CAP_EXCEEDED		= 0x81,
-	NVME_SC_NS_NOT_READY		= 0x82,
-	NVME_SC_CQ_INVALID		= 0x100,
-	NVME_SC_QID_INVALID		= 0x101,
-	NVME_SC_QUEUE_SIZE		= 0x102,
-	NVME_SC_ABORT_LIMIT		= 0x103,
-	NVME_SC_ABORT_MISSING		= 0x104,
-	NVME_SC_ASYNC_LIMIT		= 0x105,
-	NVME_SC_FIRMWARE_SLOT		= 0x106,
-	NVME_SC_FIRMWARE_IMAGE		= 0x107,
-	NVME_SC_INVALID_VECTOR		= 0x108,
-	NVME_SC_INVALID_LOG_PAGE	= 0x109,
-	NVME_SC_INVALID_FORMAT		= 0x10a,
-	NVME_SC_BAD_ATTRIBUTES		= 0x180,
-	NVME_SC_WRITE_FAULT		= 0x280,
-	NVME_SC_READ_ERROR		= 0x281,
-	NVME_SC_GUARD_CHECK		= 0x282,
-	NVME_SC_APPTAG_CHECK		= 0x283,
-	NVME_SC_REFTAG_CHECK		= 0x284,
-	NVME_SC_COMPARE_FAILED		= 0x285,
-	NVME_SC_ACCESS_DENIED		= 0x286,
-	NVME_SC_DNR			= 0x4000,
-};
-
-struct nvme_completion {
-	__le32	result;		/* Used by admin commands to return data */
-	__u32	rsvd;
-	__le16	sq_head;	/* how much of this queue may be reclaimed */
-	__le16	sq_id;		/* submission queue that generated this entry */
-	__u16	command_id;	/* of the command which completed */
-	__le16	status;		/* did the command fail, and if so, why? */
-};
-
-struct nvme_user_io {
-	__u8	opcode;
-	__u8	flags;
-	__u16	control;
-	__u16	nblocks;
-	__u16	rsvd;
-	__u64	metadata;
-	__u64	addr;
-	__u64	slba;
-	__u32	dsmgmt;
-	__u32	reftag;
-	__u16	apptag;
-	__u16	appmask;
-};
-
-struct nvme_admin_cmd {
-	__u8	opcode;
-	__u8	flags;
-	__u16	rsvd1;
-	__u32	nsid;
-	__u32	cdw2;
-	__u32	cdw3;
-	__u64	metadata;
-	__u64	addr;
-	__u32	metadata_len;
-	__u32	data_len;
-	__u32	cdw10;
-	__u32	cdw11;
-	__u32	cdw12;
-	__u32	cdw13;
-	__u32	cdw14;
-	__u32	cdw15;
-	__u32	timeout_ms;
-	__u32	result;
-};
-
-#define NVME_IOCTL_ID		_IO('N', 0x40)
-#define NVME_IOCTL_ADMIN_CMD	_IOWR('N', 0x41, struct nvme_admin_cmd)
-#define NVME_IOCTL_SUBMIT_IO	_IOW('N', 0x42, struct nvme_user_io)
-
-#if 1
-
-#define KVCMD_INLINE_KEY_MAX	(16)
-#define KVCMD_MAX_KEY_SIZE		(255)
-#define KVCMD_MIN_KEY_SIZE		(4)
-
-#define	KVS_SUCCESS		0
-#define KVS_ERR_ALIGNMENT	(-1)
-#define KVS_ERR_CAPAPCITY	(-2)
-#define KVS_ERR_CLOSE	(-3)
-#define KVS_ERR_CONT_EXIST	(-4)
-#define KVS_ERR_CONT_NAME	(-5)
-#define KVS_ERR_CONT_NOT_EXIST	(-6)
-#define KVS_ERR_DEVICE_NOT_EXIST (-7)
-#define KVS_ERR_GROUP	(-8)
-#define KVS_ERR_INDEX	(-9)
-#define KVS_ERR_IO	(-10)
-#define KVS_ERR_KEY	(-11)
-#define KVS_ERR_KEY_TYPE	(-12)
-#define KVS_ERR_MEMORY	(-13)
-#define KVS_ERR_NULL_INPUT	(-14)
-#define KVS_ERR_OFFSET	(-15)
-#define KVS_ERR_OPEN	(-16)
-#define KVS_ERR_OPTION_NOT_SUPPORTED	(-17)
-#define KVS_ERR_PERMISSION	(-18)
-#define KVS_ERR_SPACE	(-19)
-#define KVS_ERR_TIMEOUT	(-20)
-#define KVS_ERR_TUPLE_EXIST	(-21)
-#define KVS_ERR_TUPLE_NOT_EXIST	(-22)
-#define KVS_ERR_VALUE	(-23)
-
-/*
- * Extended definition for kv device
- */
-struct nvme_passthru_kv_cmd {
-	__u8	opcode;
-	__u8	flags;
-	__u16	rsvd1;
-	__u32	nsid;
-	__u32	cdw2;
-	__u32	cdw3;
-	__u32	cdw4;
-	__u32	cdw5;
-	__u64	data_addr;
-	__u32	data_length;
-	__u32	key_length;
-	__u32	cdw10;
-	__u32	cdw11;
-	union {
-		struct {
-			__u64 key_addr;
-			__u32 rsvd5;
-			__u32 rsvd6;
-		};
-		__u8 key[16];
-		struct {
-			__u32 cdw12;
-			__u32 cdw13;
-			__u32 cdw14;
-			__u32 cdw15;
-		};
-	};
-	__u32	timeout_ms;
-	__u32	result;
-	__u32	status;
-	__u32	ctxid;
-	__u64	reqid;
-};
-
-struct nvme_aioctx {
-	__u32	ctxid;
-	__u32	eventfd;
-};
-
-
-struct nvme_aioevent {
-	__u64	reqid;
-	__u32	ctxid;
-	__u32	result;
-	__u16	status;
-};
-
-#define MAX_AIO_EVENTS	128
-struct nvme_aioevents {
-	__u16	nr;
-	__u32   ctxid;
-	struct nvme_aioevent events[MAX_AIO_EVENTS];
-};
-
-#define NVME_IOCTL_AIO_CMD		_IOWR('N', 0x47, struct nvme_passthru_kv_cmd)
-#define NVME_IOCTL_SET_AIOCTX	_IOWR('N', 0x48, struct nvme_aioctx)
-#define NVME_IOCTL_DEL_AIOCTX	_IOWR('N', 0x49, struct nvme_aioctx)
-#define NVME_IOCTL_GET_AIOEVENT	_IOWR('N', 0x50, struct nvme_aioevents)
-#define NVME_IOCTL_IO_KV_CMD	_IOWR('N', 0x51, struct nvme_passthru_kv_cmd)
-#endif
-
-
-#endif /* _UAPI_LINUX_NVME_H */
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/nvme-core.c b/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/nvme-core.c
deleted file mode 100644
index b7ea7ea..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/nvme-core.c
+++ /dev/null
@@ -1,3913 +0,0 @@
-/*
- * NVM Express device driver
- * Copyright (c) 2011-2014, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- *
- * You should have received a copy of the GNU General Public License along with
- * this program; if not, write to the Free Software Foundation, Inc.,
- * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
- */
-#if 1
-#include "nvme.h"
-#else
-#include <linux/nvme.h>
-#endif
-#include <linux/bio.h>
-#include <linux/bitops.h>
-#include <linux/blkdev.h>
-#include <linux/cpu.h>
-#include <linux/delay.h>
-#include <linux/errno.h>
-#include <linux/fs.h>
-#include <linux/genhd.h>
-#include <linux/hdreg.h>
-#include <linux/idr.h>
-#include <linux/init.h>
-#include <linux/interrupt.h>
-#include <linux/io.h>
-#include <linux/kdev_t.h>
-#include <linux/kthread.h>
-#include <linux/kernel.h>
-#include <linux/mm.h>
-#include <linux/module.h>
-#include <linux/moduleparam.h>
-#include <linux/pci.h>
-#include <linux/percpu.h>
-#include <linux/poison.h>
-#include <linux/ptrace.h>
-#include <linux/sched.h>
-#include <linux/slab.h>
-#include <linux/types.h>
-#include <linux/version.h>
-#include <linux/highmem.h>
-#include <scsi/sg.h>
-
-#if 1
-#include <linux/file.h>
-#include <linux/fdtable.h>
-#include <linux/eventfd.h>
-#include <linux/kref.h>
-#endif
-
-#define NVME_Q_DEPTH 1024
-#define SQ_SIZE(depth)		(depth * sizeof(struct nvme_command))
-#define CQ_SIZE(depth)		(depth * sizeof(struct nvme_completion))
-#define ADMIN_TIMEOUT	(60 * HZ)
-#define IOD_TIMEOUT	(4 * NVME_IO_TIMEOUT)
-
-unsigned char io_timeout = 30;
-module_param(io_timeout, byte, 0644);
-MODULE_PARM_DESC(io_timeout, "timeout in seconds for I/O");
-
-static int nvme_major;
-module_param(nvme_major, int, 0);
-
-static int use_threaded_interrupts;
-module_param(use_threaded_interrupts, int, 0);
-
-static DEFINE_SPINLOCK(dev_list_lock);
-static LIST_HEAD(dev_list);
-static struct task_struct *nvme_thread;
-static struct workqueue_struct *nvme_workq;
-static wait_queue_head_t nvme_kthread_wait;
-
-static void nvme_reset_failed_dev(struct work_struct *ws);
-
-struct async_cmd_info {
-	struct kthread_work work;
-	struct kthread_worker *worker;
-	u32 result;
-	int status;
-	void *ctx;
-};
-
-/*
- * An NVM Express queue.  Each device has at least two (one for admin
- * commands and one for I/O commands).
- */
-struct nvme_queue {
-	struct rcu_head r_head;
-	struct device *q_dmadev;
-	struct nvme_dev *dev;
-	char irqname[24];	/* nvme4294967295-65535\0 */
-	spinlock_t q_lock;
-	struct nvme_command *sq_cmds;
-	volatile struct nvme_completion *cqes;
-	dma_addr_t sq_dma_addr;
-	dma_addr_t cq_dma_addr;
-	wait_queue_head_t sq_full;
-	wait_queue_t sq_cong_wait;
-	struct bio_list sq_cong;
-	struct list_head iod_bio;
-	u32 __iomem *q_db;
-	u16 q_depth;
-	u16 cq_vector;
-	u16 sq_head;
-	u16 sq_tail;
-	u16 cq_head;
-	u16 qid;
-	u8 cq_phase;
-	u8 cqe_seen;
-	u8 q_suspended;
-	cpumask_var_t cpu_mask;
-	struct async_cmd_info cmdinfo;
-	unsigned long cmdid_data[];
-};
-
-#if 1
-/*
- * PORT AIO Support logic from Kvepic.
- */
-// AIO data structures
-static struct kmem_cache        *kaioctx_cachep = 0;
-static struct kmem_cache        *kaiocb_cachep = 0;
-static mempool_t *kaiocb_mempool = 0;
-static mempool_t *kaioctx_mempool = 0;
-
-static __u32 aio_context_id;
-
-#define AIOCTX_MAX 1024
-#define AIOCB_MAX (1024*64)
-
-static __u64 debug_completed  =0;
-static int debug_outstanding  =0;
-
-struct nvme_kaioctx
-{
-	struct nvme_aioctx uctx;
-	struct eventfd_ctx *eventctx;
-	struct list_head kaiocb_list;
-	spinlock_t kaioctx_spinlock;
-	struct kref ref;
-};
-
-static struct nvme_kaioctx **g_kaioctx_tb = NULL;
-static spinlock_t g_kaioctx_tb_spinlock;
-
-
-struct aio_user_ctx {
-	int nents;
-	int len;
-	struct page ** pages;
-	struct scatterlist *sg;
-	char data[1];
-};
-
-struct nvme_kaiocb
-{
-	struct list_head aiocb_list;
-	struct nvme_aioevent event;
-	struct nvme_completion cqe;
-	int opcode;
-	bool use_meta;
-	struct nvme_iod *iod;
-	struct scatterlist meta_sg;
-	void *meta;
-	char *kv_data;
-	struct aio_user_ctx *user_ctx;
-};
-
-static void remove_kaioctx(struct nvme_kaioctx * ctx)
-{
-	struct nvme_kaiocb *tmpcb;
-	struct list_head *pos, *q;
-	unsigned long flags;
-	if (ctx) {
-		spin_lock_irqsave(&ctx->kaioctx_spinlock, flags);
-		list_for_each_safe(pos, q, &ctx->kaiocb_list){
-			tmpcb = list_entry(pos, struct nvme_kaiocb, aiocb_list);
-			list_del(pos);
-			mempool_free(tmpcb, kaiocb_mempool);
-		}
-		spin_unlock_irqrestore(&ctx->kaioctx_spinlock, flags);
-		eventfd_ctx_put(ctx->eventctx);	
-		mempool_free(ctx, kaioctx_mempool);
-	}
-}
-
-static void cleanup_kaioctx(struct kref *kref) {
-	struct nvme_kaioctx *ctx = container_of(kref, struct nvme_kaioctx, ref);
-	remove_kaioctx(ctx);
-}
-
-static void ref_kaioctx(struct nvme_kaioctx *ctx) {
-	kref_get(&ctx->ref);
-}
-
-static void deref_kaioctx(struct nvme_kaioctx *ctx) {
-	kref_put(&ctx->ref, cleanup_kaioctx);
-}
-
-
-/* destroy memory pools
-*/
-static void destroy_aio_mempool(void)
-{
-	int i = 0;
-	if (g_kaioctx_tb) {
-		for (i = 0; i < AIOCTX_MAX; i++) {
-			if (g_kaioctx_tb[i]) {
-				remove_kaioctx(g_kaioctx_tb[i]);
-				g_kaioctx_tb[i] = NULL;
-			}
-		}
-		kfree(g_kaioctx_tb);
-		g_kaioctx_tb = NULL;
-	}
-	if (kaiocb_mempool)
-		mempool_destroy(kaiocb_mempool);
-	if (kaioctx_mempool)
-		mempool_destroy(kaioctx_mempool);
-	if (kaioctx_cachep)
-		kmem_cache_destroy(kaioctx_cachep);
-	if (kaiocb_cachep)
-		kmem_cache_destroy(kaiocb_cachep);
-}
-
-/* prepare basic data structures
- * to support aio context and requests
- */
-static int aio_service_init(void)
-{
-	g_kaioctx_tb = (struct nvme_kaioctx **)kmalloc(sizeof(struct nvme_kaioctx *) * AIOCTX_MAX, GFP_KERNEL);
-	if (!g_kaioctx_tb)
-		goto fail;
-	memset(g_kaioctx_tb, 0, sizeof(struct nvme_kaioctx *) * AIOCTX_MAX);
-
-	// slap allocator and memory pool
-	kaioctx_cachep = kmem_cache_create("nvme_kaioctx", sizeof(struct nvme_kaioctx), 0, 0, NULL);
-	if (!kaioctx_cachep)
-		goto fail;
-
-	kaiocb_cachep = kmem_cache_create("nvme_kaiocb", sizeof(struct nvme_kaiocb), 0, 0, NULL);
-	if (!kaiocb_cachep)
-		goto fail;
-
-	kaiocb_mempool = mempool_create_slab_pool(AIOCB_MAX, kaiocb_cachep);
-	if (!kaiocb_mempool)
-		goto fail;
-
-	kaioctx_mempool = mempool_create_slab_pool(AIOCTX_MAX, kaioctx_cachep);
-	if (!kaioctx_mempool)
-		goto fail;
-
-	// global variables
-	// context id 0 is reserved for normal I/O operations (synchronous)
-	aio_context_id = 1;
-	spin_lock_init(&g_kaioctx_tb_spinlock);
-	printk(KERN_DEBUG"nvme-aio: initialized\n");
-	return 0;
-
-fail:
-	destroy_aio_mempool();
-	return -ENOMEM;
-}
-
-/*
- * release memory before exit
- */
-static int aio_service_exit(void)
-{
-	destroy_aio_mempool();
-	printk(KERN_DEBUG"nvme-aio: unloaded\n");
-	return 0;
-}
-
-static struct nvme_kaioctx * find_kaioctx(__u32 ctxid) {
-	struct nvme_kaioctx * tmp = NULL;
-	//unsigned long flags;
-	//spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
-	tmp = g_kaioctx_tb[ctxid];
-	if (tmp) ref_kaioctx(tmp);
-	//spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-	return tmp;
-}
-
-
-/*
- * find an aio context with a given id
- */
-static int  set_aioctx_event(__u32 ctxid, struct nvme_kaiocb * kaiocb)
-{
-	struct nvme_kaioctx *tmp;
-	unsigned long flags;
-	tmp = find_kaioctx(ctxid);
-	if (tmp) {
-		spin_lock_irqsave(&tmp->kaioctx_spinlock, flags);
-		list_add_tail(&kaiocb->aiocb_list, &tmp->kaiocb_list);
-		eventfd_signal(tmp->eventctx, 1);
-#if 0
-		printk("nvme_set_aioctx: success to signal event %d %llu\n", kaiocb->event.ctxid, kaiocb->event.reqid);
-#endif
-		spin_unlock_irqrestore(&tmp->kaioctx_spinlock, flags);
-		deref_kaioctx(tmp);
-		return 0;
-	} else {
-#if 0
-		printk("nvme_set_aioctx: failed to signal event %d.\n", ctxid);
-#endif
-	}
-
-	return -1;
-}
-
-/*
- * delete an aio context
- * it will release any resources allocated for this context
- */
-static int nvme_del_aioctx(struct nvme_aioctx __user *uctx )
-{
-	struct nvme_kaioctx ctx;
-	unsigned long flags;
-	if (copy_from_user(&ctx, uctx, sizeof(struct nvme_aioctx)))
-		return -EFAULT;
-
-	spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
-	if (g_kaioctx_tb[ctx.uctx.ctxid]) {
-		deref_kaioctx(g_kaioctx_tb[ctx.uctx.ctxid]);
-		g_kaioctx_tb[ctx.uctx.ctxid] = NULL;
-	}
-	spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-	return 0;
-}
-
-/*
- * set up an aio context
- * allocate a new context with given parameters and prepare a eventfd_context
- */
-static int nvme_set_aioctx(struct nvme_aioctx __user *uctx )
-{
-	struct nvme_kaioctx *ctx;
-	struct file *efile;
-	struct eventfd_ctx *eventfd_ctx = NULL;
-	unsigned long flags;
-	int ret = 0;
-	int i = 0;
-	if (!capable(CAP_SYS_ADMIN))
-		return -EACCES;
-
-	ctx = mempool_alloc(kaioctx_mempool, GFP_NOIO); //ctx = kmem_cache_zalloc(kaioctx_cachep, GFP_KERNEL);
-	if (!ctx)
-		return -ENOMEM;
-
-	if (copy_from_user(ctx, uctx, sizeof(struct nvme_aioctx)))
-		return -EFAULT;
-
-	efile = fget(ctx->uctx.eventfd);
-	if (!efile) {
-		printk("nvme_set_aioctx: failed to get efile for efd %d.\n", ctx->uctx.eventfd);
-		ret = -EBADF;
-		goto exit;
-	}
-
-	eventfd_ctx = eventfd_ctx_fileget(efile);
-	if (IS_ERR(eventfd_ctx)) {
-		printk("nvme_set_aioctx: failed to get eventfd_ctx for efd %d.\n", ctx->uctx.eventfd);
-		ret = PTR_ERR(eventfd_ctx);
-		goto put_efile;
-	}
-	// set context id
-	spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
-	if(g_kaioctx_tb[aio_context_id]) {
-		for(i = 0; i < AIOCTX_MAX; i++) {
-			if(g_kaioctx_tb[i] == NULL) {
-				aio_context_id = i;
-				break;
-			}
-		}
-		if (i >= AIOCTX_MAX) {
-			spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-			printk("nvme_set_aioctx: too many aioctx open.\n");
-			ret = -EMFILE;
-			goto put_event_fd;
-		}
-	}
-	ctx->uctx.ctxid = aio_context_id++;
-	if (aio_context_id == AIOCTX_MAX)
-		aio_context_id = 0;
-	ctx->eventctx = eventfd_ctx;
-	spin_lock_init(&ctx->kaioctx_spinlock);
-	INIT_LIST_HEAD(&ctx->kaiocb_list);
-	kref_init(&ctx->ref);
-	g_kaioctx_tb[ctx->uctx.ctxid] = ctx;
-	spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-
-	if (copy_to_user(&uctx->ctxid, &ctx->uctx.ctxid, sizeof(ctx->uctx.ctxid)))
-	{
-		printk("nvme_set_aioctx: failed to copy context id %d to user.\n", ctx->uctx.ctxid);
-		ret =  -EINVAL;
-		goto cleanup;
-	}
-	eventfd_ctx = NULL;
-	debug_outstanding = 0;
-	debug_completed = 0;
-	fput(efile);
-	return 0;
-cleanup:
-	spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
-	g_kaioctx_tb[ctx->uctx.ctxid - 1] = NULL;
-	spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-	mempool_free(ctx, kaiocb_mempool);
-put_event_fd:
-	eventfd_ctx_put(eventfd_ctx);	
-put_efile:
-	fput(efile);
-exit:
-	return ret;
-}
-
-/* get an aiocb, which represents a single I/O request.
-*/
-static struct nvme_kaiocb *get_aiocb(__u64 reqid) {
-	struct nvme_kaiocb *req;
-	req = mempool_alloc(kaiocb_mempool, GFP_NOIO); //ctx = kmem_cache_zalloc(kaioctx_cachep, GFP_KERNEL);
-	if (!req) return 0;
-
-	memset(req, 0, sizeof(*req));
-
-	INIT_LIST_HEAD(&req->aiocb_list);
-
-	req->event.reqid = reqid;
-
-	return req;
-}
-
-/* returns the completed events to users
-*/
-static int nvme_get_aioevents(struct nvme_aioevents __user *uevents)
-{
-	struct list_head *pos, *q;
-	struct nvme_kaiocb *tmp;
-	struct nvme_kaioctx *tmp_ctx;
-	unsigned long flags;
-	LIST_HEAD(tmp_head);
-	__u16 count =0;
-	__u16 nr = 0;
-	__u32 ctxid = 0;
-
-	if (!capable(CAP_SYS_ADMIN))
-		return -EACCES;
-
-	if (get_user(nr, &uevents->nr) < 0) { 	return -EINVAL;    }
-
-	if (nr == 0 || nr > 128) return -EINVAL;
-
-	if (get_user(ctxid, &uevents->ctxid) < 0) { return -EINVAL; }
-
-	tmp_ctx = find_kaioctx(ctxid);
-	if (tmp_ctx) {
-		spin_lock_irqsave(&tmp_ctx->kaioctx_spinlock, flags);
-		list_for_each_safe(pos, q, &tmp_ctx->kaiocb_list){
-			list_del_init(pos);
-			list_add(pos, &tmp_head);
-			count++;
-			if (nr == count) break;
-		}
-		spin_unlock_irqrestore(&tmp_ctx->kaioctx_spinlock, flags);
-		deref_kaioctx(tmp_ctx);
-		count = 0; 
-		list_for_each_safe(pos, q, &tmp_head){
-			list_del(pos);
-			tmp = list_entry(pos, struct nvme_kaiocb, aiocb_list);
-			if (copy_to_user(&uevents->events[count], &tmp->event, sizeof(struct nvme_aioevent))) {
-				mempool_free(tmp, kaiocb_mempool);
-				return -EINVAL;
-			}
-			mempool_free(tmp, kaiocb_mempool);
-			count++;
-		}
-	}
-	if (put_user(count, &uevents->nr)  < 0) { return -EINVAL; }
-
-	return 0;
-}
-
-static void kv_async_completion(struct nvme_queue *nvmeq, void *ctx, struct nvme_completion *cqe) {
-
-  struct nvme_dev *dev = nvmeq->dev;
-  struct nvme_kaiocb *cmdinfo = (struct nvme_kaiocb *)ctx;
-	cmdinfo->event.result = le32_to_cpu(cqe->result);
-	cmdinfo->event.status = le16_to_cpu(cqe->status) >> 1;
-#if 0
-	printk("kv_async_completion: call unmap...\n");
-#endif
-    if (cmdinfo->kv_data && cmdinfo->user_ctx) {
-		if (is_kv_retrieve_cmd(cmdinfo->opcode) || is_kv_iter_read_cmd(cmdinfo->opcode)) {
-#if 0
-            char *data = cmdinfo->kv_data;
-            pr_err("recevied data %c:%c:%c:%c: %c:%c:%c:%c.\n",
-                    data[0], data[1], data[2], data[3],
-                    data[4], data[5], data[6], data[7]);
-#endif
-			(void)sg_copy_from_buffer(cmdinfo->user_ctx->sg, cmdinfo->user_ctx->nents,
-					cmdinfo->kv_data, cmdinfo->user_ctx->len);
-        }
-    }
-
-	if (cmdinfo->iod) {
-		nvme_unmap_user_pages(dev, kv_nvme_is_write(cmdinfo->opcode), cmdinfo->iod);
-		nvme_free_iod(dev, cmdinfo->iod);
-	}
-
-	if (cmdinfo->user_ctx) { 
-		int i = 0;
-		for (i = 0; i < cmdinfo->user_ctx->nents; i++)
-			put_page(sg_page(&cmdinfo->user_ctx->sg[i]));
-		kfree(cmdinfo->user_ctx);
-	}
-	if (cmdinfo->kv_data) kfree(cmdinfo->kv_data);
-
-	if (cmdinfo->use_meta) {
-		dma_unmap_sg(&dev->pci_dev->dev, &cmdinfo->meta_sg, 1, DMA_TO_DEVICE);
-		put_page(sg_page(&cmdinfo->meta_sg));
-		if (cmdinfo->meta) {
-			kfree(cmdinfo->meta);
-		}
-	}
-
-	if (set_aioctx_event(cmdinfo->event.ctxid, cmdinfo)) {
-		mempool_free(cmdinfo, kaiocb_mempool);
-	}
-}
-#endif
-
-/*
- * Check we didin't inadvertently grow the command struct
- */
-static inline void _nvme_check_size(void)
-{
-	BUILD_BUG_ON(sizeof(struct nvme_rw_command) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_create_cq) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_create_sq) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_delete_queue) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_features) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_format_cmd) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_abort_cmd) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_command) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_id_ctrl) != 4096);
-	BUILD_BUG_ON(sizeof(struct nvme_id_ns) != 4096);
-	BUILD_BUG_ON(sizeof(struct nvme_lba_range_type) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_smart_log) != 512);
-}
-
-typedef void (*nvme_completion_fn)(struct nvme_queue *, void *,
-						struct nvme_completion *);
-
-struct nvme_cmd_info {
-	nvme_completion_fn fn;
-	void *ctx;
-	unsigned long timeout;
-	int aborted;
-};
-
-static struct nvme_cmd_info *nvme_cmd_info(struct nvme_queue *nvmeq)
-{
-	return (void *)&nvmeq->cmdid_data[BITS_TO_LONGS(nvmeq->q_depth)];
-}
-
-static unsigned nvme_queue_extra(int depth)
-{
-	return DIV_ROUND_UP(depth, 8) + (depth * sizeof(struct nvme_cmd_info));
-}
-
-/**
- * alloc_cmdid() - Allocate a Command ID
- * @nvmeq: The queue that will be used for this command
- * @ctx: A pointer that will be passed to the handler
- * @handler: The function to call on completion
- *
- * Allocate a Command ID for a queue.  The data passed in will
- * be passed to the completion handler.  This is implemented by using
- * the bottom two bits of the ctx pointer to store the handler ID.
- * Passing in a pointer that's not 4-byte aligned will cause a BUG.
- * We can change this if it becomes a problem.
- *
- * May be called with local interrupts disabled and the q_lock held,
- * or with interrupts enabled and no locks held.
- */
-static int alloc_cmdid(struct nvme_queue *nvmeq, void *ctx,
-				nvme_completion_fn handler, unsigned timeout)
-{
-	int depth = nvmeq->q_depth - 1;
-	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
-	int cmdid;
-
-	do {
-		cmdid = find_first_zero_bit(nvmeq->cmdid_data, depth);
-		if (cmdid >= depth)
-			return -EBUSY;
-	} while (test_and_set_bit(cmdid, nvmeq->cmdid_data));
-
-	info[cmdid].fn = handler;
-	info[cmdid].ctx = ctx;
-	info[cmdid].timeout = jiffies + timeout;
-	info[cmdid].aborted = 0;
-	return cmdid;
-}
-
-static int alloc_cmdid_killable(struct nvme_queue *nvmeq, void *ctx,
-				nvme_completion_fn handler, unsigned timeout)
-{
-	int cmdid;
-	wait_event_killable(nvmeq->sq_full,
-		(cmdid = alloc_cmdid(nvmeq, ctx, handler, timeout)) >= 0);
-	return (cmdid < 0) ? -EINTR : cmdid;
-}
-
-/* Special values must be less than 0x1000 */
-#define CMD_CTX_BASE		((void *)POISON_POINTER_DELTA)
-#define CMD_CTX_CANCELLED	(0x30C + CMD_CTX_BASE)
-#define CMD_CTX_COMPLETED	(0x310 + CMD_CTX_BASE)
-#define CMD_CTX_INVALID		(0x314 + CMD_CTX_BASE)
-#define CMD_CTX_FLUSH		(0x318 + CMD_CTX_BASE)
-#define CMD_CTX_ABORT		(0x31C + CMD_CTX_BASE)
-
-static void special_completion(struct nvme_queue *nvmeq, void *ctx,
-						struct nvme_completion *cqe)
-{
-	if (ctx == CMD_CTX_CANCELLED)
-		return;
-	if (ctx == CMD_CTX_FLUSH)
-		return;
-	if (ctx == CMD_CTX_ABORT) {
-		++nvmeq->dev->abort_limit;
-		return;
-	}
-	if (ctx == CMD_CTX_COMPLETED) {
-		dev_warn(nvmeq->q_dmadev,
-				"completed id %d twice on queue %d\n",
-				cqe->command_id, le16_to_cpup(&cqe->sq_id));
-		return;
-	}
-	if (ctx == CMD_CTX_INVALID) {
-		dev_warn(nvmeq->q_dmadev,
-				"invalid id %d completed on queue %d\n",
-				cqe->command_id, le16_to_cpup(&cqe->sq_id));
-		return;
-	}
-
-	dev_warn(nvmeq->q_dmadev, "Unknown special completion %p\n", ctx);
-}
-
-static void async_completion(struct nvme_queue *nvmeq, void *ctx,
-						struct nvme_completion *cqe)
-{
-	struct async_cmd_info *cmdinfo = ctx;
-	cmdinfo->result = le32_to_cpup(&cqe->result);
-	cmdinfo->status = le16_to_cpup(&cqe->status) >> 1;
-	queue_kthread_work(cmdinfo->worker, &cmdinfo->work);
-}
-
-/*
- * Called with local interrupts disabled and the q_lock held.  May not sleep.
- */
-static void *free_cmdid(struct nvme_queue *nvmeq, int cmdid,
-						nvme_completion_fn *fn)
-{
-	void *ctx;
-	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
-
-	if (cmdid >= nvmeq->q_depth) {
-		*fn = special_completion;
-		return CMD_CTX_INVALID;
-	}
-	if (fn)
-		*fn = info[cmdid].fn;
-	ctx = info[cmdid].ctx;
-	info[cmdid].fn = special_completion;
-	info[cmdid].ctx = CMD_CTX_COMPLETED;
-	clear_bit(cmdid, nvmeq->cmdid_data);
-	wake_up(&nvmeq->sq_full);
-	return ctx;
-}
-
-static void *cancel_cmdid(struct nvme_queue *nvmeq, int cmdid,
-						nvme_completion_fn *fn)
-{
-	void *ctx;
-	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
-	if (fn)
-		*fn = info[cmdid].fn;
-	ctx = info[cmdid].ctx;
-	info[cmdid].fn = special_completion;
-	info[cmdid].ctx = CMD_CTX_CANCELLED;
-	return ctx;
-}
-
-static struct nvme_queue *raw_nvmeq(struct nvme_dev *dev, int qid)
-{
-	return rcu_dereference_raw(dev->queues[qid]);
-}
-
-static struct nvme_queue *get_nvmeq(struct nvme_dev *dev) __acquires(RCU)
-{
-	unsigned queue_id;
-	unsigned short *io_queue;
-
-	io_queue = per_cpu_ptr(dev->io_queue, get_cpu());
-	queue_id = *io_queue;
-	put_cpu();
-	rcu_read_lock();
-	return rcu_dereference(dev->queues[queue_id]);
-}
-
-static void put_nvmeq(struct nvme_queue *nvmeq) __releases(RCU)
-{
-	rcu_read_unlock();
-	put_cpu_var(nvmeq->dev->io_queue);
-}
-
-static struct nvme_queue *lock_nvmeq(struct nvme_dev *dev, int q_idx)
-							__acquires(RCU)
-{
-	rcu_read_lock();
-	return rcu_dereference(dev->queues[q_idx]);
-}
-
-static void unlock_nvmeq(struct nvme_queue *nvmeq) __releases(RCU)
-{
-	rcu_read_unlock();
-}
-
-/**
- * nvme_submit_cmd() - Copy a command into a queue and ring the doorbell
- * @nvmeq: The queue to use
- * @cmd: The command to send
- *
- * Safe to use from interrupt context
- */
-static int nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd)
-{
-	unsigned long flags;
-	u16 tail;
-	spin_lock_irqsave(&nvmeq->q_lock, flags);
-	if (nvmeq->q_suspended) {
-		spin_unlock_irqrestore(&nvmeq->q_lock, flags);
-		return -EBUSY;
-	}
-	tail = nvmeq->sq_tail;
-	memcpy(&nvmeq->sq_cmds[tail], cmd, sizeof(*cmd));
-	if (++tail == nvmeq->q_depth)
-		tail = 0;
-	writel(tail, nvmeq->q_db);
-	nvmeq->sq_tail = tail;
-	spin_unlock_irqrestore(&nvmeq->q_lock, flags);
-
-	return 0;
-}
-
-static __le64 **iod_list(struct nvme_iod *iod)
-{
-	return ((void *)iod) + iod->offset;
-}
-
-/*
- * Will slightly overestimate the number of pages needed.  This is OK
- * as it only leads to a small amount of wasted memory for the lifetime of
- * the I/O.
- */
-static int nvme_npages(unsigned size)
-{
-	unsigned nprps = DIV_ROUND_UP(size + PAGE_SIZE, PAGE_SIZE);
-	return DIV_ROUND_UP(8 * nprps, PAGE_SIZE - 8);
-}
-
-static struct nvme_iod *
-nvme_alloc_iod(unsigned nseg, unsigned nbytes, gfp_t gfp)
-{
-	struct nvme_iod *iod = kmalloc(sizeof(struct nvme_iod) +
-				sizeof(__le64 *) * nvme_npages(nbytes) +
-				sizeof(struct scatterlist) * nseg, gfp);
-
-	if (iod) {
-		iod->offset = offsetof(struct nvme_iod, sg[nseg]);
-		iod->npages = -1;
-		iod->length = nbytes;
-		iod->nents = 0;
-		iod->first_dma = 0ULL;
-		iod->start_time = jiffies;
-	}
-
-	return iod;
-}
-
-void nvme_free_iod(struct nvme_dev *dev, struct nvme_iod *iod)
-{
-	const int last_prp = PAGE_SIZE / 8 - 1;
-	int i;
-	__le64 **list = iod_list(iod);
-	dma_addr_t prp_dma = iod->first_dma;
-
-	if (iod->npages == 0)
-		dma_pool_free(dev->prp_small_pool, list[0], prp_dma);
-	for (i = 0; i < iod->npages; i++) {
-		__le64 *prp_list = list[i];
-		dma_addr_t next_prp_dma = le64_to_cpu(prp_list[last_prp]);
-		dma_pool_free(dev->prp_page_pool, prp_list, prp_dma);
-		prp_dma = next_prp_dma;
-	}
-	kfree(iod);
-}
-
-static void nvme_start_io_acct(struct bio *bio)
-{
-	struct gendisk *disk = bio->bi_bdev->bd_disk;
-	const int rw = bio_data_dir(bio);
-	int cpu = part_stat_lock();
-	part_round_stats(cpu, &disk->part0);
-	part_stat_inc(cpu, &disk->part0, ios[rw]);
-	part_stat_add(cpu, &disk->part0, sectors[rw], bio_sectors(bio));
-	part_inc_in_flight(&disk->part0, rw);
-	part_stat_unlock();
-}
-
-static void nvme_end_io_acct(struct bio *bio, unsigned long start_time)
-{
-	struct gendisk *disk = bio->bi_bdev->bd_disk;
-	const int rw = bio_data_dir(bio);
-	unsigned long duration = jiffies - start_time;
-	int cpu = part_stat_lock();
-	part_stat_add(cpu, &disk->part0, ticks[rw], duration);
-	part_round_stats(cpu, &disk->part0);
-	part_dec_in_flight(&disk->part0, rw);
-	part_stat_unlock();
-}
-
-static void bio_completion(struct nvme_queue *nvmeq, void *ctx,
-						struct nvme_completion *cqe)
-{
-	struct nvme_iod *iod = ctx;
-	struct bio *bio = iod->private;
-	u16 status = le16_to_cpup(&cqe->status) >> 1;
-
-	if (unlikely(status)) {
-		if (!(status & NVME_SC_DNR ||
-		      (bio_rw_flagged(bio, BIO_RW_FAILFAST_DEV) ||
-		       bio_rw_flagged(bio, BIO_RW_FAILFAST_TRANSPORT) ||
-		       bio_rw_flagged(bio, BIO_RW_FAILFAST_DRIVER))) &&
-		    (jiffies - iod->start_time) < IOD_TIMEOUT) {
-			if (!waitqueue_active(&nvmeq->sq_full))
-				add_wait_queue(&nvmeq->sq_full,
-					       &nvmeq->sq_cong_wait);
-			list_add_tail(&iod->node, &nvmeq->iod_bio);
-			wake_up(&nvmeq->sq_full);
-			return;
-		}
-	}
-	if (iod->nents) {
-		dma_unmap_sg(nvmeq->q_dmadev, iod->sg, iod->nents,
-			bio_data_dir(bio) ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
-		nvme_end_io_acct(bio, iod->start_time);
-	}
-	nvme_free_iod(nvmeq->dev, iod);
-	if (status)
-		bio_endio(bio, -EIO);
-	else
-		bio_endio(bio, 0);
-}
-
-/* length is in bytes.  gfp flags indicates whether we may sleep. */
-int nvme_setup_prps(struct nvme_dev *dev, struct nvme_iod *iod, int total_len,
-								gfp_t gfp)
-{
-	struct dma_pool *pool;
-	int length = total_len;
-	struct scatterlist *sg = iod->sg;
-	int dma_len = sg_dma_len(sg);
-	u64 dma_addr = sg_dma_address(sg);
-	int offset = offset_in_page(dma_addr);
-	__le64 *prp_list;
-	__le64 **list = iod_list(iod);
-	dma_addr_t prp_dma;
-	int nprps, i;
-
-	length -= (PAGE_SIZE - offset);
-	if (length <= 0)
-		return total_len;
-
-	dma_len -= (PAGE_SIZE - offset);
-	if (dma_len) {
-		dma_addr += (PAGE_SIZE - offset);
-	} else {
-		sg = sg_next(sg);
-		dma_addr = sg_dma_address(sg);
-		dma_len = sg_dma_len(sg);
-	}
-
-	if (length <= PAGE_SIZE) {
-		iod->first_dma = dma_addr;
-		return total_len;
-	}
-
-	nprps = DIV_ROUND_UP(length, PAGE_SIZE);
-	if (nprps <= (256 / 8)) {
-		pool = dev->prp_small_pool;
-		iod->npages = 0;
-	} else {
-		pool = dev->prp_page_pool;
-		iod->npages = 1;
-	}
-
-	prp_list = dma_pool_alloc(pool, gfp, &prp_dma);
-	if (!prp_list) {
-		iod->first_dma = dma_addr;
-		iod->npages = -1;
-		return (total_len - length) + PAGE_SIZE;
-	}
-	list[0] = prp_list;
-	iod->first_dma = prp_dma;
-	i = 0;
-	for (;;) {
-		if (i == PAGE_SIZE / 8) {
-			__le64 *old_prp_list = prp_list;
-			prp_list = dma_pool_alloc(pool, gfp, &prp_dma);
-			if (!prp_list)
-				return total_len - length;
-			list[iod->npages++] = prp_list;
-			prp_list[0] = old_prp_list[i - 1];
-			old_prp_list[i - 1] = cpu_to_le64(prp_dma);
-			i = 1;
-		}
-		prp_list[i++] = cpu_to_le64(dma_addr);
-		dma_len -= PAGE_SIZE;
-		dma_addr += PAGE_SIZE;
-		length -= PAGE_SIZE;
-		if (length <= 0)
-			break;
-		if (dma_len > 0)
-			continue;
-		BUG_ON(dma_len < 0);
-		sg = sg_next(sg);
-		dma_addr = sg_dma_address(sg);
-		dma_len = sg_dma_len(sg);
-	}
-
-	return total_len;
-}
-
-struct nvme_bio_pair {
-	struct bio b1, b2, *parent;
-	struct bio_vec *bv1, *bv2;
-	int err;
-	atomic_t cnt;
-};
-
-static void nvme_bio_pair_endio(struct bio *bio, int err)
-{
-	struct nvme_bio_pair *bp = bio->bi_private;
-
-	if (err)
-		bp->err = err;
-
-	if (atomic_dec_and_test(&bp->cnt)) {
-		bio_endio(bp->parent, bp->err);
-		kfree(bp->bv1);
-		kfree(bp->bv2);
-		kfree(bp);
-	}
-}
-
-static struct nvme_bio_pair *nvme_bio_split(struct bio *bio, int idx,
-							int len, int offset)
-{
-	struct nvme_bio_pair *bp;
-
-	BUG_ON(len > bio->bi_size);
-	BUG_ON(idx > bio->bi_vcnt);
-
-	bp = kmalloc(sizeof(*bp), GFP_ATOMIC);
-	if (!bp)
-		return NULL;
-	bp->err = 0;
-
-	bp->b1 = *bio;
-	bp->b2 = *bio;
-
-	bp->b1.bi_size = len;
-	bp->b2.bi_size -= len;
-	bp->b1.bi_vcnt = idx;
-	bp->b2.bi_idx = idx;
-	bp->b2.bi_sector += len >> 9;
-
-	if (offset) {
-		bp->bv1 = kmalloc(bio->bi_max_vecs * sizeof(struct bio_vec),
-								GFP_ATOMIC);
-		if (!bp->bv1)
-			goto split_fail_1;
-
-		bp->bv2 = kmalloc(bio->bi_max_vecs * sizeof(struct bio_vec),
-								GFP_ATOMIC);
-		if (!bp->bv2)
-			goto split_fail_2;
-
-		memcpy(bp->bv1, bio->bi_io_vec,
-			bio->bi_max_vecs * sizeof(struct bio_vec));
-		memcpy(bp->bv2, bio->bi_io_vec,
-			bio->bi_max_vecs * sizeof(struct bio_vec));
-
-		bp->b1.bi_io_vec = bp->bv1;
-		bp->b2.bi_io_vec = bp->bv2;
-		bp->b2.bi_io_vec[idx].bv_offset += offset;
-		bp->b2.bi_io_vec[idx].bv_len -= offset;
-		bp->b1.bi_io_vec[idx].bv_len = offset;
-		bp->b1.bi_vcnt++;
-	} else
-		bp->bv1 = bp->bv2 = NULL;
-
-	bp->b1.bi_private = bp;
-	bp->b2.bi_private = bp;
-
-	bp->b1.bi_end_io = nvme_bio_pair_endio;
-	bp->b2.bi_end_io = nvme_bio_pair_endio;
-
-	bp->parent = bio;
-	atomic_set(&bp->cnt, 2);
-
-	return bp;
-
- split_fail_2:
-	kfree(bp->bv1);
- split_fail_1:
-	kfree(bp);
-	return NULL;
-}
-
-static int nvme_split_and_submit(struct bio *bio, struct nvme_queue *nvmeq,
-						int idx, int len, int offset)
-{
-	struct nvme_bio_pair *bp = nvme_bio_split(bio, idx, len, offset);
-	if (!bp)
-		return -ENOMEM;
-
-	if (!waitqueue_active(&nvmeq->sq_full))
-		add_wait_queue(&nvmeq->sq_full, &nvmeq->sq_cong_wait);
-	bio_list_add(&nvmeq->sq_cong, &bp->b1);
-	bio_list_add(&nvmeq->sq_cong, &bp->b2);
-	wake_up(&nvmeq->sq_full);
-
-	return 0;
-}
-
-/* NVMe scatterlists require no holes in the virtual address */
-#define BIOVEC_NOT_VIRT_MERGEABLE(vec1, vec2)	((vec2)->bv_offset || \
-			(((vec1)->bv_offset + (vec1)->bv_len) % PAGE_SIZE))
-
-static int nvme_map_bio(struct nvme_queue *nvmeq, struct nvme_iod *iod,
-		struct bio *bio, enum dma_data_direction dma_dir, int psegs)
-{
-	struct bio_vec *bvec, *bvprv = NULL;
-	struct scatterlist *sg = NULL;
-	int i, length = 0, nsegs = 0, split_len = bio->bi_size;
-
-	if (nvmeq->dev->stripe_size)
-		split_len = nvmeq->dev->stripe_size -
-			((bio->bi_sector << 9) & (nvmeq->dev->stripe_size - 1));
-
-	sg_init_table(iod->sg, psegs);
-	bio_for_each_segment(bvec, bio, i) {
-		if (bvprv && BIOVEC_PHYS_MERGEABLE(bvprv, bvec)) {
-			sg->length += bvec->bv_len;
-		} else {
-			if (bvprv && BIOVEC_NOT_VIRT_MERGEABLE(bvprv, bvec))
-				return nvme_split_and_submit(bio, nvmeq, i,
-								length, 0);
-
-			sg = sg ? sg + 1 : iod->sg;
-			sg_set_page(sg, bvec->bv_page, bvec->bv_len,
-							bvec->bv_offset);
-			nsegs++;
-		}
-
-		if (split_len - length < bvec->bv_len)
-			return nvme_split_and_submit(bio, nvmeq, i, split_len,
-							split_len - length);
-		length += bvec->bv_len;
-		bvprv = bvec;
-	}
-	iod->nents = nsegs;
-	sg_mark_end(sg);
-	if (dma_map_sg(nvmeq->q_dmadev, iod->sg, iod->nents, dma_dir) == 0)
-		return -ENOMEM;
-
-	BUG_ON(length != bio->bi_size);
-	return length;
-}
-
-static int nvme_submit_discard(struct nvme_queue *nvmeq, struct nvme_ns *ns,
-		struct bio *bio, struct nvme_iod *iod, int cmdid)
-{
-	struct nvme_dsm_range *range =
-				(struct nvme_dsm_range *)iod_list(iod)[0];
-	struct nvme_command *cmnd = &nvmeq->sq_cmds[nvmeq->sq_tail];
-
-	range->cattr = cpu_to_le32(0);
-	range->nlb = cpu_to_le32(bio->bi_size >> ns->lba_shift);
-	range->slba = cpu_to_le64(nvme_block_nr(ns, bio->bi_sector));
-
-	memset(cmnd, 0, sizeof(*cmnd));
-	cmnd->dsm.opcode = nvme_cmd_dsm;
-	cmnd->dsm.command_id = cmdid;
-	cmnd->dsm.nsid = cpu_to_le32(ns->ns_id);
-	cmnd->dsm.prp1 = cpu_to_le64(iod->first_dma);
-	cmnd->dsm.nr = 0;
-	cmnd->dsm.attributes = cpu_to_le32(NVME_DSMGMT_AD);
-
-	if (++nvmeq->sq_tail == nvmeq->q_depth)
-		nvmeq->sq_tail = 0;
-	writel(nvmeq->sq_tail, nvmeq->q_db);
-
-	return 0;
-}
-
-static int nvme_submit_flush(struct nvme_queue *nvmeq, struct nvme_ns *ns,
-								int cmdid)
-{
-	struct nvme_command *cmnd = &nvmeq->sq_cmds[nvmeq->sq_tail];
-
-	memset(cmnd, 0, sizeof(*cmnd));
-	cmnd->common.opcode = nvme_cmd_flush;
-	cmnd->common.command_id = cmdid;
-	cmnd->common.nsid = cpu_to_le32(ns->ns_id);
-
-	if (++nvmeq->sq_tail == nvmeq->q_depth)
-		nvmeq->sq_tail = 0;
-	writel(nvmeq->sq_tail, nvmeq->q_db);
-
-	return 0;
-}
-
-int nvme_submit_flush_data(struct nvme_queue *nvmeq, struct nvme_ns *ns)
-{
-	int cmdid = alloc_cmdid(nvmeq, (void *)CMD_CTX_FLUSH,
-					special_completion, NVME_IO_TIMEOUT);
-	if (unlikely(cmdid < 0))
-		return cmdid;
-
-	return nvme_submit_flush(nvmeq, ns, cmdid);
-}
-
-static int nvme_submit_iod(struct nvme_queue *nvmeq, struct nvme_iod *iod)
-{
-	struct bio *bio = iod->private;
-	struct nvme_ns *ns = bio->bi_bdev->bd_disk->private_data;
-	struct nvme_command *cmnd;
-	int cmdid;
-	u16 control;
-	u32 dsmgmt;
-
-	cmdid = alloc_cmdid(nvmeq, iod, bio_completion, NVME_IO_TIMEOUT);
-	if (unlikely(cmdid < 0))
-		return cmdid;
-
-	if (bio_rw_flagged(bio, BIO_RW_DISCARD))
-		return nvme_submit_discard(nvmeq, ns, bio, iod, cmdid);
-	if (bio_rw_flagged(bio, BIO_RW_FLUSH) && !iod->nents)
-		return nvme_submit_flush(nvmeq, ns, cmdid);
-
-	control = 0;
-	if (bio_rw_flagged(bio, BIO_RW_FUA))
-		control |= NVME_RW_FUA;
-	if (bio_rw_flagged(bio, BIO_RW_FAILFAST_DEV) || bio_rw_flagged(bio, BIO_RW_AHEAD))
-		control |= NVME_RW_LR;
-
-	dsmgmt = 0;
-	if (bio_rw_flagged(bio, BIO_RW_AHEAD))
-		dsmgmt |= NVME_RW_DSM_FREQ_PREFETCH;
-
-	cmnd = &nvmeq->sq_cmds[nvmeq->sq_tail];
-	memset(cmnd, 0, sizeof(*cmnd));
-
-	cmnd->rw.opcode = bio_data_dir(bio) ? nvme_cmd_write : nvme_cmd_read;
-	cmnd->rw.command_id = cmdid;
-	cmnd->rw.nsid = cpu_to_le32(ns->ns_id);
-	cmnd->rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
-	cmnd->rw.prp2 = cpu_to_le64(iod->first_dma);
-	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, bio->bi_sector));
-	cmnd->rw.length =
-		cpu_to_le16((bio->bi_size >> ns->lba_shift) - 1);
-	cmnd->rw.control = cpu_to_le16(control);
-	cmnd->rw.dsmgmt = cpu_to_le32(dsmgmt);
-
-	if (++nvmeq->sq_tail == nvmeq->q_depth)
-		nvmeq->sq_tail = 0;
-	writel(nvmeq->sq_tail, nvmeq->q_db);
-
-	return 0;
-
-}
-
-/*
- * Called with local interrupts disabled and the q_lock held.  May not sleep.
- */
-static int nvme_submit_bio_queue(struct nvme_queue *nvmeq, struct nvme_ns *ns,
-								struct bio *bio)
-{
-	struct nvme_iod *iod;
-	int psegs = bio_phys_segments(ns->queue, bio);
-	int result;
-
-	if (bio_rw_flagged(bio, BIO_RW_FLUSH) && psegs) {
-		result = nvme_submit_flush_data(nvmeq, ns);
-		if (result)
-			return result;
-	}
-
-	iod = nvme_alloc_iod(psegs, bio->bi_size, GFP_ATOMIC);
-	if (!iod)
-		return -ENOMEM;
-
-	iod->private = bio;
-	if (bio_rw_flagged(bio, BIO_RW_DISCARD)) {
-		void *range;
-		/*
-		 * We reuse the small pool to allocate the 16-byte range here
-		 * as it is not worth having a special pool for these or
-		 * additional cases to handle freeing the iod.
-		 */
-		range = dma_pool_alloc(nvmeq->dev->prp_small_pool,
-						GFP_ATOMIC,
-						&iod->first_dma);
-		if (!range) {
-			result = -ENOMEM;
-			goto free_iod;
-		}
-		iod_list(iod)[0] = (__le64 *)range;
-		iod->npages = 0;
-	} else if (psegs) {
-		result = nvme_map_bio(nvmeq, iod, bio,
-			bio_data_dir(bio) ? DMA_TO_DEVICE : DMA_FROM_DEVICE,
-			psegs);
-		if (result <= 0)
-			goto free_iod;
-		if (nvme_setup_prps(nvmeq->dev, iod, result, GFP_ATOMIC) !=
-								result) {
-			result = -ENOMEM;
-			goto free_iod;
-		}
-		nvme_start_io_acct(bio);
-	}
-	if (unlikely(nvme_submit_iod(nvmeq, iod))) {
-		if (!waitqueue_active(&nvmeq->sq_full))
-			add_wait_queue(&nvmeq->sq_full, &nvmeq->sq_cong_wait);
-		list_add_tail(&iod->node, &nvmeq->iod_bio);
-	}
-	return 0;
-
- free_iod:
-	nvme_free_iod(nvmeq->dev, iod);
-	return result;
-}
-
-static int nvme_process_cq(struct nvme_queue *nvmeq)
-{
-	u16 head, phase;
-
-	head = nvmeq->cq_head;
-	phase = nvmeq->cq_phase;
-
-	for (;;) {
-		void *ctx;
-		nvme_completion_fn fn;
-		struct nvme_completion cqe = nvmeq->cqes[head];
-		if ((le16_to_cpu(cqe.status) & 1) != phase)
-			break;
-		nvmeq->sq_head = le16_to_cpu(cqe.sq_head);
-		if (++head == nvmeq->q_depth) {
-			head = 0;
-			phase = !phase;
-		}
-
-		ctx = free_cmdid(nvmeq, cqe.command_id, &fn);
-		fn(nvmeq, ctx, &cqe);
-	}
-
-	/* If the controller ignores the cq head doorbell and continuously
-	 * writes to the queue, it is theoretically possible to wrap around
-	 * the queue twice and mistakenly return IRQ_NONE.  Linux only
-	 * requires that 0.1% of your interrupts are handled, so this isn't
-	 * a big problem.
-	 */
-	if (head == nvmeq->cq_head && phase == nvmeq->cq_phase)
-		return 0;
-
-	writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
-	nvmeq->cq_head = head;
-	nvmeq->cq_phase = phase;
-
-	nvmeq->cqe_seen = 1;
-	return 1;
-}
-
-static int nvme_make_request(struct request_queue *q, struct bio *bio)
-{
-	struct nvme_ns *ns = q->queuedata;
-	struct nvme_queue *nvmeq = get_nvmeq(ns->dev);
-	int result = -EBUSY;
-
-	if (!nvmeq) {
-		put_nvmeq(NULL);
-		bio_endio(bio, -EIO);
-		return 0;
-	}
-
-	spin_lock_irq(&nvmeq->q_lock);
-	if (!nvmeq->q_suspended && bio_list_empty(&nvmeq->sq_cong))
-		result = nvme_submit_bio_queue(nvmeq, ns, bio);
-	if (unlikely(result)) {
-		if (!waitqueue_active(&nvmeq->sq_full))
-			add_wait_queue(&nvmeq->sq_full, &nvmeq->sq_cong_wait);
-		bio_list_add(&nvmeq->sq_cong, bio);
-	}
-
-	nvme_process_cq(nvmeq);
-	spin_unlock_irq(&nvmeq->q_lock);
-	put_nvmeq(nvmeq);
-	return 0;
-}
-
-static irqreturn_t nvme_irq(int irq, void *data)
-{
-	irqreturn_t result;
-	struct nvme_queue *nvmeq = data;
-	spin_lock(&nvmeq->q_lock);
-	nvme_process_cq(nvmeq);
-	result = nvmeq->cqe_seen ? IRQ_HANDLED : IRQ_NONE;
-	nvmeq->cqe_seen = 0;
-	spin_unlock(&nvmeq->q_lock);
-	return result;
-}
-
-static irqreturn_t nvme_irq_check(int irq, void *data)
-{
-	struct nvme_queue *nvmeq = data;
-	struct nvme_completion cqe = nvmeq->cqes[nvmeq->cq_head];
-	if ((le16_to_cpu(cqe.status) & 1) != nvmeq->cq_phase)
-		return IRQ_NONE;
-	return IRQ_WAKE_THREAD;
-}
-
-static void nvme_abort_command(struct nvme_queue *nvmeq, int cmdid)
-{
-	spin_lock_irq(&nvmeq->q_lock);
-	cancel_cmdid(nvmeq, cmdid, NULL);
-	spin_unlock_irq(&nvmeq->q_lock);
-}
-
-struct sync_cmd_info {
-	struct task_struct *task;
-	u32 result;
-	int status;
-};
-
-static void sync_completion(struct nvme_queue *nvmeq, void *ctx,
-						struct nvme_completion *cqe)
-{
-	struct sync_cmd_info *cmdinfo = ctx;
-	cmdinfo->result = le32_to_cpup(&cqe->result);
-	cmdinfo->status = le16_to_cpup(&cqe->status) >> 1;
-	wake_up_process(cmdinfo->task);
-}
-
-/*
- * Returns 0 on success.  If the result is negative, it's a Linux error code;
- * if the result is positive, it's an NVM Express status code
- */
-static int nvme_submit_sync_cmd(struct nvme_dev *dev, int q_idx,
-						struct nvme_command *cmd,
-						u32 *result, unsigned timeout)
-{
-	int cmdid, ret;
-	struct sync_cmd_info cmdinfo;
-	struct nvme_queue *nvmeq;
-
-	nvmeq = lock_nvmeq(dev, q_idx);
-	if (!nvmeq) {
-		unlock_nvmeq(nvmeq);
-		return -ENODEV;
-	}
-
-	cmdinfo.task = current;
-	cmdinfo.status = -EINTR;
-
-	cmdid = alloc_cmdid(nvmeq, &cmdinfo, sync_completion, timeout);
-	if (cmdid < 0) {
-		unlock_nvmeq(nvmeq);
-		return cmdid;
-	}
-	cmd->common.command_id = cmdid;
-
-	set_current_state(TASK_KILLABLE);
-	ret = nvme_submit_cmd(nvmeq, cmd);
-	if (ret) {
-		free_cmdid(nvmeq, cmdid, NULL);
-		unlock_nvmeq(nvmeq);
-		set_current_state(TASK_RUNNING);
-		return ret;
-	}
-	unlock_nvmeq(nvmeq);
-	schedule_timeout(timeout);
-
-	if (cmdinfo.status == -EINTR) {
-		nvmeq = lock_nvmeq(dev, q_idx);
-		if (nvmeq)
-			nvme_abort_command(nvmeq, cmdid);
-		unlock_nvmeq(nvmeq);
-		return -EINTR;
-	}
-
-	if (result)
-		*result = cmdinfo.result;
-
-	return cmdinfo.status;
-}
-
-static int nvme_submit_async_cmd(struct nvme_queue *nvmeq,
-			struct nvme_command *cmd,
-			struct async_cmd_info *cmdinfo, unsigned timeout)
-{
-	int cmdid;
-
-	cmdid = alloc_cmdid_killable(nvmeq, cmdinfo, async_completion, timeout);
-	if (cmdid < 0)
-		return cmdid;
-	cmdinfo->status = -EINTR;
-	cmd->common.command_id = cmdid;
-	return nvme_submit_cmd(nvmeq, cmd);
-}
-
-int nvme_submit_admin_cmd(struct nvme_dev *dev, struct nvme_command *cmd,
-								u32 *result)
-{
-	return nvme_submit_sync_cmd(dev, 0, cmd, result, ADMIN_TIMEOUT);
-}
-
-int nvme_submit_io_cmd(struct nvme_dev *dev, struct nvme_command *cmd,
-								u32 *result)
-{
-	return nvme_submit_sync_cmd(dev, smp_processor_id() + 1, cmd, result,
-							NVME_IO_TIMEOUT);
-}
-
-static int nvme_submit_admin_cmd_async(struct nvme_dev *dev,
-		struct nvme_command *cmd, struct async_cmd_info *cmdinfo)
-{
-	return nvme_submit_async_cmd(raw_nvmeq(dev, 0), cmd, cmdinfo,
-								ADMIN_TIMEOUT);
-}
-
-static int adapter_delete_queue(struct nvme_dev *dev, u8 opcode, u16 id)
-{
-	int status;
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.delete_queue.opcode = opcode;
-	c.delete_queue.qid = cpu_to_le16(id);
-
-	status = nvme_submit_admin_cmd(dev, &c, NULL);
-	if (status)
-		return -EIO;
-	return 0;
-}
-
-static int adapter_alloc_cq(struct nvme_dev *dev, u16 qid,
-						struct nvme_queue *nvmeq)
-{
-	int status;
-	struct nvme_command c;
-	int flags = NVME_QUEUE_PHYS_CONTIG | NVME_CQ_IRQ_ENABLED;
-
-	memset(&c, 0, sizeof(c));
-	c.create_cq.opcode = nvme_admin_create_cq;
-	c.create_cq.prp1 = cpu_to_le64(nvmeq->cq_dma_addr);
-	c.create_cq.cqid = cpu_to_le16(qid);
-	c.create_cq.qsize = cpu_to_le16(nvmeq->q_depth - 1);
-	c.create_cq.cq_flags = cpu_to_le16(flags);
-	c.create_cq.irq_vector = cpu_to_le16(nvmeq->cq_vector);
-
-	status = nvme_submit_admin_cmd(dev, &c, NULL);
-	if (status)
-		return -EIO;
-	return 0;
-}
-
-static int adapter_alloc_sq(struct nvme_dev *dev, u16 qid,
-						struct nvme_queue *nvmeq)
-{
-	int status;
-	struct nvme_command c;
-	int flags = NVME_QUEUE_PHYS_CONTIG | NVME_SQ_PRIO_MEDIUM;
-
-	memset(&c, 0, sizeof(c));
-	c.create_sq.opcode = nvme_admin_create_sq;
-	c.create_sq.prp1 = cpu_to_le64(nvmeq->sq_dma_addr);
-	c.create_sq.sqid = cpu_to_le16(qid);
-	c.create_sq.qsize = cpu_to_le16(nvmeq->q_depth - 1);
-	c.create_sq.sq_flags = cpu_to_le16(flags);
-	c.create_sq.cqid = cpu_to_le16(qid);
-
-	status = nvme_submit_admin_cmd(dev, &c, NULL);
-	if (status)
-		return -EIO;
-	return 0;
-}
-
-static int adapter_delete_cq(struct nvme_dev *dev, u16 cqid)
-{
-	return adapter_delete_queue(dev, nvme_admin_delete_cq, cqid);
-}
-
-static int adapter_delete_sq(struct nvme_dev *dev, u16 sqid)
-{
-	return adapter_delete_queue(dev, nvme_admin_delete_sq, sqid);
-}
-
-int nvme_identify(struct nvme_dev *dev, unsigned nsid, unsigned cns,
-							dma_addr_t dma_addr)
-{
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.identify.opcode = nvme_admin_identify;
-	c.identify.nsid = cpu_to_le32(nsid);
-	c.identify.prp1 = cpu_to_le64(dma_addr);
-	c.identify.cns = cpu_to_le32(cns);
-
-	return nvme_submit_admin_cmd(dev, &c, NULL);
-}
-
-int nvme_get_features(struct nvme_dev *dev, unsigned fid, unsigned nsid,
-					dma_addr_t dma_addr, u32 *result)
-{
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.features.opcode = nvme_admin_get_features;
-	c.features.nsid = cpu_to_le32(nsid);
-	c.features.prp1 = cpu_to_le64(dma_addr);
-	c.features.fid = cpu_to_le32(fid);
-
-	return nvme_submit_admin_cmd(dev, &c, result);
-}
-
-int nvme_set_features(struct nvme_dev *dev, unsigned fid, unsigned dword11,
-					dma_addr_t dma_addr, u32 *result)
-{
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.features.opcode = nvme_admin_set_features;
-	c.features.prp1 = cpu_to_le64(dma_addr);
-	c.features.fid = cpu_to_le32(fid);
-	c.features.dword11 = cpu_to_le32(dword11);
-
-	return nvme_submit_admin_cmd(dev, &c, result);
-}
-
-/**
- * nvme_abort_cmd - Attempt aborting a command
- * @cmdid: Command id of a timed out IO
- * @queue: The queue with timed out IO
- *
- * Schedule controller reset if the command was already aborted once before and
- * still hasn't been returned to the driver, or if this is the admin queue.
- */
-static void nvme_abort_cmd(int cmdid, struct nvme_queue *nvmeq)
-{
-	int a_cmdid;
-	struct nvme_command cmd;
-	struct nvme_dev *dev = nvmeq->dev;
-	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
-	struct nvme_queue *adminq;
-
-	if (!nvmeq->qid || info[cmdid].aborted) {
-		if (work_busy(&dev->reset_work))
-			return;
-		list_del_init(&dev->node);
-		dev_warn(&dev->pci_dev->dev,
-			"I/O %d QID %d timeout, reset controller\n", cmdid,
-								nvmeq->qid);
-		PREPARE_WORK(&dev->reset_work, nvme_reset_failed_dev);
-		queue_work(nvme_workq, &dev->reset_work);
-		return;
-	}
-
-	if (!dev->abort_limit)
-		return;
-
-	adminq = rcu_dereference(dev->queues[0]);
-	a_cmdid = alloc_cmdid(adminq, CMD_CTX_ABORT, special_completion,
-								ADMIN_TIMEOUT);
-	if (a_cmdid < 0)
-		return;
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.abort.opcode = nvme_admin_abort_cmd;
-	cmd.abort.cid = cmdid;
-	cmd.abort.sqid = cpu_to_le16(nvmeq->qid);
-	cmd.abort.command_id = a_cmdid;
-
-	--dev->abort_limit;
-	info[cmdid].aborted = 1;
-	info[cmdid].timeout = jiffies + ADMIN_TIMEOUT;
-
-	dev_warn(nvmeq->q_dmadev, "Aborting I/O %d QID %d\n", cmdid,
-							nvmeq->qid);
-	nvme_submit_cmd(adminq, &cmd);
-}
-
-/**
- * nvme_cancel_ios - Cancel outstanding I/Os
- * @queue: The queue to cancel I/Os on
- * @timeout: True to only cancel I/Os which have timed out
- */
-static void nvme_cancel_ios(struct nvme_queue *nvmeq, bool timeout)
-{
-	int depth = nvmeq->q_depth - 1;
-	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
-	unsigned long now = jiffies;
-	int cmdid;
-
-	for_each_set_bit(cmdid, nvmeq->cmdid_data, depth) {
-		void *ctx;
-		nvme_completion_fn fn;
-		static struct nvme_completion cqe = {
-			.status = cpu_to_le16(NVME_SC_ABORT_REQ << 1),
-		};
-
-		if (timeout && !time_after(now, info[cmdid].timeout))
-			continue;
-		if (info[cmdid].ctx == CMD_CTX_CANCELLED)
-			continue;
-		if (timeout && nvmeq->dev->initialized) {
-			nvme_abort_cmd(cmdid, nvmeq);
-			continue;
-		}
-		dev_warn(nvmeq->q_dmadev, "Cancelling I/O %d QID %d\n", cmdid,
-								nvmeq->qid);
-		ctx = cancel_cmdid(nvmeq, cmdid, &fn);
-		fn(nvmeq, ctx, &cqe);
-	}
-}
-
-static void nvme_free_queue(struct rcu_head *r)
-{
-	struct nvme_queue *nvmeq = container_of(r, struct nvme_queue, r_head);
-
-	spin_lock_irq(&nvmeq->q_lock);
-	while (bio_list_peek(&nvmeq->sq_cong)) {
-		struct bio *bio = bio_list_pop(&nvmeq->sq_cong);
-		bio_endio(bio, -EIO);
-	}
-	while (!list_empty(&nvmeq->iod_bio)) {
-		static struct nvme_completion cqe = {
-			.status = cpu_to_le16(
-				(NVME_SC_ABORT_REQ | NVME_SC_DNR) << 1),
-		};
-		struct nvme_iod *iod = list_first_entry(&nvmeq->iod_bio,
-							struct nvme_iod,
-							node);
-		list_del(&iod->node);
-		bio_completion(nvmeq, iod, &cqe);
-	}
-	spin_unlock_irq(&nvmeq->q_lock);
-
-	dma_free_coherent(nvmeq->q_dmadev, CQ_SIZE(nvmeq->q_depth),
-				(void *)nvmeq->cqes, nvmeq->cq_dma_addr);
-	dma_free_coherent(nvmeq->q_dmadev, SQ_SIZE(nvmeq->q_depth),
-					nvmeq->sq_cmds, nvmeq->sq_dma_addr);
-	if (nvmeq->qid)
-		free_cpumask_var(nvmeq->cpu_mask);
-	kfree(nvmeq);
-}
-
-static void nvme_free_queues(struct nvme_dev *dev, int lowest)
-{
-	int i;
-
-	for (i = dev->queue_count - 1; i >= lowest; i--) {
-		struct nvme_queue *nvmeq = raw_nvmeq(dev, i);
-		rcu_assign_pointer(dev->queues[i], NULL);
-		call_rcu(&nvmeq->r_head, nvme_free_queue);
-		dev->queue_count--;
-	}
-}
-
-/**
- * nvme_suspend_queue - put queue into suspended state
- * @nvmeq - queue to suspend
- *
- * Returns 1 if already suspended, 0 otherwise.
- */
-static int nvme_suspend_queue(struct nvme_queue *nvmeq)
-{
-	int vector = nvmeq->dev->entry[nvmeq->cq_vector].vector;
-
-	spin_lock_irq(&nvmeq->q_lock);
-	if (nvmeq->q_suspended) {
-		spin_unlock_irq(&nvmeq->q_lock);
-		return 1;
-	}
-	nvmeq->q_suspended = 1;
-	nvmeq->dev->online_queues--;
-	spin_unlock_irq(&nvmeq->q_lock);
-
-	irq_set_affinity_hint(vector, NULL);
-	free_irq(vector, nvmeq);
-
-	return 0;
-}
-
-static void nvme_clear_queue(struct nvme_queue *nvmeq)
-{
-	spin_lock_irq(&nvmeq->q_lock);
-	nvme_process_cq(nvmeq);
-	nvme_cancel_ios(nvmeq, false);
-	spin_unlock_irq(&nvmeq->q_lock);
-}
-
-static void nvme_disable_queue(struct nvme_dev *dev, int qid)
-{
-	struct nvme_queue *nvmeq = raw_nvmeq(dev, qid);
-
-	if (!nvmeq)
-		return;
-	if (nvme_suspend_queue(nvmeq))
-		return;
-
-	/* Don't tell the adapter to delete the admin queue.
-	 * Don't tell a removed adapter to delete IO queues. */
-	if (qid && readl(&dev->bar->csts) != -1) {
-		adapter_delete_sq(dev, qid);
-		adapter_delete_cq(dev, qid);
-	}
-	nvme_clear_queue(nvmeq);
-}
-
-static struct nvme_queue *nvme_alloc_queue(struct nvme_dev *dev, int qid,
-							int depth, int vector)
-{
-	struct device *dmadev = &dev->pci_dev->dev;
-	unsigned extra = nvme_queue_extra(depth);
-	struct nvme_queue *nvmeq = kzalloc(sizeof(*nvmeq) + extra, GFP_KERNEL);
-	if (!nvmeq)
-		return NULL;
-
-	nvmeq->cqes = dma_alloc_coherent(dmadev, CQ_SIZE(depth),
-					&nvmeq->cq_dma_addr, GFP_KERNEL);
-	if (!nvmeq->cqes)
-		goto free_nvmeq;
-	memset((void *)nvmeq->cqes, 0, CQ_SIZE(depth));
-
-	nvmeq->sq_cmds = dma_alloc_coherent(dmadev, SQ_SIZE(depth),
-					&nvmeq->sq_dma_addr, GFP_KERNEL);
-	if (!nvmeq->sq_cmds)
-		goto free_cqdma;
-
-	if (qid && !zalloc_cpumask_var(&nvmeq->cpu_mask, GFP_KERNEL))
-		goto free_sqdma;
-
-	nvmeq->q_dmadev = dmadev;
-	nvmeq->dev = dev;
-	snprintf(nvmeq->irqname, sizeof(nvmeq->irqname), "nvme%dq%d",
-			dev->instance, qid);
-	spin_lock_init(&nvmeq->q_lock);
-	nvmeq->cq_head = 0;
-	nvmeq->cq_phase = 1;
-	init_waitqueue_head(&nvmeq->sq_full);
-	init_waitqueue_entry(&nvmeq->sq_cong_wait, nvme_thread);
-	bio_list_init(&nvmeq->sq_cong);
-	INIT_LIST_HEAD(&nvmeq->iod_bio);
-	nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
-	nvmeq->q_depth = depth;
-	nvmeq->cq_vector = vector;
-	nvmeq->qid = qid;
-	nvmeq->q_suspended = 1;
-	dev->queue_count++;
-	rcu_assign_pointer(dev->queues[qid], nvmeq);
-
-	return nvmeq;
-
- free_sqdma:
-	dma_free_coherent(dmadev, SQ_SIZE(depth), (void *)nvmeq->sq_cmds,
-							nvmeq->sq_dma_addr);
- free_cqdma:
-	dma_free_coherent(dmadev, CQ_SIZE(depth), (void *)nvmeq->cqes,
-							nvmeq->cq_dma_addr);
- free_nvmeq:
-	kfree(nvmeq);
-	return NULL;
-}
-
-static int queue_request_irq(struct nvme_dev *dev, struct nvme_queue *nvmeq,
-							const char *name)
-{
-	if (use_threaded_interrupts)
-		return request_threaded_irq(dev->entry[nvmeq->cq_vector].vector,
-					nvme_irq_check, nvme_irq, IRQF_SHARED,
-					name, nvmeq);
-	return request_irq(dev->entry[nvmeq->cq_vector].vector, nvme_irq,
-				IRQF_SHARED, name, nvmeq);
-}
-
-static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
-{
-	struct nvme_dev *dev = nvmeq->dev;
-	unsigned extra = nvme_queue_extra(nvmeq->q_depth);
-
-	nvmeq->sq_tail = 0;
-	nvmeq->cq_head = 0;
-	nvmeq->cq_phase = 1;
-	nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
-	memset(nvmeq->cmdid_data, 0, extra);
-	memset((void *)nvmeq->cqes, 0, CQ_SIZE(nvmeq->q_depth));
-	nvme_cancel_ios(nvmeq, false);
-	nvmeq->q_suspended = 0;
-	dev->online_queues++;
-}
-
-static int nvme_create_queue(struct nvme_queue *nvmeq, int qid)
-{
-	struct nvme_dev *dev = nvmeq->dev;
-	int result;
-
-	result = adapter_alloc_cq(dev, qid, nvmeq);
-	if (result < 0)
-		return result;
-
-	result = adapter_alloc_sq(dev, qid, nvmeq);
-	if (result < 0)
-		goto release_cq;
-
-	result = queue_request_irq(dev, nvmeq, nvmeq->irqname);
-	if (result < 0)
-		goto release_sq;
-
-	spin_lock_irq(&nvmeq->q_lock);
-	nvme_init_queue(nvmeq, qid);
-	spin_unlock_irq(&nvmeq->q_lock);
-
-	return result;
-
- release_sq:
-	adapter_delete_sq(dev, qid);
- release_cq:
-	adapter_delete_cq(dev, qid);
-	return result;
-}
-
-static int nvme_wait_ready(struct nvme_dev *dev, u64 cap, bool enabled)
-{
-	unsigned long timeout;
-	u32 bit = enabled ? NVME_CSTS_RDY : 0;
-
-	timeout = ((NVME_CAP_TIMEOUT(cap) + 1) * HZ / 2) + jiffies;
-
-	while ((readl(&dev->bar->csts) & NVME_CSTS_RDY) != bit) {
-		msleep(100);
-		if (fatal_signal_pending(current))
-			return -EINTR;
-		if (time_after(jiffies, timeout)) {
-			dev_err(&dev->pci_dev->dev,
-				"Device not ready; aborting initialisation\n");
-			return -ENODEV;
-		}
-	}
-
-	return 0;
-}
-
-/*
- * If the device has been passed off to us in an enabled state, just clear
- * the enabled bit.  The spec says we should set the 'shutdown notification
- * bits', but doing so may cause the device to complete commands to the
- * admin queue ... and we don't know what memory that might be pointing at!
- */
-static int nvme_disable_ctrl(struct nvme_dev *dev, u64 cap)
-{
-	u32 cc = readl(&dev->bar->cc);
-
-	if (cc & NVME_CC_ENABLE)
-		writel(cc & ~NVME_CC_ENABLE, &dev->bar->cc);
-	return nvme_wait_ready(dev, cap, false);
-}
-
-static int nvme_enable_ctrl(struct nvme_dev *dev, u64 cap)
-{
-	return nvme_wait_ready(dev, cap, true);
-}
-
-static int nvme_shutdown_ctrl(struct nvme_dev *dev)
-{
-	unsigned long timeout;
-	u32 cc;
-
-	cc = (readl(&dev->bar->cc) & ~NVME_CC_SHN_MASK) | NVME_CC_SHN_NORMAL;
-	writel(cc, &dev->bar->cc);
-
-	timeout = 2 * HZ + jiffies;
-	while ((readl(&dev->bar->csts) & NVME_CSTS_SHST_MASK) !=
-							NVME_CSTS_SHST_CMPLT) {
-		msleep(100);
-		if (fatal_signal_pending(current))
-			return -EINTR;
-		if (time_after(jiffies, timeout)) {
-			dev_err(&dev->pci_dev->dev,
-				"Device shutdown incomplete; abort shutdown\n");
-			return -ENODEV;
-		}
-	}
-
-	return 0;
-}
-
-static int nvme_configure_admin_queue(struct nvme_dev *dev)
-{
-	int result;
-	u32 aqa;
-	u64 cap = readq(&dev->bar->cap);
-	struct nvme_queue *nvmeq;
-
-	result = nvme_disable_ctrl(dev, cap);
-	if (result < 0)
-		return result;
-
-	nvmeq = raw_nvmeq(dev, 0);
-	if (!nvmeq) {
-		nvmeq = nvme_alloc_queue(dev, 0, 64, 0);
-		if (!nvmeq)
-			return -ENOMEM;
-	}
-
-	aqa = nvmeq->q_depth - 1;
-	aqa |= aqa << 16;
-
-	dev->ctrl_config = NVME_CC_ENABLE | NVME_CC_CSS_NVM;
-	dev->ctrl_config |= (PAGE_SHIFT - 12) << NVME_CC_MPS_SHIFT;
-	dev->ctrl_config |= NVME_CC_ARB_RR | NVME_CC_SHN_NONE;
-	dev->ctrl_config |= NVME_CC_IOSQES | NVME_CC_IOCQES;
-
-	writel(aqa, &dev->bar->aqa);
-	writeq(nvmeq->sq_dma_addr, &dev->bar->asq);
-	writeq(nvmeq->cq_dma_addr, &dev->bar->acq);
-	writel(dev->ctrl_config, &dev->bar->cc);
-
-	result = nvme_enable_ctrl(dev, cap);
-	if (result)
-		return result;
-
-	result = queue_request_irq(dev, nvmeq, nvmeq->irqname);
-	if (result)
-		return result;
-
-	spin_lock_irq(&nvmeq->q_lock);
-	nvme_init_queue(nvmeq, 0);
-	spin_unlock_irq(&nvmeq->q_lock);
-	return result;
-}
-
-struct nvme_iod *nvme_map_user_pages(struct nvme_dev *dev, int write,
-				unsigned long addr, unsigned length)
-{
-	int i, err, count, nents, offset;
-	struct scatterlist *sg;
-	struct page **pages;
-	struct nvme_iod *iod;
-
-	if (addr & 3)
-		return ERR_PTR(-EINVAL);
-	if (!length || length > INT_MAX - PAGE_SIZE)
-		return ERR_PTR(-EINVAL);
-
-	offset = offset_in_page(addr);
-	count = DIV_ROUND_UP(offset + length, PAGE_SIZE);
-	pages = kcalloc(count, sizeof(*pages), GFP_KERNEL);
-	if (!pages)
-		return ERR_PTR(-ENOMEM);
-
-	err = get_user_pages_fast(addr, count, 1, pages);
-	if (err < count) {
-		count = err;
-		err = -EFAULT;
-		goto put_pages;
-	}
-
-	iod = nvme_alloc_iod(count, length, GFP_KERNEL);
-	sg = iod->sg;
-	sg_init_table(sg, count);
-	for (i = 0; i < count; i++) {
-		sg_set_page(&sg[i], pages[i],
-			    min_t(unsigned, length, PAGE_SIZE - offset),
-			    offset);
-		length -= (PAGE_SIZE - offset);
-		offset = 0;
-	}
-	sg_mark_end(&sg[i - 1]);
-	iod->nents = count;
-
-	err = -ENOMEM;
-	nents = dma_map_sg(&dev->pci_dev->dev, sg, count,
-				write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
-	if (!nents)
-		goto free_iod;
-
-	kfree(pages);
-	return iod;
-
- free_iod:
-	kfree(iod);
- put_pages:
-	for (i = 0; i < count; i++)
-		put_page(pages[i]);
-	kfree(pages);
-	return ERR_PTR(err);
-}
-
-void nvme_unmap_user_pages(struct nvme_dev *dev, int write,
-			struct nvme_iod *iod)
-{
-	int i;
-
-	dma_unmap_sg(&dev->pci_dev->dev, iod->sg, iod->nents,
-				write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
-
-	for (i = 0; i < iod->nents; i++)
-		put_page(sg_page(&iod->sg[i]));
-}
-
-static int nvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
-{
-	struct nvme_dev *dev = ns->dev;
-	struct nvme_user_io io;
-	struct nvme_command c;
-	unsigned length, meta_len;
-	int status, i;
-	struct nvme_iod *iod, *meta_iod = NULL;
-	dma_addr_t meta_dma_addr;
-	void *meta, *uninitialized_var(meta_mem);
-
-	if (copy_from_user(&io, uio, sizeof(io)))
-		return -EFAULT;
-	length = (io.nblocks + 1) << ns->lba_shift;
-	meta_len = (io.nblocks + 1) * ns->ms;
-
-	if (meta_len && ((io.metadata & 3) || !io.metadata))
-		return -EINVAL;
-
-	switch (io.opcode) {
-	case nvme_cmd_write:
-	case nvme_cmd_read:
-	case nvme_cmd_compare:
-		iod = nvme_map_user_pages(dev, io.opcode & 1, io.addr, length);
-		break;
-	default:
-		return -EINVAL;
-	}
-
-	if (IS_ERR(iod))
-		return PTR_ERR(iod);
-
-	memset(&c, 0, sizeof(c));
-	c.rw.opcode = io.opcode;
-	c.rw.flags = io.flags;
-	c.rw.nsid = cpu_to_le32(ns->ns_id);
-	c.rw.slba = cpu_to_le64(io.slba);
-	c.rw.length = cpu_to_le16(io.nblocks);
-	c.rw.control = cpu_to_le16(io.control);
-	c.rw.dsmgmt = cpu_to_le32(io.dsmgmt);
-	c.rw.reftag = cpu_to_le32(io.reftag);
-	c.rw.apptag = cpu_to_le16(io.apptag);
-	c.rw.appmask = cpu_to_le16(io.appmask);
-
-	if (meta_len) {
-		meta_iod = nvme_map_user_pages(dev, io.opcode & 1, io.metadata,
-								meta_len);
-		if (IS_ERR(meta_iod)) {
-			status = PTR_ERR(meta_iod);
-			meta_iod = NULL;
-			goto unmap;
-		}
-
-		meta_mem = dma_alloc_coherent(&dev->pci_dev->dev, meta_len,
-						&meta_dma_addr, GFP_KERNEL);
-		if (!meta_mem) {
-			status = -ENOMEM;
-			goto unmap;
-		}
-
-		if (io.opcode & 1) {
-			int meta_offset = 0;
-
-			for (i = 0; i < meta_iod->nents; i++) {
-				meta = kmap_atomic(sg_page(&meta_iod->sg[i]), KM_USER0) +
-						meta_iod->sg[i].offset;
-				memcpy(meta_mem + meta_offset, meta,
-						meta_iod->sg[i].length);
-				kunmap_atomic(meta, KM_USER0);
-				meta_offset += meta_iod->sg[i].length;
-			}
-		}
-
-		c.rw.metadata = cpu_to_le64(meta_dma_addr);
-	}
-
-	length = nvme_setup_prps(dev, iod, length, GFP_KERNEL);
-	c.rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
-	c.rw.prp2 = cpu_to_le64(iod->first_dma);
-
-	if (length != (io.nblocks + 1) << ns->lba_shift)
-		status = -ENOMEM;
-	else
-		status = nvme_submit_io_cmd(dev, &c, NULL);
-
-	if (meta_len) {
-		if (status == NVME_SC_SUCCESS && !(io.opcode & 1)) {
-			int meta_offset = 0;
-
-			for (i = 0; i < meta_iod->nents; i++) {
-				meta = kmap_atomic(sg_page(&meta_iod->sg[i]), KM_USER0) +
-						meta_iod->sg[i].offset;
-				memcpy(meta, meta_mem + meta_offset,
-						meta_iod->sg[i].length);
-				kunmap_atomic(meta, KM_USER0);
-				meta_offset += meta_iod->sg[i].length;
-			}
-		}
-
-		dma_free_coherent(&dev->pci_dev->dev, meta_len, meta_mem,
-								meta_dma_addr);
-	}
-
- unmap:
-	nvme_unmap_user_pages(dev, io.opcode & 1, iod);
-	nvme_free_iod(dev, iod);
-
-	if (meta_iod) {
-		nvme_unmap_user_pages(dev, io.opcode & 1, meta_iod);
-		nvme_free_iod(dev, meta_iod);
-	}
-
-	return status;
-}
-
-static int nvme_user_admin_cmd(struct nvme_dev *dev,
-					struct nvme_admin_cmd __user *ucmd)
-{
-	struct nvme_admin_cmd cmd;
-	struct nvme_command c;
-	int status, length;
-	struct nvme_iod *uninitialized_var(iod);
-	unsigned timeout;
-
-	if (!capable(CAP_SYS_ADMIN))
-		return -EACCES;
-	if (copy_from_user(&cmd, ucmd, sizeof(cmd)))
-		return -EFAULT;
-
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = cmd.opcode;
-	c.common.flags = cmd.flags;
-	c.common.nsid = cpu_to_le32(cmd.nsid);
-	c.common.cdw2[0] = cpu_to_le32(cmd.cdw2);
-	c.common.cdw2[1] = cpu_to_le32(cmd.cdw3);
-	c.common.cdw10[0] = cpu_to_le32(cmd.cdw10);
-	c.common.cdw10[1] = cpu_to_le32(cmd.cdw11);
-	c.common.cdw10[2] = cpu_to_le32(cmd.cdw12);
-	c.common.cdw10[3] = cpu_to_le32(cmd.cdw13);
-	c.common.cdw10[4] = cpu_to_le32(cmd.cdw14);
-	c.common.cdw10[5] = cpu_to_le32(cmd.cdw15);
-
-	length = cmd.data_len;
-	if (cmd.data_len) {
-		iod = nvme_map_user_pages(dev, cmd.opcode & 1, cmd.addr,
-								length);
-		if (IS_ERR(iod))
-			return PTR_ERR(iod);
-		length = nvme_setup_prps(dev, iod, length, GFP_KERNEL);
-		c.common.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
-		c.common.prp2 = cpu_to_le64(iod->first_dma);
-	}
-
-	timeout = cmd.timeout_ms ? msecs_to_jiffies(cmd.timeout_ms) :
-								ADMIN_TIMEOUT;
-	if (length != cmd.data_len)
-		status = -ENOMEM;
-	else
-		status = nvme_submit_sync_cmd(dev, 0, &c, &cmd.result, timeout);
-
-	if (cmd.data_len) {
-		nvme_unmap_user_pages(dev, cmd.opcode & 1, iod);
-		nvme_free_iod(dev, iod);
-	}
-
-	if ((status >= 0) && copy_to_user(&ucmd->result, &cmd.result,
-							sizeof(cmd.result)))
-		status = -EFAULT;
-
-	return status;
-}
-
-
-#if 1
-struct nvme_iod *nvme_map_kernel_pages(struct nvme_dev *dev, int write,
-		unsigned long addr, unsigned length)
-{
-	int i, err, count, nents, offset;
-	struct scatterlist *sg;
-	struct page **pages;
-	struct page *page = NULL;
-	struct nvme_iod *iod;
-	char *src_data = NULL;
-
-	if (addr & 3)
-		return ERR_PTR(-EINVAL);
-	if (!length || length > INT_MAX - PAGE_SIZE)
-		return ERR_PTR(-EINVAL);
-
-	offset = offset_in_page(addr);
-	count = DIV_ROUND_UP(offset + length, PAGE_SIZE);
-	pages = kcalloc(count, sizeof(*pages), GFP_KERNEL);
-	if (!pages)
-		return ERR_PTR(-ENOMEM);
-
-	src_data = (char *)addr;
-	for (i = 0; i < count; i ++) {
-		page = virt_to_page(src_data);
-		get_page(page);
-		pages[i] = page;
-		src_data += PAGE_SIZE;
-	}				
-
-	iod = nvme_alloc_iod(count, length, GFP_KERNEL);
-	sg = iod->sg;
-	sg_init_table(sg, count);
-	for (i = 0; i < count; i++) {
-		sg_set_page(&sg[i], pages[i],
-				min_t(unsigned, length, PAGE_SIZE - offset),
-				offset);
-		length -= (PAGE_SIZE - offset);
-		offset = 0;
-	}
-	sg_mark_end(&sg[i - 1]);
-	iod->nents = count;
-
-	err = -ENOMEM;
-	nents = dma_map_sg(&dev->pci_dev->dev, sg, count,
-			write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
-	if (!nents)
-		goto free_iod;
-
-	kfree(pages);
-	return iod;
-
-free_iod:
-	kfree(iod);
-	for (i = 0; i < count; i++)
-		put_page(pages[i]);
-	kfree(pages);
-	return ERR_PTR(err);
-}
-
-
-
-static int user_addr_npages(int offset, int size)
-{
-	unsigned count = DIV_ROUND_UP(offset + size, PAGE_SIZE);
-	return count;
-}
-
-static struct aio_user_ctx *get_aio_user_ctx(void __user *addr, unsigned len)
-{
-	int offset = offset_in_page(addr);
-	int datalen = len;
-	int num_page = user_addr_npages(offset,len);
-	int size = 0;
-	struct aio_user_ctx  *user_ctx = NULL;
-	int mapped_pages = 0;
-	int i = 0;
-
-	size = sizeof (struct aio_user_ctx) + sizeof(__le64 *) * num_page
-			+ sizeof(struct scatterlist) * num_page -1;
-	/* need to keep user address to map to copy when complete request */
-	user_ctx = (struct aio_user_ctx *)kmalloc(size, GFP_KERNEL);
-	if (!user_ctx) {
-		return NULL;
-    }
-
-    user_ctx->nents = 0;
-	user_ctx->pages =(struct page **)user_ctx->data;
-	user_ctx->sg = (struct scatterlist *)(user_ctx->data + sizeof(__le64 *) * num_page);
-	mapped_pages = get_user_pages_fast((unsigned long)addr, num_page,
-			1, user_ctx->pages);
-	if (mapped_pages != num_page) {
-		user_ctx->nents = mapped_pages;
-		goto exit;
-	}
-    user_ctx->nents = num_page;
-    user_ctx->len = datalen;
-    sg_init_table(user_ctx->sg, num_page);
-    for(i = 0; i < num_page; i++) {
-        sg_set_page(&user_ctx->sg[i], user_ctx->pages[i],
-                min_t(unsigned, datalen, PAGE_SIZE - offset),
-                offset);
-        datalen -= (PAGE_SIZE - offset);
-        offset = 0;
-    }
-    sg_mark_end(&user_ctx->sg[i -1]);
-	return user_ctx;
-exit:
-	if (user_ctx) {
-		for (i = 0; i < user_ctx->nents; i++)
-			put_page(user_ctx->pages[i]);
-		kfree(user_ctx);
-	}
-	return NULL;
-}
-
-int nvme_submit_async_kv_cmd(struct nvme_queue *nvmeq, struct nvme_command* cmd,
-		u32 *result, unsigned timeout, struct nvme_kaiocb *aiocb) {
-	int cmdid;
-	aiocb->event.status = -EINTR;
-	cmdid = alloc_cmdid_killable(nvmeq, aiocb, kv_async_completion, timeout);
-	if (cmdid < 0)
-		return cmdid;
-	cmd->common.command_id = cmdid;
-	nvme_submit_cmd(nvmeq, cmd);
-	return 0;
-}
-
-#define KV_QUEUE_DAM_ALIGNMENT (0x03)
-static bool check_for_single_phyaddress(void __user* address, unsigned length) {
-	unsigned offset = 0;
-	unsigned count = 0;
-	offset = offset_in_page(address);
-	count = DIV_ROUND_UP(offset + length, PAGE_SIZE);
-	if (count > 1 && ((unsigned long)address & KV_QUEUE_DAM_ALIGNMENT)) {
-		return false;
-	}
-	return true;
-}
-int __nvme_submit_kv_user_cmd(struct nvme_ns *ns, struct nvme_command *cmd,
-		struct nvme_passthru_kv_cmd *pthr_cmd,
-		void __user *ubuffer, unsigned bufflen,
-		void __user *meta_buffer, unsigned meta_len,
-		u32 *result, u32 *status, unsigned timeout, bool aio)
-{
-	struct nvme_dev *dev = ns->dev;
-	struct nvme_queue *nvmeq = NULL;
-	struct nvme_kaiocb *aiocb = NULL;
-	struct scatterlist *meta_sg_ptr = NULL;
-	struct scatterlist meta_sg;
-	struct nvme_iod *iod = NULL;
-	unsigned length = 0;
-	int ret = 0;
-	void* meta = NULL;
-	char* kv_data = NULL;
-	bool set_meta = false;
-	bool need_to_copy = false;
-	struct page *p_page = NULL;
-	struct aio_user_ctx * user_ctx = NULL;
-	if (aio) {
-		aiocb = get_aiocb(pthr_cmd->reqid);
-		if (!aiocb)
-			return  -ENOMEM;
-	}
-
-	if (ubuffer && bufflen) {
-		if ((unsigned long)ubuffer & KV_QUEUE_DAM_ALIGNMENT) {
-			int len = DIV_ROUND_UP(bufflen, PAGE_SIZE)*PAGE_SIZE;
-			need_to_copy = true;
-			kv_data = kmalloc(len, GFP_KERNEL);
-			if (kv_data == NULL) {
-				ret = -ENOMEM;
-				goto out;	
-			}
-
-			user_ctx = get_aio_user_ctx(ubuffer, bufflen);
-			if (user_ctx == NULL) {
-				ret = -ENOMEM;
-				goto free_out;
-			}
-			if (is_kv_store_cmd(cmd->common.opcode) || is_kv_append_cmd(cmd->common.opcode)) {
-				(void)sg_copy_to_buffer(user_ctx->sg, user_ctx->nents,
-						kv_data, user_ctx->len);
-#if 0
-				pr_err("copied data %c:%c:%c:%c: %c:%c:%c:%c.\n",
-						kv_data[0], kv_data[1], kv_data[2], kv_data[3],
-						kv_data[4], kv_data[5], kv_data[6], kv_data[7]);
-#endif
-			}
-
-			iod = nvme_map_kernel_pages(dev, kv_nvme_is_write(pthr_cmd->opcode), (unsigned long)kv_data, bufflen);
-		} else {
-			iod = nvme_map_user_pages(dev, kv_nvme_is_write(pthr_cmd->opcode), (unsigned long)ubuffer, bufflen);
-		}
-		if (IS_ERR(iod)) {
-			ret = -ENOMEM;
-			goto free_out;
-		}
-		length = nvme_setup_prps(dev, iod, bufflen, GFP_KERNEL);
-		if (length != bufflen) {
-			ret = -ENOMEM;
-			goto out_unmap;
-		}
-		cmd->common.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
-		cmd->common.prp2 = cpu_to_le64(iod->first_dma);
-
-		if (aio) {
-			aiocb->iod = iod;
-			aiocb->kv_data = kv_data;
-			aiocb->user_ctx = user_ctx;
-		}
-	}
-	if (meta_buffer && meta_len) {
-		int offset = 0, len = 0;
-		if (!check_for_single_phyaddress(meta_buffer, meta_len)) {
-			len = DIV_ROUND_UP(meta_len, 256)*256;
-			meta = kmalloc(len, GFP_KERNEL);
-			if (copy_from_user(meta, meta_buffer, meta_len)) {
-				ret = -EFAULT;
-				goto out;
-			}
-			offset = offset_in_page(meta);
-			p_page = virt_to_page(meta);
-			page_cache_get(p_page);
-		} else {
-			ret = get_user_pages_fast((unsigned long)meta_buffer, 1, 1, &p_page);
-			if (ret < 1) {
-				ret = -ENOMEM;
-				goto out; 
-			}
-			offset = offset_in_page(meta_buffer);
-		}
-		if (aio) {
-			aiocb->use_meta = true;
-			aiocb->meta = meta;
-			meta_sg_ptr = &aiocb->meta_sg;
-		} else {
-			meta_sg_ptr = &meta_sg;
-		}
-		sg_init_table(meta_sg_ptr, 1);
-		sg_set_page(meta_sg_ptr, p_page, meta_len, offset);
-		sg_mark_end(meta_sg_ptr);
-		if (!dma_map_sg(&dev->pci_dev->dev, meta_sg_ptr, 1, DMA_TO_DEVICE)) {
-			ret = -ENOMEM;
-			goto out_unmap;
-		}
-		cmd->kv_store.key_prp = cpu_to_le64(sg_dma_address(meta_sg_ptr)); 
-		set_meta = true;
-	}
-	
-	/*
-	 * Since nvme_submit_sync_cmd sleeps, we can't keep preemption
-	 * disabled.  We may be preempted at any point, and be rescheduled
-	 * to a different CPU.  That will cause cacheline bouncing, but no
-	 * additional races since q_lock already protects against other CPUs.
-	 */
-	if (aio) {
-		nvmeq = get_nvmeq(dev);
-		put_nvmeq(nvmeq);
-		aiocb->event.ctxid = pthr_cmd->ctxid;
-		aiocb->event.reqid = pthr_cmd->reqid;
-		aiocb->opcode = cmd->common.opcode;
-		ret = nvme_submit_async_kv_cmd(nvmeq, cmd, result, timeout, aiocb);
-		if (ret < 0) {
-			*status = ret;
-			goto  out_unmap;
-		}
-		return 0;
-	} else {
-		ret = nvme_submit_io_cmd(dev, cmd, result);
-		*status = ret;
-	}
-
-    if (need_to_copy) {
-		if ((is_kv_retrieve_cmd(cmd->common.opcode) && !ret) ||
-              (is_kv_iter_read_cmd(cmd->common.opcode) && (!ret || ((ret& 0x00ff) == 0x0093)))) {
-#if 0
-            char *data = kv_data;
-            pr_err("recevied data %c:%c:%c:%c: %c:%c:%c:%c.\n",
-                    data[0], data[1], data[2], data[3],
-                    data[4], data[5], data[6], data[7]);
-#endif
-			(void)sg_copy_from_buffer(user_ctx->sg, user_ctx->nents,
-					kv_data, user_ctx->len);
-        }
-    }
-
-out_unmap:
-	if (iod) {
-		nvme_unmap_user_pages(dev, kv_nvme_is_write(pthr_cmd->opcode), iod);
-		nvme_free_iod(dev, iod);
-	}
-	if (p_page) {
-		if (set_meta)
-			dma_unmap_sg(&dev->pci_dev->dev, meta_sg_ptr, 1, DMA_TO_DEVICE);
-		put_page(sg_page(meta_sg_ptr));
-	}
-free_out:
-	if (user_ctx)  {
-		int i = 0;
-		for (i = 0; i < user_ctx->nents; i++)
-			put_page(sg_page(&user_ctx->sg[i]));
-		kfree(user_ctx);
-	}
-	if (kv_data) kfree(kv_data);
-out:
-	if (aio)
-		mempool_free(aiocb, kaiocb_mempool);
-	return ret;
-}
-
-
-static int nvme_user_kv_cmd(struct nvme_ns *ns, struct nvme_passthru_kv_cmd __user *ucmd, bool aio)
-{
-	struct nvme_passthru_kv_cmd cmd;
-	struct nvme_command c;
-	int status = 0;
-	unsigned timeout = NVME_IO_TIMEOUT;
-	void __user *metadata = NULL;
-	unsigned meta_len = 0;
-	unsigned option = 0;
-	unsigned iter_handle = 0;
-
-	if (copy_from_user(&cmd, ucmd, sizeof(cmd)))
-		return -EFAULT;
-	if (cmd.flags)
-		return -EINVAL;
-	if (!is_kv_cmd(cmd.opcode))
-		return -EINVAL;
-
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = cmd.opcode;
-	c.common.flags = cmd.flags;
-#ifdef KSID_SUPPORT
-	c.common.nsid = cmd.cdw3;
-#else
-	c.common.nsid = cpu_to_le32(cmd.nsid);
-#endif
-	if (cmd.timeout_ms)
-		timeout = msecs_to_jiffies(cmd.timeout_ms);
-
-	switch(cmd.opcode) {
-		case nvme_cmd_kv_store:
-		case nvme_cmd_kv_append:
-			option = cpu_to_le32(cmd.cdw4);
-			c.kv_store.offset = cpu_to_le32(cmd.cdw5);
-			/* validate key length */
-			if (cmd.key_length >  KVCMD_MAX_KEY_SIZE) {
-				cmd.result = KVS_ERR_VALUE;
-				status = -EINVAL;
-				goto exit;
-			}
-			c.kv_store.key_len = cpu_to_le32(cmd.key_length -1); /* key len -1 */
-			c.kv_store.option = (option & 0xff);
-            /* set value size */
-            if (cmd.data_length % 4) {
-                c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2) + 1);
-                c.kv_store.invalid_byte = 4 - (cmd.data_length % 4);
-            } else {
-                c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2));
-            }
-
-			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-				metadata = (void __user*)cmd.key_addr;
-				meta_len = cmd.key_length;
-			} else {
-				memcpy(c.kv_store.key, cmd.key, cmd.key_length);
-			}
-			break;
-		case nvme_cmd_kv_retrieve:
-			option = cpu_to_le32(cmd.cdw4);
-			c.kv_retrieve.offset = cpu_to_le32(cmd.cdw5);
-			/* validate key length */
-			if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-					cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-				cmd.result = KVS_ERR_VALUE;
-				status = -EINVAL;
-				goto exit;
-			}
-			c.kv_retrieve.key_len = cpu_to_le32(cmd.key_length -1); /* key len - 1 */
-			c.kv_retrieve.option = (option & 0xff);
-			c.kv_retrieve.value_len = cpu_to_le32((cmd.data_length >> 2));
-
-			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-				metadata = (void __user*)cmd.key_addr;
-				meta_len = cmd.key_length;
-			} else {
-				memcpy(c.kv_retrieve.key, cmd.key, cmd.key_length);
-			}
-			break;
-		case nvme_cmd_kv_delete:
-			option = cpu_to_le32(cmd.cdw4);
-			/* validate key length */
-			if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-					cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-				cmd.result = KVS_ERR_VALUE;
-				status = -EINVAL;
-				goto exit;
-			}
-			c.kv_delete.key_len = cpu_to_le32(cmd.key_length -1);
-			c.kv_delete.option = (option & 0xff);	
-			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-				metadata = (void __user *)cmd.key_addr;
-				meta_len = cmd.key_length;
-			} else {
-				memcpy(c.kv_delete.key, cmd.key, cmd.key_length);
-			}
-			break;
-		case nvme_cmd_kv_exist:
-			option = cpu_to_le32(cmd.cdw4);
-			/* validate key length */
-			if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-					cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-				cmd.result = KVS_ERR_VALUE;
-				status = -EINVAL;
-				goto exit;
-			}
-			c.kv_exist.key_len = cpu_to_le32(cmd.key_length -1);
-			c.kv_exist.option = (option & 0xff);	
-			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-				metadata = (void __user *)cmd.key_addr;
-				meta_len = cmd.key_length;
-			} else {
-				memcpy(c.kv_exist.key, cmd.key, cmd.key_length);
-			}
-			break;
-
-		case nvme_cmd_kv_iter_req:
-			option = cpu_to_le32(cmd.cdw4);
-			iter_handle = cpu_to_le32(cmd.cdw5);
-			c.kv_iter_req.iter_handle = iter_handle & 0xff;
-			c.kv_iter_req.option = option & 0xff;
-			c.kv_iter_req.iter_val = cpu_to_le32(cmd.cdw12);
-			c.kv_iter_req.iter_bitmask = cpu_to_le32(cmd.cdw13);
-			break;
-		case nvme_cmd_kv_iter_read:
-			option = cpu_to_le32(cmd.cdw4);
-			iter_handle = cpu_to_le32(cmd.cdw5);
-			c.kv_iter_read.iter_handle = iter_handle & 0xff;
-			c.kv_iter_read.option = option & 0xff;
-			c.kv_iter_read.value_len = cpu_to_le32((cmd.data_length >> 2));
-			break;
-		default:
-			cmd.result = KVS_ERR_IO;
-			status = -EINVAL;
-			goto exit;
-	}
-
-	status = __nvme_submit_kv_user_cmd(ns, &c, &cmd,
-			(void __user *)(uintptr_t)cmd.data_addr, cmd.data_length, metadata, meta_len,
-			&cmd.result, &cmd.status, timeout, aio);
-exit:
-	if (!aio) {
-		if (put_user(cmd.result, &ucmd->result))
-			return -EFAULT;
-		if (put_user(cmd.status, &ucmd->status))
-			return -EFAULT;
-	}
-	return status;
-}
-#endif
-
-
-
-static int nvme_ioctl(struct block_device *bdev, fmode_t mode, unsigned int cmd,
-							unsigned long arg)
-{
-	struct nvme_ns *ns = bdev->bd_disk->private_data;
-
-	switch (cmd) {
-	case NVME_IOCTL_ID:
-		force_successful_syscall_return();
-		return ns->ns_id;
-	case NVME_IOCTL_ADMIN_CMD:
-		return nvme_user_admin_cmd(ns->dev, (void __user *)arg);
-	case NVME_IOCTL_SUBMIT_IO:
-		return nvme_submit_io(ns, (void __user *)arg);
-	case SG_GET_VERSION_NUM:
-		return nvme_sg_get_version_num((void __user *)arg);
-	case SG_IO:
-		return nvme_sg_io(ns, (void __user *)arg);
-#if 1
-	/*
-	* kv device ioctl.   
-	*/
-	case NVME_IOCTL_AIO_CMD:
-		return nvme_user_kv_cmd(ns, (void __user *)arg, true);
-	case NVME_IOCTL_IO_KV_CMD:
-		return nvme_user_kv_cmd(ns, (void __user *)arg, false);
-	case NVME_IOCTL_SET_AIOCTX:
-		return nvme_set_aioctx((void __user *)arg);
-	case NVME_IOCTL_DEL_AIOCTX:
-		return nvme_del_aioctx((void __user *)arg);
-	case NVME_IOCTL_GET_AIOEVENT:
-		return nvme_get_aioevents((void __user *)arg);
-#endif
-	default:
-		return -ENOTTY;
-	}
-}
-
-#ifdef CONFIG_COMPAT
-static int nvme_compat_ioctl(struct block_device *bdev, fmode_t mode,
-					unsigned int cmd, unsigned long arg)
-{
-	struct nvme_ns *ns = bdev->bd_disk->private_data;
-
-	switch (cmd) {
-	case SG_IO:
-		return nvme_sg_io32(ns, arg);
-	}
-	return nvme_ioctl(bdev, mode, cmd, arg);
-}
-#else
-#define nvme_compat_ioctl	NULL
-#endif
-
-static int nvme_open(struct block_device *bdev, fmode_t mode)
-{
-	struct nvme_ns *ns = bdev->bd_disk->private_data;
-	struct nvme_dev *dev = ns->dev;
-
-	kref_get(&dev->kref);
-	return 0;
-}
-
-static void nvme_free_dev(struct kref *kref);
-
-static int nvme_release(struct gendisk *disk, fmode_t mode)
-{
-	struct nvme_ns *ns = disk->private_data;
-	struct nvme_dev *dev = ns->dev;
-
-	kref_put(&dev->kref, nvme_free_dev);
-	return 0;
-}
-
-static int nvme_getgeo(struct block_device *bd, struct hd_geometry *geo)
-{
-	/* some standard values */
-	geo->heads = 1 << 6;
-	geo->sectors = 1 << 5;
-	geo->cylinders = get_capacity(bd->bd_disk) >> 11;
-	return 0;
-}
-
-static const struct block_device_operations nvme_fops = {
-	.owner		= THIS_MODULE,
-	.ioctl		= nvme_ioctl,
-	.compat_ioctl	= nvme_compat_ioctl,
-	.open		= nvme_open,
-	.release	= nvme_release,
-	.getgeo		= nvme_getgeo,
-};
-
-static void nvme_resubmit_iods(struct nvme_queue *nvmeq)
-{
-	struct nvme_iod *iod, *next;
-
-	list_for_each_entry_safe(iod, next, &nvmeq->iod_bio, node) {
-		if (unlikely(nvme_submit_iod(nvmeq, iod)))
-			break;
-		list_del(&iod->node);
-		if (bio_list_empty(&nvmeq->sq_cong) &&
-						list_empty(&nvmeq->iod_bio))
-			remove_wait_queue(&nvmeq->sq_full,
-						&nvmeq->sq_cong_wait);
-	}
-}
-
-static void nvme_resubmit_bios(struct nvme_queue *nvmeq)
-{
-	while (bio_list_peek(&nvmeq->sq_cong)) {
-		struct bio *bio = bio_list_pop(&nvmeq->sq_cong);
-		struct nvme_ns *ns = bio->bi_bdev->bd_disk->private_data;
-
-		if (bio_list_empty(&nvmeq->sq_cong) &&
-						list_empty(&nvmeq->iod_bio))
-			remove_wait_queue(&nvmeq->sq_full,
-							&nvmeq->sq_cong_wait);
-		if (nvme_submit_bio_queue(nvmeq, ns, bio)) {
-			if (!waitqueue_active(&nvmeq->sq_full))
-				add_wait_queue(&nvmeq->sq_full,
-							&nvmeq->sq_cong_wait);
-			bio_list_add_head(&nvmeq->sq_cong, bio);
-			break;
-		}
-	}
-}
-
-static int nvme_kthread(void *data)
-{
-	struct nvme_dev *dev, *next;
-
-	while (!kthread_should_stop()) {
-		set_current_state(TASK_INTERRUPTIBLE);
-		spin_lock(&dev_list_lock);
-		list_for_each_entry_safe(dev, next, &dev_list, node) {
-			int i;
-			if (readl(&dev->bar->csts) & NVME_CSTS_CFS &&
-							dev->initialized) {
-				if (work_busy(&dev->reset_work))
-					continue;
-				list_del_init(&dev->node);
-				dev_warn(&dev->pci_dev->dev,
-					"Failed status, reset controller\n");
-				PREPARE_WORK(&dev->reset_work,
-							nvme_reset_failed_dev);
-				queue_work(nvme_workq, &dev->reset_work);
-				continue;
-			}
-			rcu_read_lock();
-			for (i = 0; i < dev->queue_count; i++) {
-				struct nvme_queue *nvmeq =
-						rcu_dereference(dev->queues[i]);
-				if (!nvmeq)
-					continue;
-				spin_lock_irq(&nvmeq->q_lock);
-				if (nvmeq->q_suspended)
-					goto unlock;
-				nvme_process_cq(nvmeq);
-				nvme_cancel_ios(nvmeq, true);
-				nvme_resubmit_bios(nvmeq);
-				nvme_resubmit_iods(nvmeq);
- unlock:
-				spin_unlock_irq(&nvmeq->q_lock);
-			}
-			rcu_read_unlock();
-		}
-		spin_unlock(&dev_list_lock);
-		schedule_timeout(round_jiffies_relative(HZ));
-	}
-	return 0;
-}
-
-static void nvme_config_discard(struct nvme_ns *ns)
-{
-	u32 logical_block_size = queue_logical_block_size(ns->queue);
-	ns->queue->limits.discard_zeroes_data = 0;
-	ns->queue->limits.discard_alignment = logical_block_size;
-	ns->queue->limits.discard_granularity = logical_block_size;
-	ns->queue->limits.max_discard_sectors = 0xffffffff;
-	queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, ns->queue);
-}
-
-static struct nvme_ns *nvme_alloc_ns(struct nvme_dev *dev, unsigned nsid,
-			struct nvme_id_ns *id, struct nvme_lba_range_type *rt)
-{
-	struct nvme_ns *ns;
-	struct gendisk *disk;
-	int lbaf;
-
-	if (rt->attributes & NVME_LBART_ATTRIB_HIDE)
-		return NULL;
-
-	ns = kzalloc(sizeof(*ns), GFP_KERNEL);
-	if (!ns)
-		return NULL;
-	ns->queue = blk_alloc_queue(GFP_KERNEL);
-	if (!ns->queue)
-		goto out_free_ns;
-	ns->queue->queue_flags = QUEUE_FLAG_DEFAULT;
-	queue_flag_clear_unlocked(QUEUE_FLAG_STACKABLE, ns->queue);
-	queue_flag_set_unlocked(QUEUE_FLAG_NOMERGES, ns->queue);
-	queue_flag_set_unlocked(QUEUE_FLAG_NONROT, ns->queue);
-	blk_queue_make_request(ns->queue, nvme_make_request);
-	ns->dev = dev;
-	ns->queue->queuedata = ns;
-
-	disk = alloc_disk(0);
-	if (!disk)
-		goto out_free_queue;
-	ns->ns_id = nsid;
-	ns->disk = disk;
-	lbaf = id->flbas & 0xf;
-	ns->lba_shift = id->lbaf[lbaf].ds;
-	ns->ms = le16_to_cpu(id->lbaf[lbaf].ms);
-	blk_queue_logical_block_size(ns->queue, 1 << ns->lba_shift);
-	if (dev->max_hw_sectors)
-		blk_queue_max_hw_sectors(ns->queue, dev->max_hw_sectors);
-
-	disk->major = nvme_major;
-	disk->first_minor = 0;
-	disk->fops = &nvme_fops;
-	disk->private_data = ns;
-	disk->queue = ns->queue;
-	disk->driverfs_dev = &dev->pci_dev->dev;
-	disk->flags = GENHD_FL_EXT_DEVT;
-	sprintf(disk->disk_name, "nvme%dn%d", dev->instance, nsid);
-	set_capacity(disk, le64_to_cpup(&id->nsze) << (ns->lba_shift - 9));
-
-	if (dev->oncs & NVME_CTRL_ONCS_DSM)
-		nvme_config_discard(ns);
-
-	return ns;
-
- out_free_queue:
-	blk_cleanup_queue(ns->queue);
- out_free_ns:
-	kfree(ns);
-	return NULL;
-}
-
-static int nvme_find_closest_node(int node)
-{
-	int n, val, min_val = INT_MAX, best_node = node;
-
-	for_each_online_node(n) {
-		if (n == node)
-			continue;
-		val = node_distance(node, n);
-		if (val < min_val) {
-			min_val = val;
-			best_node = n;
-		}
-	}
-	return best_node;
-}
-
-static void nvme_set_queue_cpus(cpumask_t *qmask, struct nvme_queue *nvmeq,
-								int count)
-{
-	int cpu;
-	for_each_cpu(cpu, qmask) {
-		if (cpumask_weight(nvmeq->cpu_mask) >= count)
-			break;
-		if (!cpumask_test_and_set_cpu(cpu, nvmeq->cpu_mask))
-			*per_cpu_ptr(nvmeq->dev->io_queue, cpu) = nvmeq->qid;
-	}
-}
-
-static void nvme_add_cpus(cpumask_t *mask, const cpumask_t *unassigned_cpus,
-	const cpumask_t *new_mask, struct nvme_queue *nvmeq, int cpus_per_queue)
-{
-	int next_cpu;
-	for_each_cpu(next_cpu, new_mask) {
-		cpumask_or(mask, mask, get_cpu_mask(next_cpu));
-		cpumask_or(mask, mask, topology_thread_cpumask(next_cpu));
-		cpumask_and(mask, mask, unassigned_cpus);
-		nvme_set_queue_cpus(mask, nvmeq, cpus_per_queue);
-	}
-}
-
-static void nvme_create_io_queues(struct nvme_dev *dev)
-{
-	unsigned i, max;
-
-	max = min(dev->max_qid, num_online_cpus());
-	for (i = dev->queue_count; i <= max; i++)
-		if (!nvme_alloc_queue(dev, i, dev->q_depth, i - 1))
-			break;
-
-	max = min(dev->queue_count - 1, num_online_cpus());
-	for (i = dev->online_queues; i <= max; i++)
-		if (nvme_create_queue(raw_nvmeq(dev, i), i))
-			break;
-}
-
-/*
- * If there are fewer queues than online cpus, this will try to optimally
- * assign a queue to multiple cpus by grouping cpus that are "close" together:
- * thread siblings, core, socket, closest node, then whatever else is
- * available.
- */
-static void nvme_assign_io_queues(struct nvme_dev *dev)
-{
-	unsigned cpu, cpus_per_queue, queues, remainder, i;
-	cpumask_var_t unassigned_cpus;
-
-	nvme_create_io_queues(dev);
-
-	queues = min(dev->online_queues - 1, num_online_cpus());
-	if (!queues)
-		return;
-
-	cpus_per_queue = num_online_cpus() / queues;
-	remainder = queues - (num_online_cpus() - queues * cpus_per_queue);
-
-	if (!alloc_cpumask_var(&unassigned_cpus, GFP_KERNEL))
-		return;
-
-	cpumask_copy(unassigned_cpus, cpu_online_mask);
-	cpu = cpumask_first(unassigned_cpus);
-	for (i = 1; i <= queues; i++) {
-		struct nvme_queue *nvmeq = lock_nvmeq(dev, i);
-		cpumask_t mask;
-
-		cpumask_clear(nvmeq->cpu_mask);
-		if (!cpumask_weight(unassigned_cpus)) {
-			unlock_nvmeq(nvmeq);
-			break;
-		}
-
-		mask = *get_cpu_mask(cpu);
-		nvme_set_queue_cpus(&mask, nvmeq, cpus_per_queue);
-		if (cpus_weight(mask) < cpus_per_queue)
-			nvme_add_cpus(&mask, unassigned_cpus,
-				topology_thread_cpumask(cpu),
-				nvmeq, cpus_per_queue);
-		if (cpus_weight(mask) < cpus_per_queue)
-			nvme_add_cpus(&mask, unassigned_cpus,
-				topology_core_cpumask(cpu),
-				nvmeq, cpus_per_queue);
-		if (cpus_weight(mask) < cpus_per_queue)
-			nvme_add_cpus(&mask, unassigned_cpus,
-				cpumask_of_node(cpu_to_node(cpu)),
-				nvmeq, cpus_per_queue);
-		if (cpus_weight(mask) < cpus_per_queue)
-			nvme_add_cpus(&mask, unassigned_cpus,
-				cpumask_of_node(
-					nvme_find_closest_node(
-						cpu_to_node(cpu))),
-				nvmeq, cpus_per_queue);
-		if (cpus_weight(mask) < cpus_per_queue)
-			nvme_add_cpus(&mask, unassigned_cpus,
-				unassigned_cpus,
-				nvmeq, cpus_per_queue);
-
-		WARN(cpumask_weight(nvmeq->cpu_mask) != cpus_per_queue,
-			"nvme%d qid:%d mis-matched queue-to-cpu assignment\n",
-			dev->instance, i);
-
-		irq_set_affinity_hint(dev->entry[nvmeq->cq_vector].vector,
-							nvmeq->cpu_mask);
-		cpumask_andnot(unassigned_cpus, unassigned_cpus,
-						nvmeq->cpu_mask);
-		cpu = cpumask_next(cpu, unassigned_cpus);
-		if (remainder && !--remainder)
-			cpus_per_queue++;
-		unlock_nvmeq(nvmeq);
-	}
-	WARN(cpumask_weight(unassigned_cpus), "nvme%d unassigned online cpus\n",
-								dev->instance);
-	i = 0;
-	cpumask_andnot(unassigned_cpus, cpu_possible_mask, cpu_online_mask);
-	for_each_cpu(cpu, unassigned_cpus)
-		*per_cpu_ptr(dev->io_queue, cpu) = (i++ % queues) + 1;
-	free_cpumask_var(unassigned_cpus);
-}
-
-static int set_queue_count(struct nvme_dev *dev, int count)
-{
-	int status;
-	u32 result;
-	u32 q_count = (count - 1) | ((count - 1) << 16);
-
-	status = nvme_set_features(dev, NVME_FEAT_NUM_QUEUES, q_count, 0,
-								&result);
-	if (status)
-		return status < 0 ? -EIO : -EBUSY;
-	return min(result & 0xffff, result >> 16) + 1;
-}
-
-static size_t db_bar_size(struct nvme_dev *dev, unsigned nr_io_queues)
-{
-	return 4096 + ((nr_io_queues + 1) * 8 * dev->db_stride);
-}
-
-static int nvme_setup_io_queues(struct nvme_dev *dev)
-{
-	struct nvme_queue *adminq = raw_nvmeq(dev, 0);
-	struct pci_dev *pdev = dev->pci_dev;
-	int result, i, vecs, nr_io_queues, size;
-
-	nr_io_queues = num_possible_cpus();
-	result = set_queue_count(dev, nr_io_queues);
-	if (result < 0)
-		return result;
-	if (result < nr_io_queues)
-		nr_io_queues = result;
-
-	size = db_bar_size(dev, nr_io_queues);
-	if (size > 8192) {
-		iounmap(dev->bar);
-		do {
-			dev->bar = ioremap(pci_resource_start(pdev, 0), size);
-			if (dev->bar)
-				break;
-			if (!--nr_io_queues)
-				return -ENOMEM;
-			size = db_bar_size(dev, nr_io_queues);
-		} while (1);
-		dev->dbs = ((void __iomem *)dev->bar) + 4096;
-		adminq->q_db = dev->dbs;
-	}
-
-	/* Deregister the admin queue's interrupt */
-	free_irq(dev->entry[0].vector, adminq);
-
-	vecs = nr_io_queues;
-	for (i = 0; i < vecs; i++)
-		dev->entry[i].entry = i;
-	for (;;) {
-		result = pci_enable_msix(pdev, dev->entry, vecs);
-		if (result <= 0)
-			break;
-		vecs = result;
-	}
-
-	if (result < 0) {
-		vecs = nr_io_queues;
-		if (vecs > 32)
-			vecs = 32;
-		for (;;) {
-			result = pci_enable_msi_block(pdev, vecs);
-			if (result == 0) {
-				for (i = 0; i < vecs; i++)
-					dev->entry[i].vector = i + pdev->irq;
-				break;
-			} else if (result < 0) {
-				vecs = 1;
-				break;
-			}
-			vecs = result;
-		}
-	}
-
-	/*
-	 * Should investigate if there's a performance win from allocating
-	 * more queues than interrupt vectors; it might allow the submission
-	 * path to scale better, even if the receive path is limited by the
-	 * number of interrupts.
-	 */
-	nr_io_queues = vecs;
-	dev->max_qid = nr_io_queues;
-
-	result = queue_request_irq(dev, adminq, adminq->irqname);
-	if (result) {
-		adminq->q_suspended = 1;
-		goto free_queues;
-	}
-
-	/* Free previously allocated queues that are no longer usable */
-	nvme_free_queues(dev, nr_io_queues + 1);
-	nvme_assign_io_queues(dev);
-
-	return 0;
-
- free_queues:
-	nvme_free_queues(dev, 1);
-	return result;
-}
-
-/*
- * Return: error value if an error occurred setting up the queues or calling
- * Identify Device.  0 if these succeeded, even if adding some of the
- * namespaces failed.  At the moment, these failures are silent.  TBD which
- * failures should be reported.
- */
-static int __devinit nvme_dev_add(struct nvme_dev *dev)
-{
-	struct pci_dev *pdev = dev->pci_dev;
-	int res;
-	unsigned nn, i;
-	struct nvme_ns *ns;
-	struct nvme_id_ctrl *ctrl;
-	struct nvme_id_ns *id_ns;
-	void *mem;
-	dma_addr_t dma_addr;
-	int shift = NVME_CAP_MPSMIN(readq(&dev->bar->cap)) + 12;
-
-	mem = dma_alloc_coherent(&pdev->dev, 8192, &dma_addr, GFP_KERNEL);
-	if (!mem)
-		return -ENOMEM;
-
-	res = nvme_identify(dev, 0, 1, dma_addr);
-	if (res) {
-		res = -EIO;
-		goto out;
-	}
-
-	ctrl = mem;
-	nn = le32_to_cpup(&ctrl->nn);
-	dev->oncs = le16_to_cpup(&ctrl->oncs);
-	dev->abort_limit = ctrl->acl + 1;
-	memcpy(dev->serial, ctrl->sn, sizeof(ctrl->sn));
-	memcpy(dev->model, ctrl->mn, sizeof(ctrl->mn));
-	memcpy(dev->firmware_rev, ctrl->fr, sizeof(ctrl->fr));
-	if (ctrl->mdts)
-		dev->max_hw_sectors = 1 << (ctrl->mdts + shift - 9);
-	if ((pdev->vendor == PCI_VENDOR_ID_INTEL) &&
-			(pdev->device == 0x0953) && ctrl->vs[3])
-		dev->stripe_size = 1 << (ctrl->vs[3] + shift);
-
-	id_ns = mem;
-	for (i = 1; i <= nn; i++) {
-		res = nvme_identify(dev, i, 0, dma_addr);
-		if (res)
-			continue;
-
-		if (id_ns->ncap == 0)
-			continue;
-
-		res = nvme_get_features(dev, NVME_FEAT_LBA_RANGE, i,
-							dma_addr + 4096, NULL);
-		if (res)
-			memset(mem + 4096, 0, 4096);
-
-		ns = nvme_alloc_ns(dev, i, mem, mem + 4096);
-		if (ns)
-			list_add_tail(&ns->list, &dev->namespaces);
-	}
-	list_for_each_entry(ns, &dev->namespaces, list)
-		add_disk(ns->disk);
-	res = 0;
-
- out:
-	dma_free_coherent(&dev->pci_dev->dev, 8192, mem, dma_addr);
-	return res;
-}
-
-static int nvme_dev_map(struct nvme_dev *dev)
-{
-	u64 cap;
-	int bars, result = -ENOMEM;
-	struct pci_dev *pdev = dev->pci_dev;
-
-	if (pci_enable_device_mem(pdev))
-		return result;
-
-	dev->entry[0].vector = pdev->irq;
-	pci_set_master(pdev);
-	bars = pci_select_bars(pdev, IORESOURCE_MEM);
-	if (pci_request_selected_regions(pdev, bars, "nvme"))
-		goto disable_pci;
-
-	if (!dma_set_mask(&pdev->dev, DMA_BIT_MASK(64)))
-		dma_set_coherent_mask(&pdev->dev, DMA_BIT_MASK(64));
-	else if (!dma_set_mask(&pdev->dev, DMA_BIT_MASK(32)))
-		dma_set_coherent_mask(&pdev->dev, DMA_BIT_MASK(32));
-	else
-		goto disable_pci;
-
-	dev->bar = ioremap(pci_resource_start(pdev, 0), 8192);
-	if (!dev->bar)
-		goto disable;
-	if (readl(&dev->bar->csts) == -1) {
-		result = -ENODEV;
-		goto unmap;
-	}
-	cap = readq(&dev->bar->cap);
-	dev->q_depth = min_t(int, NVME_CAP_MQES(cap) + 1, NVME_Q_DEPTH);
-	dev->db_stride = 1 << NVME_CAP_STRIDE(cap);
-	dev->dbs = ((void __iomem *)dev->bar) + 4096;
-
-	return 0;
-
- unmap:
-	iounmap(dev->bar);
-	dev->bar = NULL;
- disable:
-	pci_release_regions(pdev);
- disable_pci:
-	pci_disable_device(pdev);
-	return result;
-}
-
-static void nvme_dev_unmap(struct nvme_dev *dev)
-{
-	if (dev->pci_dev->msi_enabled)
-		pci_disable_msi(dev->pci_dev);
-	else if (dev->pci_dev->msix_enabled)
-		pci_disable_msix(dev->pci_dev);
-
-	if (dev->bar) {
-		iounmap(dev->bar);
-		dev->bar = NULL;
-		pci_release_regions(dev->pci_dev);
-	}
-
-	if (pci_is_enabled(dev->pci_dev))
-		pci_disable_device(dev->pci_dev);
-}
-
-struct nvme_delq_ctx {
-	struct task_struct *waiter;
-	struct kthread_worker *worker;
-	atomic_t refcount;
-};
-
-static void nvme_wait_dq(struct nvme_delq_ctx *dq, struct nvme_dev *dev)
-{
-	dq->waiter = current;
-	mb();
-
-	for (;;) {
-		set_current_state(TASK_KILLABLE);
-		if (!atomic_read(&dq->refcount))
-			break;
-		if (!schedule_timeout(ADMIN_TIMEOUT) ||
-					fatal_signal_pending(current)) {
-			set_current_state(TASK_RUNNING);
-
-			nvme_disable_ctrl(dev, readq(&dev->bar->cap));
-			nvme_disable_queue(dev, 0);
-
-			send_sig(SIGKILL, dq->worker->task, 1);
-			flush_kthread_worker(dq->worker);
-			return;
-		}
-	}
-	set_current_state(TASK_RUNNING);
-}
-
-static void nvme_put_dq(struct nvme_delq_ctx *dq)
-{
-	atomic_dec(&dq->refcount);
-	if (dq->waiter)
-		wake_up_process(dq->waiter);
-}
-
-static struct nvme_delq_ctx *nvme_get_dq(struct nvme_delq_ctx *dq)
-{
-	atomic_inc(&dq->refcount);
-	return dq;
-}
-
-static void nvme_del_queue_end(struct nvme_queue *nvmeq)
-{
-	struct nvme_delq_ctx *dq = nvmeq->cmdinfo.ctx;
-
-	nvme_clear_queue(nvmeq);
-	nvme_put_dq(dq);
-}
-
-static int adapter_async_del_queue(struct nvme_queue *nvmeq, u8 opcode,
-						kthread_work_func_t fn)
-{
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.delete_queue.opcode = opcode;
-	c.delete_queue.qid = cpu_to_le16(nvmeq->qid);
-
-	init_kthread_work(&nvmeq->cmdinfo.work, fn);
-	return nvme_submit_admin_cmd_async(nvmeq->dev, &c, &nvmeq->cmdinfo);
-}
-
-static void nvme_del_cq_work_handler(struct kthread_work *work)
-{
-	struct nvme_queue *nvmeq = container_of(work, struct nvme_queue,
-							cmdinfo.work);
-	nvme_del_queue_end(nvmeq);
-}
-
-static int nvme_delete_cq(struct nvme_queue *nvmeq)
-{
-	return adapter_async_del_queue(nvmeq, nvme_admin_delete_cq,
-						nvme_del_cq_work_handler);
-}
-
-static void nvme_del_sq_work_handler(struct kthread_work *work)
-{
-	struct nvme_queue *nvmeq = container_of(work, struct nvme_queue,
-							cmdinfo.work);
-	int status = nvmeq->cmdinfo.status;
-
-	if (!status)
-		status = nvme_delete_cq(nvmeq);
-	if (status)
-		nvme_del_queue_end(nvmeq);
-}
-
-static int nvme_delete_sq(struct nvme_queue *nvmeq)
-{
-	return adapter_async_del_queue(nvmeq, nvme_admin_delete_sq,
-						nvme_del_sq_work_handler);
-}
-
-static void nvme_del_queue_start(struct kthread_work *work)
-{
-	struct nvme_queue *nvmeq = container_of(work, struct nvme_queue,
-							cmdinfo.work);
-	allow_signal(SIGKILL);
-	if (nvme_delete_sq(nvmeq))
-		nvme_del_queue_end(nvmeq);
-}
-
-static void nvme_disable_io_queues(struct nvme_dev *dev)
-{
-	int i;
-	DEFINE_KTHREAD_WORKER_ONSTACK(worker);
-	struct nvme_delq_ctx dq;
-	struct task_struct *kworker_task = kthread_run(kthread_worker_fn,
-					&worker, "nvme%d", dev->instance);
-
-	if (IS_ERR(kworker_task)) {
-		dev_err(&dev->pci_dev->dev,
-			"Failed to create queue del task\n");
-		for (i = dev->queue_count - 1; i > 0; i--)
-			nvme_disable_queue(dev, i);
-		return;
-	}
-
-	dq.waiter = NULL;
-	atomic_set(&dq.refcount, 0);
-	dq.worker = &worker;
-	for (i = dev->queue_count - 1; i > 0; i--) {
-		struct nvme_queue *nvmeq = raw_nvmeq(dev, i);
-
-		if (nvme_suspend_queue(nvmeq))
-			continue;
-		nvmeq->cmdinfo.ctx = nvme_get_dq(&dq);
-		nvmeq->cmdinfo.worker = dq.worker;
-		init_kthread_work(&nvmeq->cmdinfo.work, nvme_del_queue_start);
-		queue_kthread_work(dq.worker, &nvmeq->cmdinfo.work);
-	}
-	nvme_wait_dq(&dq, dev);
-	kthread_stop(kworker_task);
-}
-
-/*
-* Remove the node from the device list and check
-* for whether or not we need to stop the nvme_thread.
-*/
-static void nvme_dev_list_remove(struct nvme_dev *dev)
-{
-	struct task_struct *tmp = NULL;
-
-	spin_lock(&dev_list_lock);
-	list_del_init(&dev->node);
-	if (list_empty(&dev_list) && !IS_ERR_OR_NULL(nvme_thread)) {
-		tmp = nvme_thread;
-		nvme_thread = NULL;
-	}
-	spin_unlock(&dev_list_lock);
-
-	if (tmp)
-		kthread_stop(tmp);
-}
-
-static void nvme_dev_shutdown(struct nvme_dev *dev)
-{
-	int i;
-
-	dev->initialized = 0;
-
-	nvme_dev_list_remove(dev);
-
-	if (!dev->bar || (dev->bar && readl(&dev->bar->csts) == -1)) {
-		for (i = dev->queue_count - 1; i >= 0; i--) {
-			struct nvme_queue *nvmeq = raw_nvmeq(dev, i);
-			nvme_suspend_queue(nvmeq);
-			nvme_clear_queue(nvmeq);
-		}
-	} else {
-		nvme_disable_io_queues(dev);
-		nvme_shutdown_ctrl(dev);
-		nvme_disable_queue(dev, 0);
-	}
-	nvme_dev_unmap(dev);
-}
-
-static void nvme_dev_remove(struct nvme_dev *dev)
-{
-	struct nvme_ns *ns;
-
-	list_for_each_entry(ns, &dev->namespaces, list) {
-		if (ns->disk->flags & GENHD_FL_UP)
-			del_gendisk(ns->disk);
-		if (!blk_queue_dead(ns->queue))
-			blk_cleanup_queue(ns->queue);
-	}
-}
-
-static int nvme_setup_prp_pools(struct nvme_dev *dev)
-{
-	struct device *dmadev = &dev->pci_dev->dev;
-	dev->prp_page_pool = dma_pool_create("prp list page", dmadev,
-						PAGE_SIZE, PAGE_SIZE, 0);
-	if (!dev->prp_page_pool)
-		return -ENOMEM;
-
-	/* Optimisation for I/Os between 4k and 128k */
-	dev->prp_small_pool = dma_pool_create("prp list 256", dmadev,
-						256, 256, 0);
-	if (!dev->prp_small_pool) {
-		dma_pool_destroy(dev->prp_page_pool);
-		return -ENOMEM;
-	}
-	return 0;
-}
-
-static void nvme_release_prp_pools(struct nvme_dev *dev)
-{
-	dma_pool_destroy(dev->prp_page_pool);
-	dma_pool_destroy(dev->prp_small_pool);
-}
-
-static DEFINE_IDA(nvme_instance_ida);
-
-static int nvme_set_instance(struct nvme_dev *dev)
-{
-	int instance, error;
-
-	do {
-		if (!ida_pre_get(&nvme_instance_ida, GFP_KERNEL))
-			return -ENODEV;
-
-		spin_lock(&dev_list_lock);
-		error = ida_get_new(&nvme_instance_ida, &instance);
-		spin_unlock(&dev_list_lock);
-	} while (error == -EAGAIN);
-
-	if (error)
-		return -ENODEV;
-
-	dev->instance = instance;
-	return 0;
-}
-
-static void nvme_release_instance(struct nvme_dev *dev)
-{
-	spin_lock(&dev_list_lock);
-	ida_remove(&nvme_instance_ida, dev->instance);
-	spin_unlock(&dev_list_lock);
-}
-
-static void nvme_free_namespaces(struct nvme_dev *dev)
-{
-	struct nvme_ns *ns, *next;
-
-	list_for_each_entry_safe(ns, next, &dev->namespaces, list) {
-		list_del(&ns->list);
-		put_disk(ns->disk);
-		kfree(ns);
-	}
-}
-
-static void nvme_free_dev(struct kref *kref)
-{
-	struct nvme_dev *dev = container_of(kref, struct nvme_dev, kref);
-
-	nvme_free_namespaces(dev);
-	free_percpu(dev->io_queue);
-	kfree(dev->queues);
-	kfree(dev->entry);
-	kfree(dev);
-}
-
-static int nvme_dev_open(struct inode *inode, struct file *f)
-{
-	struct nvme_dev *dev = container_of(f->private_data, struct nvme_dev,
-								miscdev);
-	kref_get(&dev->kref);
-	f->private_data = dev;
-	return 0;
-}
-
-static int nvme_dev_release(struct inode *inode, struct file *f)
-{
-	struct nvme_dev *dev = f->private_data;
-	kref_put(&dev->kref, nvme_free_dev);
-	return 0;
-}
-
-static long nvme_dev_ioctl(struct file *f, unsigned int cmd, unsigned long arg)
-{
-	struct nvme_dev *dev = f->private_data;
-	switch (cmd) {
-	case NVME_IOCTL_ADMIN_CMD:
-		return nvme_user_admin_cmd(dev, (void __user *)arg);
-	default:
-		return -ENOTTY;
-	}
-}
-
-static const struct file_operations nvme_dev_fops = {
-	.owner		= THIS_MODULE,
-	.open		= nvme_dev_open,
-	.release	= nvme_dev_release,
-	.unlocked_ioctl	= nvme_dev_ioctl,
-	.compat_ioctl	= nvme_dev_ioctl,
-};
-
-static int nvme_dev_start(struct nvme_dev *dev)
-{
-	int result;
-	bool start_thread = false;
-
-	result = nvme_dev_map(dev);
-	if (result)
-		return result;
-
-	result = nvme_configure_admin_queue(dev);
-	if (result)
-		goto unmap;
-
-	spin_lock(&dev_list_lock);
-	if (list_empty(&dev_list) && IS_ERR_OR_NULL(nvme_thread)) {
-		start_thread = true;
-		nvme_thread = NULL;
-	}
-	list_add(&dev->node, &dev_list);
-	spin_unlock(&dev_list_lock);
-
-	if (start_thread) {
-		nvme_thread = kthread_run(nvme_kthread, NULL, "nvme");
-		wake_up(&nvme_kthread_wait);
-	} else
-		wait_event_killable(nvme_kthread_wait, nvme_thread);
-
-	if (IS_ERR_OR_NULL(nvme_thread)) {
-		result = nvme_thread ? PTR_ERR(nvme_thread) : -EINTR;
-		goto disable;
-	}
-
-	result = nvme_setup_io_queues(dev);
-	if (result && result != -EBUSY)
-		goto disable;
-
-	return result;
-
- disable:
-	nvme_disable_queue(dev, 0);
-	nvme_dev_list_remove(dev);
- unmap:
-	nvme_dev_unmap(dev);
-	return result;
-}
-
-static int nvme_remove_dead_ctrl(void *arg)
-{
-	struct nvme_dev *dev = (struct nvme_dev *)arg;
-	struct pci_dev *pdev = dev->pci_dev;
-
-	if (pci_get_drvdata(pdev)) {
-		pci_stop_bus_device(pdev);
-		pci_remove_bus_device(pdev);
-	}
-	kref_put(&dev->kref, nvme_free_dev);
-	return 0;
-}
-
-static void nvme_remove_disks(struct work_struct *ws)
-{
-	struct nvme_dev *dev = container_of(ws, struct nvme_dev, reset_work);
-
-	nvme_dev_remove(dev);
-	nvme_free_queues(dev, 1);
-}
-
-static int nvme_dev_resume(struct nvme_dev *dev)
-{
-	int ret;
-
-	ret = nvme_dev_start(dev);
-	if (ret && ret != -EBUSY)
-		return ret;
-	if (ret == -EBUSY) {
-		spin_lock(&dev_list_lock);
-		PREPARE_WORK(&dev->reset_work, nvme_remove_disks);
-		queue_work(nvme_workq, &dev->reset_work);
-		spin_unlock(&dev_list_lock);
-	}
-	dev->initialized = 1;
-	return 0;
-}
-
-static void nvme_dev_reset(struct nvme_dev *dev)
-{
-	nvme_dev_shutdown(dev);
-	if (nvme_dev_resume(dev)) {
-		dev_err(&dev->pci_dev->dev, "Device failed to resume\n");
-		kref_get(&dev->kref);
-		if (IS_ERR(kthread_run(nvme_remove_dead_ctrl, dev, "nvme%d",
-							dev->instance))) {
-			dev_err(&dev->pci_dev->dev,
-				"Failed to start controller remove task\n");
-			kref_put(&dev->kref, nvme_free_dev);
-		}
-	}
-}
-
-static void nvme_reset_failed_dev(struct work_struct *ws)
-{
-	struct nvme_dev *dev = container_of(ws, struct nvme_dev, reset_work);
-	nvme_dev_reset(dev);
-}
-
-static int __devinit nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
-{
-	int result = -ENOMEM;
-	struct nvme_dev *dev;
-
-	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
-	if (!dev)
-		return -ENOMEM;
-	dev->entry = kcalloc(num_possible_cpus(), sizeof(*dev->entry),
-								GFP_KERNEL);
-	if (!dev->entry)
-		goto free;
-	dev->queues = kcalloc(num_possible_cpus() + 1, sizeof(void *),
-								GFP_KERNEL);
-	if (!dev->queues)
-		goto free;
-	dev->io_queue = alloc_percpu(unsigned short);
-	if (!dev->io_queue)
-		goto free;
-
-	INIT_LIST_HEAD(&dev->namespaces);
-	INIT_WORK(&dev->reset_work, nvme_reset_failed_dev);
-	dev->pci_dev = pdev;
-	pci_set_drvdata(pdev, dev);
-	result = nvme_set_instance(dev);
-	if (result)
-		goto free;
-
-	result = nvme_setup_prp_pools(dev);
-	if (result)
-		goto release;
-
-	kref_init(&dev->kref);
-	result = nvme_dev_start(dev);
-	if (result) {
-		if (result == -EBUSY)
-			goto create_cdev;
-		goto release_pools;
-	}
-
-	result = nvme_dev_add(dev);
-	if (result)
-		goto shutdown;
-
- create_cdev:
-	scnprintf(dev->name, sizeof(dev->name), "nvme%d", dev->instance);
-	dev->miscdev.minor = MISC_DYNAMIC_MINOR;
-	dev->miscdev.parent = &pdev->dev;
-	dev->miscdev.name = dev->name;
-	dev->miscdev.fops = &nvme_dev_fops;
-	result = misc_register(&dev->miscdev);
-	if (result)
-		goto remove;
-
-	dev->initialized = 1;
-	return 0;
-
- remove:
-	nvme_dev_remove(dev);
-	nvme_free_namespaces(dev);
- shutdown:
-	nvme_dev_shutdown(dev);
- release_pools:
-	nvme_free_queues(dev, 0);
-	nvme_release_prp_pools(dev);
- release:
-	nvme_release_instance(dev);
- free:
-	free_percpu(dev->io_queue);
-	kfree(dev->queues);
-	kfree(dev->entry);
-	kfree(dev);
-	return result;
-}
-
-static void nvme_shutdown(struct pci_dev *pdev)
-{
-	struct nvme_dev *dev = pci_get_drvdata(pdev);
-	nvme_dev_shutdown(dev);
-}
-
-static void __devexit nvme_remove(struct pci_dev *pdev)
-{
-	struct nvme_dev *dev = pci_get_drvdata(pdev);
-
-	spin_lock(&dev_list_lock);
-	list_del_init(&dev->node);
-	spin_unlock(&dev_list_lock);
-
-	pci_set_drvdata(pdev, NULL);
-	flush_work(&dev->reset_work);
-	misc_deregister(&dev->miscdev);
-	nvme_dev_remove(dev);
-	nvme_dev_shutdown(dev);
-	nvme_free_queues(dev, 0);
-	rcu_barrier();
-	nvme_release_instance(dev);
-	nvme_release_prp_pools(dev);
-	kref_put(&dev->kref, nvme_free_dev);
-}
-
-/* These functions are yet to be implemented */
-#define nvme_error_detected NULL
-#define nvme_dump_registers NULL
-#define nvme_link_reset NULL
-#define nvme_slot_reset NULL
-#define nvme_error_resume NULL
-
-#ifdef CONFIG_PM_SLEEP
-static int nvme_suspend(struct device *dev)
-{
-	struct pci_dev *pdev = to_pci_dev(dev);
-	struct nvme_dev *ndev = pci_get_drvdata(pdev);
-
-	nvme_dev_shutdown(ndev);
-	return 0;
-}
-
-static int nvme_resume(struct device *dev)
-{
-	struct pci_dev *pdev = to_pci_dev(dev);
-	struct nvme_dev *ndev = pci_get_drvdata(pdev);
-
-	if (nvme_dev_resume(ndev) && !work_busy(&ndev->reset_work)) {
-		PREPARE_WORK(&ndev->reset_work, nvme_reset_failed_dev);
-		queue_work(nvme_workq, &ndev->reset_work);
-	}
-	return 0;
-}
-#endif
-
-static SIMPLE_DEV_PM_OPS(nvme_dev_pm_ops, nvme_suspend, nvme_resume);
-
-static struct pci_error_handlers nvme_err_handler = {
-	.error_detected	= nvme_error_detected,
-	.mmio_enabled	= nvme_dump_registers,
-	.link_reset	= nvme_link_reset,
-	.slot_reset	= nvme_slot_reset,
-	.resume		= nvme_error_resume,
-};
-
-/* Move to pci_ids.h later */
-#define PCI_CLASS_STORAGE_EXPRESS	0x010802
-
-static const struct pci_device_id nvme_id_table[] = {
-	{ PCI_DEVICE_CLASS(PCI_CLASS_STORAGE_EXPRESS, 0xffffff) },
-	{ 0, }
-};
-MODULE_DEVICE_TABLE(pci, nvme_id_table);
-
-static struct pci_driver nvme_driver = {
-	.name		= "nvme",
-	.id_table	= nvme_id_table,
-	.probe		= nvme_probe,
-	.remove		= __devexit_p(nvme_remove),
-	.shutdown	= nvme_shutdown,
-	.driver		= {
-		.pm	= &nvme_dev_pm_ops,
-	},
-	.err_handler	= &nvme_err_handler,
-};
-
-static int __init nvme_init(void)
-{
-	int result;
-
-	init_waitqueue_head(&nvme_kthread_wait);
-
-	nvme_workq = create_singlethread_workqueue("nvme");
-	if (!nvme_workq)
-		return -ENOMEM;
-
-	result = register_blkdev(nvme_major, "nvme");
-	if (result < 0)
-		goto kill_workq;
-	else if (result > 0)
-		nvme_major = result;
-
-	result = pci_register_driver(&nvme_driver);
-	if (result)
-		goto unregister_blkdev;
-#if 1
-	result = aio_service_init();
-	if (result)
-		goto unregister_pcidev;
-#endif
-	return 0;
-
-#if 1
-unregister_pcidev:
-	pci_unregister_driver(&nvme_driver);
-#endif
-unregister_blkdev:
-	unregister_blkdev(nvme_major, "nvme");
- kill_workq:
-	destroy_workqueue(nvme_workq);
-	return result;
-}
-
-static void __exit nvme_exit(void)
-{
-#if 1
-	aio_service_exit();
-#endif
-	pci_unregister_driver(&nvme_driver);
-	unregister_blkdev(nvme_major, "nvme");
-	destroy_workqueue(nvme_workq);
-	BUG_ON(nvme_thread && !IS_ERR(nvme_thread));
-}
-
-MODULE_AUTHOR("Matthew Wilcox <willy@linux.intel.com>");
-MODULE_LICENSE("GPL");
-MODULE_VERSION("0.9");
-module_init(nvme_init);
-module_exit(nvme_exit);
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/nvme-scsi.c b/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/nvme-scsi.c
deleted file mode 100644
index dc2946e..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/nvme-scsi.c
+++ /dev/null
@@ -1,3173 +0,0 @@
-/*
- * NVM Express device driver
- * Copyright (c) 2011, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- *
- * You should have received a copy of the GNU General Public License along with
- * this program; if not, write to the Free Software Foundation, Inc.,
- * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
- */
-
-/*
- * Refer to the SCSI-NVMe Translation spec for details on how
- * each command is translated.
- */
-#if 1
-#include "nvme.h"
-#else
-#include <linux/nvme.h>
-#endif
-#include <linux/bio.h>
-#include <linux/bitops.h>
-#include <linux/blkdev.h>
-#include <linux/compat.h>
-#include <linux/delay.h>
-#include <linux/errno.h>
-#include <linux/fs.h>
-#include <linux/genhd.h>
-#include <linux/idr.h>
-#include <linux/init.h>
-#include <linux/interrupt.h>
-#include <linux/io.h>
-#include <linux/kdev_t.h>
-#include <linux/kthread.h>
-#include <linux/kernel.h>
-#include <linux/mm.h>
-#include <linux/module.h>
-#include <linux/moduleparam.h>
-#include <linux/pci.h>
-#include <linux/poison.h>
-#include <linux/sched.h>
-#include <linux/slab.h>
-#include <linux/types.h>
-#include <scsi/sg.h>
-#include <scsi/scsi.h>
-
-
-static int sg_version_num = 30534;	/* 2 digits for each component */
-
-#define SNTI_TRANSLATION_SUCCESS			0
-#define SNTI_INTERNAL_ERROR				1
-
-/* VPD Page Codes */
-#define VPD_SUPPORTED_PAGES				0x00
-#define VPD_SERIAL_NUMBER				0x80
-#define VPD_DEVICE_IDENTIFIERS				0x83
-#define VPD_EXTENDED_INQUIRY				0x86
-#define VPD_BLOCK_DEV_CHARACTERISTICS			0xB1
-
-/* CDB offsets */
-#define REPORT_LUNS_CDB_ALLOC_LENGTH_OFFSET		6
-#define REPORT_LUNS_SR_OFFSET				2
-#define READ_CAP_16_CDB_ALLOC_LENGTH_OFFSET		10
-#define REQUEST_SENSE_CDB_ALLOC_LENGTH_OFFSET		4
-#define REQUEST_SENSE_DESC_OFFSET			1
-#define REQUEST_SENSE_DESC_MASK				0x01
-#define DESCRIPTOR_FORMAT_SENSE_DATA_TYPE		1
-#define INQUIRY_EVPD_BYTE_OFFSET			1
-#define INQUIRY_PAGE_CODE_BYTE_OFFSET			2
-#define INQUIRY_EVPD_BIT_MASK				1
-#define INQUIRY_CDB_ALLOCATION_LENGTH_OFFSET		3
-#define START_STOP_UNIT_CDB_IMMED_OFFSET		1
-#define START_STOP_UNIT_CDB_IMMED_MASK			0x1
-#define START_STOP_UNIT_CDB_POWER_COND_MOD_OFFSET	3
-#define START_STOP_UNIT_CDB_POWER_COND_MOD_MASK		0xF
-#define START_STOP_UNIT_CDB_POWER_COND_OFFSET		4
-#define START_STOP_UNIT_CDB_POWER_COND_MASK		0xF0
-#define START_STOP_UNIT_CDB_NO_FLUSH_OFFSET		4
-#define START_STOP_UNIT_CDB_NO_FLUSH_MASK		0x4
-#define START_STOP_UNIT_CDB_START_OFFSET		4
-#define START_STOP_UNIT_CDB_START_MASK			0x1
-#define WRITE_BUFFER_CDB_MODE_OFFSET			1
-#define WRITE_BUFFER_CDB_MODE_MASK			0x1F
-#define WRITE_BUFFER_CDB_BUFFER_ID_OFFSET		2
-#define WRITE_BUFFER_CDB_BUFFER_OFFSET_OFFSET		3
-#define WRITE_BUFFER_CDB_PARM_LIST_LENGTH_OFFSET	6
-#define FORMAT_UNIT_CDB_FORMAT_PROT_INFO_OFFSET		1
-#define FORMAT_UNIT_CDB_FORMAT_PROT_INFO_MASK		0xC0
-#define FORMAT_UNIT_CDB_FORMAT_PROT_INFO_SHIFT		6
-#define FORMAT_UNIT_CDB_LONG_LIST_OFFSET		1
-#define FORMAT_UNIT_CDB_LONG_LIST_MASK			0x20
-#define FORMAT_UNIT_CDB_FORMAT_DATA_OFFSET		1
-#define FORMAT_UNIT_CDB_FORMAT_DATA_MASK		0x10
-#define FORMAT_UNIT_SHORT_PARM_LIST_LEN			4
-#define FORMAT_UNIT_LONG_PARM_LIST_LEN			8
-#define FORMAT_UNIT_PROT_INT_OFFSET			3
-#define FORMAT_UNIT_PROT_FIELD_USAGE_OFFSET		0
-#define FORMAT_UNIT_PROT_FIELD_USAGE_MASK		0x07
-#define UNMAP_CDB_PARAM_LIST_LENGTH_OFFSET		7
-
-/* Misc. defines */
-#define NIBBLE_SHIFT					4
-#define FIXED_SENSE_DATA				0x70
-#define DESC_FORMAT_SENSE_DATA				0x72
-#define FIXED_SENSE_DATA_ADD_LENGTH			10
-#define LUN_ENTRY_SIZE					8
-#define LUN_DATA_HEADER_SIZE				8
-#define ALL_LUNS_RETURNED				0x02
-#define ALL_WELL_KNOWN_LUNS_RETURNED			0x01
-#define RESTRICTED_LUNS_RETURNED			0x00
-#define NVME_POWER_STATE_START_VALID			0x00
-#define NVME_POWER_STATE_ACTIVE				0x01
-#define NVME_POWER_STATE_IDLE				0x02
-#define NVME_POWER_STATE_STANDBY			0x03
-#define NVME_POWER_STATE_LU_CONTROL			0x07
-#define POWER_STATE_0					0
-#define POWER_STATE_1					1
-#define POWER_STATE_2					2
-#define POWER_STATE_3					3
-#define DOWNLOAD_SAVE_ACTIVATE				0x05
-#define DOWNLOAD_SAVE_DEFER_ACTIVATE			0x0E
-#define ACTIVATE_DEFERRED_MICROCODE			0x0F
-#define FORMAT_UNIT_IMMED_MASK				0x2
-#define FORMAT_UNIT_IMMED_OFFSET			1
-#define KELVIN_TEMP_FACTOR				273
-#define FIXED_FMT_SENSE_DATA_SIZE			18
-#define DESC_FMT_SENSE_DATA_SIZE			8
-
-/* SCSI/NVMe defines and bit masks */
-#define INQ_STANDARD_INQUIRY_PAGE			0x00
-#define INQ_SUPPORTED_VPD_PAGES_PAGE			0x00
-#define INQ_UNIT_SERIAL_NUMBER_PAGE			0x80
-#define INQ_DEVICE_IDENTIFICATION_PAGE			0x83
-#define INQ_EXTENDED_INQUIRY_DATA_PAGE			0x86
-#define INQ_BDEV_CHARACTERISTICS_PAGE			0xB1
-#define INQ_SERIAL_NUMBER_LENGTH			0x14
-#define INQ_NUM_SUPPORTED_VPD_PAGES			5
-#define VERSION_SPC_4					0x06
-#define ACA_UNSUPPORTED					0
-#define STANDARD_INQUIRY_LENGTH				36
-#define ADDITIONAL_STD_INQ_LENGTH			31
-#define EXTENDED_INQUIRY_DATA_PAGE_LENGTH		0x3C
-#define RESERVED_FIELD					0
-
-/* SCSI READ/WRITE Defines */
-#define IO_CDB_WP_MASK					0xE0
-#define IO_CDB_WP_SHIFT					5
-#define IO_CDB_FUA_MASK					0x8
-#define IO_6_CDB_LBA_OFFSET				0
-#define IO_6_CDB_LBA_MASK				0x001FFFFF
-#define IO_6_CDB_TX_LEN_OFFSET				4
-#define IO_6_DEFAULT_TX_LEN				256
-#define IO_10_CDB_LBA_OFFSET				2
-#define IO_10_CDB_TX_LEN_OFFSET				7
-#define IO_10_CDB_WP_OFFSET				1
-#define IO_10_CDB_FUA_OFFSET				1
-#define IO_12_CDB_LBA_OFFSET				2
-#define IO_12_CDB_TX_LEN_OFFSET				6
-#define IO_12_CDB_WP_OFFSET				1
-#define IO_12_CDB_FUA_OFFSET				1
-#define IO_16_CDB_FUA_OFFSET				1
-#define IO_16_CDB_WP_OFFSET				1
-#define IO_16_CDB_LBA_OFFSET				2
-#define IO_16_CDB_TX_LEN_OFFSET				10
-
-/* Mode Sense/Select defines */
-#define MODE_PAGE_INFO_EXCEP				0x1C
-#define MODE_PAGE_CACHING				0x08
-#define MODE_PAGE_CONTROL				0x0A
-#define MODE_PAGE_POWER_CONDITION			0x1A
-#define MODE_PAGE_RETURN_ALL				0x3F
-#define MODE_PAGE_BLK_DES_LEN				0x08
-#define MODE_PAGE_LLBAA_BLK_DES_LEN			0x10
-#define MODE_PAGE_CACHING_LEN				0x14
-#define MODE_PAGE_CONTROL_LEN				0x0C
-#define MODE_PAGE_POW_CND_LEN				0x28
-#define MODE_PAGE_INF_EXC_LEN				0x0C
-#define MODE_PAGE_ALL_LEN				0x54
-#define MODE_SENSE6_MPH_SIZE				4
-#define MODE_SENSE6_ALLOC_LEN_OFFSET			4
-#define MODE_SENSE_PAGE_CONTROL_OFFSET			2
-#define MODE_SENSE_PAGE_CONTROL_MASK			0xC0
-#define MODE_SENSE_PAGE_CODE_OFFSET			2
-#define MODE_SENSE_PAGE_CODE_MASK			0x3F
-#define MODE_SENSE_LLBAA_OFFSET				1
-#define MODE_SENSE_LLBAA_MASK				0x10
-#define MODE_SENSE_LLBAA_SHIFT				4
-#define MODE_SENSE_DBD_OFFSET				1
-#define MODE_SENSE_DBD_MASK				8
-#define MODE_SENSE_DBD_SHIFT				3
-#define MODE_SENSE10_MPH_SIZE				8
-#define MODE_SENSE10_ALLOC_LEN_OFFSET			7
-#define MODE_SELECT_CDB_PAGE_FORMAT_OFFSET		1
-#define MODE_SELECT_CDB_SAVE_PAGES_OFFSET		1
-#define MODE_SELECT_6_CDB_PARAM_LIST_LENGTH_OFFSET	4
-#define MODE_SELECT_10_CDB_PARAM_LIST_LENGTH_OFFSET	7
-#define MODE_SELECT_CDB_PAGE_FORMAT_MASK		0x10
-#define MODE_SELECT_CDB_SAVE_PAGES_MASK			0x1
-#define MODE_SELECT_6_BD_OFFSET				3
-#define MODE_SELECT_10_BD_OFFSET			6
-#define MODE_SELECT_10_LLBAA_OFFSET			4
-#define MODE_SELECT_10_LLBAA_MASK			1
-#define MODE_SELECT_6_MPH_SIZE				4
-#define MODE_SELECT_10_MPH_SIZE				8
-#define CACHING_MODE_PAGE_WCE_MASK			0x04
-#define MODE_SENSE_BLK_DESC_ENABLED			0
-#define MODE_SENSE_BLK_DESC_COUNT			1
-#define MODE_SELECT_PAGE_CODE_MASK			0x3F
-#define SHORT_DESC_BLOCK				8
-#define LONG_DESC_BLOCK					16
-#define MODE_PAGE_POW_CND_LEN_FIELD			0x26
-#define MODE_PAGE_INF_EXC_LEN_FIELD			0x0A
-#define MODE_PAGE_CACHING_LEN_FIELD			0x12
-#define MODE_PAGE_CONTROL_LEN_FIELD			0x0A
-#define MODE_SENSE_PC_CURRENT_VALUES			0
-
-/* Log Sense defines */
-#define LOG_PAGE_SUPPORTED_LOG_PAGES_PAGE		0x00
-#define LOG_PAGE_SUPPORTED_LOG_PAGES_LENGTH		0x07
-#define LOG_PAGE_INFORMATIONAL_EXCEPTIONS_PAGE		0x2F
-#define LOG_PAGE_TEMPERATURE_PAGE			0x0D
-#define LOG_SENSE_CDB_SP_OFFSET				1
-#define LOG_SENSE_CDB_SP_NOT_ENABLED			0
-#define LOG_SENSE_CDB_PC_OFFSET				2
-#define LOG_SENSE_CDB_PC_MASK				0xC0
-#define LOG_SENSE_CDB_PC_SHIFT				6
-#define LOG_SENSE_CDB_PC_CUMULATIVE_VALUES		1
-#define LOG_SENSE_CDB_PAGE_CODE_MASK			0x3F
-#define LOG_SENSE_CDB_ALLOC_LENGTH_OFFSET		7
-#define REMAINING_INFO_EXCP_PAGE_LENGTH			0x8
-#define LOG_INFO_EXCP_PAGE_LENGTH			0xC
-#define REMAINING_TEMP_PAGE_LENGTH			0xC
-#define LOG_TEMP_PAGE_LENGTH				0x10
-#define LOG_TEMP_UNKNOWN				0xFF
-#define SUPPORTED_LOG_PAGES_PAGE_LENGTH			0x3
-
-/* Read Capacity defines */
-#define READ_CAP_10_RESP_SIZE				8
-#define READ_CAP_16_RESP_SIZE				32
-
-/* NVMe Namespace and Command Defines */
-#define NVME_GET_SMART_LOG_PAGE				0x02
-#define NVME_GET_FEAT_TEMP_THRESH			0x04
-#define BYTES_TO_DWORDS					4
-#define NVME_MAX_FIRMWARE_SLOT				7
-
-/* Report LUNs defines */
-#define REPORT_LUNS_FIRST_LUN_OFFSET			8
-
-/* SCSI ADDITIONAL SENSE Codes */
-
-#define SCSI_ASC_NO_SENSE				0x00
-#define SCSI_ASC_PERIPHERAL_DEV_WRITE_FAULT		0x03
-#define SCSI_ASC_LUN_NOT_READY				0x04
-#define SCSI_ASC_WARNING				0x0B
-#define SCSI_ASC_LOG_BLOCK_GUARD_CHECK_FAILED		0x10
-#define SCSI_ASC_LOG_BLOCK_APPTAG_CHECK_FAILED		0x10
-#define SCSI_ASC_LOG_BLOCK_REFTAG_CHECK_FAILED		0x10
-#define SCSI_ASC_UNRECOVERED_READ_ERROR			0x11
-#define SCSI_ASC_MISCOMPARE_DURING_VERIFY		0x1D
-#define SCSI_ASC_ACCESS_DENIED_INVALID_LUN_ID		0x20
-#define SCSI_ASC_ILLEGAL_COMMAND			0x20
-#define SCSI_ASC_ILLEGAL_BLOCK				0x21
-#define SCSI_ASC_INVALID_CDB				0x24
-#define SCSI_ASC_INVALID_LUN				0x25
-#define SCSI_ASC_INVALID_PARAMETER			0x26
-#define SCSI_ASC_FORMAT_COMMAND_FAILED			0x31
-#define SCSI_ASC_INTERNAL_TARGET_FAILURE		0x44
-
-/* SCSI ADDITIONAL SENSE Code Qualifiers */
-
-#define SCSI_ASCQ_CAUSE_NOT_REPORTABLE			0x00
-#define SCSI_ASCQ_FORMAT_COMMAND_FAILED			0x01
-#define SCSI_ASCQ_LOG_BLOCK_GUARD_CHECK_FAILED		0x01
-#define SCSI_ASCQ_LOG_BLOCK_APPTAG_CHECK_FAILED		0x02
-#define SCSI_ASCQ_LOG_BLOCK_REFTAG_CHECK_FAILED		0x03
-#define SCSI_ASCQ_FORMAT_IN_PROGRESS			0x04
-#define SCSI_ASCQ_POWER_LOSS_EXPECTED			0x08
-#define SCSI_ASCQ_INVALID_LUN_ID			0x09
-
-/**
- * DEVICE_SPECIFIC_PARAMETER in mode parameter header (see sbc2r16) to
- * enable DPOFUA support type 0x10 value.
- */
-#define DEVICE_SPECIFIC_PARAMETER			0
-#define VPD_ID_DESCRIPTOR_LENGTH sizeof(VPD_IDENTIFICATION_DESCRIPTOR)
-
-/* MACROs to extract information from CDBs */
-
-#define GET_OPCODE(cdb)		cdb[0]
-
-#define GET_U8_FROM_CDB(cdb, index) (cdb[index] << 0)
-
-#define GET_U16_FROM_CDB(cdb, index) ((cdb[index] << 8) | (cdb[index + 1] << 0))
-
-#define GET_U24_FROM_CDB(cdb, index) ((cdb[index] << 16) | \
-(cdb[index + 1] <<  8) | \
-(cdb[index + 2] <<  0))
-
-#define GET_U32_FROM_CDB(cdb, index) ((cdb[index] << 24) | \
-(cdb[index + 1] << 16) | \
-(cdb[index + 2] <<  8) | \
-(cdb[index + 3] <<  0))
-
-#define GET_U64_FROM_CDB(cdb, index) ((((u64)cdb[index]) << 56) | \
-(((u64)cdb[index + 1]) << 48) | \
-(((u64)cdb[index + 2]) << 40) | \
-(((u64)cdb[index + 3]) << 32) | \
-(((u64)cdb[index + 4]) << 24) | \
-(((u64)cdb[index + 5]) << 16) | \
-(((u64)cdb[index + 6]) <<  8) | \
-(((u64)cdb[index + 7]) <<  0))
-
-/* Inquiry Helper Macros */
-#define GET_INQ_EVPD_BIT(cdb) \
-((GET_U8_FROM_CDB(cdb, INQUIRY_EVPD_BYTE_OFFSET) &		\
-INQUIRY_EVPD_BIT_MASK) ? 1 : 0)
-
-#define GET_INQ_PAGE_CODE(cdb)					\
-(GET_U8_FROM_CDB(cdb, INQUIRY_PAGE_CODE_BYTE_OFFSET))
-
-#define GET_INQ_ALLOC_LENGTH(cdb)				\
-(GET_U16_FROM_CDB(cdb, INQUIRY_CDB_ALLOCATION_LENGTH_OFFSET))
-
-/* Report LUNs Helper Macros */
-#define GET_REPORT_LUNS_ALLOC_LENGTH(cdb)			\
-(GET_U32_FROM_CDB(cdb, REPORT_LUNS_CDB_ALLOC_LENGTH_OFFSET))
-
-/* Read Capacity Helper Macros */
-#define GET_READ_CAP_16_ALLOC_LENGTH(cdb)			\
-(GET_U32_FROM_CDB(cdb, READ_CAP_16_CDB_ALLOC_LENGTH_OFFSET))
-
-#define IS_READ_CAP_16(cdb)					\
-((cdb[0] == SERVICE_ACTION_IN && cdb[1] == SAI_READ_CAPACITY_16) ? 1 : 0)
-
-/* Request Sense Helper Macros */
-#define GET_REQUEST_SENSE_ALLOC_LENGTH(cdb)			\
-(GET_U8_FROM_CDB(cdb, REQUEST_SENSE_CDB_ALLOC_LENGTH_OFFSET))
-
-/* Mode Sense Helper Macros */
-#define GET_MODE_SENSE_DBD(cdb)					\
-((GET_U8_FROM_CDB(cdb, MODE_SENSE_DBD_OFFSET) & MODE_SENSE_DBD_MASK) >>	\
-MODE_SENSE_DBD_SHIFT)
-
-#define GET_MODE_SENSE_LLBAA(cdb)				\
-((GET_U8_FROM_CDB(cdb, MODE_SENSE_LLBAA_OFFSET) &		\
-MODE_SENSE_LLBAA_MASK) >> MODE_SENSE_LLBAA_SHIFT)
-
-#define GET_MODE_SENSE_MPH_SIZE(cdb10)				\
-(cdb10 ? MODE_SENSE10_MPH_SIZE : MODE_SENSE6_MPH_SIZE)
-
-
-/* Struct to gather data that needs to be extracted from a SCSI CDB.
-   Not conforming to any particular CDB variant, but compatible with all. */
-
-struct nvme_trans_io_cdb {
-	u8 fua;
-	u8 prot_info;
-	u64 lba;
-	u32 xfer_len;
-};
-
-
-/* Internal Helper Functions */
-
-
-/* Copy data to userspace memory */
-
-static int nvme_trans_copy_to_user(struct sg_io_hdr *hdr, void *from,
-								unsigned long n)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	unsigned long not_copied;
-	int i;
-	void *index = from;
-	size_t remaining = n;
-	size_t xfer_len;
-
-	if (hdr->iovec_count > 0) {
-		struct sg_iovec sgl;
-
-		for (i = 0; i < hdr->iovec_count; i++) {
-			not_copied = copy_from_user(&sgl, hdr->dxferp +
-						i * sizeof(struct sg_iovec),
-						sizeof(struct sg_iovec));
-			if (not_copied)
-				return -EFAULT;
-			xfer_len = min(remaining, sgl.iov_len);
-			not_copied = copy_to_user(sgl.iov_base, index,
-								xfer_len);
-			if (not_copied) {
-				res = -EFAULT;
-				break;
-			}
-			index += xfer_len;
-			remaining -= xfer_len;
-			if (remaining == 0)
-				break;
-		}
-		return res;
-	}
-	not_copied = copy_to_user(hdr->dxferp, from, n);
-	if (not_copied)
-		res = -EFAULT;
-	return res;
-}
-
-/* Copy data from userspace memory */
-
-static int nvme_trans_copy_from_user(struct sg_io_hdr *hdr, void *to,
-								unsigned long n)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	unsigned long not_copied;
-	int i;
-	void *index = to;
-	size_t remaining = n;
-	size_t xfer_len;
-
-	if (hdr->iovec_count > 0) {
-		struct sg_iovec sgl;
-
-		for (i = 0; i < hdr->iovec_count; i++) {
-			not_copied = copy_from_user(&sgl, hdr->dxferp +
-						i * sizeof(struct sg_iovec),
-						sizeof(struct sg_iovec));
-			if (not_copied)
-				return -EFAULT;
-			xfer_len = min(remaining, sgl.iov_len);
-			not_copied = copy_from_user(index, sgl.iov_base,
-								xfer_len);
-			if (not_copied) {
-				res = -EFAULT;
-				break;
-			}
-			index += xfer_len;
-			remaining -= xfer_len;
-			if (remaining == 0)
-				break;
-		}
-		return res;
-	}
-
-	not_copied = copy_from_user(to, hdr->dxferp, n);
-	if (not_copied)
-		res = -EFAULT;
-	return res;
-}
-
-/* Status/Sense Buffer Writeback */
-
-static int nvme_trans_completion(struct sg_io_hdr *hdr, u8 status, u8 sense_key,
-				 u8 asc, u8 ascq)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 xfer_len;
-	u8 resp[DESC_FMT_SENSE_DATA_SIZE];
-
-	if (scsi_status_is_good(status)) {
-		hdr->status = SAM_STAT_GOOD;
-		hdr->masked_status = GOOD;
-		hdr->host_status = DID_OK;
-		hdr->driver_status = DRIVER_OK;
-		hdr->sb_len_wr = 0;
-	} else {
-		hdr->status = status;
-		hdr->masked_status = status >> 1;
-		hdr->host_status = DID_OK;
-		hdr->driver_status = DRIVER_OK;
-
-		memset(resp, 0, DESC_FMT_SENSE_DATA_SIZE);
-		resp[0] = DESC_FORMAT_SENSE_DATA;
-		resp[1] = sense_key;
-		resp[2] = asc;
-		resp[3] = ascq;
-
-		xfer_len = min_t(u8, hdr->mx_sb_len, DESC_FMT_SENSE_DATA_SIZE);
-		hdr->sb_len_wr = xfer_len;
-		if (copy_to_user(hdr->sbp, resp, xfer_len) > 0)
-			res = -EFAULT;
-	}
-
-	return res;
-}
-
-static int nvme_trans_status_code(struct sg_io_hdr *hdr, int nvme_sc)
-{
-	u8 status, sense_key, asc, ascq;
-	int res = SNTI_TRANSLATION_SUCCESS;
-
-	/* For non-nvme (Linux) errors, simply return the error code */
-	if (nvme_sc < 0)
-		return nvme_sc;
-
-	/* Mask DNR, More, and reserved fields */
-	nvme_sc &= 0x7FF;
-
-	switch (nvme_sc) {
-	/* Generic Command Status */
-	case NVME_SC_SUCCESS:
-		status = SAM_STAT_GOOD;
-		sense_key = NO_SENSE;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_INVALID_OPCODE:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_ILLEGAL_COMMAND;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_INVALID_FIELD:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_INVALID_CDB;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_DATA_XFER_ERROR:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_POWER_LOSS:
-		status = SAM_STAT_TASK_ABORTED;
-		sense_key = ABORTED_COMMAND;
-		asc = SCSI_ASC_WARNING;
-		ascq = SCSI_ASCQ_POWER_LOSS_EXPECTED;
-		break;
-	case NVME_SC_INTERNAL:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = HARDWARE_ERROR;
-		asc = SCSI_ASC_INTERNAL_TARGET_FAILURE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_ABORT_REQ:
-		status = SAM_STAT_TASK_ABORTED;
-		sense_key = ABORTED_COMMAND;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_ABORT_QUEUE:
-		status = SAM_STAT_TASK_ABORTED;
-		sense_key = ABORTED_COMMAND;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_FUSED_FAIL:
-		status = SAM_STAT_TASK_ABORTED;
-		sense_key = ABORTED_COMMAND;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_FUSED_MISSING:
-		status = SAM_STAT_TASK_ABORTED;
-		sense_key = ABORTED_COMMAND;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_INVALID_NS:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_ACCESS_DENIED_INVALID_LUN_ID;
-		ascq = SCSI_ASCQ_INVALID_LUN_ID;
-		break;
-	case NVME_SC_LBA_RANGE:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_ILLEGAL_BLOCK;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_CAP_EXCEEDED:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_NS_NOT_READY:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = NOT_READY;
-		asc = SCSI_ASC_LUN_NOT_READY;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-
-	/* Command Specific Status */
-	case NVME_SC_INVALID_FORMAT:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_FORMAT_COMMAND_FAILED;
-		ascq = SCSI_ASCQ_FORMAT_COMMAND_FAILED;
-		break;
-	case NVME_SC_BAD_ATTRIBUTES:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_INVALID_CDB;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-
-	/* Media Errors */
-	case NVME_SC_WRITE_FAULT:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_PERIPHERAL_DEV_WRITE_FAULT;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_READ_ERROR:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_UNRECOVERED_READ_ERROR;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_GUARD_CHECK:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_LOG_BLOCK_GUARD_CHECK_FAILED;
-		ascq = SCSI_ASCQ_LOG_BLOCK_GUARD_CHECK_FAILED;
-		break;
-	case NVME_SC_APPTAG_CHECK:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_LOG_BLOCK_APPTAG_CHECK_FAILED;
-		ascq = SCSI_ASCQ_LOG_BLOCK_APPTAG_CHECK_FAILED;
-		break;
-	case NVME_SC_REFTAG_CHECK:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_LOG_BLOCK_REFTAG_CHECK_FAILED;
-		ascq = SCSI_ASCQ_LOG_BLOCK_REFTAG_CHECK_FAILED;
-		break;
-	case NVME_SC_COMPARE_FAILED:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MISCOMPARE;
-		asc = SCSI_ASC_MISCOMPARE_DURING_VERIFY;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_ACCESS_DENIED:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_ACCESS_DENIED_INVALID_LUN_ID;
-		ascq = SCSI_ASCQ_INVALID_LUN_ID;
-		break;
-
-	/* Unspecified/Default */
-	case NVME_SC_CMDID_CONFLICT:
-	case NVME_SC_CMD_SEQ_ERROR:
-	case NVME_SC_CQ_INVALID:
-	case NVME_SC_QID_INVALID:
-	case NVME_SC_QUEUE_SIZE:
-	case NVME_SC_ABORT_LIMIT:
-	case NVME_SC_ABORT_MISSING:
-	case NVME_SC_ASYNC_LIMIT:
-	case NVME_SC_FIRMWARE_SLOT:
-	case NVME_SC_FIRMWARE_IMAGE:
-	case NVME_SC_INVALID_VECTOR:
-	case NVME_SC_INVALID_LOG_PAGE:
-	default:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	}
-
-	res = nvme_trans_completion(hdr, status, sense_key, asc, ascq);
-
-	return res;
-}
-
-/* INQUIRY Helper Functions */
-
-static int nvme_trans_standard_inquiry_page(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *inq_response,
-					int alloc_len)
-{
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ns *id_ns;
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	int xfer_len;
-	u8 resp_data_format = 0x02;
-	u8 protect;
-	u8 cmdque = 0x01 << 1;
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-				&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out_dma;
-	}
-
-	/* nvme ns identify - use DPS value for PROTECT field */
-	nvme_sc = nvme_identify(dev, ns->ns_id, 0, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	/*
-	 * If nvme_sc was -ve, res will be -ve here.
-	 * If nvme_sc was +ve, the status would bace been translated, and res
-	 *  can only be 0 or -ve.
-	 *    - If 0 && nvme_sc > 0, then go into next if where res gets nvme_sc
-	 *    - If -ve, return because its a Linux error.
-	 */
-	if (res)
-		goto out_free;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_free;
-	}
-	id_ns = mem;
-	(id_ns->dps) ? (protect = 0x01) : (protect = 0);
-
-	memset(inq_response, 0, STANDARD_INQUIRY_LENGTH);
-	inq_response[2] = VERSION_SPC_4;
-	inq_response[3] = resp_data_format;	/*normaca=0 | hisup=0 */
-	inq_response[4] = ADDITIONAL_STD_INQ_LENGTH;
-	inq_response[5] = protect;	/* sccs=0 | acc=0 | tpgs=0 | pc3=0 */
-	inq_response[7] = cmdque;	/* wbus16=0 | sync=0 | vs=0 */
-	strncpy(&inq_response[8], "NVMe    ", 8);
-	strncpy(&inq_response[16], dev->model, 16);
-	strncpy(&inq_response[32], dev->firmware_rev, 4);
-
-	xfer_len = min(alloc_len, STANDARD_INQUIRY_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
-
- out_free:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
- out_dma:
-	return res;
-}
-
-static int nvme_trans_supported_vpd_pages(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *inq_response,
-					int alloc_len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-
-	memset(inq_response, 0, STANDARD_INQUIRY_LENGTH);
-	inq_response[1] = INQ_SUPPORTED_VPD_PAGES_PAGE;   /* Page Code */
-	inq_response[3] = INQ_NUM_SUPPORTED_VPD_PAGES;    /* Page Length */
-	inq_response[4] = INQ_SUPPORTED_VPD_PAGES_PAGE;
-	inq_response[5] = INQ_UNIT_SERIAL_NUMBER_PAGE;
-	inq_response[6] = INQ_DEVICE_IDENTIFICATION_PAGE;
-	inq_response[7] = INQ_EXTENDED_INQUIRY_DATA_PAGE;
-	inq_response[8] = INQ_BDEV_CHARACTERISTICS_PAGE;
-
-	xfer_len = min(alloc_len, STANDARD_INQUIRY_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
-
-	return res;
-}
-
-static int nvme_trans_unit_serial_page(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *inq_response,
-					int alloc_len)
-{
-	struct nvme_dev *dev = ns->dev;
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-
-	memset(inq_response, 0, STANDARD_INQUIRY_LENGTH);
-	inq_response[1] = INQ_UNIT_SERIAL_NUMBER_PAGE; /* Page Code */
-	inq_response[3] = INQ_SERIAL_NUMBER_LENGTH;    /* Page Length */
-	strncpy(&inq_response[4], dev->serial, INQ_SERIAL_NUMBER_LENGTH);
-
-	xfer_len = min(alloc_len, STANDARD_INQUIRY_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
-
-	return res;
-}
-
-static int nvme_trans_device_id_page(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					u8 *inq_response, int alloc_len)
-{
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ctrl *id_ctrl;
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	u8 ieee[4];
-	int xfer_len;
-	__be32 tmp_id = cpu_to_be32(ns->ns_id);
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-					&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out_dma;
-	}
-
-	/* nvme controller identify */
-	nvme_sc = nvme_identify(dev, 0, 1, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_free;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_free;
-	}
-	id_ctrl = mem;
-
-	/* Since SCSI tried to save 4 bits... [SPC-4(r34) Table 591] */
-	ieee[0] = id_ctrl->ieee[0] << 4;
-	ieee[1] = id_ctrl->ieee[0] >> 4 | id_ctrl->ieee[1] << 4;
-	ieee[2] = id_ctrl->ieee[1] >> 4 | id_ctrl->ieee[2] << 4;
-	ieee[3] = id_ctrl->ieee[2] >> 4;
-
-	memset(inq_response, 0, STANDARD_INQUIRY_LENGTH);
-	inq_response[1] = INQ_DEVICE_IDENTIFICATION_PAGE;    /* Page Code */
-	inq_response[3] = 20;      /* Page Length */
-	/* Designation Descriptor start */
-	inq_response[4] = 0x01;    /* Proto ID=0h | Code set=1h */
-	inq_response[5] = 0x03;    /* PIV=0b | Asso=00b | Designator Type=3h */
-	inq_response[6] = 0x00;    /* Rsvd */
-	inq_response[7] = 16;      /* Designator Length */
-	/* Designator start */
-	inq_response[8] = 0x60 | ieee[3]; /* NAA=6h | IEEE ID MSB, High nibble*/
-	inq_response[9] = ieee[2];        /* IEEE ID */
-	inq_response[10] = ieee[1];       /* IEEE ID */
-	inq_response[11] = ieee[0];       /* IEEE ID| Vendor Specific ID... */
-	inq_response[12] = (dev->pci_dev->vendor & 0xFF00) >> 8;
-	inq_response[13] = (dev->pci_dev->vendor & 0x00FF);
-	inq_response[14] = dev->serial[0];
-	inq_response[15] = dev->serial[1];
-	inq_response[16] = dev->model[0];
-	inq_response[17] = dev->model[1];
-	memcpy(&inq_response[18], &tmp_id, sizeof(u32));
-	/* Last 2 bytes are zero */
-
-	xfer_len = min(alloc_len, STANDARD_INQUIRY_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
-
- out_free:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
- out_dma:
-	return res;
-}
-
-static int nvme_trans_ext_inq_page(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					int alloc_len)
-{
-	u8 *inq_response;
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ctrl *id_ctrl;
-	struct nvme_id_ns *id_ns;
-	int xfer_len;
-	u8 microcode = 0x80;
-	u8 spt;
-	u8 spt_lut[8] = {0, 0, 2, 1, 4, 6, 5, 7};
-	u8 grd_chk, app_chk, ref_chk, protect;
-	u8 uask_sup = 0x20;
-	u8 v_sup;
-	u8 luiclr = 0x01;
-
-	inq_response = kmalloc(EXTENDED_INQUIRY_DATA_PAGE_LENGTH, GFP_KERNEL);
-	if (inq_response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-							&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out_dma;
-	}
-
-	/* nvme ns identify */
-	nvme_sc = nvme_identify(dev, ns->ns_id, 0, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_free;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_free;
-	}
-	id_ns = mem;
-	spt = spt_lut[(id_ns->dpc) & 0x07] << 3;
-	(id_ns->dps) ? (protect = 0x01) : (protect = 0);
-	grd_chk = protect << 2;
-	app_chk = protect << 1;
-	ref_chk = protect;
-
-	/* nvme controller identify */
-	nvme_sc = nvme_identify(dev, 0, 1, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_free;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_free;
-	}
-	id_ctrl = mem;
-	v_sup = id_ctrl->vwc;
-
-	memset(inq_response, 0, EXTENDED_INQUIRY_DATA_PAGE_LENGTH);
-	inq_response[1] = INQ_EXTENDED_INQUIRY_DATA_PAGE;    /* Page Code */
-	inq_response[2] = 0x00;    /* Page Length MSB */
-	inq_response[3] = 0x3C;    /* Page Length LSB */
-	inq_response[4] = microcode | spt | grd_chk | app_chk | ref_chk;
-	inq_response[5] = uask_sup;
-	inq_response[6] = v_sup;
-	inq_response[7] = luiclr;
-	inq_response[8] = 0;
-	inq_response[9] = 0;
-
-	xfer_len = min(alloc_len, EXTENDED_INQUIRY_DATA_PAGE_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
-
- out_free:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
- out_dma:
-	kfree(inq_response);
- out_mem:
-	return res;
-}
-
-static int nvme_trans_bdev_char_page(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					int alloc_len)
-{
-	u8 *inq_response;
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-
-	inq_response = kzalloc(EXTENDED_INQUIRY_DATA_PAGE_LENGTH, GFP_KERNEL);
-	if (inq_response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	inq_response[1] = INQ_BDEV_CHARACTERISTICS_PAGE;    /* Page Code */
-	inq_response[2] = 0x00;    /* Page Length MSB */
-	inq_response[3] = 0x3C;    /* Page Length LSB */
-	inq_response[4] = 0x00;    /* Medium Rotation Rate MSB */
-	inq_response[5] = 0x01;    /* Medium Rotation Rate LSB */
-	inq_response[6] = 0x00;    /* Form Factor */
-
-	xfer_len = min(alloc_len, EXTENDED_INQUIRY_DATA_PAGE_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
-
-	kfree(inq_response);
- out_mem:
-	return res;
-}
-
-/* LOG SENSE Helper Functions */
-
-static int nvme_trans_log_supp_pages(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					int alloc_len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-	u8 *log_response;
-
-	log_response = kzalloc(LOG_PAGE_SUPPORTED_LOG_PAGES_LENGTH, GFP_KERNEL);
-	if (log_response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	log_response[0] = LOG_PAGE_SUPPORTED_LOG_PAGES_PAGE;
-	/* Subpage=0x00, Page Length MSB=0 */
-	log_response[3] = SUPPORTED_LOG_PAGES_PAGE_LENGTH;
-	log_response[4] = LOG_PAGE_SUPPORTED_LOG_PAGES_PAGE;
-	log_response[5] = LOG_PAGE_INFORMATIONAL_EXCEPTIONS_PAGE;
-	log_response[6] = LOG_PAGE_TEMPERATURE_PAGE;
-
-	xfer_len = min(alloc_len, LOG_PAGE_SUPPORTED_LOG_PAGES_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, log_response, xfer_len);
-
-	kfree(log_response);
- out_mem:
-	return res;
-}
-
-static int nvme_trans_log_info_exceptions(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, int alloc_len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-	u8 *log_response;
-	struct nvme_command c;
-	struct nvme_dev *dev = ns->dev;
-	struct nvme_smart_log *smart_log;
-	dma_addr_t dma_addr;
-	void *mem;
-	u8 temp_c;
-	u16 temp_k;
-
-	log_response = kzalloc(LOG_INFO_EXCP_PAGE_LENGTH, GFP_KERNEL);
-	if (log_response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev,
-					sizeof(struct nvme_smart_log),
-					&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out_dma;
-	}
-
-	/* Get SMART Log Page */
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = nvme_admin_get_log_page;
-	c.common.nsid = cpu_to_le32(0xFFFFFFFF);
-	c.common.prp1 = cpu_to_le64(dma_addr);
-	c.common.cdw10[0] = cpu_to_le32(((sizeof(struct nvme_smart_log) /
-			BYTES_TO_DWORDS) << 16) | NVME_GET_SMART_LOG_PAGE);
-	res = nvme_submit_admin_cmd(dev, &c, NULL);
-	if (res != NVME_SC_SUCCESS) {
-		temp_c = LOG_TEMP_UNKNOWN;
-	} else {
-		smart_log = mem;
-		temp_k = (smart_log->temperature[1] << 8) +
-				(smart_log->temperature[0]);
-		temp_c = temp_k - KELVIN_TEMP_FACTOR;
-	}
-
-	log_response[0] = LOG_PAGE_INFORMATIONAL_EXCEPTIONS_PAGE;
-	/* Subpage=0x00, Page Length MSB=0 */
-	log_response[3] = REMAINING_INFO_EXCP_PAGE_LENGTH;
-	/* Informational Exceptions Log Parameter 1 Start */
-	/* Parameter Code=0x0000 bytes 4,5 */
-	log_response[6] = 0x23; /* DU=0, TSD=1, ETC=0, TMC=0, FMT_AND_LNK=11b */
-	log_response[7] = 0x04; /* PARAMETER LENGTH */
-	/* Add sense Code and qualifier = 0x00 each */
-	/* Use Temperature from NVMe Get Log Page, convert to C from K */
-	log_response[10] = temp_c;
-
-	xfer_len = min(alloc_len, LOG_INFO_EXCP_PAGE_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, log_response, xfer_len);
-
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_smart_log),
-			  mem, dma_addr);
- out_dma:
-	kfree(log_response);
- out_mem:
-	return res;
-}
-
-static int nvme_trans_log_temperature(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					int alloc_len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-	u8 *log_response;
-	struct nvme_command c;
-	struct nvme_dev *dev = ns->dev;
-	struct nvme_smart_log *smart_log;
-	dma_addr_t dma_addr;
-	void *mem;
-	u32 feature_resp;
-	u8 temp_c_cur, temp_c_thresh;
-	u16 temp_k;
-
-	log_response = kzalloc(LOG_TEMP_PAGE_LENGTH, GFP_KERNEL);
-	if (log_response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev,
-					sizeof(struct nvme_smart_log),
-					&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out_dma;
-	}
-
-	/* Get SMART Log Page */
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = nvme_admin_get_log_page;
-	c.common.nsid = cpu_to_le32(0xFFFFFFFF);
-	c.common.prp1 = cpu_to_le64(dma_addr);
-	c.common.cdw10[0] = cpu_to_le32(((sizeof(struct nvme_smart_log) /
-			BYTES_TO_DWORDS) << 16) | NVME_GET_SMART_LOG_PAGE);
-	res = nvme_submit_admin_cmd(dev, &c, NULL);
-	if (res != NVME_SC_SUCCESS) {
-		temp_c_cur = LOG_TEMP_UNKNOWN;
-	} else {
-		smart_log = mem;
-		temp_k = (smart_log->temperature[1] << 8) +
-				(smart_log->temperature[0]);
-		temp_c_cur = temp_k - KELVIN_TEMP_FACTOR;
-	}
-
-	/* Get Features for Temp Threshold */
-	res = nvme_get_features(dev, NVME_FEAT_TEMP_THRESH, 0, 0,
-								&feature_resp);
-	if (res != NVME_SC_SUCCESS)
-		temp_c_thresh = LOG_TEMP_UNKNOWN;
-	else
-		temp_c_thresh = (feature_resp & 0xFFFF) - KELVIN_TEMP_FACTOR;
-
-	log_response[0] = LOG_PAGE_TEMPERATURE_PAGE;
-	/* Subpage=0x00, Page Length MSB=0 */
-	log_response[3] = REMAINING_TEMP_PAGE_LENGTH;
-	/* Temperature Log Parameter 1 (Temperature) Start */
-	/* Parameter Code = 0x0000 */
-	log_response[6] = 0x01;		/* Format and Linking = 01b */
-	log_response[7] = 0x02;		/* Parameter Length */
-	/* Use Temperature from NVMe Get Log Page, convert to C from K */
-	log_response[9] = temp_c_cur;
-	/* Temperature Log Parameter 2 (Reference Temperature) Start */
-	log_response[11] = 0x01;	/* Parameter Code = 0x0001 */
-	log_response[12] = 0x01;	/* Format and Linking = 01b */
-	log_response[13] = 0x02;	/* Parameter Length */
-	/* Use Temperature Thresh from NVMe Get Log Page, convert to C from K */
-	log_response[15] = temp_c_thresh;
-
-	xfer_len = min(alloc_len, LOG_TEMP_PAGE_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, log_response, xfer_len);
-
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_smart_log),
-			  mem, dma_addr);
- out_dma:
-	kfree(log_response);
- out_mem:
-	return res;
-}
-
-/* MODE SENSE Helper Functions */
-
-static int nvme_trans_fill_mode_parm_hdr(u8 *resp, int len, u8 cdb10, u8 llbaa,
-					u16 mode_data_length, u16 blk_desc_len)
-{
-	/* Quick check to make sure I don't stomp on my own memory... */
-	if ((cdb10 && len < 8) || (!cdb10 && len < 4))
-		return SNTI_INTERNAL_ERROR;
-
-	if (cdb10) {
-		resp[0] = (mode_data_length & 0xFF00) >> 8;
-		resp[1] = (mode_data_length & 0x00FF);
-		/* resp[2] and [3] are zero */
-		resp[4] = llbaa;
-		resp[5] = RESERVED_FIELD;
-		resp[6] = (blk_desc_len & 0xFF00) >> 8;
-		resp[7] = (blk_desc_len & 0x00FF);
-	} else {
-		resp[0] = (mode_data_length & 0x00FF);
-		/* resp[1] and [2] are zero */
-		resp[3] = (blk_desc_len & 0x00FF);
-	}
-
-	return SNTI_TRANSLATION_SUCCESS;
-}
-
-static int nvme_trans_fill_blk_desc(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-				    u8 *resp, int len, u8 llbaa)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ns *id_ns;
-	u8 flbas;
-	u32 lba_length;
-
-	if (llbaa == 0 && len < MODE_PAGE_BLK_DES_LEN)
-		return SNTI_INTERNAL_ERROR;
-	else if (llbaa > 0 && len < MODE_PAGE_LLBAA_BLK_DES_LEN)
-		return SNTI_INTERNAL_ERROR;
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-							&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-
-	/* nvme ns identify */
-	nvme_sc = nvme_identify(dev, ns->ns_id, 0, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_dma;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_dma;
-	}
-	id_ns = mem;
-	flbas = (id_ns->flbas) & 0x0F;
-	lba_length = (1 << (id_ns->lbaf[flbas].ds));
-
-	if (llbaa == 0) {
-		__be32 tmp_cap = cpu_to_be32(le64_to_cpu(id_ns->ncap));
-		/* Byte 4 is reserved */
-		__be32 tmp_len = cpu_to_be32(lba_length & 0x00FFFFFF);
-
-		memcpy(resp, &tmp_cap, sizeof(u32));
-		memcpy(&resp[4], &tmp_len, sizeof(u32));
-	} else {
-		__be64 tmp_cap = cpu_to_be64(le64_to_cpu(id_ns->ncap));
-		__be32 tmp_len = cpu_to_be32(lba_length);
-
-		memcpy(resp, &tmp_cap, sizeof(u64));
-		/* Bytes 8, 9, 10, 11 are reserved */
-		memcpy(&resp[12], &tmp_len, sizeof(u32));
-	}
-
- out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
- out:
-	return res;
-}
-
-static int nvme_trans_fill_control_page(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *resp,
-					int len)
-{
-	if (len < MODE_PAGE_CONTROL_LEN)
-		return SNTI_INTERNAL_ERROR;
-
-	resp[0] = MODE_PAGE_CONTROL;
-	resp[1] = MODE_PAGE_CONTROL_LEN_FIELD;
-	resp[2] = 0x0E;		/* TST=000b, TMF_ONLY=0, DPICZ=1,
-				 * D_SENSE=1, GLTSD=1, RLEC=0 */
-	resp[3] = 0x12;		/* Q_ALGO_MODIFIER=1h, NUAR=0, QERR=01b */
-	/* Byte 4:  VS=0, RAC=0, UA_INT=0, SWP=0 */
-	resp[5] = 0x40;		/* ATO=0, TAS=1, ATMPE=0, RWWP=0, AUTOLOAD=0 */
-	/* resp[6] and [7] are obsolete, thus zero */
-	resp[8] = 0xFF;		/* Busy timeout period = 0xffff */
-	resp[9] = 0xFF;
-	/* Bytes 10,11: Extended selftest completion time = 0x0000 */
-
-	return SNTI_TRANSLATION_SUCCESS;
-}
-
-static int nvme_trans_fill_caching_page(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr,
-					u8 *resp, int len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	u32 feature_resp;
-	u8 vwc;
-
-	if (len < MODE_PAGE_CACHING_LEN)
-		return SNTI_INTERNAL_ERROR;
-
-	nvme_sc = nvme_get_features(dev, NVME_FEAT_VOLATILE_WC, 0, 0,
-								&feature_resp);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out;
-	}
-	vwc = feature_resp & 0x00000001;
-
-	resp[0] = MODE_PAGE_CACHING;
-	resp[1] = MODE_PAGE_CACHING_LEN_FIELD;
-	resp[2] = vwc << 2;
-
- out:
-	return res;
-}
-
-static int nvme_trans_fill_pow_cnd_page(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *resp,
-					int len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-
-	if (len < MODE_PAGE_POW_CND_LEN)
-		return SNTI_INTERNAL_ERROR;
-
-	resp[0] = MODE_PAGE_POWER_CONDITION;
-	resp[1] = MODE_PAGE_POW_CND_LEN_FIELD;
-	/* All other bytes are zero */
-
-	return res;
-}
-
-static int nvme_trans_fill_inf_exc_page(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *resp,
-					int len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-
-	if (len < MODE_PAGE_INF_EXC_LEN)
-		return SNTI_INTERNAL_ERROR;
-
-	resp[0] = MODE_PAGE_INFO_EXCEP;
-	resp[1] = MODE_PAGE_INF_EXC_LEN_FIELD;
-	resp[2] = 0x88;
-	/* All other bytes are zero */
-
-	return res;
-}
-
-static int nvme_trans_fill_all_pages(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-				     u8 *resp, int len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u16 mode_pages_offset_1 = 0;
-	u16 mode_pages_offset_2, mode_pages_offset_3, mode_pages_offset_4;
-
-	mode_pages_offset_2 = mode_pages_offset_1 + MODE_PAGE_CACHING_LEN;
-	mode_pages_offset_3 = mode_pages_offset_2 + MODE_PAGE_CONTROL_LEN;
-	mode_pages_offset_4 = mode_pages_offset_3 + MODE_PAGE_POW_CND_LEN;
-
-	res = nvme_trans_fill_caching_page(ns, hdr, &resp[mode_pages_offset_1],
-					MODE_PAGE_CACHING_LEN);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-	res = nvme_trans_fill_control_page(ns, hdr, &resp[mode_pages_offset_2],
-					MODE_PAGE_CONTROL_LEN);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-	res = nvme_trans_fill_pow_cnd_page(ns, hdr, &resp[mode_pages_offset_3],
-					MODE_PAGE_POW_CND_LEN);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-	res = nvme_trans_fill_inf_exc_page(ns, hdr, &resp[mode_pages_offset_4],
-					MODE_PAGE_INF_EXC_LEN);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-
- out:
-	return res;
-}
-
-static inline int nvme_trans_get_blk_desc_len(u8 dbd, u8 llbaa)
-{
-	if (dbd == MODE_SENSE_BLK_DESC_ENABLED) {
-		/* SPC-4: len = 8 x Num_of_descriptors if llbaa = 0, 16x if 1 */
-		return 8 * (llbaa + 1) * MODE_SENSE_BLK_DESC_COUNT;
-	} else {
-		return 0;
-	}
-}
-
-static int nvme_trans_mode_page_create(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *cmd,
-					u16 alloc_len, u8 cdb10,
-					int (*mode_page_fill_func)
-					(struct nvme_ns *,
-					struct sg_io_hdr *hdr, u8 *, int),
-					u16 mode_pages_tot_len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-	u8 *response;
-	u8 dbd, llbaa;
-	u16 resp_size;
-	int mph_size;
-	u16 mode_pages_offset_1;
-	u16 blk_desc_len, blk_desc_offset, mode_data_length;
-
-	dbd = GET_MODE_SENSE_DBD(cmd);
-	llbaa = GET_MODE_SENSE_LLBAA(cmd);
-	mph_size = GET_MODE_SENSE_MPH_SIZE(cdb10);
-	blk_desc_len = nvme_trans_get_blk_desc_len(dbd, llbaa);
-
-	resp_size = mph_size + blk_desc_len + mode_pages_tot_len;
-	/* Refer spc4r34 Table 440 for calculation of Mode data Length field */
-	mode_data_length = 3 + (3 * cdb10) + blk_desc_len + mode_pages_tot_len;
-
-	blk_desc_offset = mph_size;
-	mode_pages_offset_1 = blk_desc_offset + blk_desc_len;
-
-	response = kzalloc(resp_size, GFP_KERNEL);
-	if (response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	res = nvme_trans_fill_mode_parm_hdr(&response[0], mph_size, cdb10,
-					llbaa, mode_data_length, blk_desc_len);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out_free;
-	if (blk_desc_len > 0) {
-		res = nvme_trans_fill_blk_desc(ns, hdr,
-					       &response[blk_desc_offset],
-					       blk_desc_len, llbaa);
-		if (res != SNTI_TRANSLATION_SUCCESS)
-			goto out_free;
-	}
-	res = mode_page_fill_func(ns, hdr, &response[mode_pages_offset_1],
-					mode_pages_tot_len);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out_free;
-
-	xfer_len = min(alloc_len, resp_size);
-	res = nvme_trans_copy_to_user(hdr, response, xfer_len);
-
- out_free:
-	kfree(response);
- out_mem:
-	return res;
-}
-
-/* Read Capacity Helper Functions */
-
-static void nvme_trans_fill_read_cap(u8 *response, struct nvme_id_ns *id_ns,
-								u8 cdb16)
-{
-	u8 flbas;
-	u32 lba_length;
-	u64 rlba;
-	u8 prot_en;
-	u8 p_type_lut[4] = {0, 0, 1, 2};
-	__be64 tmp_rlba;
-	__be32 tmp_rlba_32;
-	__be32 tmp_len;
-
-	flbas = (id_ns->flbas) & 0x0F;
-	lba_length = (1 << (id_ns->lbaf[flbas].ds));
-	rlba = le64_to_cpup(&id_ns->nsze) - 1;
-	(id_ns->dps) ? (prot_en = 0x01) : (prot_en = 0);
-
-	if (!cdb16) {
-		if (rlba > 0xFFFFFFFF)
-			rlba = 0xFFFFFFFF;
-		tmp_rlba_32 = cpu_to_be32(rlba);
-		tmp_len = cpu_to_be32(lba_length);
-		memcpy(response, &tmp_rlba_32, sizeof(u32));
-		memcpy(&response[4], &tmp_len, sizeof(u32));
-	} else {
-		tmp_rlba = cpu_to_be64(rlba);
-		tmp_len = cpu_to_be32(lba_length);
-		memcpy(response, &tmp_rlba, sizeof(u64));
-		memcpy(&response[8], &tmp_len, sizeof(u32));
-		response[12] = (p_type_lut[id_ns->dps & 0x3] << 1) | prot_en;
-		/* P_I_Exponent = 0x0 | LBPPBE = 0x0 */
-		/* LBPME = 0 | LBPRZ = 0 | LALBA = 0x00 */
-		/* Bytes 16-31 - Reserved */
-	}
-}
-
-/* Start Stop Unit Helper Functions */
-
-static int nvme_trans_power_state(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-						u8 pc, u8 pcmod, u8 start)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ctrl *id_ctrl;
-	int lowest_pow_st;	/* max npss = lowest power consumption */
-	unsigned ps_desired = 0;
-
-	/* NVMe Controller Identify */
-	mem = dma_alloc_coherent(&dev->pci_dev->dev,
-				sizeof(struct nvme_id_ctrl),
-				&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-	nvme_sc = nvme_identify(dev, 0, 1, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_dma;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_dma;
-	}
-	id_ctrl = mem;
-	lowest_pow_st = id_ctrl->npss - 1;
-
-	switch (pc) {
-	case NVME_POWER_STATE_START_VALID:
-		/* Action unspecified if POWER CONDITION MODIFIER != 0 */
-		if (pcmod == 0 && start == 0x1)
-			ps_desired = POWER_STATE_0;
-		if (pcmod == 0 && start == 0x0)
-			ps_desired = lowest_pow_st;
-		break;
-	case NVME_POWER_STATE_ACTIVE:
-		/* Action unspecified if POWER CONDITION MODIFIER != 0 */
-		if (pcmod == 0)
-			ps_desired = POWER_STATE_0;
-		break;
-	case NVME_POWER_STATE_IDLE:
-		/* Action unspecified if POWER CONDITION MODIFIER != [0,1,2] */
-		/* min of desired state and (lps-1) because lps is STOP */
-		if (pcmod == 0x0)
-			ps_desired = min(POWER_STATE_1, (lowest_pow_st - 1));
-		else if (pcmod == 0x1)
-			ps_desired = min(POWER_STATE_2, (lowest_pow_st - 1));
-		else if (pcmod == 0x2)
-			ps_desired = min(POWER_STATE_3, (lowest_pow_st - 1));
-		break;
-	case NVME_POWER_STATE_STANDBY:
-		/* Action unspecified if POWER CONDITION MODIFIER != [0,1] */
-		if (pcmod == 0x0)
-			ps_desired = max(0, (lowest_pow_st - 2));
-		else if (pcmod == 0x1)
-			ps_desired = max(0, (lowest_pow_st - 1));
-		break;
-	case NVME_POWER_STATE_LU_CONTROL:
-	default:
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-				ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-				SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		break;
-	}
-	nvme_sc = nvme_set_features(dev, NVME_FEAT_POWER_MGMT, ps_desired, 0,
-				    NULL);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_dma;
-	if (nvme_sc)
-		res = nvme_sc;
- out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ctrl), mem,
-			  dma_addr);
- out:
-	return res;
-}
-
-/* Write Buffer Helper Functions */
-/* Also using this for Format Unit with hdr passed as NULL, and buffer_id, 0 */
-
-static int nvme_trans_send_fw_cmd(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					u8 opcode, u32 tot_len, u32 offset,
-					u8 buffer_id)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	struct nvme_command c;
-	struct nvme_iod *iod = NULL;
-	unsigned length;
-
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = opcode;
-	if (opcode == nvme_admin_download_fw) {
-		if (hdr->iovec_count > 0) {
-			/* Assuming SGL is not allowed for this command */
-			res = nvme_trans_completion(hdr,
-						SAM_STAT_CHECK_CONDITION,
-						ILLEGAL_REQUEST,
-						SCSI_ASC_INVALID_CDB,
-						SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-			goto out;
-		}
-		iod = nvme_map_user_pages(dev, DMA_TO_DEVICE,
-				(unsigned long)hdr->dxferp, tot_len);
-		if (IS_ERR(iod)) {
-			res = PTR_ERR(iod);
-			goto out;
-		}
-		length = nvme_setup_prps(dev, iod, tot_len, GFP_KERNEL);
-		if (length != tot_len) {
-			res = -ENOMEM;
-			goto out_unmap;
-		}
-
-		c.dlfw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
-		c.dlfw.prp2 = cpu_to_le64(iod->first_dma);
-		c.dlfw.numd = cpu_to_le32((tot_len/BYTES_TO_DWORDS) - 1);
-		c.dlfw.offset = cpu_to_le32(offset/BYTES_TO_DWORDS);
-	} else if (opcode == nvme_admin_activate_fw) {
-		u32 cdw10 = buffer_id | NVME_FWACT_REPL_ACTV;
-		c.common.cdw10[0] = cpu_to_le32(cdw10);
-	}
-
-	nvme_sc = nvme_submit_admin_cmd(dev, &c, NULL);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_unmap;
-	if (nvme_sc)
-		res = nvme_sc;
-
- out_unmap:
-	if (opcode == nvme_admin_download_fw) {
-		nvme_unmap_user_pages(dev, DMA_TO_DEVICE, iod);
-		nvme_free_iod(dev, iod);
-	}
- out:
-	return res;
-}
-
-/* Mode Select Helper Functions */
-
-static inline void nvme_trans_modesel_get_bd_len(u8 *parm_list, u8 cdb10,
-						u16 *bd_len, u8 *llbaa)
-{
-	if (cdb10) {
-		/* 10 Byte CDB */
-		*bd_len = (parm_list[MODE_SELECT_10_BD_OFFSET] << 8) +
-			parm_list[MODE_SELECT_10_BD_OFFSET + 1];
-		*llbaa = parm_list[MODE_SELECT_10_LLBAA_OFFSET] &&
-				MODE_SELECT_10_LLBAA_MASK;
-	} else {
-		/* 6 Byte CDB */
-		*bd_len = parm_list[MODE_SELECT_6_BD_OFFSET];
-	}
-}
-
-static void nvme_trans_modesel_save_bd(struct nvme_ns *ns, u8 *parm_list,
-					u16 idx, u16 bd_len, u8 llbaa)
-{
-	u16 bd_num;
-
-	bd_num = bd_len / ((llbaa == 0) ?
-			SHORT_DESC_BLOCK : LONG_DESC_BLOCK);
-	/* Store block descriptor info if a FORMAT UNIT comes later */
-	/* TODO Saving 1st BD info; what to do if multiple BD received? */
-	if (llbaa == 0) {
-		/* Standard Block Descriptor - spc4r34 7.5.5.1 */
-		ns->mode_select_num_blocks =
-				(parm_list[idx + 1] << 16) +
-				(parm_list[idx + 2] << 8) +
-				(parm_list[idx + 3]);
-
-		ns->mode_select_block_len =
-				(parm_list[idx + 5] << 16) +
-				(parm_list[idx + 6] << 8) +
-				(parm_list[idx + 7]);
-	} else {
-		/* Long LBA Block Descriptor - sbc3r27 6.4.2.3 */
-		ns->mode_select_num_blocks =
-				(((u64)parm_list[idx + 0]) << 56) +
-				(((u64)parm_list[idx + 1]) << 48) +
-				(((u64)parm_list[idx + 2]) << 40) +
-				(((u64)parm_list[idx + 3]) << 32) +
-				(((u64)parm_list[idx + 4]) << 24) +
-				(((u64)parm_list[idx + 5]) << 16) +
-				(((u64)parm_list[idx + 6]) << 8) +
-				((u64)parm_list[idx + 7]);
-
-		ns->mode_select_block_len =
-				(parm_list[idx + 12] << 24) +
-				(parm_list[idx + 13] << 16) +
-				(parm_list[idx + 14] << 8) +
-				(parm_list[idx + 15]);
-	}
-}
-
-static int nvme_trans_modesel_get_mp(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					u8 *mode_page, u8 page_code)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	unsigned dword11;
-
-	switch (page_code) {
-	case MODE_PAGE_CACHING:
-		dword11 = ((mode_page[2] & CACHING_MODE_PAGE_WCE_MASK) ? 1 : 0);
-		nvme_sc = nvme_set_features(dev, NVME_FEAT_VOLATILE_WC, dword11,
-					    0, NULL);
-		res = nvme_trans_status_code(hdr, nvme_sc);
-		if (res)
-			break;
-		if (nvme_sc) {
-			res = nvme_sc;
-			break;
-		}
-		break;
-	case MODE_PAGE_CONTROL:
-		break;
-	case MODE_PAGE_POWER_CONDITION:
-		/* Verify the OS is not trying to set timers */
-		if ((mode_page[2] & 0x01) != 0 || (mode_page[3] & 0x0F) != 0) {
-			res = nvme_trans_completion(hdr,
-						SAM_STAT_CHECK_CONDITION,
-						ILLEGAL_REQUEST,
-						SCSI_ASC_INVALID_PARAMETER,
-						SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-			if (!res)
-				res = SNTI_INTERNAL_ERROR;
-			break;
-		}
-		break;
-	default:
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		if (!res)
-			res = SNTI_INTERNAL_ERROR;
-		break;
-	}
-
-	return res;
-}
-
-static int nvme_trans_modesel_data(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					u8 *cmd, u16 parm_list_len, u8 pf,
-					u8 sp, u8 cdb10)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 *parm_list;
-	u16 bd_len;
-	u8 llbaa = 0;
-	u16 index, saved_index;
-	u8 page_code;
-	u16 mp_size;
-
-	/* Get parm list from data-in/out buffer */
-	parm_list = kmalloc(parm_list_len, GFP_KERNEL);
-	if (parm_list == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-
-	res = nvme_trans_copy_from_user(hdr, parm_list, parm_list_len);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out_mem;
-
-	nvme_trans_modesel_get_bd_len(parm_list, cdb10, &bd_len, &llbaa);
-	index = (cdb10) ? (MODE_SELECT_10_MPH_SIZE) : (MODE_SELECT_6_MPH_SIZE);
-
-	if (bd_len != 0) {
-		/* Block Descriptors present, parse */
-		nvme_trans_modesel_save_bd(ns, parm_list, index, bd_len, llbaa);
-		index += bd_len;
-	}
-	saved_index = index;
-
-	/* Multiple mode pages may be present; iterate through all */
-	/* In 1st Iteration, don't do NVME Command, only check for CDB errors */
-	do {
-		page_code = parm_list[index] & MODE_SELECT_PAGE_CODE_MASK;
-		mp_size = parm_list[index + 1] + 2;
-		if ((page_code != MODE_PAGE_CACHING) &&
-		    (page_code != MODE_PAGE_CONTROL) &&
-		    (page_code != MODE_PAGE_POWER_CONDITION)) {
-			res = nvme_trans_completion(hdr,
-						SAM_STAT_CHECK_CONDITION,
-						ILLEGAL_REQUEST,
-						SCSI_ASC_INVALID_CDB,
-						SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-			goto out_mem;
-		}
-		index += mp_size;
-	} while (index < parm_list_len);
-
-	/* In 2nd Iteration, do the NVME Commands */
-	index = saved_index;
-	do {
-		page_code = parm_list[index] & MODE_SELECT_PAGE_CODE_MASK;
-		mp_size = parm_list[index + 1] + 2;
-		res = nvme_trans_modesel_get_mp(ns, hdr, &parm_list[index],
-								page_code);
-		if (res != SNTI_TRANSLATION_SUCCESS)
-			break;
-		index += mp_size;
-	} while (index < parm_list_len);
-
- out_mem:
-	kfree(parm_list);
- out:
-	return res;
-}
-
-/* Format Unit Helper Functions */
-
-static int nvme_trans_fmt_set_blk_size_count(struct nvme_ns *ns,
-					     struct sg_io_hdr *hdr)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ns *id_ns;
-	u8 flbas;
-
-	/*
-	 * SCSI Expects a MODE SELECT would have been issued prior to
-	 * a FORMAT UNIT, and the block size and number would be used
-	 * from the block descriptor in it. If a MODE SELECT had not
-	 * been issued, FORMAT shall use the current values for both.
-	 */
-
-	if (ns->mode_select_num_blocks == 0 || ns->mode_select_block_len == 0) {
-		mem = dma_alloc_coherent(&dev->pci_dev->dev,
-			sizeof(struct nvme_id_ns), &dma_addr, GFP_KERNEL);
-		if (mem == NULL) {
-			res = -ENOMEM;
-			goto out;
-		}
-		/* nvme ns identify */
-		nvme_sc = nvme_identify(dev, ns->ns_id, 0, dma_addr);
-		res = nvme_trans_status_code(hdr, nvme_sc);
-		if (res)
-			goto out_dma;
-		if (nvme_sc) {
-			res = nvme_sc;
-			goto out_dma;
-		}
-		id_ns = mem;
-
-		if (ns->mode_select_num_blocks == 0)
-			ns->mode_select_num_blocks = le64_to_cpu(id_ns->ncap);
-		if (ns->mode_select_block_len == 0) {
-			flbas = (id_ns->flbas) & 0x0F;
-			ns->mode_select_block_len =
-						(1 << (id_ns->lbaf[flbas].ds));
-		}
- out_dma:
-		dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-				  mem, dma_addr);
-	}
- out:
-	return res;
-}
-
-static int nvme_trans_fmt_get_parm_header(struct sg_io_hdr *hdr, u8 len,
-					u8 format_prot_info, u8 *nvme_pf_code)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 *parm_list;
-	u8 pf_usage, pf_code;
-
-	parm_list = kmalloc(len, GFP_KERNEL);
-	if (parm_list == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-	res = nvme_trans_copy_from_user(hdr, parm_list, len);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out_mem;
-
-	if ((parm_list[FORMAT_UNIT_IMMED_OFFSET] &
-				FORMAT_UNIT_IMMED_MASK) != 0) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out_mem;
-	}
-
-	if (len == FORMAT_UNIT_LONG_PARM_LIST_LEN &&
-	    (parm_list[FORMAT_UNIT_PROT_INT_OFFSET] & 0x0F) != 0) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out_mem;
-	}
-	pf_usage = parm_list[FORMAT_UNIT_PROT_FIELD_USAGE_OFFSET] &
-			FORMAT_UNIT_PROT_FIELD_USAGE_MASK;
-	pf_code = (pf_usage << 2) | format_prot_info;
-	switch (pf_code) {
-	case 0:
-		*nvme_pf_code = 0;
-		break;
-	case 2:
-		*nvme_pf_code = 1;
-		break;
-	case 3:
-		*nvme_pf_code = 2;
-		break;
-	case 7:
-		*nvme_pf_code = 3;
-		break;
-	default:
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		break;
-	}
-
- out_mem:
-	kfree(parm_list);
- out:
-	return res;
-}
-
-static int nvme_trans_fmt_send_cmd(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-				   u8 prot_info)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ns *id_ns;
-	u8 i;
-	u8 flbas, nlbaf;
-	u8 selected_lbaf = 0xFF;
-	u32 cdw10 = 0;
-	struct nvme_command c;
-
-	/* Loop thru LBAF's in id_ns to match reqd lbaf, put in cdw10 */
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-							&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-	/* nvme ns identify */
-	nvme_sc = nvme_identify(dev, ns->ns_id, 0, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_dma;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_dma;
-	}
-	id_ns = mem;
-	flbas = (id_ns->flbas) & 0x0F;
-	nlbaf = id_ns->nlbaf;
-
-	for (i = 0; i < nlbaf; i++) {
-		if (ns->mode_select_block_len == (1 << (id_ns->lbaf[i].ds))) {
-			selected_lbaf = i;
-			break;
-		}
-	}
-	if (selected_lbaf > 0x0F) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-				ILLEGAL_REQUEST, SCSI_ASC_INVALID_PARAMETER,
-				SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-	}
-	if (ns->mode_select_num_blocks != le64_to_cpu(id_ns->ncap)) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-				ILLEGAL_REQUEST, SCSI_ASC_INVALID_PARAMETER,
-				SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-	}
-
-	cdw10 |= prot_info << 5;
-	cdw10 |= selected_lbaf & 0x0F;
-	memset(&c, 0, sizeof(c));
-	c.format.opcode = nvme_admin_format_nvm;
-	c.format.nsid = cpu_to_le32(ns->ns_id);
-	c.format.cdw10 = cpu_to_le32(cdw10);
-
-	nvme_sc = nvme_submit_admin_cmd(dev, &c, NULL);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_dma;
-	if (nvme_sc)
-		res = nvme_sc;
-
- out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
- out:
-	return res;
-}
-
-/* Read/Write Helper Functions */
-
-static inline void nvme_trans_get_io_cdb6(u8 *cmd,
-					struct nvme_trans_io_cdb *cdb_info)
-{
-	cdb_info->fua = 0;
-	cdb_info->prot_info = 0;
-	cdb_info->lba = GET_U32_FROM_CDB(cmd, IO_6_CDB_LBA_OFFSET) &
-					IO_6_CDB_LBA_MASK;
-	cdb_info->xfer_len = GET_U8_FROM_CDB(cmd, IO_6_CDB_TX_LEN_OFFSET);
-
-	/* sbc3r27 sec 5.32 - TRANSFER LEN of 0 implies a 256 Block transfer */
-	if (cdb_info->xfer_len == 0)
-		cdb_info->xfer_len = IO_6_DEFAULT_TX_LEN;
-}
-
-static inline void nvme_trans_get_io_cdb10(u8 *cmd,
-					struct nvme_trans_io_cdb *cdb_info)
-{
-	cdb_info->fua = GET_U8_FROM_CDB(cmd, IO_10_CDB_FUA_OFFSET) &
-					IO_CDB_FUA_MASK;
-	cdb_info->prot_info = GET_U8_FROM_CDB(cmd, IO_10_CDB_WP_OFFSET) &
-					IO_CDB_WP_MASK >> IO_CDB_WP_SHIFT;
-	cdb_info->lba = GET_U32_FROM_CDB(cmd, IO_10_CDB_LBA_OFFSET);
-	cdb_info->xfer_len = GET_U16_FROM_CDB(cmd, IO_10_CDB_TX_LEN_OFFSET);
-}
-
-static inline void nvme_trans_get_io_cdb12(u8 *cmd,
-					struct nvme_trans_io_cdb *cdb_info)
-{
-	cdb_info->fua = GET_U8_FROM_CDB(cmd, IO_12_CDB_FUA_OFFSET) &
-					IO_CDB_FUA_MASK;
-	cdb_info->prot_info = GET_U8_FROM_CDB(cmd, IO_12_CDB_WP_OFFSET) &
-					IO_CDB_WP_MASK >> IO_CDB_WP_SHIFT;
-	cdb_info->lba = GET_U32_FROM_CDB(cmd, IO_12_CDB_LBA_OFFSET);
-	cdb_info->xfer_len = GET_U32_FROM_CDB(cmd, IO_12_CDB_TX_LEN_OFFSET);
-}
-
-static inline void nvme_trans_get_io_cdb16(u8 *cmd,
-					struct nvme_trans_io_cdb *cdb_info)
-{
-	cdb_info->fua = GET_U8_FROM_CDB(cmd, IO_16_CDB_FUA_OFFSET) &
-					IO_CDB_FUA_MASK;
-	cdb_info->prot_info = GET_U8_FROM_CDB(cmd, IO_16_CDB_WP_OFFSET) &
-					IO_CDB_WP_MASK >> IO_CDB_WP_SHIFT;
-	cdb_info->lba = GET_U64_FROM_CDB(cmd, IO_16_CDB_LBA_OFFSET);
-	cdb_info->xfer_len = GET_U32_FROM_CDB(cmd, IO_16_CDB_TX_LEN_OFFSET);
-}
-
-static inline u32 nvme_trans_io_get_num_cmds(struct sg_io_hdr *hdr,
-					struct nvme_trans_io_cdb *cdb_info,
-					u32 max_blocks)
-{
-	/* If using iovecs, send one nvme command per vector */
-	if (hdr->iovec_count > 0)
-		return hdr->iovec_count;
-	else if (cdb_info->xfer_len > max_blocks)
-		return ((cdb_info->xfer_len - 1) / max_blocks) + 1;
-	else
-		return 1;
-}
-
-static u16 nvme_trans_io_get_control(struct nvme_ns *ns,
-					struct nvme_trans_io_cdb *cdb_info)
-{
-	u16 control = 0;
-
-	/* When Protection information support is added, implement here */
-
-	if (cdb_info->fua > 0)
-		control |= NVME_RW_FUA;
-
-	return control;
-}
-
-static int nvme_trans_do_nvme_io(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-				struct nvme_trans_io_cdb *cdb_info, u8 is_write)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	u32 num_cmds;
-	struct nvme_iod *iod;
-	u64 unit_len;
-	u64 unit_num_blocks;	/* Number of blocks to xfer in each nvme cmd */
-	u32 retcode;
-	u32 i = 0;
-	u64 nvme_offset = 0;
-	void __user *next_mapping_addr;
-	struct nvme_command c;
-	u8 opcode = (is_write ? nvme_cmd_write : nvme_cmd_read);
-	u16 control;
-	u32 max_blocks = queue_max_hw_sectors(ns->queue);
-
-	num_cmds = nvme_trans_io_get_num_cmds(hdr, cdb_info, max_blocks);
-
-	/*
-	 * This loop handles two cases.
-	 * First, when an SGL is used in the form of an iovec list:
-	 *   - Use iov_base as the next mapping address for the nvme command_id
-	 *   - Use iov_len as the data transfer length for the command.
-	 * Second, when we have a single buffer
-	 *   - If larger than max_blocks, split into chunks, offset
-	 *        each nvme command accordingly.
-	 */
-	for (i = 0; i < num_cmds; i++) {
-		memset(&c, 0, sizeof(c));
-		if (hdr->iovec_count > 0) {
-			struct sg_iovec sgl;
-
-			retcode = copy_from_user(&sgl, hdr->dxferp +
-					i * sizeof(struct sg_iovec),
-					sizeof(struct sg_iovec));
-			if (retcode)
-				return -EFAULT;
-			unit_len = sgl.iov_len;
-			unit_num_blocks = unit_len >> ns->lba_shift;
-			next_mapping_addr = sgl.iov_base;
-		} else {
-			unit_num_blocks = min((u64)max_blocks,
-					(cdb_info->xfer_len - nvme_offset));
-			unit_len = unit_num_blocks << ns->lba_shift;
-			next_mapping_addr = hdr->dxferp +
-					((1 << ns->lba_shift) * nvme_offset);
-		}
-
-		c.rw.opcode = opcode;
-		c.rw.nsid = cpu_to_le32(ns->ns_id);
-		c.rw.slba = cpu_to_le64(cdb_info->lba + nvme_offset);
-		c.rw.length = cpu_to_le16(unit_num_blocks - 1);
-		control = nvme_trans_io_get_control(ns, cdb_info);
-		c.rw.control = cpu_to_le16(control);
-
-		iod = nvme_map_user_pages(dev,
-			(is_write) ? DMA_TO_DEVICE : DMA_FROM_DEVICE,
-			(unsigned long)next_mapping_addr, unit_len);
-		if (IS_ERR(iod)) {
-			res = PTR_ERR(iod);
-			goto out;
-		}
-		retcode = nvme_setup_prps(dev, iod, unit_len, GFP_KERNEL);
-		if (retcode != unit_len) {
-			nvme_unmap_user_pages(dev,
-				(is_write) ? DMA_TO_DEVICE : DMA_FROM_DEVICE,
-				iod);
-			nvme_free_iod(dev, iod);
-			res = -ENOMEM;
-			goto out;
-		}
-		c.rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
-		c.rw.prp2 = cpu_to_le64(iod->first_dma);
-
-		nvme_offset += unit_num_blocks;
-
-		nvme_sc = nvme_submit_io_cmd(dev, &c, NULL);
-		if (nvme_sc != NVME_SC_SUCCESS) {
-			nvme_unmap_user_pages(dev,
-				(is_write) ? DMA_TO_DEVICE : DMA_FROM_DEVICE,
-				iod);
-			nvme_free_iod(dev, iod);
-			res = nvme_trans_status_code(hdr, nvme_sc);
-			goto out;
-		}
-		nvme_unmap_user_pages(dev,
-				(is_write) ? DMA_TO_DEVICE : DMA_FROM_DEVICE,
-				iod);
-		nvme_free_iod(dev, iod);
-	}
-	res = nvme_trans_status_code(hdr, NVME_SC_SUCCESS);
-
- out:
-	return res;
-}
-
-
-/* SCSI Command Translation Functions */
-
-static int nvme_trans_io(struct nvme_ns *ns, struct sg_io_hdr *hdr, u8 is_write,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	struct nvme_trans_io_cdb cdb_info;
-	u8 opcode = cmd[0];
-	u64 xfer_bytes;
-	u64 sum_iov_len = 0;
-	struct sg_iovec sgl;
-	int i;
-	size_t not_copied;
-
-	/* Extract Fields from CDB */
-	switch (opcode) {
-	case WRITE_6:
-	case READ_6:
-		nvme_trans_get_io_cdb6(cmd, &cdb_info);
-		break;
-	case WRITE_10:
-	case READ_10:
-		nvme_trans_get_io_cdb10(cmd, &cdb_info);
-		break;
-	case WRITE_12:
-	case READ_12:
-		nvme_trans_get_io_cdb12(cmd, &cdb_info);
-		break;
-	case WRITE_16:
-	case READ_16:
-		nvme_trans_get_io_cdb16(cmd, &cdb_info);
-		break;
-	default:
-		/* Will never really reach here */
-		res = SNTI_INTERNAL_ERROR;
-		goto out;
-	}
-
-	/* Calculate total length of transfer (in bytes) */
-	if (hdr->iovec_count > 0) {
-		for (i = 0; i < hdr->iovec_count; i++) {
-			not_copied = copy_from_user(&sgl, hdr->dxferp +
-						i * sizeof(struct sg_iovec),
-						sizeof(struct sg_iovec));
-			if (not_copied)
-				return -EFAULT;
-			sum_iov_len += sgl.iov_len;
-			/* IO vector sizes should be multiples of block size */
-			if (sgl.iov_len % (1 << ns->lba_shift) != 0) {
-				res = nvme_trans_completion(hdr,
-						SAM_STAT_CHECK_CONDITION,
-						ILLEGAL_REQUEST,
-						SCSI_ASC_INVALID_PARAMETER,
-						SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-				goto out;
-			}
-		}
-	} else {
-		sum_iov_len = hdr->dxfer_len;
-	}
-
-	/* As Per sg ioctl howto, if the lengths differ, use the lower one */
-	xfer_bytes = min(((u64)hdr->dxfer_len), sum_iov_len);
-
-	/* If block count and actual data buffer size dont match, error out */
-	if (xfer_bytes != (cdb_info.xfer_len << ns->lba_shift)) {
-		res = -EINVAL;
-		goto out;
-	}
-
-	/* Check for 0 length transfer - it is not illegal */
-	if (cdb_info.xfer_len == 0)
-		goto out;
-
-	/* Send NVMe IO Command(s) */
-	res = nvme_trans_do_nvme_io(ns, hdr, &cdb_info, is_write);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-
- out:
-	return res;
-}
-
-static int nvme_trans_inquiry(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 evpd;
-	u8 page_code;
-	int alloc_len;
-	u8 *inq_response;
-
-	evpd = GET_INQ_EVPD_BIT(cmd);
-	page_code = GET_INQ_PAGE_CODE(cmd);
-	alloc_len = GET_INQ_ALLOC_LENGTH(cmd);
-
-	inq_response = kmalloc(STANDARD_INQUIRY_LENGTH, GFP_KERNEL);
-	if (inq_response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	if (evpd == 0) {
-		if (page_code == INQ_STANDARD_INQUIRY_PAGE) {
-			res = nvme_trans_standard_inquiry_page(ns, hdr,
-						inq_response, alloc_len);
-		} else {
-			res = nvme_trans_completion(hdr,
-						SAM_STAT_CHECK_CONDITION,
-						ILLEGAL_REQUEST,
-						SCSI_ASC_INVALID_CDB,
-						SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		}
-	} else {
-		switch (page_code) {
-		case VPD_SUPPORTED_PAGES:
-			res = nvme_trans_supported_vpd_pages(ns, hdr,
-						inq_response, alloc_len);
-			break;
-		case VPD_SERIAL_NUMBER:
-			res = nvme_trans_unit_serial_page(ns, hdr, inq_response,
-								alloc_len);
-			break;
-		case VPD_DEVICE_IDENTIFIERS:
-			res = nvme_trans_device_id_page(ns, hdr, inq_response,
-								alloc_len);
-			break;
-		case VPD_EXTENDED_INQUIRY:
-			res = nvme_trans_ext_inq_page(ns, hdr, alloc_len);
-			break;
-		case VPD_BLOCK_DEV_CHARACTERISTICS:
-			res = nvme_trans_bdev_char_page(ns, hdr, alloc_len);
-			break;
-		default:
-			res = nvme_trans_completion(hdr,
-						SAM_STAT_CHECK_CONDITION,
-						ILLEGAL_REQUEST,
-						SCSI_ASC_INVALID_CDB,
-						SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-			break;
-		}
-	}
-	kfree(inq_response);
- out_mem:
-	return res;
-}
-
-static int nvme_trans_log_sense(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u16 alloc_len;
-	u8 sp;
-	u8 pc;
-	u8 page_code;
-
-	sp = GET_U8_FROM_CDB(cmd, LOG_SENSE_CDB_SP_OFFSET);
-	if (sp != LOG_SENSE_CDB_SP_NOT_ENABLED) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	}
-	pc = GET_U8_FROM_CDB(cmd, LOG_SENSE_CDB_PC_OFFSET);
-	page_code = pc & LOG_SENSE_CDB_PAGE_CODE_MASK;
-	pc = (pc & LOG_SENSE_CDB_PC_MASK) >> LOG_SENSE_CDB_PC_SHIFT;
-	if (pc != LOG_SENSE_CDB_PC_CUMULATIVE_VALUES) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	}
-	alloc_len = GET_U16_FROM_CDB(cmd, LOG_SENSE_CDB_ALLOC_LENGTH_OFFSET);
-	switch (page_code) {
-	case LOG_PAGE_SUPPORTED_LOG_PAGES_PAGE:
-		res = nvme_trans_log_supp_pages(ns, hdr, alloc_len);
-		break;
-	case LOG_PAGE_INFORMATIONAL_EXCEPTIONS_PAGE:
-		res = nvme_trans_log_info_exceptions(ns, hdr, alloc_len);
-		break;
-	case LOG_PAGE_TEMPERATURE_PAGE:
-		res = nvme_trans_log_temperature(ns, hdr, alloc_len);
-		break;
-	default:
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		break;
-	}
-
- out:
-	return res;
-}
-
-static int nvme_trans_mode_select(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 cdb10 = 0;
-	u16 parm_list_len;
-	u8 page_format;
-	u8 save_pages;
-
-	page_format = GET_U8_FROM_CDB(cmd, MODE_SELECT_CDB_PAGE_FORMAT_OFFSET);
-	page_format &= MODE_SELECT_CDB_PAGE_FORMAT_MASK;
-
-	save_pages = GET_U8_FROM_CDB(cmd, MODE_SELECT_CDB_SAVE_PAGES_OFFSET);
-	save_pages &= MODE_SELECT_CDB_SAVE_PAGES_MASK;
-
-	if (GET_OPCODE(cmd) == MODE_SELECT) {
-		parm_list_len = GET_U8_FROM_CDB(cmd,
-				MODE_SELECT_6_CDB_PARAM_LIST_LENGTH_OFFSET);
-	} else {
-		parm_list_len = GET_U16_FROM_CDB(cmd,
-				MODE_SELECT_10_CDB_PARAM_LIST_LENGTH_OFFSET);
-		cdb10 = 1;
-	}
-
-	if (parm_list_len != 0) {
-		/*
-		 * According to SPC-4 r24, a paramter list length field of 0
-		 * shall not be considered an error
-		 */
-		res = nvme_trans_modesel_data(ns, hdr, cmd, parm_list_len,
-						page_format, save_pages, cdb10);
-	}
-
-	return res;
-}
-
-static int nvme_trans_mode_sense(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u16 alloc_len;
-	u8 cdb10 = 0;
-	u8 page_code;
-	u8 pc;
-
-	if (GET_OPCODE(cmd) == MODE_SENSE) {
-		alloc_len = GET_U8_FROM_CDB(cmd, MODE_SENSE6_ALLOC_LEN_OFFSET);
-	} else {
-		alloc_len = GET_U16_FROM_CDB(cmd,
-						MODE_SENSE10_ALLOC_LEN_OFFSET);
-		cdb10 = 1;
-	}
-
-	pc = GET_U8_FROM_CDB(cmd, MODE_SENSE_PAGE_CONTROL_OFFSET) &
-						MODE_SENSE_PAGE_CONTROL_MASK;
-	if (pc != MODE_SENSE_PC_CURRENT_VALUES) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	}
-
-	page_code = GET_U8_FROM_CDB(cmd, MODE_SENSE_PAGE_CODE_OFFSET) &
-					MODE_SENSE_PAGE_CODE_MASK;
-	switch (page_code) {
-	case MODE_PAGE_CACHING:
-		res = nvme_trans_mode_page_create(ns, hdr, cmd, alloc_len,
-						cdb10,
-						&nvme_trans_fill_caching_page,
-						MODE_PAGE_CACHING_LEN);
-		break;
-	case MODE_PAGE_CONTROL:
-		res = nvme_trans_mode_page_create(ns, hdr, cmd, alloc_len,
-						cdb10,
-						&nvme_trans_fill_control_page,
-						MODE_PAGE_CONTROL_LEN);
-		break;
-	case MODE_PAGE_POWER_CONDITION:
-		res = nvme_trans_mode_page_create(ns, hdr, cmd, alloc_len,
-						cdb10,
-						&nvme_trans_fill_pow_cnd_page,
-						MODE_PAGE_POW_CND_LEN);
-		break;
-	case MODE_PAGE_INFO_EXCEP:
-		res = nvme_trans_mode_page_create(ns, hdr, cmd, alloc_len,
-						cdb10,
-						&nvme_trans_fill_inf_exc_page,
-						MODE_PAGE_INF_EXC_LEN);
-		break;
-	case MODE_PAGE_RETURN_ALL:
-		res = nvme_trans_mode_page_create(ns, hdr, cmd, alloc_len,
-						cdb10,
-						&nvme_trans_fill_all_pages,
-						MODE_PAGE_ALL_LEN);
-		break;
-	default:
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		break;
-	}
-
- out:
-	return res;
-}
-
-static int nvme_trans_read_capacity(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	u32 alloc_len = READ_CAP_10_RESP_SIZE;
-	u32 resp_size = READ_CAP_10_RESP_SIZE;
-	u32 xfer_len;
-	u8 cdb16;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ns *id_ns;
-	u8 *response;
-
-	cdb16 = IS_READ_CAP_16(cmd);
-	if (cdb16) {
-		alloc_len = GET_READ_CAP_16_ALLOC_LENGTH(cmd);
-		resp_size = READ_CAP_16_RESP_SIZE;
-	}
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-							&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-	/* nvme ns identify */
-	nvme_sc = nvme_identify(dev, ns->ns_id, 0, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_dma;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_dma;
-	}
-	id_ns = mem;
-
-	response = kzalloc(resp_size, GFP_KERNEL);
-	if (response == NULL) {
-		res = -ENOMEM;
-		goto out_dma;
-	}
-	nvme_trans_fill_read_cap(response, id_ns, cdb16);
-
-	xfer_len = min(alloc_len, resp_size);
-	res = nvme_trans_copy_to_user(hdr, response, xfer_len);
-
-	kfree(response);
- out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
- out:
-	return res;
-}
-
-static int nvme_trans_report_luns(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	u32 alloc_len, xfer_len, resp_size;
-	u8 select_report;
-	u8 *response;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ctrl *id_ctrl;
-	u32 ll_length, lun_id;
-	u8 lun_id_offset = REPORT_LUNS_FIRST_LUN_OFFSET;
-	__be32 tmp_len;
-
-	alloc_len = GET_REPORT_LUNS_ALLOC_LENGTH(cmd);
-	select_report = GET_U8_FROM_CDB(cmd, REPORT_LUNS_SR_OFFSET);
-
-	if ((select_report != ALL_LUNS_RETURNED) &&
-	    (select_report != ALL_WELL_KNOWN_LUNS_RETURNED) &&
-	    (select_report != RESTRICTED_LUNS_RETURNED)) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	} else {
-		/* NVMe Controller Identify */
-		mem = dma_alloc_coherent(&dev->pci_dev->dev,
-					sizeof(struct nvme_id_ctrl),
-					&dma_addr, GFP_KERNEL);
-		if (mem == NULL) {
-			res = -ENOMEM;
-			goto out;
-		}
-		nvme_sc = nvme_identify(dev, 0, 1, dma_addr);
-		res = nvme_trans_status_code(hdr, nvme_sc);
-		if (res)
-			goto out_dma;
-		if (nvme_sc) {
-			res = nvme_sc;
-			goto out_dma;
-		}
-		id_ctrl = mem;
-		ll_length = le32_to_cpu(id_ctrl->nn) * LUN_ENTRY_SIZE;
-		resp_size = ll_length + LUN_DATA_HEADER_SIZE;
-
-		if (alloc_len < resp_size) {
-			res = nvme_trans_completion(hdr,
-					SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-			goto out_dma;
-		}
-
-		response = kzalloc(resp_size, GFP_KERNEL);
-		if (response == NULL) {
-			res = -ENOMEM;
-			goto out_dma;
-		}
-
-		/* The first LUN ID will always be 0 per the SAM spec */
-		for (lun_id = 0; lun_id < le32_to_cpu(id_ctrl->nn); lun_id++) {
-			/*
-			 * Set the LUN Id and then increment to the next LUN
-			 * location in the parameter data.
-			 */
-			__be64 tmp_id = cpu_to_be64(lun_id);
-			memcpy(&response[lun_id_offset], &tmp_id, sizeof(u64));
-			lun_id_offset += LUN_ENTRY_SIZE;
-		}
-		tmp_len = cpu_to_be32(ll_length);
-		memcpy(response, &tmp_len, sizeof(u32));
-	}
-
-	xfer_len = min(alloc_len, resp_size);
-	res = nvme_trans_copy_to_user(hdr, response, xfer_len);
-
-	kfree(response);
- out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ctrl), mem,
-			  dma_addr);
- out:
-	return res;
-}
-
-static int nvme_trans_request_sense(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 alloc_len, xfer_len, resp_size;
-	u8 desc_format;
-	u8 *response;
-
-	alloc_len = GET_REQUEST_SENSE_ALLOC_LENGTH(cmd);
-	desc_format = GET_U8_FROM_CDB(cmd, REQUEST_SENSE_DESC_OFFSET);
-	desc_format &= REQUEST_SENSE_DESC_MASK;
-
-	resp_size = ((desc_format) ? (DESC_FMT_SENSE_DATA_SIZE) :
-					(FIXED_FMT_SENSE_DATA_SIZE));
-	response = kzalloc(resp_size, GFP_KERNEL);
-	if (response == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-
-	if (desc_format == DESCRIPTOR_FORMAT_SENSE_DATA_TYPE) {
-		/* Descriptor Format Sense Data */
-		response[0] = DESC_FORMAT_SENSE_DATA;
-		response[1] = NO_SENSE;
-		/* TODO How is LOW POWER CONDITION ON handled? (byte 2) */
-		response[2] = SCSI_ASC_NO_SENSE;
-		response[3] = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		/* SDAT_OVFL = 0 | Additional Sense Length = 0 */
-	} else {
-		/* Fixed Format Sense Data */
-		response[0] = FIXED_SENSE_DATA;
-		/* Byte 1 = Obsolete */
-		response[2] = NO_SENSE; /* FM, EOM, ILI, SDAT_OVFL = 0 */
-		/* Bytes 3-6 - Information - set to zero */
-		response[7] = FIXED_SENSE_DATA_ADD_LENGTH;
-		/* Bytes 8-11 - Cmd Specific Information - set to zero */
-		response[12] = SCSI_ASC_NO_SENSE;
-		response[13] = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		/* Byte 14 = Field Replaceable Unit Code = 0 */
-		/* Bytes 15-17 - SKSV=0; Sense Key Specific = 0 */
-	}
-
-	xfer_len = min(alloc_len, resp_size);
-	res = nvme_trans_copy_to_user(hdr, response, xfer_len);
-
-	kfree(response);
- out:
-	return res;
-}
-
-static int nvme_trans_security_protocol(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr,
-					u8 *cmd)
-{
-	return nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-				ILLEGAL_REQUEST, SCSI_ASC_ILLEGAL_COMMAND,
-				SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-}
-
-static int nvme_trans_start_stop(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_command c;
-	u8 immed, pcmod, pc, no_flush, start;
-
-	immed = GET_U8_FROM_CDB(cmd, START_STOP_UNIT_CDB_IMMED_OFFSET);
-	pcmod = GET_U8_FROM_CDB(cmd, START_STOP_UNIT_CDB_POWER_COND_MOD_OFFSET);
-	pc = GET_U8_FROM_CDB(cmd, START_STOP_UNIT_CDB_POWER_COND_OFFSET);
-	no_flush = GET_U8_FROM_CDB(cmd, START_STOP_UNIT_CDB_NO_FLUSH_OFFSET);
-	start = GET_U8_FROM_CDB(cmd, START_STOP_UNIT_CDB_START_OFFSET);
-
-	immed &= START_STOP_UNIT_CDB_IMMED_MASK;
-	pcmod &= START_STOP_UNIT_CDB_POWER_COND_MOD_MASK;
-	pc = (pc & START_STOP_UNIT_CDB_POWER_COND_MASK) >> NIBBLE_SHIFT;
-	no_flush &= START_STOP_UNIT_CDB_NO_FLUSH_MASK;
-	start &= START_STOP_UNIT_CDB_START_MASK;
-
-	if (immed != 0) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-	} else {
-		if (no_flush == 0) {
-			/* Issue NVME FLUSH command prior to START STOP UNIT */
-			memset(&c, 0, sizeof(c));
-			c.common.opcode = nvme_cmd_flush;
-			c.common.nsid = cpu_to_le32(ns->ns_id);
-
-			nvme_sc = nvme_submit_io_cmd(ns->dev, &c, NULL);
-			res = nvme_trans_status_code(hdr, nvme_sc);
-			if (res)
-				goto out;
-			if (nvme_sc) {
-				res = nvme_sc;
-				goto out;
-			}
-		}
-		/* Setup the expected power state transition */
-		res = nvme_trans_power_state(ns, hdr, pc, pcmod, start);
-	}
-
- out:
-	return res;
-}
-
-static int nvme_trans_synchronize_cache(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = nvme_cmd_flush;
-	c.common.nsid = cpu_to_le32(ns->ns_id);
-
-	nvme_sc = nvme_submit_io_cmd(ns->dev, &c, NULL);
-
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out;
-	if (nvme_sc)
-		res = nvme_sc;
-
- out:
-	return res;
-}
-
-static int nvme_trans_format_unit(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 parm_hdr_len = 0;
-	u8 nvme_pf_code = 0;
-	u8 format_prot_info, long_list, format_data;
-
-	format_prot_info = GET_U8_FROM_CDB(cmd,
-				FORMAT_UNIT_CDB_FORMAT_PROT_INFO_OFFSET);
-	long_list = GET_U8_FROM_CDB(cmd, FORMAT_UNIT_CDB_LONG_LIST_OFFSET);
-	format_data = GET_U8_FROM_CDB(cmd, FORMAT_UNIT_CDB_FORMAT_DATA_OFFSET);
-
-	format_prot_info = (format_prot_info &
-				FORMAT_UNIT_CDB_FORMAT_PROT_INFO_MASK) >>
-				FORMAT_UNIT_CDB_FORMAT_PROT_INFO_SHIFT;
-	long_list &= FORMAT_UNIT_CDB_LONG_LIST_MASK;
-	format_data &= FORMAT_UNIT_CDB_FORMAT_DATA_MASK;
-
-	if (format_data != 0) {
-		if (format_prot_info != 0) {
-			if (long_list == 0)
-				parm_hdr_len = FORMAT_UNIT_SHORT_PARM_LIST_LEN;
-			else
-				parm_hdr_len = FORMAT_UNIT_LONG_PARM_LIST_LEN;
-		}
-	} else if (format_data == 0 && format_prot_info != 0) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	}
-
-	/* Get parm header from data-in/out buffer */
-	/*
-	 * According to the translation spec, the only fields in the parameter
-	 * list we are concerned with are in the header. So allocate only that.
-	 */
-	if (parm_hdr_len > 0) {
-		res = nvme_trans_fmt_get_parm_header(hdr, parm_hdr_len,
-					format_prot_info, &nvme_pf_code);
-		if (res != SNTI_TRANSLATION_SUCCESS)
-			goto out;
-	}
-
-	/* Attempt to activate any previously downloaded firmware image */
-	res = nvme_trans_send_fw_cmd(ns, hdr, nvme_admin_activate_fw, 0, 0, 0);
-
-	/* Determine Block size and count and send format command */
-	res = nvme_trans_fmt_set_blk_size_count(ns, hdr);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-
-	res = nvme_trans_fmt_send_cmd(ns, hdr, nvme_pf_code);
-
- out:
-	return res;
-}
-
-static int nvme_trans_test_unit_ready(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr,
-					u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	struct nvme_dev *dev = ns->dev;
-
-	if (!(readl(&dev->bar->csts) & NVME_CSTS_RDY))
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					    NOT_READY, SCSI_ASC_LUN_NOT_READY,
-					    SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-	else
-		res = nvme_trans_completion(hdr, SAM_STAT_GOOD, NO_SENSE, 0, 0);
-
-	return res;
-}
-
-static int nvme_trans_write_buffer(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u32 buffer_offset, parm_list_length;
-	u8 buffer_id, mode;
-
-	parm_list_length =
-		GET_U24_FROM_CDB(cmd, WRITE_BUFFER_CDB_PARM_LIST_LENGTH_OFFSET);
-	if (parm_list_length % BYTES_TO_DWORDS != 0) {
-		/* NVMe expects Firmware file to be a whole number of DWORDS */
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	}
-	buffer_id = GET_U8_FROM_CDB(cmd, WRITE_BUFFER_CDB_BUFFER_ID_OFFSET);
-	if (buffer_id > NVME_MAX_FIRMWARE_SLOT) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	}
-	mode = GET_U8_FROM_CDB(cmd, WRITE_BUFFER_CDB_MODE_OFFSET) &
-						WRITE_BUFFER_CDB_MODE_MASK;
-	buffer_offset =
-		GET_U24_FROM_CDB(cmd, WRITE_BUFFER_CDB_BUFFER_OFFSET_OFFSET);
-
-	switch (mode) {
-	case DOWNLOAD_SAVE_ACTIVATE:
-		res = nvme_trans_send_fw_cmd(ns, hdr, nvme_admin_download_fw,
-						parm_list_length, buffer_offset,
-						buffer_id);
-		if (res != SNTI_TRANSLATION_SUCCESS)
-			goto out;
-		res = nvme_trans_send_fw_cmd(ns, hdr, nvme_admin_activate_fw,
-						parm_list_length, buffer_offset,
-						buffer_id);
-		break;
-	case DOWNLOAD_SAVE_DEFER_ACTIVATE:
-		res = nvme_trans_send_fw_cmd(ns, hdr, nvme_admin_download_fw,
-						parm_list_length, buffer_offset,
-						buffer_id);
-		break;
-	case ACTIVATE_DEFERRED_MICROCODE:
-		res = nvme_trans_send_fw_cmd(ns, hdr, nvme_admin_activate_fw,
-						parm_list_length, buffer_offset,
-						buffer_id);
-		break;
-	default:
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		break;
-	}
-
- out:
-	return res;
-}
-
-struct scsi_unmap_blk_desc {
-	__be64	slba;
-	__be32	nlb;
-	u32	resv;
-};
-
-struct scsi_unmap_parm_list {
-	__be16	unmap_data_len;
-	__be16	unmap_blk_desc_data_len;
-	u32	resv;
-	struct scsi_unmap_blk_desc desc[0];
-};
-
-static int nvme_trans_unmap(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	struct nvme_dev *dev = ns->dev;
-	struct scsi_unmap_parm_list *plist;
-	struct nvme_dsm_range *range;
-	struct nvme_command c;
-	int i, nvme_sc, res = -ENOMEM;
-	u16 ndesc, list_len;
-	dma_addr_t dma_addr;
-
-	list_len = GET_U16_FROM_CDB(cmd, UNMAP_CDB_PARAM_LIST_LENGTH_OFFSET);
-	if (!list_len)
-		return -EINVAL;
-
-	plist = kmalloc(list_len, GFP_KERNEL);
-	if (!plist)
-		return -ENOMEM;
-
-	res = nvme_trans_copy_from_user(hdr, plist, list_len);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-
-	ndesc = be16_to_cpu(plist->unmap_blk_desc_data_len) >> 4;
-	if (!ndesc || ndesc > 256) {
-		res = -EINVAL;
-		goto out;
-	}
-
-	range = dma_alloc_coherent(&dev->pci_dev->dev, ndesc * sizeof(*range),
-							&dma_addr, GFP_KERNEL);
-	if (!range)
-		goto out;
-
-	for (i = 0; i < ndesc; i++) {
-		range[i].nlb = cpu_to_le32(be32_to_cpu(plist->desc[i].nlb));
-		range[i].slba = cpu_to_le64(be64_to_cpu(plist->desc[i].slba));
-		range[i].cattr = 0;
-	}
-
-	memset(&c, 0, sizeof(c));
-	c.dsm.opcode = nvme_cmd_dsm;
-	c.dsm.nsid = cpu_to_le32(ns->ns_id);
-	c.dsm.prp1 = cpu_to_le64(dma_addr);
-	c.dsm.nr = cpu_to_le32(ndesc - 1);
-	c.dsm.attributes = cpu_to_le32(NVME_DSMGMT_AD);
-
-	nvme_sc = nvme_submit_io_cmd(dev, &c, NULL);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-
-	dma_free_coherent(&dev->pci_dev->dev, ndesc * sizeof(*range),
-							range, dma_addr);
- out:
-	kfree(plist);
-	return res;
-}
-
-static int nvme_scsi_translate(struct nvme_ns *ns, struct sg_io_hdr *hdr)
-{
-	u8 cmd[BLK_MAX_CDB];
-	int retcode;
-	unsigned int opcode;
-
-	if (hdr->cmdp == NULL)
-		return -EMSGSIZE;
-	if (copy_from_user(cmd, hdr->cmdp, hdr->cmd_len))
-		return -EFAULT;
-
-	opcode = cmd[0];
-
-	switch (opcode) {
-	case READ_6:
-	case READ_10:
-	case READ_12:
-	case READ_16:
-		retcode = nvme_trans_io(ns, hdr, 0, cmd);
-		break;
-	case WRITE_6:
-	case WRITE_10:
-	case WRITE_12:
-	case WRITE_16:
-		retcode = nvme_trans_io(ns, hdr, 1, cmd);
-		break;
-	case INQUIRY:
-		retcode = nvme_trans_inquiry(ns, hdr, cmd);
-		break;
-	case LOG_SENSE:
-		retcode = nvme_trans_log_sense(ns, hdr, cmd);
-		break;
-	case MODE_SELECT:
-	case MODE_SELECT_10:
-		retcode = nvme_trans_mode_select(ns, hdr, cmd);
-		break;
-	case MODE_SENSE:
-	case MODE_SENSE_10:
-		retcode = nvme_trans_mode_sense(ns, hdr, cmd);
-		break;
-	case READ_CAPACITY:
-		retcode = nvme_trans_read_capacity(ns, hdr, cmd);
-		break;
-	case SERVICE_ACTION_IN:
-		if (IS_READ_CAP_16(cmd))
-			retcode = nvme_trans_read_capacity(ns, hdr, cmd);
-		else
-			goto out;
-		break;
-	case REPORT_LUNS:
-		retcode = nvme_trans_report_luns(ns, hdr, cmd);
-		break;
-	case REQUEST_SENSE:
-		retcode = nvme_trans_request_sense(ns, hdr, cmd);
-		break;
-	case SECURITY_PROTOCOL_IN:
-	case SECURITY_PROTOCOL_OUT:
-		retcode = nvme_trans_security_protocol(ns, hdr, cmd);
-		break;
-	case START_STOP:
-		retcode = nvme_trans_start_stop(ns, hdr, cmd);
-		break;
-	case SYNCHRONIZE_CACHE:
-		retcode = nvme_trans_synchronize_cache(ns, hdr, cmd);
-		break;
-	case FORMAT_UNIT:
-		retcode = nvme_trans_format_unit(ns, hdr, cmd);
-		break;
-	case TEST_UNIT_READY:
-		retcode = nvme_trans_test_unit_ready(ns, hdr, cmd);
-		break;
-	case WRITE_BUFFER:
-		retcode = nvme_trans_write_buffer(ns, hdr, cmd);
-		break;
-	case UNMAP:
-		retcode = nvme_trans_unmap(ns, hdr, cmd);
-		break;
-	default:
- out:
-		retcode = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-				ILLEGAL_REQUEST, SCSI_ASC_ILLEGAL_COMMAND,
-				SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		break;
-	}
-	return retcode;
-}
-
-int nvme_sg_io(struct nvme_ns *ns, struct sg_io_hdr __user *u_hdr)
-{
-	struct sg_io_hdr hdr;
-	int retcode;
-
-	if (!capable(CAP_SYS_ADMIN))
-		return -EACCES;
-	if (copy_from_user(&hdr, u_hdr, sizeof(hdr)))
-		return -EFAULT;
-	if (hdr.interface_id != 'S')
-		return -EINVAL;
-	if (hdr.cmd_len > BLK_MAX_CDB)
-		return -EINVAL;
-
-	retcode = nvme_scsi_translate(ns, &hdr);
-	if (retcode < 0)
-		return retcode;
-	if (retcode > 0)
-		retcode = SNTI_TRANSLATION_SUCCESS;
-	if (copy_to_user(u_hdr, &hdr, sizeof(sg_io_hdr_t)) > 0)
-		return -EFAULT;
-
-	return retcode;
-}
-
-#ifdef CONFIG_COMPAT
-typedef struct sg_io_hdr32 {
-	compat_int_t interface_id;	/* [i] 'S' for SCSI generic (required) */
-	compat_int_t dxfer_direction;	/* [i] data transfer direction  */
-	unsigned char cmd_len;		/* [i] SCSI command length ( <= 16 bytes) */
-	unsigned char mx_sb_len;		/* [i] max length to write to sbp */
-	unsigned short iovec_count;	/* [i] 0 implies no scatter gather */
-	compat_uint_t dxfer_len;		/* [i] byte count of data transfer */
-	compat_uint_t dxferp;		/* [i], [*io] points to data transfer memory
-					      or scatter gather list */
-	compat_uptr_t cmdp;		/* [i], [*i] points to command to perform */
-	compat_uptr_t sbp;		/* [i], [*o] points to sense_buffer memory */
-	compat_uint_t timeout;		/* [i] MAX_UINT->no timeout (unit: millisec) */
-	compat_uint_t flags;		/* [i] 0 -> default, see SG_FLAG... */
-	compat_int_t pack_id;		/* [i->o] unused internally (normally) */
-	compat_uptr_t usr_ptr;		/* [i->o] unused internally */
-	unsigned char status;		/* [o] scsi status */
-	unsigned char masked_status;	/* [o] shifted, masked scsi status */
-	unsigned char msg_status;		/* [o] messaging level data (optional) */
-	unsigned char sb_len_wr;		/* [o] byte count actually written to sbp */
-	unsigned short host_status;	/* [o] errors from host adapter */
-	unsigned short driver_status;	/* [o] errors from software driver */
-	compat_int_t resid;		/* [o] dxfer_len - actual_transferred */
-	compat_uint_t duration;		/* [o] time taken by cmd (unit: millisec) */
-	compat_uint_t info;		/* [o] auxiliary information */
-} sg_io_hdr32_t;  /* 64 bytes long (on sparc32) */
-
-typedef struct sg_iovec32 {
-	compat_uint_t iov_base;
-	compat_uint_t iov_len;
-} sg_iovec32_t;
-
-static int sg_build_iovec(sg_io_hdr_t __user *sgio, void __user *dxferp, u16 iovec_count)
-{
-	sg_iovec_t __user *iov = (sg_iovec_t __user *) (sgio + 1);
-	sg_iovec32_t __user *iov32 = dxferp;
-	int i;
-
-	for (i = 0; i < iovec_count; i++) {
-		u32 base, len;
-
-		if (get_user(base, &iov32[i].iov_base) ||
-		    get_user(len, &iov32[i].iov_len) ||
-		    put_user(compat_ptr(base), &iov[i].iov_base) ||
-		    put_user(len, &iov[i].iov_len))
-			return -EFAULT;
-	}
-
-	if (put_user(iov, &sgio->dxferp))
-		return -EFAULT;
-	return 0;
-}
-
-int nvme_sg_io32(struct nvme_ns *ns, unsigned long arg)
-{
-	sg_io_hdr32_t __user *sgio32 = (sg_io_hdr32_t __user *)arg;
-	sg_io_hdr_t __user *sgio;
-	u16 iovec_count;
-	u32 data;
-	void __user *dxferp;
-	int err;
-	int interface_id;
-
-	if (get_user(interface_id, &sgio32->interface_id))
-		return -EFAULT;
-	if (interface_id != 'S')
-		return -EINVAL;
-
-	if (get_user(iovec_count, &sgio32->iovec_count))
-		return -EFAULT;
-
-	{
-		void __user *top = compat_alloc_user_space(0);
-		void __user *new = compat_alloc_user_space(sizeof(sg_io_hdr_t) +
-				       (iovec_count * sizeof(sg_iovec_t)));
-		if (new > top)
-			return -EINVAL;
-
-		sgio = new;
-	}
-
-	/* Ok, now construct.  */
-	if (copy_in_user(&sgio->interface_id, &sgio32->interface_id,
-			 (2 * sizeof(int)) +
-			 (2 * sizeof(unsigned char)) +
-			 (1 * sizeof(unsigned short)) +
-			 (1 * sizeof(unsigned int))))
-		return -EFAULT;
-
-	if (get_user(data, &sgio32->dxferp))
-		return -EFAULT;
-	dxferp = compat_ptr(data);
-	if (iovec_count) {
-		if (sg_build_iovec(sgio, dxferp, iovec_count))
-			return -EFAULT;
-	} else {
-		if (put_user(dxferp, &sgio->dxferp))
-			return -EFAULT;
-	}
-
-	{
-		unsigned char __user *cmdp;
-		unsigned char __user *sbp;
-
-		if (get_user(data, &sgio32->cmdp))
-			return -EFAULT;
-		cmdp = compat_ptr(data);
-
-		if (get_user(data, &sgio32->sbp))
-			return -EFAULT;
-		sbp = compat_ptr(data);
-
-		if (put_user(cmdp, &sgio->cmdp) ||
-		    put_user(sbp, &sgio->sbp))
-			return -EFAULT;
-	}
-
-	if (copy_in_user(&sgio->timeout, &sgio32->timeout,
-			 3 * sizeof(int)))
-		return -EFAULT;
-
-	if (get_user(data, &sgio32->usr_ptr))
-		return -EFAULT;
-	if (put_user(compat_ptr(data), &sgio->usr_ptr))
-		return -EFAULT;
-
-	err = nvme_sg_io(ns, sgio);
-	if (err >= 0) {
-		void __user *datap;
-
-		if (copy_in_user(&sgio32->pack_id, &sgio->pack_id,
-				 sizeof(int)) ||
-		    get_user(datap, &sgio->usr_ptr) ||
-		    put_user((u32)(unsigned long)datap,
-			     &sgio32->usr_ptr) ||
-		    copy_in_user(&sgio32->status, &sgio->status,
-				 (4 * sizeof(unsigned char)) +
-				 (2 * sizeof(unsigned short)) +
-				 (3 * sizeof(int))))
-			err = -EFAULT;
-	}
-
-	return err;
-}
-#endif
-
-int nvme_sg_get_version_num(int __user *ip)
-{
-	return put_user(sg_version_num, ip);
-}
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/nvme.h b/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/nvme.h
deleted file mode 100644
index 6488429..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/nvme.h
+++ /dev/null
@@ -1,181 +0,0 @@
-/*
- * Definitions for the NVM Express interface
- * Copyright (c) 2011-2013, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- *
- * You should have received a copy of the GNU General Public License along with
- * this program; if not, write to the Free Software Foundation, Inc., 
- * 51 Franklin St - Fifth Floor, Boston, MA 02110-1301 USA.
- */
-
-#ifndef _LINUX_NVME_H
-#define _LINUX_NVME_H
-
-#if 1
-#define KSID_SUPPORT
-#include "linux_nvme_ioctl.h"
-#else
-#include <uapi/linux/nvme.h>
-#endif
-#include <linux/pci.h>
-#include <linux/miscdevice.h>
-#include <linux/kref.h>
-
-struct nvme_bar {
-	__u64			cap;	/* Controller Capabilities */
-	__u32			vs;	/* Version */
-	__u32			intms;	/* Interrupt Mask Set */
-	__u32			intmc;	/* Interrupt Mask Clear */
-	__u32			cc;	/* Controller Configuration */
-	__u32			rsvd1;	/* Reserved */
-	__u32			csts;	/* Controller Status */
-	__u32			rsvd2;	/* Reserved */
-	__u32			aqa;	/* Admin Queue Attributes */
-	__u64			asq;	/* Admin SQ Base Address */
-	__u64			acq;	/* Admin CQ Base Address */
-};
-
-#define NVME_CAP_MQES(cap)	((cap) & 0xffff)
-#define NVME_CAP_TIMEOUT(cap)	(((cap) >> 24) & 0xff)
-#define NVME_CAP_STRIDE(cap)	(((cap) >> 32) & 0xf)
-#define NVME_CAP_MPSMIN(cap)	(((cap) >> 48) & 0xf)
-
-enum {
-	NVME_CC_ENABLE		= 1 << 0,
-	NVME_CC_CSS_NVM		= 0 << 4,
-	NVME_CC_MPS_SHIFT	= 7,
-	NVME_CC_ARB_RR		= 0 << 11,
-	NVME_CC_ARB_WRRU	= 1 << 11,
-	NVME_CC_ARB_VS		= 7 << 11,
-	NVME_CC_SHN_NONE	= 0 << 14,
-	NVME_CC_SHN_NORMAL	= 1 << 14,
-	NVME_CC_SHN_ABRUPT	= 2 << 14,
-	NVME_CC_SHN_MASK	= 3 << 14,
-	NVME_CC_IOSQES		= 6 << 16,
-	NVME_CC_IOCQES		= 4 << 20,
-	NVME_CSTS_RDY		= 1 << 0,
-	NVME_CSTS_CFS		= 1 << 1,
-	NVME_CSTS_SHST_NORMAL	= 0 << 2,
-	NVME_CSTS_SHST_OCCUR	= 1 << 2,
-	NVME_CSTS_SHST_CMPLT	= 2 << 2,
-	NVME_CSTS_SHST_MASK	= 3 << 2,
-};
-
-#define NVME_VS(major, minor)	(major << 16 | minor)
-
-extern unsigned char io_timeout;
-#define NVME_IO_TIMEOUT	(io_timeout * HZ)
-
-/*
- * Represents an NVM Express device.  Each nvme_dev is a PCI function.
- */
-struct nvme_dev {
-	struct list_head node;
-	struct nvme_queue __rcu **queues;
-	unsigned short __percpu *io_queue;
-	u32 __iomem *dbs;
-	struct pci_dev *pci_dev;
-	struct dma_pool *prp_page_pool;
-	struct dma_pool *prp_small_pool;
-	int instance;
-	unsigned queue_count;
-	unsigned online_queues;
-	unsigned max_qid;
-	int q_depth;
-	u32 db_stride;
-	u32 ctrl_config;
-	struct msix_entry *entry;
-	struct nvme_bar __iomem *bar;
-	struct list_head namespaces;
-	struct kref kref;
-	struct miscdevice miscdev;
-	struct work_struct reset_work;
-	char name[12];
-	char serial[20];
-	char model[40];
-	char firmware_rev[8];
-	u32 max_hw_sectors;
-	u32 stripe_size;
-	u16 oncs;
-	u16 abort_limit;
-	u8 initialized;
-};
-
-/*
- * An NVM Express namespace is equivalent to a SCSI LUN
- */
-struct nvme_ns {
-	struct list_head list;
-
-	struct nvme_dev *dev;
-	struct request_queue *queue;
-	struct gendisk *disk;
-
-	unsigned ns_id;
-	int lba_shift;
-	int ms;
-	u64 mode_select_num_blocks;
-	u32 mode_select_block_len;
-};
-
-/*
- * The nvme_iod describes the data in an I/O, including the list of PRP
- * entries.  You can't see it in this data structure because C doesn't let
- * me express that.  Use nvme_alloc_iod to ensure there's enough space
- * allocated to store the PRP list.
- */
-struct nvme_iod {
-	void *private;		/* For the use of the submitter of the I/O */
-	int npages;		/* In the PRP list. 0 means small pool in use */
-	int offset;		/* Of PRP list */
-	int nents;		/* Used in scatterlist */
-	int length;		/* Of data, in bytes */
-	unsigned long start_time;
-	dma_addr_t first_dma;
-	struct list_head node;
-	struct scatterlist sg[0];
-};
-
-static inline u64 nvme_block_nr(struct nvme_ns *ns, sector_t sector)
-{
-	return (sector >> (ns->lba_shift - 9));
-}
-
-/**
- * nvme_free_iod - frees an nvme_iod
- * @dev: The device that the I/O was submitted to
- * @iod: The memory to free
- */
-void nvme_free_iod(struct nvme_dev *dev, struct nvme_iod *iod);
-
-int nvme_setup_prps(struct nvme_dev *, struct nvme_iod *, int , gfp_t);
-struct nvme_iod *nvme_map_user_pages(struct nvme_dev *dev, int write,
-				unsigned long addr, unsigned length);
-void nvme_unmap_user_pages(struct nvme_dev *dev, int write,
-			struct nvme_iod *iod);
-int nvme_submit_io_cmd(struct nvme_dev *, struct nvme_command *, u32 *);
-int nvme_submit_flush_data(struct nvme_queue *nvmeq, struct nvme_ns *ns);
-int nvme_submit_admin_cmd(struct nvme_dev *, struct nvme_command *,
-							u32 *result);
-int nvme_identify(struct nvme_dev *, unsigned nsid, unsigned cns,
-							dma_addr_t dma_addr);
-int nvme_get_features(struct nvme_dev *dev, unsigned fid, unsigned nsid,
-			dma_addr_t dma_addr, u32 *result);
-int nvme_set_features(struct nvme_dev *dev, unsigned fid, unsigned dword11,
-			dma_addr_t dma_addr, u32 *result);
-
-struct sg_io_hdr;
-
-int nvme_sg_io(struct nvme_ns *ns, struct sg_io_hdr __user *u_hdr);
-int nvme_sg_io32(struct nvme_ns *ns, unsigned long arg);
-int nvme_sg_get_version_num(int __user *ip);
-
-#endif /* _LINUX_NVME_H */
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/re_insmod.sh b/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/re_insmod.sh
deleted file mode 100644
index 955898a..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v2.6.32-centos-6.6/re_insmod.sh
+++ /dev/null
@@ -1,5 +0,0 @@
-#!/bin/bash
-
-sudo rmmod nvme
-
-sudo insmod nvme.ko
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v3.10-depreciated/re_insmod.sh b/PDK/driver/PCIe/kernel_driver/kernel_v3.10-depreciated/re_insmod.sh
old mode 100644
new mode 100755
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-1062-centos-7_7/Makefile.orig b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-1062-centos-7_7/Makefile.orig
new file mode 100644
index 0000000..a287b40
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-1062-centos-7_7/Makefile.orig
@@ -0,0 +1,16 @@
+obj-$(CONFIG_NVME_CORE)			+= nvme-core.o
+obj-$(CONFIG_BLK_DEV_NVME)		+= nvme.o
+obj-$(CONFIG_NVME_FABRICS)		+= nvme-fabrics.o
+obj-$(CONFIG_NVME_RDMA)			+= nvme-rdma.o
+obj-$(CONFIG_NVME_FC)			+= nvme-fc.o
+
+nvme-core-y				:= core.o
+nvme-core-$(CONFIG_BLK_DEV_NVME_SCSI)	+= scsi.o
+
+nvme-y					+= pci.o
+
+nvme-fabrics-y				+= fabrics.o
+
+nvme-rdma-y				+= rdma.o
+
+nvme-fc-y				+= fc.o
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-1062-centos-7_7/core.c b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-1062-centos-7_7/core.c
index 4ceed89..b08a3b8 100644
--- a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-1062-centos-7_7/core.c
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-1062-centos-7_7/core.c
@@ -751,12 +751,6 @@ EXPORT_SYMBOL_GPL(nvme_delete_ctrl_sync);
 
 static int nvme_error_status(struct request *req)
 {
-	if(nvme_req(req)->cmd) {
-		// return status directly if kv command
-		if(is_kv_cmd(nvme_req(req)->cmd->common.opcode))
-			return nvme_req(req)->status;
-	}
-
 	switch (nvme_req(req)->status & 0x7ff) {
 	case NVME_SC_SUCCESS:
 		return 0;
@@ -1357,8 +1351,7 @@ int __nvme_submit_kv_user_cmd(struct request_queue *q, struct nvme_command *cmd,
 		return 0;
 	} else {
 		blk_execute_rq(req->q, disk, req, 0);
-		//It's different in this kernel version, req->errors does not equal to status
-		ret = nvme_req(req)->status;
+		ret = req->errors;
 		if (result)
 			*result = le32_to_cpu(nvme_req(req)->result.u32);
 		if (status)
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-1062-centos-7_7/linux_nvme_ioctl.h b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-1062-centos-7_7/linux_nvme_ioctl.h
index 8edc08e..359f29e 100644
--- a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-1062-centos-7_7/linux_nvme_ioctl.h
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-1062-centos-7_7/linux_nvme_ioctl.h
@@ -88,15 +88,28 @@ struct nvme_passthru_kv_cmd {
 	__u32	cdw3;
 	__u32	cdw4;
 	__u32	cdw5;
-	__u64	data_addr;
-	__u32	data_length;
-	__u32	key_length;
+	struct {
+		__u64	data_addr;
+		union {
+			struct {
+				__u32	data_length;
+				__u8	rsvd2;//Not used
+				__u8	list_key_offset;//List key offset
+				__u16	list_max_keys;//List max keys
+			};
+			struct {
+				__u8    length[3];//Length in 24 bit
+				__u8    rkey[4];//RDMA remote key
+				__u8    type;//SGL Type
+			};//Keyed SGL second half
+		};
+	};//dptr
 	__u32	cdw10;
 	__u32	cdw11;
 	union {
 		struct {
 			__u64 key_addr;
-			__u32 rsvd5;
+			__u32 key_length;
 			__u32 rsvd6;
 		};
 		__u8 key[16];
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-1062-centos-7_7/re_insmod.sh b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-1062-centos-7_7/re_insmod.sh
old mode 100644
new mode 100755
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/Makefile b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/Makefile
deleted file mode 100644
index 13694d2..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/Makefile
+++ /dev/null
@@ -1,8 +0,0 @@
-CONFIG_MODULE_SIG=7
-obj-m   += nvme.o
-nvme-y  := nvme-core.o nvme-scsi.o
-
-all:
-	make -C /lib/modules/`uname -r`/build M=`pwd` modules
-clean:
-	make -C /lib/modules/`uname -r`/build M=`pwd` clean
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/nvme-core.c b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/nvme-core.c
deleted file mode 100644
index 23c3a76..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/nvme-core.c
+++ /dev/null
@@ -1,4006 +0,0 @@
-/*
- * NVM Express device driver
- * Copyright (c) 2011-2014, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- */
-
-#include <linux/bio.h>
-#include <linux/bitops.h>
-#include <linux/blkdev.h>
-#include <linux/cpu.h>
-#include <linux/delay.h>
-#include <linux/errno.h>
-#include <linux/fs.h>
-#include <linux/genhd.h>
-#include <linux/hdreg.h>
-#include <linux/idr.h>
-#include <linux/init.h>
-#include <linux/interrupt.h>
-#include <linux/io.h>
-#include <linux/kdev_t.h>
-#include <linux/kthread.h>
-#include <linux/kernel.h>
-#include <linux/mm.h>
-#include <linux/module.h>
-#include <linux/moduleparam.h>
-#include <linux/pci.h>
-#include <linux/percpu.h>
-#include <linux/poison.h>
-#include <linux/ptrace.h>
-#include <linux/sched.h>
-#include <linux/slab.h>
-#include <linux/types.h>
-#include <scsi/sg.h>
-#include <asm-generic/io-64-nonatomic-lo-hi.h>
-
-#include <trace/events/block.h>
-#include <linux/file.h>
-#include <linux/fdtable.h>
-#include <linux/eventfd.h>
-#include <linux/kref.h>
-#include "nvme.h"
-
-#define NVME_Q_DEPTH		1024
-#define SQ_SIZE(depth)		(depth * sizeof(struct nvme_command))
-#define CQ_SIZE(depth)		(depth * sizeof(struct nvme_completion))
-#define ADMIN_TIMEOUT		(admin_timeout * HZ)
-#define IOD_TIMEOUT		(retry_time * HZ)
-
-static unsigned char admin_timeout = 60;
-module_param(admin_timeout, byte, 0644);
-MODULE_PARM_DESC(admin_timeout, "timeout in seconds for admin commands");
-
-unsigned char nvme_io_timeout = 30;
-module_param_named(io_timeout, nvme_io_timeout, byte, 0644);
-MODULE_PARM_DESC(io_timeout, "timeout in seconds for I/O");
-
-static unsigned char retry_time = 30;
-module_param(retry_time, byte, 0644);
-MODULE_PARM_DESC(retry_time, "time in seconds to retry failed I/O");
-
-static int nvme_major;
-module_param(nvme_major, int, 0);
-
-static int use_threaded_interrupts;
-module_param(use_threaded_interrupts, int, 0);
-
-static DEFINE_SPINLOCK(dev_list_lock);
-static LIST_HEAD(dev_list);
-static struct task_struct *nvme_thread;
-static struct workqueue_struct *nvme_workq;
-static wait_queue_head_t nvme_kthread_wait;
-static struct notifier_block nvme_nb;
-
-static void nvme_reset_failed_dev(struct work_struct *ws);
-
-struct async_cmd_info {
-	struct kthread_work work;
-	struct kthread_worker *worker;
-	u32 result;
-	int status;
-	void *ctx;
-};
-
-/*
- * PORT AIO Support logic from Kvepic.
- */
-// AIO data structures
-static struct kmem_cache        *kaioctx_cachep = 0;
-static struct kmem_cache        *kaiocb_cachep = 0;
-static mempool_t *kaiocb_mempool = 0;
-static mempool_t *kaioctx_mempool = 0;
-
-static __u32 aio_context_id;
-
-#define AIOCTX_MAX 1024
-#define AIOCB_MAX (1024*64)
-
-static __u64 debug_completed  =0;
-static int debug_outstanding  =0;
-
-struct nvme_kaioctx
-{
-	struct nvme_aioctx uctx;
-	struct eventfd_ctx *eventctx;
-	struct list_head kaiocb_list;
-	spinlock_t kaioctx_spinlock;
-	struct kref ref;
-};
-
-static struct nvme_kaioctx **g_kaioctx_tb = NULL;
-static spinlock_t g_kaioctx_tb_spinlock;
-
-struct aio_user_ctx {
-	int nents;
-	int len;
-	struct page ** pages;
-	struct scatterlist *sg;
-	char data[1];
-};
-
-struct nvme_kaiocb
-{
-	struct list_head aiocb_list;
-	struct nvme_aioevent event;
-	struct nvme_completion cqe;
-	int opcode;
-	bool use_meta;
-	struct nvme_iod *iod;
-	struct scatterlist meta_sg;
-	void *meta;
-	char *kv_data;
-	struct aio_user_ctx *user_ctx;
-};
-
-static void remove_kaioctx(struct nvme_kaioctx * ctx)
-{
-	struct nvme_kaiocb *tmpcb;
-	struct list_head *pos, *q;
-	unsigned long flags;
-	if (ctx) {
-		spin_lock_irqsave(&ctx->kaioctx_spinlock, flags);
-		list_for_each_safe(pos, q, &ctx->kaiocb_list){
-			tmpcb = list_entry(pos, struct nvme_kaiocb, aiocb_list);
-			list_del(pos);
-			mempool_free(tmpcb, kaiocb_mempool);
-		}
-		spin_unlock_irqrestore(&ctx->kaioctx_spinlock, flags);
-		eventfd_ctx_put(ctx->eventctx);	
-		mempool_free(ctx, kaioctx_mempool);
-	}
-}
-
-static void cleanup_kaioctx(struct kref *kref) {
-	struct nvme_kaioctx *ctx = container_of(kref, struct nvme_kaioctx, ref);
-	remove_kaioctx(ctx);
-}
-
-static void ref_kaioctx(struct nvme_kaioctx *ctx) {
-	kref_get(&ctx->ref);
-}
-
-static void deref_kaioctx(struct nvme_kaioctx *ctx) {
-	kref_put(&ctx->ref, cleanup_kaioctx);
-}
-
-/* destroy memory pools
-*/
-static void destroy_aio_mempool(void)
-{
-	int i = 0;
-	if (g_kaioctx_tb) {
-		for (i = 0; i < AIOCTX_MAX; i++) {
-			if (g_kaioctx_tb[i]) {
-				remove_kaioctx(g_kaioctx_tb[i]);
-				g_kaioctx_tb[i] = NULL;
-			}
-		}
-		kfree(g_kaioctx_tb);
-		g_kaioctx_tb = NULL;
-	}
-	if (kaiocb_mempool)
-		mempool_destroy(kaiocb_mempool);
-	if (kaioctx_mempool)
-		mempool_destroy(kaioctx_mempool);
-	if (kaioctx_cachep)
-		kmem_cache_destroy(kaioctx_cachep);
-	if (kaiocb_cachep)
-		kmem_cache_destroy(kaiocb_cachep);
-}
-
-/* prepare basic data structures
- * to support aio context and requests
- */
-static int aio_service_init(void)
-{
-	g_kaioctx_tb = (struct nvme_kaioctx **)kmalloc(sizeof(struct nvme_kaioctx *) * AIOCTX_MAX, GFP_KERNEL);
-	if (!g_kaioctx_tb)
-		goto fail;
-	memset(g_kaioctx_tb, 0, sizeof(struct nvme_kaioctx *) * AIOCTX_MAX);
-
-	// slap allocator and memory pool
-	kaioctx_cachep = kmem_cache_create("nvme_kaioctx", sizeof(struct nvme_kaioctx), 0, 0, NULL);
-	if (!kaioctx_cachep)
-		goto fail;
-
-	kaiocb_cachep = kmem_cache_create("nvme_kaiocb", sizeof(struct nvme_kaiocb), 0, 0, NULL);
-	if (!kaiocb_cachep)
-		goto fail;
-
-	kaiocb_mempool = mempool_create_slab_pool(AIOCB_MAX, kaiocb_cachep);
-	if (!kaiocb_mempool)
-		goto fail;
-
-	kaioctx_mempool = mempool_create_slab_pool(AIOCTX_MAX, kaioctx_cachep);
-	if (!kaioctx_mempool)
-		goto fail;
-
-	// global variables
-	// context id 0 is reserved for normal I/O operations (synchronous)
-	aio_context_id = 1;
-	spin_lock_init(&g_kaioctx_tb_spinlock);
-	printk(KERN_DEBUG"nvme-aio: initialized\n");
-	return 0;
-
-fail:
-	destroy_aio_mempool();
-	return -ENOMEM;
-}
-
-/*
- * release memory before exit
- */
-static int aio_service_exit(void)
-{
-	destroy_aio_mempool();
-	printk(KERN_DEBUG"nvme-aio: unloaded\n");
-	return 0;
-}
-
-static struct nvme_kaioctx * find_kaioctx(__u32 ctxid) {
-	struct nvme_kaioctx * tmp = NULL;
-	//unsigned long flags;
-	//spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
-	tmp = g_kaioctx_tb[ctxid];
-	if (tmp) ref_kaioctx(tmp);
-	//spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-	return tmp;
-}
-
-
-/*
- * find an aio context with a given id
- */
-static int  set_aioctx_event(__u32 ctxid, struct nvme_kaiocb * kaiocb)
-{
-	struct nvme_kaioctx *tmp;
-	unsigned long flags;
-	tmp = find_kaioctx(ctxid);
-	if (tmp) {
-		spin_lock_irqsave(&tmp->kaioctx_spinlock, flags);
-		list_add_tail(&kaiocb->aiocb_list, &tmp->kaiocb_list);
-		eventfd_signal(tmp->eventctx, 1);
-#if 0
-		printk("nvme_set_aioctx: success to signal event %d %llu\n", kaiocb->event.ctxid, kaiocb->event.reqid);
-#endif
-		spin_unlock_irqrestore(&tmp->kaioctx_spinlock, flags);
-		deref_kaioctx(tmp);
-		return 0;
-	} else {
-#if 0
-		printk("nvme_set_aioctx: failed to signal event %d.\n", ctxid);
-#endif
-	}
-
-	return -1;
-}
-
-/*
- * delete an aio context
- * it will release any resources allocated for this context
- */
-static int nvme_del_aioctx(struct nvme_aioctx __user *uctx )
-{
-	struct nvme_kaioctx ctx;
-	unsigned long flags;
-	if (copy_from_user(&ctx, uctx, sizeof(struct nvme_aioctx)))
-		return -EFAULT;
-
-	spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
-	if (g_kaioctx_tb[ctx.uctx.ctxid]) {
-		deref_kaioctx(g_kaioctx_tb[ctx.uctx.ctxid]);
-		g_kaioctx_tb[ctx.uctx.ctxid] = NULL;
-	}
-	spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-	return 0;
-}
-
-/*
- * set up an aio context
- * allocate a new context with given parameters and prepare a eventfd_context
- */
-static int nvme_set_aioctx(struct nvme_aioctx __user *uctx )
-{
-	struct nvme_kaioctx *ctx;
-	struct fd efile;
-	struct eventfd_ctx *eventfd_ctx = NULL;
-	unsigned long flags;
-	int ret = 0;
-	int i = 0;
-	if (!capable(CAP_SYS_ADMIN))
-		return -EACCES;
-
-	ctx = mempool_alloc(kaioctx_mempool, GFP_NOIO); //ctx = kmem_cache_zalloc(kaioctx_cachep, GFP_KERNEL);
-	if (!ctx)
-		return -ENOMEM;
-
-	if (copy_from_user(ctx, uctx, sizeof(struct nvme_aioctx)))
-		return -EFAULT;
-
-	efile = fdget(ctx->uctx.eventfd);
-	if (!efile.file) {
-		printk("nvme_set_aioctx: failed to get efile for efd %d.\n", ctx->uctx.eventfd);
-		ret = -EBADF;
-		goto exit;
-	}
-
-	eventfd_ctx = eventfd_ctx_fileget(efile.file);
-	if (IS_ERR(eventfd_ctx)) {
-		printk("nvme_set_aioctx: failed to get eventfd_ctx for efd %d.\n", ctx->uctx.eventfd);
-		ret = PTR_ERR(eventfd_ctx);
-		goto put_efile;
-	}
-	// set context id
-	spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
-	if(g_kaioctx_tb[aio_context_id]) {
-		for(i = 0; i < AIOCTX_MAX; i++) {
-			if(g_kaioctx_tb[i] == NULL) {
-				aio_context_id = i;
-				break;
-			}
-		}
-		if (i >= AIOCTX_MAX) {
-			spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-			printk("nvme_set_aioctx: too many aioctx open.\n");
-			ret = -EMFILE;
-			goto put_event_fd;
-		}
-	}
-	ctx->uctx.ctxid = aio_context_id++;
-	if (aio_context_id == AIOCTX_MAX)
-		aio_context_id = 0;
-	ctx->eventctx = eventfd_ctx;
-	spin_lock_init(&ctx->kaioctx_spinlock);
-	INIT_LIST_HEAD(&ctx->kaiocb_list);
-	kref_init(&ctx->ref);
-	g_kaioctx_tb[ctx->uctx.ctxid] = ctx;
-	spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-
-	if (copy_to_user(&uctx->ctxid, &ctx->uctx.ctxid, sizeof(ctx->uctx.ctxid)))
-	{
-		printk("nvme_set_aioctx: failed to copy context id %d to user.\n", ctx->uctx.ctxid);
-		ret =  -EINVAL;
-		goto cleanup;
-	}
-	eventfd_ctx = NULL;
-	debug_outstanding = 0;
-	debug_completed = 0;
-	fdput(efile);
-	return 0;
-cleanup:
-	spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
-	g_kaioctx_tb[ctx->uctx.ctxid - 1] = NULL;
-	spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-	mempool_free(ctx, kaiocb_mempool);
-put_event_fd:
-	eventfd_ctx_put(eventfd_ctx);	
-put_efile:
-	fdput(efile);
-exit:
-	return ret;
-}
-
-/* get an aiocb, which represents a single I/O request.
-*/
-static struct nvme_kaiocb *get_aiocb(__u64 reqid) {
-	struct nvme_kaiocb *req;
-	req = mempool_alloc(kaiocb_mempool, GFP_NOIO); //ctx = kmem_cache_zalloc(kaioctx_cachep, GFP_KERNEL);
-	if (!req) return 0;
-
-	memset(req, 0, sizeof(*req));
-
-	INIT_LIST_HEAD(&req->aiocb_list);
-
-	req->event.reqid = reqid;
-
-	return req;
-}
-
-/* returns the completed events to users
-*/
-static int nvme_get_aioevents(struct nvme_aioevents __user *uevents)
-{
-	struct list_head *pos, *q;
-	struct nvme_kaiocb *tmp;
-	struct nvme_kaioctx *tmp_ctx;
-	unsigned long flags;
-	LIST_HEAD(tmp_head);
-	__u16 count =0;
-	__u16 nr = 0;
-	__u32 ctxid = 0;
-
-	if (!capable(CAP_SYS_ADMIN))
-		return -EACCES;
-
-	if (get_user(nr, &uevents->nr) < 0) { 	return -EINVAL;    }
-
-	if (nr == 0 || nr > 128) return -EINVAL;
-
-	if (get_user(ctxid, &uevents->ctxid) < 0) { return -EINVAL; }
-
-	tmp_ctx = find_kaioctx(ctxid);
-	if (tmp_ctx) {
-		spin_lock_irqsave(&tmp_ctx->kaioctx_spinlock, flags);
-		list_for_each_safe(pos, q, &tmp_ctx->kaiocb_list){
-			list_del_init(pos);
-			list_add(pos, &tmp_head);
-			count++;
-			if (nr == count) break;
-		}
-		spin_unlock_irqrestore(&tmp_ctx->kaioctx_spinlock, flags);
-		deref_kaioctx(tmp_ctx);
-		count = 0; 
-		list_for_each_safe(pos, q, &tmp_head){
-			list_del(pos);
-			tmp = list_entry(pos, struct nvme_kaiocb, aiocb_list);
-			copy_to_user(&uevents->events[count], &tmp->event, sizeof(struct nvme_aioevent));
-			mempool_free(tmp, kaiocb_mempool);
-			count++;
-		}
-	}
-	if (put_user(count, &uevents->nr)  < 0) { return -EINVAL; }
-
-	return 0;
-}
-
-/*
- * An NVM Express queue.  Each device has at least two (one for admin
- * commands and one for I/O commands).
- */
-struct nvme_queue {
-	struct rcu_head r_head;
-	struct device *q_dmadev;
-	struct nvme_dev *dev;
-	char irqname[24];	/* nvme4294967295-65535\0 */
-	spinlock_t q_lock;
-	struct nvme_command *sq_cmds;
-	volatile struct nvme_completion *cqes;
-	dma_addr_t sq_dma_addr;
-	dma_addr_t cq_dma_addr;
-	wait_queue_head_t sq_full;
-	wait_queue_t sq_cong_wait;
-	struct bio_list sq_cong;
-	struct list_head iod_bio;
-	u32 __iomem *q_db;
-	u16 q_depth;
-	u16 cq_vector;
-	u16 sq_head;
-	u16 sq_tail;
-	u16 cq_head;
-	u16 qid;
-	u8 cq_phase;
-	u8 cqe_seen;
-	u8 q_suspended;
-	cpumask_var_t cpu_mask;
-	struct async_cmd_info cmdinfo;
-	unsigned long cmdid_data[];
-};
-
-static void kv_async_completion(struct nvme_queue *nvmeq, void *ctx, struct nvme_completion *cqe){
-	struct nvme_dev* dev = nvmeq->dev;
-	struct nvme_kaiocb *cmdinfo = (struct nvme_kaiocb *)ctx;
-	cmdinfo->event.result = le32_to_cpu(cqe->result);
-	cmdinfo->event.status = le16_to_cpu(cqe->status) >> 1;
-	if (cmdinfo->kv_data && cmdinfo->user_ctx) {
-	if (is_kv_retrieve_cmd(cmdinfo->opcode) || is_kv_iter_read_cmd(cmdinfo->opcode)) {
-		(void)sg_copy_from_buffer(cmdinfo->user_ctx->sg, cmdinfo->user_ctx->nents,
-				cmdinfo->kv_data, cmdinfo->user_ctx->len);
-			}
-	}
-
-	if (cmdinfo->iod) {
-		nvme_unmap_user_pages(dev, kv_nvme_is_write(cmdinfo->opcode), cmdinfo->iod);
-		nvme_free_iod(dev, cmdinfo->iod);
-	}
-
-	if (cmdinfo->user_ctx) { 
-		int i = 0;
-		for (i = 0; i < cmdinfo->user_ctx->nents; i++)
-			put_page(sg_page(&cmdinfo->user_ctx->sg[i]));
-		kfree(cmdinfo->user_ctx);
-	}
-	if (cmdinfo->kv_data) kfree(cmdinfo->kv_data);
-
-	if (cmdinfo->use_meta) {
-		dma_unmap_sg(&dev->pci_dev->dev, &cmdinfo->meta_sg, 1, DMA_TO_DEVICE);
-		put_page(sg_page(&cmdinfo->meta_sg));
-		if (cmdinfo->meta) {
-			kfree(cmdinfo->meta);
-		}
-	}
-
-	if (set_aioctx_event(cmdinfo->event.ctxid, cmdinfo)) {
-		mempool_free(cmdinfo, kaiocb_mempool);
-	}
-}
-
-/*
- * Check we didin't inadvertently grow the command struct
- */
-static inline void _nvme_check_size(void)
-{
-	BUILD_BUG_ON(sizeof(struct nvme_rw_command) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_create_cq) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_create_sq) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_delete_queue) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_features) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_format_cmd) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_abort_cmd) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_command) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_id_ctrl) != 4096);
-	BUILD_BUG_ON(sizeof(struct nvme_id_ns) != 4096);
-	BUILD_BUG_ON(sizeof(struct nvme_lba_range_type) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_smart_log) != 512);
-}
-
-typedef void (*nvme_completion_fn)(struct nvme_queue *, void *,
-						struct nvme_completion *);
-
-struct nvme_cmd_info {
-	nvme_completion_fn fn;
-	void *ctx;
-	unsigned long timeout;
-	int aborted;
-};
-
-static struct nvme_cmd_info *nvme_cmd_info(struct nvme_queue *nvmeq)
-{
-	return (void *)&nvmeq->cmdid_data[BITS_TO_LONGS(nvmeq->q_depth)];
-}
-
-static unsigned nvme_queue_extra(int depth)
-{
-	return DIV_ROUND_UP(depth, 8) + (depth * sizeof(struct nvme_cmd_info));
-}
-
-/**
- * alloc_cmdid() - Allocate a Command ID
- * @nvmeq: The queue that will be used for this command
- * @ctx: A pointer that will be passed to the handler
- * @handler: The function to call on completion
- *
- * Allocate a Command ID for a queue.  The data passed in will
- * be passed to the completion handler.  This is implemented by using
- * the bottom two bits of the ctx pointer to store the handler ID.
- * Passing in a pointer that's not 4-byte aligned will cause a BUG.
- * We can change this if it becomes a problem.
- *
- * May be called with local interrupts disabled and the q_lock held,
- * or with interrupts enabled and no locks held.
- */
-static int alloc_cmdid(struct nvme_queue *nvmeq, void *ctx,
-				nvme_completion_fn handler, unsigned timeout)
-{
-	int depth = nvmeq->q_depth - 1;
-	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
-	int cmdid;
-
-	do {
-		cmdid = find_first_zero_bit(nvmeq->cmdid_data, depth);
-		if (cmdid >= depth)
-			return -EBUSY;
-	} while (test_and_set_bit(cmdid, nvmeq->cmdid_data));
-
-	info[cmdid].fn = handler;
-	info[cmdid].ctx = ctx;
-	info[cmdid].timeout = jiffies + timeout;
-	info[cmdid].aborted = 0;
-	return cmdid;
-}
-
-static int alloc_cmdid_killable(struct nvme_queue *nvmeq, void *ctx,
-				nvme_completion_fn handler, unsigned timeout)
-{
-	int cmdid;
-	wait_event_killable(nvmeq->sq_full,
-		(cmdid = alloc_cmdid(nvmeq, ctx, handler, timeout)) >= 0);
-	return (cmdid < 0) ? -EINTR : cmdid;
-}
-
-/* Special values must be less than 0x1000 */
-#define CMD_CTX_BASE		((void *)POISON_POINTER_DELTA)
-#define CMD_CTX_CANCELLED	(0x30C + CMD_CTX_BASE)
-#define CMD_CTX_COMPLETED	(0x310 + CMD_CTX_BASE)
-#define CMD_CTX_INVALID		(0x314 + CMD_CTX_BASE)
-#define CMD_CTX_ABORT		(0x318 + CMD_CTX_BASE)
-
-static void special_completion(struct nvme_queue *nvmeq, void *ctx,
-						struct nvme_completion *cqe)
-{
-	if (ctx == CMD_CTX_CANCELLED)
-		return;
-	if (ctx == CMD_CTX_ABORT) {
-		++nvmeq->dev->abort_limit;
-		return;
-	}
-	if (ctx == CMD_CTX_COMPLETED) {
-		dev_warn(nvmeq->q_dmadev,
-				"completed id %d twice on queue %d\n",
-				cqe->command_id, le16_to_cpup(&cqe->sq_id));
-		return;
-	}
-	if (ctx == CMD_CTX_INVALID) {
-		dev_warn(nvmeq->q_dmadev,
-				"invalid id %d completed on queue %d\n",
-				cqe->command_id, le16_to_cpup(&cqe->sq_id));
-		return;
-	}
-
-	dev_warn(nvmeq->q_dmadev, "Unknown special completion %p\n", ctx);
-}
-
-static void async_completion(struct nvme_queue *nvmeq, void *ctx,
-						struct nvme_completion *cqe)
-{
-	struct async_cmd_info *cmdinfo = ctx;
-	cmdinfo->result = le32_to_cpup(&cqe->result);
-	cmdinfo->status = le16_to_cpup(&cqe->status) >> 1;
-	queue_kthread_work(cmdinfo->worker, &cmdinfo->work);
-}
-
-/*
- * Called with local interrupts disabled and the q_lock held.  May not sleep.
- */
-static void *free_cmdid(struct nvme_queue *nvmeq, int cmdid,
-						nvme_completion_fn *fn)
-{
-	void *ctx;
-	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
-
-	if (cmdid >= nvmeq->q_depth || !info[cmdid].fn) {
-		if (fn)
-			*fn = special_completion;
-		return CMD_CTX_INVALID;
-	}
-	if (fn)
-		*fn = info[cmdid].fn;
-	ctx = info[cmdid].ctx;
-	info[cmdid].fn = special_completion;
-	info[cmdid].ctx = CMD_CTX_COMPLETED;
-	clear_bit(cmdid, nvmeq->cmdid_data);
-	wake_up(&nvmeq->sq_full);
-	return ctx;
-}
-
-static void *cancel_cmdid(struct nvme_queue *nvmeq, int cmdid,
-						nvme_completion_fn *fn)
-{
-	void *ctx;
-	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
-	if (fn)
-		*fn = info[cmdid].fn;
-	ctx = info[cmdid].ctx;
-	info[cmdid].fn = special_completion;
-	info[cmdid].ctx = CMD_CTX_CANCELLED;
-	return ctx;
-}
-
-static struct nvme_queue *raw_nvmeq(struct nvme_dev *dev, int qid)
-{
-	return rcu_dereference_raw(dev->queues[qid]);
-}
-
-static struct nvme_queue *get_nvmeq(struct nvme_dev *dev) __acquires(RCU)
-{
-	struct nvme_queue *nvmeq;
-	unsigned queue_id = get_cpu_var(*dev->io_queue);
-
-	rcu_read_lock();
-	nvmeq = rcu_dereference(dev->queues[queue_id]);
-	if (nvmeq)
-		return nvmeq;
-
-	rcu_read_unlock();
-	put_cpu_var(*dev->io_queue);
-	return NULL;
-}
-
-static void put_nvmeq(struct nvme_queue *nvmeq) __releases(RCU)
-{
-	rcu_read_unlock();
-	put_cpu_var(nvmeq->dev->io_queue);
-}
-
-static struct nvme_queue *lock_nvmeq(struct nvme_dev *dev, int q_idx)
-							__acquires(RCU)
-{
-	struct nvme_queue *nvmeq;
-
-	rcu_read_lock();
-	nvmeq = rcu_dereference(dev->queues[q_idx]);
-	if (nvmeq)
-		return nvmeq;
-
-	rcu_read_unlock();
-	return NULL;
-}
-
-static void unlock_nvmeq(struct nvme_queue *nvmeq) __releases(RCU)
-{
-	rcu_read_unlock();
-}
-
-/**
- * nvme_submit_cmd() - Copy a command into a queue and ring the doorbell
- * @nvmeq: The queue to use
- * @cmd: The command to send
- *
- * Safe to use from interrupt context
- */
-static int nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd)
-{
-	unsigned long flags;
-	u16 tail;
-	spin_lock_irqsave(&nvmeq->q_lock, flags);
-	if (nvmeq->q_suspended) {
-		spin_unlock_irqrestore(&nvmeq->q_lock, flags);
-		return -EBUSY;
-	}
-	tail = nvmeq->sq_tail;
-	memcpy(&nvmeq->sq_cmds[tail], cmd, sizeof(*cmd));
-	if (++tail == nvmeq->q_depth)
-		tail = 0;
-	writel(tail, nvmeq->q_db);
-	nvmeq->sq_tail = tail;
-	spin_unlock_irqrestore(&nvmeq->q_lock, flags);
-
-	return 0;
-}
-
-static __le64 **iod_list(struct nvme_iod *iod)
-{
-	return ((void *)iod) + iod->offset;
-}
-
-/*
- * Will slightly overestimate the number of pages needed.  This is OK
- * as it only leads to a small amount of wasted memory for the lifetime of
- * the I/O.
- */
-static int nvme_npages(unsigned size)
-{
-	unsigned nprps = DIV_ROUND_UP(size + PAGE_SIZE, PAGE_SIZE);
-	return DIV_ROUND_UP(8 * nprps, PAGE_SIZE - 8);
-}
-
-static struct nvme_iod *
-nvme_alloc_iod(unsigned nseg, unsigned nbytes, gfp_t gfp)
-{
-	struct nvme_iod *iod = kmalloc(sizeof(struct nvme_iod) +
-				sizeof(__le64 *) * nvme_npages(nbytes) +
-				sizeof(struct scatterlist) * nseg, gfp);
-
-	if (iod) {
-		iod->offset = offsetof(struct nvme_iod, sg[nseg]);
-		iod->npages = -1;
-		iod->length = nbytes;
-		iod->nents = 0;
-		iod->first_dma = 0ULL;
-		iod->start_time = jiffies;
-	}
-
-	return iod;
-}
-
-void nvme_free_iod(struct nvme_dev *dev, struct nvme_iod *iod)
-{
-	const int last_prp = PAGE_SIZE / 8 - 1;
-	int i;
-	__le64 **list = iod_list(iod);
-	dma_addr_t prp_dma = iod->first_dma;
-
-	if (iod->npages == 0)
-		dma_pool_free(dev->prp_small_pool, list[0], prp_dma);
-	for (i = 0; i < iod->npages; i++) {
-		__le64 *prp_list = list[i];
-		dma_addr_t next_prp_dma = le64_to_cpu(prp_list[last_prp]);
-		dma_pool_free(dev->prp_page_pool, prp_list, prp_dma);
-		prp_dma = next_prp_dma;
-	}
-	kfree(iod);
-}
-
-static void nvme_start_io_acct(struct bio *bio)
-{
-	struct gendisk *disk = bio->bi_bdev->bd_disk;
-	if (blk_queue_io_stat(disk->queue)) {
-		const int rw = bio_data_dir(bio);
-		int cpu = part_stat_lock();
-		part_round_stats(cpu, &disk->part0);
-		part_stat_inc(cpu, &disk->part0, ios[rw]);
-		part_stat_add(cpu, &disk->part0, sectors[rw],
-							bio_sectors(bio));
-		part_inc_in_flight(&disk->part0, rw);
-		part_stat_unlock();
-	}
-}
-
-static void nvme_end_io_acct(struct bio *bio, unsigned long start_time)
-{
-	struct gendisk *disk = bio->bi_bdev->bd_disk;
-	if (blk_queue_io_stat(disk->queue)) {
-		const int rw = bio_data_dir(bio);
-		unsigned long duration = jiffies - start_time;
-		int cpu = part_stat_lock();
-		part_stat_add(cpu, &disk->part0, ticks[rw], duration);
-		part_round_stats(cpu, &disk->part0);
-		part_dec_in_flight(&disk->part0, rw);
-		part_stat_unlock();
-	}
-}
-
-static void bio_completion(struct nvme_queue *nvmeq, void *ctx,
-						struct nvme_completion *cqe)
-{
-	struct nvme_iod *iod = ctx;
-	struct bio *bio = iod->private;
-	u16 status = le16_to_cpup(&cqe->status) >> 1;
-	int error = 0;
-
-	if (unlikely(status)) {
-		if (!(status & NVME_SC_DNR ||
-				bio->bi_rw & REQ_FAILFAST_MASK) &&
-				(jiffies - iod->start_time) < IOD_TIMEOUT) {
-			if (!waitqueue_active(&nvmeq->sq_full))
-				add_wait_queue(&nvmeq->sq_full,
-							&nvmeq->sq_cong_wait);
-			list_add_tail(&iod->node, &nvmeq->iod_bio);
-			wake_up(&nvmeq->sq_full);
-			return;
-		}
-		error = -EIO;
-	}
-	if (iod->nents) {
-		dma_unmap_sg(nvmeq->q_dmadev, iod->sg, iod->nents,
-			bio_data_dir(bio) ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
-		nvme_end_io_acct(bio, iod->start_time);
-	}
-	nvme_free_iod(nvmeq->dev, iod);
-
-	trace_block_bio_complete(bdev_get_queue(bio->bi_bdev), bio, error);
-	bio_endio(bio, error);
-}
-
-static int nvme_setup_kv_prps(struct nvme_dev *dev, struct nvme_common_command *cmd,
-		struct nvme_iod *iod, int total_len, gfp_t gfp)
-{
-	struct dma_pool *pool;
-	int length = total_len;
-	struct scatterlist *sg = iod->sg;
-	int dma_len = sg_dma_len(sg);
-	u64 dma_addr = sg_dma_address(sg);
-	int offset = offset_in_page(dma_addr);
-	__le64 *prp_list;
-	__le64 **list = iod_list(iod);
-	dma_addr_t prp_dma;
-	int nprps, i;
-
-	cmd->prp1 = cpu_to_le64(dma_addr);
-	length -= (PAGE_SIZE - offset);
-	if (length <= 0)
-		return total_len;
-
-	dma_len -= (PAGE_SIZE - offset);
-	if (dma_len) {
-		dma_addr += (PAGE_SIZE - offset);
-	} else {
-		sg = sg_next(sg);
-		dma_addr = sg_dma_address(sg);
-		dma_len = sg_dma_len(sg);
-	}
-
-	if (length <= PAGE_SIZE) {
-		cmd->prp2 = cpu_to_le64(dma_addr);
-		return total_len;
-	}
-
-	nprps = DIV_ROUND_UP(length, PAGE_SIZE);
-	if (nprps <= (256 / 8)) {
-		pool = dev->prp_small_pool;
-		iod->npages = 0;
-	} else {
-		pool = dev->prp_page_pool;
-		iod->npages = 1;
-	}
-
-	prp_list = dma_pool_alloc(pool, gfp, &prp_dma);
-	if (!prp_list) {
-		cmd->prp2 = cpu_to_le64(dma_addr);
-		iod->npages = -1;
-		return (total_len - length) + PAGE_SIZE;
-	}
-	list[0] = prp_list;
-	iod->first_dma = prp_dma;
-	cmd->prp2 = cpu_to_le64(prp_dma);
-	i = 0;
-	for (;;) {
-		if (i == PAGE_SIZE / 8) {
-			__le64 *old_prp_list = prp_list;
-			prp_list = dma_pool_alloc(pool, gfp, &prp_dma);
-			if (!prp_list)
-				return total_len - length;
-			list[iod->npages++] = prp_list;
-			prp_list[0] = old_prp_list[i - 1];
-			old_prp_list[i - 1] = cpu_to_le64(prp_dma);
-			i = 1;
-		}
-		prp_list[i++] = cpu_to_le64(dma_addr);
-		dma_len -= PAGE_SIZE;
-		dma_addr += PAGE_SIZE;
-		length -= PAGE_SIZE;
-		if (length <= 0)
-			break;
-		if (dma_len > 0)
-			continue;
-		BUG_ON(dma_len < 0);
-		sg = sg_next(sg);
-		dma_addr = sg_dma_address(sg);
-		dma_len = sg_dma_len(sg);
-	}
-
-	return total_len;
-}
-
-/* length is in bytes.  gfp flags indicates whether we may sleep. */
-int nvme_setup_prps(struct nvme_dev *dev, struct nvme_iod *iod, int total_len,
-								gfp_t gfp)
-{
-	struct dma_pool *pool;
-	int length = total_len;
-	struct scatterlist *sg = iod->sg;
-	int dma_len = sg_dma_len(sg);
-	u64 dma_addr = sg_dma_address(sg);
-	int offset = offset_in_page(dma_addr);
-	__le64 *prp_list;
-	__le64 **list = iod_list(iod);
-	dma_addr_t prp_dma;
-	int nprps, i;
-
-	length -= (PAGE_SIZE - offset);
-	if (length <= 0)
-		return total_len;
-
-	dma_len -= (PAGE_SIZE - offset);
-	if (dma_len) {
-		dma_addr += (PAGE_SIZE - offset);
-	} else {
-		sg = sg_next(sg);
-		dma_addr = sg_dma_address(sg);
-		dma_len = sg_dma_len(sg);
-	}
-
-	if (length <= PAGE_SIZE) {
-		iod->first_dma = dma_addr;
-		return total_len;
-	}
-
-	nprps = DIV_ROUND_UP(length, PAGE_SIZE);
-	if (nprps <= (256 / 8)) {
-		pool = dev->prp_small_pool;
-		iod->npages = 0;
-	} else {
-		pool = dev->prp_page_pool;
-		iod->npages = 1;
-	}
-
-	prp_list = dma_pool_alloc(pool, gfp, &prp_dma);
-	if (!prp_list) {
-		iod->first_dma = dma_addr;
-		iod->npages = -1;
-		return (total_len - length) + PAGE_SIZE;
-	}
-	list[0] = prp_list;
-	iod->first_dma = prp_dma;
-	i = 0;
-	for (;;) {
-		if (i == PAGE_SIZE / 8) {
-			__le64 *old_prp_list = prp_list;
-			prp_list = dma_pool_alloc(pool, gfp, &prp_dma);
-			if (!prp_list)
-				return total_len - length;
-			list[iod->npages++] = prp_list;
-			prp_list[0] = old_prp_list[i - 1];
-			old_prp_list[i - 1] = cpu_to_le64(prp_dma);
-			i = 1;
-		}
-		prp_list[i++] = cpu_to_le64(dma_addr);
-		dma_len -= PAGE_SIZE;
-		dma_addr += PAGE_SIZE;
-		length -= PAGE_SIZE;
-		if (length <= 0)
-			break;
-		if (dma_len > 0)
-			continue;
-		BUG_ON(dma_len < 0);
-		sg = sg_next(sg);
-		dma_addr = sg_dma_address(sg);
-		dma_len = sg_dma_len(sg);
-	}
-
-	return total_len;
-}
-
-struct nvme_bio_pair {
-	struct bio b1, b2, *parent;
-	struct bio_vec *bv1, *bv2;
-	int err;
-	atomic_t cnt;
-};
-
-static void nvme_bio_pair_endio(struct bio *bio, int err)
-{
-	struct nvme_bio_pair *bp = bio->bi_private;
-
-	if (err)
-		bp->err = err;
-
-	if (atomic_dec_and_test(&bp->cnt)) {
-		bio_endio(bp->parent, bp->err);
-		kfree(bp->bv1);
-		kfree(bp->bv2);
-		kfree(bp);
-	}
-}
-
-static struct nvme_bio_pair *nvme_bio_split(struct bio *bio, int idx,
-							int len, int offset)
-{
-	struct nvme_bio_pair *bp;
-
-	BUG_ON(len > bio->bi_size);
-	BUG_ON(idx > bio->bi_vcnt);
-
-	bp = kmalloc(sizeof(*bp), GFP_ATOMIC);
-	if (!bp)
-		return NULL;
-	bp->err = 0;
-
-	bp->b1 = *bio;
-	bp->b2 = *bio;
-
-	bp->b1.bi_size = len;
-	bp->b2.bi_size -= len;
-	bp->b1.bi_vcnt = idx;
-	bp->b2.bi_idx = idx;
-	bp->b2.bi_sector += len >> 9;
-
-	if (offset) {
-		bp->bv1 = kmalloc(bio->bi_max_vecs * sizeof(struct bio_vec),
-								GFP_ATOMIC);
-		if (!bp->bv1)
-			goto split_fail_1;
-
-		bp->bv2 = kmalloc(bio->bi_max_vecs * sizeof(struct bio_vec),
-								GFP_ATOMIC);
-		if (!bp->bv2)
-			goto split_fail_2;
-
-		memcpy(bp->bv1, bio->bi_io_vec,
-			bio->bi_max_vecs * sizeof(struct bio_vec));
-		memcpy(bp->bv2, bio->bi_io_vec,
-			bio->bi_max_vecs * sizeof(struct bio_vec));
-
-		bp->b1.bi_io_vec = bp->bv1;
-		bp->b2.bi_io_vec = bp->bv2;
-		bp->b2.bi_io_vec[idx].bv_offset += offset;
-		bp->b2.bi_io_vec[idx].bv_len -= offset;
-		bp->b1.bi_io_vec[idx].bv_len = offset;
-		bp->b1.bi_vcnt++;
-	} else
-		bp->bv1 = bp->bv2 = NULL;
-
-	bp->b1.bi_private = bp;
-	bp->b2.bi_private = bp;
-
-	bp->b1.bi_end_io = nvme_bio_pair_endio;
-	bp->b2.bi_end_io = nvme_bio_pair_endio;
-
-	bp->parent = bio;
-	atomic_set(&bp->cnt, 2);
-
-	return bp;
-
- split_fail_2:
-	kfree(bp->bv1);
- split_fail_1:
-	kfree(bp);
-	return NULL;
-}
-
-static int nvme_split_and_submit(struct bio *bio, struct nvme_queue *nvmeq,
-						int idx, int len, int offset)
-{
-	struct nvme_bio_pair *bp = nvme_bio_split(bio, idx, len, offset);
-	if (!bp)
-		return -ENOMEM;
-
-	trace_block_split(bdev_get_queue(bio->bi_bdev), bio,
-					bio->bi_sector);
-
-	if (!waitqueue_active(&nvmeq->sq_full))
-		add_wait_queue(&nvmeq->sq_full, &nvmeq->sq_cong_wait);
-	bio_list_add(&nvmeq->sq_cong, &bp->b1);
-	bio_list_add(&nvmeq->sq_cong, &bp->b2);
-	wake_up(&nvmeq->sq_full);
-
-	return 0;
-}
-
-/* NVMe scatterlists require no holes in the virtual address */
-#define BIOVEC_NOT_VIRT_MERGEABLE(vec1, vec2)	((vec2)->bv_offset || \
-			(((vec1)->bv_offset + (vec1)->bv_len) % PAGE_SIZE))
-
-static int nvme_map_bio(struct nvme_queue *nvmeq, struct nvme_iod *iod,
-		struct bio *bio, enum dma_data_direction dma_dir, int psegs)
-{
-	struct bio_vec *bvec, *bvprv = NULL;
-	struct scatterlist *sg = NULL;
-	int i, length = 0, nsegs = 0, split_len = bio->bi_size;
-
-	if (nvmeq->dev->stripe_size)
-		split_len = nvmeq->dev->stripe_size -
-			((bio->bi_sector << 9) & (nvmeq->dev->stripe_size - 1));
-
-	sg_init_table(iod->sg, psegs);
-	bio_for_each_segment(bvec, bio, i) {
-		if (bvprv && BIOVEC_PHYS_MERGEABLE(bvprv, bvec)) {
-			sg->length += bvec->bv_len;
-		} else {
-			if (bvprv && BIOVEC_NOT_VIRT_MERGEABLE(bvprv, bvec))
-				return nvme_split_and_submit(bio, nvmeq, i,
-								length, 0);
-
-			sg = sg ? sg + 1 : iod->sg;
-			sg_set_page(sg, bvec->bv_page, bvec->bv_len,
-							bvec->bv_offset);
-			nsegs++;
-		}
-
-		if (split_len - length < bvec->bv_len)
-			return nvme_split_and_submit(bio, nvmeq, i, split_len,
-							split_len - length);
-		length += bvec->bv_len;
-		bvprv = bvec;
-	}
-	iod->nents = nsegs;
-	sg_mark_end(sg);
-	if (dma_map_sg(nvmeq->q_dmadev, iod->sg, iod->nents, dma_dir) == 0)
-		return -ENOMEM;
-
-	BUG_ON(length != bio->bi_size);
-	return length;
-}
-
-static int nvme_submit_discard(struct nvme_queue *nvmeq, struct nvme_ns *ns,
-		struct bio *bio, struct nvme_iod *iod, int cmdid)
-{
-	struct nvme_dsm_range *range =
-				(struct nvme_dsm_range *)iod_list(iod)[0];
-	struct nvme_command *cmnd = &nvmeq->sq_cmds[nvmeq->sq_tail];
-
-	range->cattr = cpu_to_le32(0);
-	range->nlb = cpu_to_le32(bio->bi_size >> ns->lba_shift);
-	range->slba = cpu_to_le64(nvme_block_nr(ns, bio->bi_sector));
-
-	memset(cmnd, 0, sizeof(*cmnd));
-	cmnd->dsm.opcode = nvme_cmd_dsm;
-	cmnd->dsm.command_id = cmdid;
-	cmnd->dsm.nsid = cpu_to_le32(ns->ns_id);
-	cmnd->dsm.prp1 = cpu_to_le64(iod->first_dma);
-	cmnd->dsm.nr = 0;
-	cmnd->dsm.attributes = cpu_to_le32(NVME_DSMGMT_AD);
-
-	if (++nvmeq->sq_tail == nvmeq->q_depth)
-		nvmeq->sq_tail = 0;
-	writel(nvmeq->sq_tail, nvmeq->q_db);
-
-	return 0;
-}
-
-static int nvme_submit_flush(struct nvme_queue *nvmeq, struct nvme_ns *ns,
-								int cmdid)
-{
-	struct nvme_command *cmnd = &nvmeq->sq_cmds[nvmeq->sq_tail];
-
-	memset(cmnd, 0, sizeof(*cmnd));
-	cmnd->common.opcode = nvme_cmd_flush;
-	cmnd->common.command_id = cmdid;
-	cmnd->common.nsid = cpu_to_le32(ns->ns_id);
-
-	if (++nvmeq->sq_tail == nvmeq->q_depth)
-		nvmeq->sq_tail = 0;
-	writel(nvmeq->sq_tail, nvmeq->q_db);
-
-	return 0;
-}
-
-static int nvme_submit_iod(struct nvme_queue *nvmeq, struct nvme_iod *iod)
-{
-	struct bio *bio = iod->private;
-	struct nvme_ns *ns = bio->bi_bdev->bd_disk->private_data;
-	struct nvme_command *cmnd;
-	int cmdid;
-	u16 control;
-	u32 dsmgmt;
-
-	cmdid = alloc_cmdid(nvmeq, iod, bio_completion, NVME_IO_TIMEOUT);
-	if (unlikely(cmdid < 0))
-		return cmdid;
-
-	if (bio->bi_rw & REQ_DISCARD)
-		return nvme_submit_discard(nvmeq, ns, bio, iod, cmdid);
-	if (bio->bi_rw & REQ_FLUSH)
-		return nvme_submit_flush(nvmeq, ns, cmdid);
-
-	control = 0;
-	if (bio->bi_rw & REQ_FUA)
-		control |= NVME_RW_FUA;
-	if (bio->bi_rw & (REQ_FAILFAST_DEV | REQ_RAHEAD))
-		control |= NVME_RW_LR;
-
-	dsmgmt = 0;
-	if (bio->bi_rw & REQ_RAHEAD)
-		dsmgmt |= NVME_RW_DSM_FREQ_PREFETCH;
-
-	cmnd = &nvmeq->sq_cmds[nvmeq->sq_tail];
-	memset(cmnd, 0, sizeof(*cmnd));
-
-	cmnd->rw.opcode = bio_data_dir(bio) ? nvme_cmd_write : nvme_cmd_read;
-	cmnd->rw.command_id = cmdid;
-	cmnd->rw.nsid = cpu_to_le32(ns->ns_id);
-	cmnd->rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
-	cmnd->rw.prp2 = cpu_to_le64(iod->first_dma);
-	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, bio->bi_sector));
-	cmnd->rw.length =
-		cpu_to_le16((bio->bi_size >> ns->lba_shift) - 1);
-	cmnd->rw.control = cpu_to_le16(control);
-	cmnd->rw.dsmgmt = cpu_to_le32(dsmgmt);
-
-	if (++nvmeq->sq_tail == nvmeq->q_depth)
-		nvmeq->sq_tail = 0;
-	writel(nvmeq->sq_tail, nvmeq->q_db);
-
-	return 0;
-
-}
-
-static int nvme_split_flush_data(struct nvme_queue *nvmeq, struct bio *bio)
-{
-	struct nvme_bio_pair *bp = nvme_bio_split(bio, 0, 0, 0);
-	if (!bp)
-		return -ENOMEM;
-
-	bp->b1.bi_phys_segments = 0;
-	bp->b2.bi_rw &= ~REQ_FLUSH;
-
-	if (!waitqueue_active(&nvmeq->sq_full))
-		add_wait_queue(&nvmeq->sq_full, &nvmeq->sq_cong_wait);
-	bio_list_add(&nvmeq->sq_cong, &bp->b1);
-	bio_list_add(&nvmeq->sq_cong, &bp->b2);
-	wake_up(&nvmeq->sq_full);
-
-	return 0;
-}
-
-/*
- * Called with local interrupts disabled and the q_lock held.  May not sleep.
- */
-static int nvme_submit_bio_queue(struct nvme_queue *nvmeq, struct nvme_ns *ns,
-								struct bio *bio)
-{
-	struct nvme_iod *iod;
-	int psegs = bio_phys_segments(ns->queue, bio);
-	int result;
-
-	if ((bio->bi_rw & REQ_FLUSH) && psegs)
-		return nvme_split_flush_data(nvmeq, bio);
-
-	iod = nvme_alloc_iod(psegs, bio->bi_size, GFP_ATOMIC);
-	if (!iod)
-		return -ENOMEM;
-
-	iod->private = bio;
-	if (bio->bi_rw & REQ_DISCARD) {
-		void *range;
-		/*
-		 * We reuse the small pool to allocate the 16-byte range here
-		 * as it is not worth having a special pool for these or
-		 * additional cases to handle freeing the iod.
-		 */
-		range = dma_pool_alloc(nvmeq->dev->prp_small_pool,
-						GFP_ATOMIC,
-						&iod->first_dma);
-		if (!range) {
-			result = -ENOMEM;
-			goto free_iod;
-		}
-		iod_list(iod)[0] = (__le64 *)range;
-		iod->npages = 0;
-	} else if (psegs) {
-		result = nvme_map_bio(nvmeq, iod, bio,
-			bio_data_dir(bio) ? DMA_TO_DEVICE : DMA_FROM_DEVICE,
-			psegs);
-		if (result <= 0)
-			goto free_iod;
-		if (nvme_setup_prps(nvmeq->dev, iod, result, GFP_ATOMIC) !=
-								result) {
-			result = -ENOMEM;
-			goto free_iod;
-		}
-		nvme_start_io_acct(bio);
-	}
-	if (unlikely(nvme_submit_iod(nvmeq, iod))) {
-		if (!waitqueue_active(&nvmeq->sq_full))
-			add_wait_queue(&nvmeq->sq_full, &nvmeq->sq_cong_wait);
-		list_add_tail(&iod->node, &nvmeq->iod_bio);
-	}
-	return 0;
-
- free_iod:
-	nvme_free_iod(nvmeq->dev, iod);
-	return result;
-}
-
-static int nvme_process_cq(struct nvme_queue *nvmeq)
-{
-	u16 head, phase;
-
-	head = nvmeq->cq_head;
-	phase = nvmeq->cq_phase;
-
-	for (;;) {
-		void *ctx;
-		nvme_completion_fn fn;
-		struct nvme_completion cqe = nvmeq->cqes[head];
-		if ((le16_to_cpu(cqe.status) & 1) != phase)
-			break;
-		nvmeq->sq_head = le16_to_cpu(cqe.sq_head);
-		if (++head == nvmeq->q_depth) {
-			head = 0;
-			phase = !phase;
-		}
-
-		ctx = free_cmdid(nvmeq, cqe.command_id, &fn);
-		fn(nvmeq, ctx, &cqe);
-	}
-
-	/* If the controller ignores the cq head doorbell and continuously
-	 * writes to the queue, it is theoretically possible to wrap around
-	 * the queue twice and mistakenly return IRQ_NONE.  Linux only
-	 * requires that 0.1% of your interrupts are handled, so this isn't
-	 * a big problem.
-	 */
-	if (head == nvmeq->cq_head && phase == nvmeq->cq_phase)
-		return 0;
-
-	writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
-	nvmeq->cq_head = head;
-	nvmeq->cq_phase = phase;
-
-	nvmeq->cqe_seen = 1;
-	return 1;
-}
-
-static void nvme_make_request(struct request_queue *q, struct bio *bio)
-{
-	struct nvme_ns *ns = q->queuedata;
-	struct nvme_queue *nvmeq = get_nvmeq(ns->dev);
-	int result = -EBUSY;
-
-	if (!nvmeq) {
-		bio_endio(bio, -EIO);
-		return;
-	}
-
-	spin_lock_irq(&nvmeq->q_lock);
-	if (!nvmeq->q_suspended && bio_list_empty(&nvmeq->sq_cong))
-		result = nvme_submit_bio_queue(nvmeq, ns, bio);
-	if (unlikely(result)) {
-		if (!waitqueue_active(&nvmeq->sq_full))
-			add_wait_queue(&nvmeq->sq_full, &nvmeq->sq_cong_wait);
-		bio_list_add(&nvmeq->sq_cong, bio);
-	}
-
-	nvme_process_cq(nvmeq);
-	spin_unlock_irq(&nvmeq->q_lock);
-	put_nvmeq(nvmeq);
-}
-
-static irqreturn_t nvme_irq(int irq, void *data)
-{
-	irqreturn_t result;
-	struct nvme_queue *nvmeq = data;
-	spin_lock(&nvmeq->q_lock);
-	nvme_process_cq(nvmeq);
-	result = nvmeq->cqe_seen ? IRQ_HANDLED : IRQ_NONE;
-	nvmeq->cqe_seen = 0;
-	spin_unlock(&nvmeq->q_lock);
-	return result;
-}
-
-static irqreturn_t nvme_irq_check(int irq, void *data)
-{
-	struct nvme_queue *nvmeq = data;
-	struct nvme_completion cqe = nvmeq->cqes[nvmeq->cq_head];
-	if ((le16_to_cpu(cqe.status) & 1) != nvmeq->cq_phase)
-		return IRQ_NONE;
-	return IRQ_WAKE_THREAD;
-}
-
-static void nvme_abort_command(struct nvme_queue *nvmeq, int cmdid)
-{
-	spin_lock_irq(&nvmeq->q_lock);
-	cancel_cmdid(nvmeq, cmdid, NULL);
-	spin_unlock_irq(&nvmeq->q_lock);
-}
-
-struct sync_cmd_info {
-	struct task_struct *task;
-	u32 result;
-	int status;
-};
-
-static void sync_completion(struct nvme_queue *nvmeq, void *ctx,
-						struct nvme_completion *cqe)
-{
-	struct sync_cmd_info *cmdinfo = ctx;
-	cmdinfo->result = le32_to_cpup(&cqe->result);
-	cmdinfo->status = le16_to_cpup(&cqe->status) >> 1;
-	wake_up_process(cmdinfo->task);
-}
-
-/*
- * Returns 0 on success.  If the result is negative, it's a Linux error code;
- * if the result is positive, it's an NVM Express status code
- */
-static int nvme_submit_sync_cmd(struct nvme_dev *dev, int q_idx,
-						struct nvme_command *cmd,
-						u32 *result, unsigned timeout)
-{
-	int cmdid, ret;
-	struct sync_cmd_info cmdinfo;
-	struct nvme_queue *nvmeq;
-
-	nvmeq = lock_nvmeq(dev, q_idx);
-	if (!nvmeq)
-		return -ENODEV;
-
-	cmdinfo.task = current;
-	cmdinfo.status = -EINTR;
-
-	cmdid = alloc_cmdid(nvmeq, &cmdinfo, sync_completion, timeout);
-	if (cmdid < 0) {
-		unlock_nvmeq(nvmeq);
-		return cmdid;
-	}
-	cmd->common.command_id = cmdid;
-
-	set_current_state(TASK_KILLABLE);
-	ret = nvme_submit_cmd(nvmeq, cmd);
-	if (ret) {
-		free_cmdid(nvmeq, cmdid, NULL);
-		unlock_nvmeq(nvmeq);
-		set_current_state(TASK_RUNNING);
-		return ret;
-	}
-	unlock_nvmeq(nvmeq);
-	schedule_timeout(timeout);
-
-	if (cmdinfo.status == -EINTR) {
-		nvmeq = lock_nvmeq(dev, q_idx);
-		if (nvmeq) {
-			nvme_abort_command(nvmeq, cmdid);
-			unlock_nvmeq(nvmeq);
-		}
-		return -EINTR;
-	}
-
-	if (result)
-		*result = cmdinfo.result;
-
-	return cmdinfo.status;
-}
-
-int nvme_submit_async_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
-						struct async_cmd_info *cmdinfo,
-						unsigned timeout)
-{
-	int cmdid;
-
-	cmdid = alloc_cmdid_killable(nvmeq, cmdinfo, async_completion, timeout);
-	if (cmdid < 0)
-		return cmdid;
-	cmdinfo->status = -EINTR;
-	cmd->common.command_id = cmdid;
-	return nvme_submit_cmd(nvmeq, cmd);
-}
-
-int nvme_submit_admin_cmd(struct nvme_dev *dev, struct nvme_command *cmd,
-								u32 *result)
-{
-	return nvme_submit_sync_cmd(dev, 0, cmd, result, ADMIN_TIMEOUT);
-}
-
-int nvme_submit_io_cmd(struct nvme_dev *dev, struct nvme_command *cmd,
-								u32 *result)
-{
-	return nvme_submit_sync_cmd(dev, smp_processor_id() + 1, cmd, result,
-							NVME_IO_TIMEOUT);
-}
-
-int nvme_submit_admin_cmd_async(struct nvme_dev *dev, struct nvme_command *cmd,
-						struct async_cmd_info *cmdinfo)
-{
-	return nvme_submit_async_cmd(raw_nvmeq(dev, 0), cmd, cmdinfo,
-								ADMIN_TIMEOUT);
-}
-
-static int adapter_delete_queue(struct nvme_dev *dev, u8 opcode, u16 id)
-{
-	int status;
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.delete_queue.opcode = opcode;
-	c.delete_queue.qid = cpu_to_le16(id);
-
-	status = nvme_submit_admin_cmd(dev, &c, NULL);
-	if (status)
-		return -EIO;
-	return 0;
-}
-
-static int adapter_alloc_cq(struct nvme_dev *dev, u16 qid,
-						struct nvme_queue *nvmeq)
-{
-	int status;
-	struct nvme_command c;
-	int flags = NVME_QUEUE_PHYS_CONTIG | NVME_CQ_IRQ_ENABLED;
-
-	memset(&c, 0, sizeof(c));
-	c.create_cq.opcode = nvme_admin_create_cq;
-	c.create_cq.prp1 = cpu_to_le64(nvmeq->cq_dma_addr);
-	c.create_cq.cqid = cpu_to_le16(qid);
-	c.create_cq.qsize = cpu_to_le16(nvmeq->q_depth - 1);
-	c.create_cq.cq_flags = cpu_to_le16(flags);
-	c.create_cq.irq_vector = cpu_to_le16(nvmeq->cq_vector);
-
-	status = nvme_submit_admin_cmd(dev, &c, NULL);
-	if (status)
-		return -EIO;
-	return 0;
-}
-
-static int adapter_alloc_sq(struct nvme_dev *dev, u16 qid,
-						struct nvme_queue *nvmeq)
-{
-	int status;
-	struct nvme_command c;
-	int flags = NVME_QUEUE_PHYS_CONTIG | NVME_SQ_PRIO_MEDIUM;
-
-	memset(&c, 0, sizeof(c));
-	c.create_sq.opcode = nvme_admin_create_sq;
-	c.create_sq.prp1 = cpu_to_le64(nvmeq->sq_dma_addr);
-	c.create_sq.sqid = cpu_to_le16(qid);
-	c.create_sq.qsize = cpu_to_le16(nvmeq->q_depth - 1);
-	c.create_sq.sq_flags = cpu_to_le16(flags);
-	c.create_sq.cqid = cpu_to_le16(qid);
-
-	status = nvme_submit_admin_cmd(dev, &c, NULL);
-	if (status)
-		return -EIO;
-	return 0;
-}
-
-static int adapter_delete_cq(struct nvme_dev *dev, u16 cqid)
-{
-	return adapter_delete_queue(dev, nvme_admin_delete_cq, cqid);
-}
-
-static int adapter_delete_sq(struct nvme_dev *dev, u16 sqid)
-{
-	return adapter_delete_queue(dev, nvme_admin_delete_sq, sqid);
-}
-
-int nvme_identify(struct nvme_dev *dev, unsigned nsid, unsigned cns,
-							dma_addr_t dma_addr)
-{
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.identify.opcode = nvme_admin_identify;
-	c.identify.nsid = cpu_to_le32(nsid);
-	c.identify.prp1 = cpu_to_le64(dma_addr);
-	c.identify.cns = cpu_to_le32(cns);
-
-	return nvme_submit_admin_cmd(dev, &c, NULL);
-}
-
-int nvme_get_features(struct nvme_dev *dev, unsigned fid, unsigned nsid,
-					dma_addr_t dma_addr, u32 *result)
-{
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.features.opcode = nvme_admin_get_features;
-	c.features.nsid = cpu_to_le32(nsid);
-	c.features.prp1 = cpu_to_le64(dma_addr);
-	c.features.fid = cpu_to_le32(fid);
-
-	return nvme_submit_admin_cmd(dev, &c, result);
-}
-
-int nvme_set_features(struct nvme_dev *dev, unsigned fid, unsigned dword11,
-					dma_addr_t dma_addr, u32 *result)
-{
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.features.opcode = nvme_admin_set_features;
-	c.features.prp1 = cpu_to_le64(dma_addr);
-	c.features.fid = cpu_to_le32(fid);
-	c.features.dword11 = cpu_to_le32(dword11);
-
-	return nvme_submit_admin_cmd(dev, &c, result);
-}
-
-/**
- * nvme_abort_cmd - Attempt aborting a command
- * @cmdid: Command id of a timed out IO
- * @queue: The queue with timed out IO
- *
- * Schedule controller reset if the command was already aborted once before and
- * still hasn't been returned to the driver, or if this is the admin queue.
- */
-static void nvme_abort_cmd(int cmdid, struct nvme_queue *nvmeq)
-{
-	int a_cmdid;
-	struct nvme_command cmd;
-	struct nvme_dev *dev = nvmeq->dev;
-	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
-	struct nvme_queue *adminq;
-
-	if (!nvmeq->qid || info[cmdid].aborted) {
-		if (work_busy(&dev->reset_work))
-			return;
-		list_del_init(&dev->node);
-		dev_warn(&dev->pci_dev->dev,
-			"I/O %d QID %d timeout, reset controller\n", cmdid,
-								nvmeq->qid);
-		PREPARE_WORK(&dev->reset_work, nvme_reset_failed_dev);
-		queue_work(nvme_workq, &dev->reset_work);
-		return;
-	}
-
-	if (!dev->abort_limit)
-		return;
-
-	adminq = rcu_dereference(dev->queues[0]);
-	a_cmdid = alloc_cmdid(adminq, CMD_CTX_ABORT, special_completion,
-								ADMIN_TIMEOUT);
-	if (a_cmdid < 0)
-		return;
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.abort.opcode = nvme_admin_abort_cmd;
-	cmd.abort.cid = cmdid;
-	cmd.abort.sqid = nvmeq->qid;
-	cmd.abort.command_id = a_cmdid;
-
-	--dev->abort_limit;
-	info[cmdid].aborted = 1;
-	info[cmdid].timeout = jiffies + ADMIN_TIMEOUT;
-
-	dev_warn(nvmeq->q_dmadev, "Aborting I/O %d QID %d\n", cmdid,
-							nvmeq->qid);
-	nvme_submit_cmd(adminq, &cmd);
-}
-
-/**
- * nvme_cancel_ios - Cancel outstanding I/Os
- * @queue: The queue to cancel I/Os on
- * @timeout: True to only cancel I/Os which have timed out
- */
-static void nvme_cancel_ios(struct nvme_queue *nvmeq, bool timeout)
-{
-	int depth = nvmeq->q_depth - 1;
-	struct nvme_cmd_info *info = nvme_cmd_info(nvmeq);
-	unsigned long now = jiffies;
-	int cmdid;
-
-	for_each_set_bit(cmdid, nvmeq->cmdid_data, depth) {
-		void *ctx;
-		nvme_completion_fn fn;
-		static struct nvme_completion cqe = {
-			.status = cpu_to_le16(NVME_SC_ABORT_REQ << 1),
-		};
-
-		if (timeout && !time_after(now, info[cmdid].timeout))
-			continue;
-		if (info[cmdid].ctx == CMD_CTX_CANCELLED)
-			continue;
-		if (timeout && nvmeq->dev->initialized) {
-			nvme_abort_cmd(cmdid, nvmeq);
-			continue;
-		}
-		dev_warn(nvmeq->q_dmadev, "Cancelling I/O %d QID %d\n", cmdid,
-								nvmeq->qid);
-		ctx = cancel_cmdid(nvmeq, cmdid, &fn);
-		fn(nvmeq, ctx, &cqe);
-	}
-}
-
-static void nvme_free_queue(struct rcu_head *r)
-{
-	struct nvme_queue *nvmeq = container_of(r, struct nvme_queue, r_head);
-
-	spin_lock_irq(&nvmeq->q_lock);
-	while (bio_list_peek(&nvmeq->sq_cong)) {
-		struct bio *bio = bio_list_pop(&nvmeq->sq_cong);
-		bio_endio(bio, -EIO);
-	}
-	while (!list_empty(&nvmeq->iod_bio)) {
-		static struct nvme_completion cqe = {
-			.status = cpu_to_le16(
-				(NVME_SC_ABORT_REQ | NVME_SC_DNR) << 1),
-		};
-		struct nvme_iod *iod = list_first_entry(&nvmeq->iod_bio,
-							struct nvme_iod,
-							node);
-		list_del(&iod->node);
-		bio_completion(nvmeq, iod, &cqe);
-	}
-	spin_unlock_irq(&nvmeq->q_lock);
-
-	dma_free_coherent(nvmeq->q_dmadev, CQ_SIZE(nvmeq->q_depth),
-				(void *)nvmeq->cqes, nvmeq->cq_dma_addr);
-	dma_free_coherent(nvmeq->q_dmadev, SQ_SIZE(nvmeq->q_depth),
-					nvmeq->sq_cmds, nvmeq->sq_dma_addr);
-	if (nvmeq->qid)
-		free_cpumask_var(nvmeq->cpu_mask);
-	kfree(nvmeq);
-}
-
-static void nvme_free_queues(struct nvme_dev *dev, int lowest)
-{
-	int i;
-
-	for (i = dev->queue_count - 1; i >= lowest; i--) {
-		struct nvme_queue *nvmeq = raw_nvmeq(dev, i);
-		rcu_assign_pointer(dev->queues[i], NULL);
-		call_rcu(&nvmeq->r_head, nvme_free_queue);
-		dev->queue_count--;
-	}
-}
-
-/**
- * nvme_suspend_queue - put queue into suspended state
- * @nvmeq - queue to suspend
- *
- * Returns 1 if already suspended, 0 otherwise.
- */
-static int nvme_suspend_queue(struct nvme_queue *nvmeq)
-{
-	int vector = nvmeq->dev->entry[nvmeq->cq_vector].vector;
-
-	spin_lock_irq(&nvmeq->q_lock);
-	if (nvmeq->q_suspended) {
-		spin_unlock_irq(&nvmeq->q_lock);
-		return 1;
-	}
-	nvmeq->q_suspended = 1;
-	nvmeq->dev->online_queues--;
-	spin_unlock_irq(&nvmeq->q_lock);
-
-	irq_set_affinity_hint(vector, NULL);
-	free_irq(vector, nvmeq);
-
-	return 0;
-}
-
-static void nvme_clear_queue(struct nvme_queue *nvmeq)
-{
-	spin_lock_irq(&nvmeq->q_lock);
-	nvme_process_cq(nvmeq);
-	nvme_cancel_ios(nvmeq, false);
-	spin_unlock_irq(&nvmeq->q_lock);
-}
-
-static void nvme_disable_queue(struct nvme_dev *dev, int qid)
-{
-	struct nvme_queue *nvmeq = raw_nvmeq(dev, qid);
-
-	if (!nvmeq)
-		return;
-	if (nvme_suspend_queue(nvmeq))
-		return;
-
-	/* Don't tell the adapter to delete the admin queue.
-	 * Don't tell a removed adapter to delete IO queues. */
-	if (qid && readl(&dev->bar->csts) != -1) {
-		adapter_delete_sq(dev, qid);
-		adapter_delete_cq(dev, qid);
-	}
-	nvme_clear_queue(nvmeq);
-}
-
-static struct nvme_queue *nvme_alloc_queue(struct nvme_dev *dev, int qid,
-							int depth, int vector)
-{
-	struct device *dmadev = &dev->pci_dev->dev;
-	unsigned extra = nvme_queue_extra(depth);
-	struct nvme_queue *nvmeq = kzalloc(sizeof(*nvmeq) + extra, GFP_KERNEL);
-	if (!nvmeq)
-		return NULL;
-
-	nvmeq->cqes = dma_alloc_coherent(dmadev, CQ_SIZE(depth),
-					&nvmeq->cq_dma_addr, GFP_KERNEL);
-	if (!nvmeq->cqes)
-		goto free_nvmeq;
-	memset((void *)nvmeq->cqes, 0, CQ_SIZE(depth));
-
-	nvmeq->sq_cmds = dma_alloc_coherent(dmadev, SQ_SIZE(depth),
-					&nvmeq->sq_dma_addr, GFP_KERNEL);
-	if (!nvmeq->sq_cmds)
-		goto free_cqdma;
-
-	if (qid && !zalloc_cpumask_var(&nvmeq->cpu_mask, GFP_KERNEL))
-		goto free_sqdma;
-
-	nvmeq->q_dmadev = dmadev;
-	nvmeq->dev = dev;
-	snprintf(nvmeq->irqname, sizeof(nvmeq->irqname), "nvme%dq%d",
-			dev->instance, qid);
-	spin_lock_init(&nvmeq->q_lock);
-	nvmeq->cq_head = 0;
-	nvmeq->cq_phase = 1;
-	init_waitqueue_head(&nvmeq->sq_full);
-	init_waitqueue_entry(&nvmeq->sq_cong_wait, nvme_thread);
-	bio_list_init(&nvmeq->sq_cong);
-	INIT_LIST_HEAD(&nvmeq->iod_bio);
-	nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
-	nvmeq->q_depth = depth;
-	nvmeq->cq_vector = vector;
-	nvmeq->qid = qid;
-	nvmeq->q_suspended = 1;
-	dev->queue_count++;
-	rcu_assign_pointer(dev->queues[qid], nvmeq);
-
-	return nvmeq;
-
- free_sqdma:
-	dma_free_coherent(dmadev, SQ_SIZE(depth), (void *)nvmeq->sq_cmds,
-							nvmeq->sq_dma_addr);
- free_cqdma:
-	dma_free_coherent(dmadev, CQ_SIZE(depth), (void *)nvmeq->cqes,
-							nvmeq->cq_dma_addr);
- free_nvmeq:
-	kfree(nvmeq);
-	return NULL;
-}
-
-static int queue_request_irq(struct nvme_dev *dev, struct nvme_queue *nvmeq,
-							const char *name)
-{
-	if (use_threaded_interrupts)
-		return request_threaded_irq(dev->entry[nvmeq->cq_vector].vector,
-					nvme_irq_check, nvme_irq, IRQF_SHARED,
-					name, nvmeq);
-	return request_irq(dev->entry[nvmeq->cq_vector].vector, nvme_irq,
-				IRQF_SHARED, name, nvmeq);
-}
-
-static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
-{
-	struct nvme_dev *dev = nvmeq->dev;
-	unsigned extra = nvme_queue_extra(nvmeq->q_depth);
-
-	nvmeq->sq_tail = 0;
-	nvmeq->cq_head = 0;
-	nvmeq->cq_phase = 1;
-	nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
-	memset(nvmeq->cmdid_data, 0, extra);
-	memset((void *)nvmeq->cqes, 0, CQ_SIZE(nvmeq->q_depth));
-	nvme_cancel_ios(nvmeq, false);
-	nvmeq->q_suspended = 0;
-	dev->online_queues++;
-}
-
-static int nvme_create_queue(struct nvme_queue *nvmeq, int qid)
-{
-	struct nvme_dev *dev = nvmeq->dev;
-	int result;
-
-	result = adapter_alloc_cq(dev, qid, nvmeq);
-	if (result < 0)
-		return result;
-
-	result = adapter_alloc_sq(dev, qid, nvmeq);
-	if (result < 0)
-		goto release_cq;
-
-	result = queue_request_irq(dev, nvmeq, nvmeq->irqname);
-	if (result < 0)
-		goto release_sq;
-
-	spin_lock_irq(&nvmeq->q_lock);
-	nvme_init_queue(nvmeq, qid);
-	spin_unlock_irq(&nvmeq->q_lock);
-
-	return result;
-
- release_sq:
-	adapter_delete_sq(dev, qid);
- release_cq:
-	adapter_delete_cq(dev, qid);
-	return result;
-}
-
-static int nvme_wait_ready(struct nvme_dev *dev, u64 cap, bool enabled)
-{
-	unsigned long timeout;
-	u32 bit = enabled ? NVME_CSTS_RDY : 0;
-
-	timeout = ((NVME_CAP_TIMEOUT(cap) + 1) * HZ / 2) + jiffies;
-
-	while ((readl(&dev->bar->csts) & NVME_CSTS_RDY) != bit) {
-		msleep(100);
-		if (fatal_signal_pending(current))
-			return -EINTR;
-		if (time_after(jiffies, timeout)) {
-			dev_err(&dev->pci_dev->dev,
-				"Device not ready; aborting %s\n", enabled ?
-						"initialisation" : "reset");
-			return -ENODEV;
-		}
-	}
-
-	return 0;
-}
-
-/*
- * If the device has been passed off to us in an enabled state, just clear
- * the enabled bit.  The spec says we should set the 'shutdown notification
- * bits', but doing so may cause the device to complete commands to the
- * admin queue ... and we don't know what memory that might be pointing at!
- */
-static int nvme_disable_ctrl(struct nvme_dev *dev, u64 cap)
-{
-	u32 cc = readl(&dev->bar->cc);
-
-	if (cc & NVME_CC_ENABLE)
-		writel(cc & ~NVME_CC_ENABLE, &dev->bar->cc);
-	return nvme_wait_ready(dev, cap, false);
-}
-
-static int nvme_enable_ctrl(struct nvme_dev *dev, u64 cap)
-{
-	return nvme_wait_ready(dev, cap, true);
-}
-
-static int nvme_shutdown_ctrl(struct nvme_dev *dev)
-{
-	unsigned long timeout;
-	u32 cc;
-
-	cc = (readl(&dev->bar->cc) & ~NVME_CC_SHN_MASK) | NVME_CC_SHN_NORMAL;
-	writel(cc, &dev->bar->cc);
-
-	timeout = 2 * HZ + jiffies;
-	while ((readl(&dev->bar->csts) & NVME_CSTS_SHST_MASK) !=
-							NVME_CSTS_SHST_CMPLT) {
-		msleep(100);
-		if (fatal_signal_pending(current))
-			return -EINTR;
-		if (time_after(jiffies, timeout)) {
-			dev_err(&dev->pci_dev->dev,
-				"Device shutdown incomplete; abort shutdown\n");
-			return -ENODEV;
-		}
-	}
-
-	return 0;
-}
-
-static int nvme_configure_admin_queue(struct nvme_dev *dev)
-{
-	int result;
-	u32 aqa;
-	u64 cap = readq(&dev->bar->cap);
-	struct nvme_queue *nvmeq;
-
-	result = nvme_disable_ctrl(dev, cap);
-	if (result < 0)
-		return result;
-
-	nvmeq = raw_nvmeq(dev, 0);
-	if (!nvmeq) {
-		nvmeq = nvme_alloc_queue(dev, 0, 64, 0);
-		if (!nvmeq)
-			return -ENOMEM;
-	}
-
-	aqa = nvmeq->q_depth - 1;
-	aqa |= aqa << 16;
-
-	dev->ctrl_config = NVME_CC_ENABLE | NVME_CC_CSS_NVM;
-	dev->ctrl_config |= (PAGE_SHIFT - 12) << NVME_CC_MPS_SHIFT;
-	dev->ctrl_config |= NVME_CC_ARB_RR | NVME_CC_SHN_NONE;
-	dev->ctrl_config |= NVME_CC_IOSQES | NVME_CC_IOCQES;
-
-	writel(aqa, &dev->bar->aqa);
-	writeq(nvmeq->sq_dma_addr, &dev->bar->asq);
-	writeq(nvmeq->cq_dma_addr, &dev->bar->acq);
-	writel(dev->ctrl_config, &dev->bar->cc);
-
-	result = nvme_enable_ctrl(dev, cap);
-	if (result)
-		return result;
-
-	result = queue_request_irq(dev, nvmeq, nvmeq->irqname);
-	if (result)
-		return result;
-
-	spin_lock_irq(&nvmeq->q_lock);
-	nvme_init_queue(nvmeq, 0);
-	spin_unlock_irq(&nvmeq->q_lock);
-	return result;
-}
-
-struct nvme_iod *nvme_map_user_pages(struct nvme_dev *dev, int write,
-				unsigned long addr, unsigned length)
-{
-	int i, err, count, nents, offset;
-	struct scatterlist *sg;
-	struct page **pages;
-	struct nvme_iod *iod;
-
-	if (addr & 3)
-		return ERR_PTR(-EINVAL);
-	if (!length || length > INT_MAX - PAGE_SIZE)
-		return ERR_PTR(-EINVAL);
-
-	offset = offset_in_page(addr);
-	count = DIV_ROUND_UP(offset + length, PAGE_SIZE);
-	pages = kcalloc(count, sizeof(*pages), GFP_KERNEL);
-	if (!pages)
-		return ERR_PTR(-ENOMEM);
-
-	err = get_user_pages_fast(addr, count, 1, pages);
-	if (err < count) {
-		count = err;
-		err = -EFAULT;
-		goto put_pages;
-	}
-
-	err = -ENOMEM;
-	iod = nvme_alloc_iod(count, length, GFP_KERNEL);
-	if (!iod)
-		goto put_pages;
-
-	sg = iod->sg;
-	sg_init_table(sg, count);
-	for (i = 0; i < count; i++) {
-		sg_set_page(&sg[i], pages[i],
-			    min_t(unsigned, length, PAGE_SIZE - offset),
-			    offset);
-		length -= (PAGE_SIZE - offset);
-		offset = 0;
-	}
-	sg_mark_end(&sg[i - 1]);
-	iod->nents = count;
-
-	nents = dma_map_sg(&dev->pci_dev->dev, sg, count,
-				write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
-	if (!nents)
-		goto free_iod;
-
-	kfree(pages);
-	return iod;
-
- free_iod:
-	kfree(iod);
- put_pages:
-	for (i = 0; i < count; i++)
-		put_page(pages[i]);
-	kfree(pages);
-	return ERR_PTR(err);
-}
-
-void nvme_unmap_user_pages(struct nvme_dev *dev, int write,
-			struct nvme_iod *iod)
-{
-	int i;
-
-	dma_unmap_sg(&dev->pci_dev->dev, iod->sg, iod->nents,
-				write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
-
-	for (i = 0; i < iod->nents; i++)
-		put_page(sg_page(&iod->sg[i]));
-}
-
-static int nvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
-{
-	struct nvme_dev *dev = ns->dev;
-	struct nvme_user_io io;
-	struct nvme_command c;
-	unsigned length, meta_len;
-	int status, i;
-	struct nvme_iod *iod, *meta_iod = NULL;
-	dma_addr_t meta_dma_addr;
-	void *meta, *uninitialized_var(meta_mem);
-
-	if (copy_from_user(&io, uio, sizeof(io)))
-		return -EFAULT;
-	length = (io.nblocks + 1) << ns->lba_shift;
-	meta_len = (io.nblocks + 1) * ns->ms;
-
-	if (meta_len && ((io.metadata & 3) || !io.metadata))
-		return -EINVAL;
-
-	switch (io.opcode) {
-	case nvme_cmd_write:
-	case nvme_cmd_read:
-	case nvme_cmd_compare:
-		iod = nvme_map_user_pages(dev, io.opcode & 1, io.addr, length);
-		break;
-	default:
-		return -EINVAL;
-	}
-
-	if (IS_ERR(iod))
-		return PTR_ERR(iod);
-
-	memset(&c, 0, sizeof(c));
-	c.rw.opcode = io.opcode;
-	c.rw.flags = io.flags;
-	c.rw.nsid = cpu_to_le32(ns->ns_id);
-	c.rw.slba = cpu_to_le64(io.slba);
-	c.rw.length = cpu_to_le16(io.nblocks);
-	c.rw.control = cpu_to_le16(io.control);
-	c.rw.dsmgmt = cpu_to_le32(io.dsmgmt);
-	c.rw.reftag = cpu_to_le32(io.reftag);
-	c.rw.apptag = cpu_to_le16(io.apptag);
-	c.rw.appmask = cpu_to_le16(io.appmask);
-
-	if (meta_len) {
-		meta_iod = nvme_map_user_pages(dev, io.opcode & 1, io.metadata,
-								meta_len);
-		if (IS_ERR(meta_iod)) {
-			status = PTR_ERR(meta_iod);
-			meta_iod = NULL;
-			goto unmap;
-		}
-
-		meta_mem = dma_alloc_coherent(&dev->pci_dev->dev, meta_len,
-						&meta_dma_addr, GFP_KERNEL);
-		if (!meta_mem) {
-			status = -ENOMEM;
-			goto unmap;
-		}
-
-		if (io.opcode & 1) {
-			int meta_offset = 0;
-
-			for (i = 0; i < meta_iod->nents; i++) {
-				meta = kmap_atomic(sg_page(&meta_iod->sg[i])) +
-						meta_iod->sg[i].offset;
-				memcpy(meta_mem + meta_offset, meta,
-						meta_iod->sg[i].length);
-				kunmap_atomic(meta);
-				meta_offset += meta_iod->sg[i].length;
-			}
-		}
-
-		c.rw.metadata = cpu_to_le64(meta_dma_addr);
-	}
-
-	length = nvme_setup_prps(dev, iod, length, GFP_KERNEL);
-	c.rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
-	c.rw.prp2 = cpu_to_le64(iod->first_dma);
-
-	if (length != (io.nblocks + 1) << ns->lba_shift)
-		status = -ENOMEM;
-	else
-		status = nvme_submit_io_cmd(dev, &c, NULL);
-
-	if (meta_len) {
-		if (status == NVME_SC_SUCCESS && !(io.opcode & 1)) {
-			int meta_offset = 0;
-
-			for (i = 0; i < meta_iod->nents; i++) {
-				meta = kmap_atomic(sg_page(&meta_iod->sg[i])) +
-						meta_iod->sg[i].offset;
-				memcpy(meta, meta_mem + meta_offset,
-						meta_iod->sg[i].length);
-				kunmap_atomic(meta);
-				meta_offset += meta_iod->sg[i].length;
-			}
-		}
-
-		dma_free_coherent(&dev->pci_dev->dev, meta_len, meta_mem,
-								meta_dma_addr);
-	}
-
- unmap:
-	nvme_unmap_user_pages(dev, io.opcode & 1, iod);
-	nvme_free_iod(dev, iod);
-
-	if (meta_iod) {
-		nvme_unmap_user_pages(dev, io.opcode & 1, meta_iod);
-		nvme_free_iod(dev, meta_iod);
-	}
-
-	return status;
-}
-
-static int nvme_user_admin_cmd(struct nvme_dev *dev,
-					struct nvme_admin_cmd __user *ucmd)
-{
-	struct nvme_admin_cmd cmd;
-	struct nvme_command c;
-	int status, length;
-	struct nvme_iod *uninitialized_var(iod);
-	unsigned timeout;
-
-	if (!capable(CAP_SYS_ADMIN))
-		return -EACCES;
-	if (copy_from_user(&cmd, ucmd, sizeof(cmd)))
-		return -EFAULT;
-
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = cmd.opcode;
-	c.common.flags = cmd.flags;
-	c.common.nsid = cpu_to_le32(cmd.nsid);
-	c.common.cdw2[0] = cpu_to_le32(cmd.cdw2);
-	c.common.cdw2[1] = cpu_to_le32(cmd.cdw3);
-	c.common.cdw10[0] = cpu_to_le32(cmd.cdw10);
-	c.common.cdw10[1] = cpu_to_le32(cmd.cdw11);
-	c.common.cdw10[2] = cpu_to_le32(cmd.cdw12);
-	c.common.cdw10[3] = cpu_to_le32(cmd.cdw13);
-	c.common.cdw10[4] = cpu_to_le32(cmd.cdw14);
-	c.common.cdw10[5] = cpu_to_le32(cmd.cdw15);
-
-	length = cmd.data_len;
-	if (cmd.data_len) {
-		iod = nvme_map_user_pages(dev, cmd.opcode & 1, cmd.addr,
-								length);
-		if (IS_ERR(iod))
-			return PTR_ERR(iod);
-		length = nvme_setup_prps(dev, iod, length, GFP_KERNEL);
-		c.common.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
-		c.common.prp2 = cpu_to_le64(iod->first_dma);
-	}
-
-	timeout = cmd.timeout_ms ? msecs_to_jiffies(cmd.timeout_ms) :
-								ADMIN_TIMEOUT;
-	if (length != cmd.data_len)
-		status = -ENOMEM;
-	else
-		status = nvme_submit_sync_cmd(dev, 0, &c, &cmd.result, timeout);
-
-	if (cmd.data_len) {
-		nvme_unmap_user_pages(dev, cmd.opcode & 1, iod);
-		nvme_free_iod(dev, iod);
-	}
-
-	if ((status >= 0) && copy_to_user(&ucmd->result, &cmd.result,
-							sizeof(cmd.result)))
-		status = -EFAULT;
-
-	return status;
-}
-
-static int user_addr_npages(int offset, int size)
-{
-	unsigned count = DIV_ROUND_UP(offset + size, PAGE_SIZE);
-	return count;
-}
-
-static struct aio_user_ctx *get_aio_user_ctx(void __user *addr, unsigned len)
-{
-	int offset = offset_in_page(addr);
-	int datalen = len;
-	int num_page = user_addr_npages(offset,len);
-	int size = 0;
-	struct aio_user_ctx  *user_ctx = NULL;
-	int mapped_pages = 0;
-	int i = 0;
-
-	size = sizeof (struct aio_user_ctx) + sizeof(__le64 *) * num_page
-			+ sizeof(struct scatterlist) * num_page -1;
-	/* need to keep user address to map to copy when complete request */
-	user_ctx = (struct aio_user_ctx *)kmalloc(size, GFP_KERNEL);
-	if (!user_ctx)
-		return NULL;
-
-	user_ctx->nents = 0;
-	user_ctx->pages =(struct page **)user_ctx->data;
-	user_ctx->sg = (struct scatterlist *)(user_ctx->data + sizeof(__le64 *) * num_page);
-	mapped_pages = get_user_pages_fast((unsigned long)addr, num_page, 1, user_ctx->pages);
-	if (mapped_pages != num_page) {
-		user_ctx->nents = mapped_pages;
-		goto exit;
-	}
-	user_ctx->nents = num_page;
-	user_ctx->len = datalen;
-	sg_init_table(user_ctx->sg, num_page);
-	for(i = 0; i < num_page; i++) {
-			sg_set_page(&user_ctx->sg[i], user_ctx->pages[i],
-							min_t(unsigned, datalen, PAGE_SIZE - offset), offset);
-			datalen -= (PAGE_SIZE - offset);
-			offset = 0;
-	}
-	sg_mark_end(&user_ctx->sg[i -1]);
-	return user_ctx;
-exit:
-	if (user_ctx) {
-		for (i = 0; i < user_ctx->nents; i++)
-			put_page(user_ctx->pages[i]);
-		kfree(user_ctx);
-	}
-	return NULL;
-}
-
-struct nvme_iod *nvme_map_kernel_pages(struct nvme_dev *dev, int write,
-		unsigned long addr, unsigned length)
-{
-	int i, err, count, nents, offset;
-	struct scatterlist *sg;
-	struct page **pages;
-	struct page *page = NULL;
-	struct nvme_iod *iod;
-	char *src_data = NULL;
-
-	if (addr & 3)
-		return ERR_PTR(-EINVAL);
-	if (!length || length > INT_MAX - PAGE_SIZE)
-		return ERR_PTR(-EINVAL);
-
-	offset = offset_in_page(addr);
-	count = DIV_ROUND_UP(offset + length, PAGE_SIZE);
-	pages = kcalloc(count, sizeof(*pages), GFP_KERNEL);
-	if (!pages)
-		return ERR_PTR(-ENOMEM);
-
-	src_data = (char *)addr;
-	for (i = 0; i < count; i ++) {
-		page = virt_to_page(src_data);
-		get_page(page);
-		pages[i] = page;
-		src_data += PAGE_SIZE;
-	}				
-	iod = nvme_alloc_iod(count, length, GFP_KERNEL);
-	sg = iod->sg;
-	sg_init_table(sg, count);
-	for (i = 0; i < count; i++) {
-		sg_set_page(&sg[i], pages[i],
-				min_t(unsigned, length, PAGE_SIZE - offset),
-				offset);
-		length -= (PAGE_SIZE - offset);
-		offset = 0;
-	}
-	sg_mark_end(&sg[i - 1]);
-	iod->nents = count;
-	err = -ENOMEM;
-	nents = dma_map_sg(&dev->pci_dev->dev, sg, count,
-			write ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
-	if (!nents)
-		goto free_iod;
-
-	kfree(pages);
-	return iod;
-
-free_iod:
-	kfree(iod);
-	for (i = 0; i < count; i++)
-		put_page(pages[i]);
-	kfree(pages);
-	return ERR_PTR(err);
-}
-
-int nvme_submit_async_kv_cmd(struct nvme_queue *nvmeq, struct nvme_command* cmd,
-		u32 *result, unsigned timeout, struct nvme_kaiocb *aiocb) {
-	int cmdid;
-	aiocb->event.status = -EINTR;
-	cmdid = alloc_cmdid_killable(nvmeq, aiocb, kv_async_completion, timeout);
-	if (cmdid < 0)
-		return cmdid;
-	cmd->common.command_id = cmdid;
-	nvme_submit_cmd(nvmeq, cmd);
-	return 0;
-}
-
-#define KV_QUEUE_DAM_ALIGNMENT (0x03)
-static bool check_for_single_phyaddress(void __user* address, unsigned length) {
-	unsigned offset = 0;
-	unsigned count = 0;
-	offset = offset_in_page(address);
-	count = DIV_ROUND_UP(offset + length, PAGE_SIZE);
-	if (count > 1 && ((unsigned long)address & KV_QUEUE_DAM_ALIGNMENT)) {
-		return false;
-	}
-	return true;
-}
-
-int __nvme_submit_kv_user_cmd(struct nvme_ns *ns, struct nvme_command *cmd,
-		struct nvme_passthru_kv_cmd *pthr_cmd,
-		void __user *ubuffer, unsigned bufflen,
-		void __user *meta_buffer, unsigned meta_len,
-		u32 *result, u32 *status, unsigned timeout, bool aio)
-{
-	struct nvme_dev *dev = ns->dev;
-	struct nvme_queue *nvmeq = NULL;
-	struct nvme_kaiocb *aiocb = NULL;
-	struct scatterlist *meta_sg_ptr = NULL;
-	struct scatterlist meta_sg;
-	struct nvme_iod *iod = NULL;
-	unsigned length = 0;
-	int ret = 0;
-	void* meta = NULL;
-	char* kv_data = NULL;
-	bool set_meta = false;
-	bool need_to_copy = false;
-	struct page *p_page = NULL;
-	struct aio_user_ctx * user_ctx = NULL;
-	if (aio) {
-		aiocb = get_aiocb(pthr_cmd->reqid);
-		if (!aiocb)
-			return  -ENOMEM;
-	}
-
-	if (ubuffer && bufflen) {
-		if ((unsigned long)ubuffer & KV_QUEUE_DAM_ALIGNMENT) {
-			int len = DIV_ROUND_UP(bufflen, PAGE_SIZE)*PAGE_SIZE;
-			need_to_copy = true;
-			kv_data = kmalloc(len, GFP_KERNEL);
-			if (kv_data == NULL) {
-				ret = -ENOMEM;
-				goto out;	
-			}
-
-			user_ctx = get_aio_user_ctx(ubuffer, bufflen);
-			if (user_ctx == NULL) {
-				ret = -ENOMEM;
-				goto free_out;
-			}
-			if (is_kv_store_cmd(cmd->common.opcode) || is_kv_append_cmd(cmd->common.opcode)) {
-				(void)sg_copy_to_buffer(user_ctx->sg, user_ctx->nents,
-						kv_data, user_ctx->len);
-			}
-			iod = nvme_map_kernel_pages(dev, kv_nvme_is_write(pthr_cmd->opcode), (unsigned long)kv_data, bufflen);
-		} else {
-			iod = nvme_map_user_pages(dev, kv_nvme_is_write(pthr_cmd->opcode), (unsigned long)ubuffer, bufflen);
-		}
-		if (IS_ERR(iod)) {
-			ret = -ENOMEM;
-			goto free_out;
-		}
-		length = nvme_setup_kv_prps(dev, &cmd->common, iod, bufflen, GFP_KERNEL);
-		if (length != bufflen) {
-			ret = -ENOMEM;
-			goto out_unmap;
-		}
-		if (aio) {
-			aiocb->iod = iod;
-			aiocb->kv_data = kv_data;
-			aiocb->user_ctx = user_ctx;
-		}
-	}
-	if (meta_buffer && meta_len) {
-		int offset = 0, len = 0;
-		if (!check_for_single_phyaddress(meta_buffer, meta_len)) {
-			len = DIV_ROUND_UP(meta_len, 256)*256;
-			meta = kmalloc(len, GFP_KERNEL);
-			if (copy_from_user(meta, meta_buffer, meta_len)) {
-				ret = -EFAULT;
-				goto out;
-			}
-			offset = offset_in_page(meta);
-			p_page = virt_to_page(meta);
-			page_cache_get(p_page);
-		} else {
-			ret = get_user_pages_fast((unsigned long)meta_buffer, 1, 1, &p_page);
-			if (ret < 1) {
-				ret = -ENOMEM;
-				goto out; 
-			}
-			offset = offset_in_page(meta_buffer);
-		}
-		if (aio) {
-			aiocb->use_meta = true;
-			aiocb->meta = meta;
-			meta_sg_ptr = &aiocb->meta_sg;
-		} else {
-			meta_sg_ptr = &meta_sg;
-		}
-		sg_init_table(meta_sg_ptr, 1);
-		sg_set_page(meta_sg_ptr, p_page, meta_len, offset);
-		sg_mark_end(meta_sg_ptr);
-		if (!dma_map_sg(&dev->pci_dev->dev, meta_sg_ptr, 1, DMA_TO_DEVICE)) {
-			ret = -ENOMEM;
-			goto out_unmap;
-		}
-		cmd->kv_store.key_prp = cpu_to_le64(sg_dma_address(meta_sg_ptr)); 
-		set_meta = true;
-	}
-
-	nvmeq = get_nvmeq(dev);
-	/*
-	 * Since nvme_submit_sync_cmd sleeps, we can't keep preemption
-	 * disabled.  We may be preempted at any point, and be rescheduled
-	 * to a different CPU.  That will cause cacheline bouncing, but no
-	 * additional races since q_lock already protects against other CPUs.
-	 */
-	put_nvmeq(nvmeq);
-	if (aio) {
-		aiocb->event.ctxid = pthr_cmd->ctxid;
-		aiocb->event.reqid = pthr_cmd->reqid;
-		aiocb->opcode = cmd->common.opcode;
-		ret = nvme_submit_async_kv_cmd(nvmeq, cmd, result, timeout, aiocb);
-		if (ret < 0) {
-			*status = ret;
-			goto  out_unmap;
-		}
-		return 0;
-	} else {
-		ret = nvme_submit_sync_cmd(dev, smp_processor_id() + 1, cmd, result, timeout);
-		*status = ret;
-	}
-
-    if (need_to_copy) {
-		if ((is_kv_retrieve_cmd(cmd->common.opcode) && !ret) ||
-              (is_kv_iter_read_cmd(cmd->common.opcode) && (!ret || ((ret& 0x00ff) == 0x0093)))) {
-#if 0
-            char *data = kv_data;
-            pr_err("recevied data %c:%c:%c:%c: %c:%c:%c:%c.\n",
-                    data[0], data[1], data[2], data[3],
-                    data[4], data[5], data[6], data[7]);
-#endif
-			(void)sg_copy_from_buffer(user_ctx->sg, user_ctx->nents,
-					kv_data, user_ctx->len);
-        }
-    }
-
-out_unmap:
-	if (iod) {
-		nvme_unmap_user_pages(dev, kv_nvme_is_write(pthr_cmd->opcode), iod);
-		nvme_free_iod(dev, iod);
-	}
-	if (p_page) {
-		if (set_meta)
-			dma_unmap_sg(&dev->pci_dev->dev, meta_sg_ptr, 1, DMA_TO_DEVICE);
-		put_page(sg_page(meta_sg_ptr));
-	}
-free_out:
-	if (user_ctx)  {
-		int i = 0;
-		for (i = 0; i < user_ctx->nents; i++)
-			put_page(sg_page(&user_ctx->sg[i]));
-		kfree(user_ctx);
-	}
-	if (kv_data) kfree(kv_data);
-out:
-	if (aio)
-		mempool_free(aiocb, kaiocb_mempool);
-	return ret;
-}
-
-
-static int nvme_user_kv_cmd(struct nvme_ns *ns, struct nvme_passthru_kv_cmd __user *ucmd, bool aio)
-{
-	struct nvme_passthru_kv_cmd cmd;
-	struct nvme_command c;
-	int status = 0;
-	unsigned timeout = NVME_IO_TIMEOUT;
-	void __user *metadata = NULL;
-	unsigned meta_len = 0;
-	unsigned option = 0;
-	unsigned iter_handle = 0;
-
-	if (copy_from_user(&cmd, ucmd, sizeof(cmd)))
-		return -EFAULT;
-	if (cmd.flags)
-		return -EINVAL;
-	if (!is_kv_cmd(cmd.opcode))
-		return -EINVAL;
-
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = cmd.opcode;
-	c.common.flags = cmd.flags;
-#ifdef KSID_SUPPORT
-	c.common.nsid = cmd.cdw3;
-#else
-	c.common.nsid = cpu_to_le32(cmd.nsid);
-#endif
-	if (cmd.timeout_ms)
-		timeout = msecs_to_jiffies(cmd.timeout_ms);
-
-	switch(cmd.opcode) {
-		case nvme_cmd_kv_store:
-		case nvme_cmd_kv_append:
-			option = cpu_to_le32(cmd.cdw4);
-			c.kv_store.offset = cpu_to_le32(cmd.cdw5);
-			/* validate key length */
-			if (cmd.key_length >  KVCMD_MAX_KEY_SIZE) {
-				cmd.result = KVS_ERR_VALUE;
-				status = -EINVAL;
-				goto exit;
-			}
-			c.kv_store.key_len = cpu_to_le32(cmd.key_length -1); /* key len -1 */
-			c.kv_store.option = (option & 0xff);
-            /* set value size */
-            if (cmd.data_length % 4) {
-                c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2) + 1);
-                c.kv_store.invalid_byte = 4 - (cmd.data_length % 4);
-            } else {
-                c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2));
-            }
-
-			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-				metadata = (void __user*)cmd.key_addr;
-				meta_len = cmd.key_length;
-			} else {
-				memcpy(c.kv_store.key, cmd.key, cmd.key_length);
-			}
-			break;
-		case nvme_cmd_kv_retrieve:
-			option = cpu_to_le32(cmd.cdw4);
-			c.kv_retrieve.offset = cpu_to_le32(cmd.cdw5);
-			/* validate key length */
-			if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-					cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-				cmd.result = KVS_ERR_VALUE;
-				status = -EINVAL;
-				goto exit;
-			}
-			c.kv_retrieve.key_len = cpu_to_le32(cmd.key_length -1); /* key len - 1 */
-			c.kv_retrieve.option = (option & 0xff);
-			c.kv_retrieve.value_len = cpu_to_le32((cmd.data_length >> 2));
-
-			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-				metadata = (void __user*)cmd.key_addr;
-				meta_len = cmd.key_length;
-			} else {
-				memcpy(c.kv_retrieve.key, cmd.key, cmd.key_length);
-			}
-			break;
-		case nvme_cmd_kv_delete:
-			option = cpu_to_le32(cmd.cdw4);
-			/* validate key length */
-			if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-					cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-				cmd.result = KVS_ERR_VALUE;
-				status = -EINVAL;
-				goto exit;
-			}
-			c.kv_delete.key_len = cpu_to_le32(cmd.key_length -1);
-			c.kv_delete.option = (option & 0xff);	
-			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-				metadata = (void __user *)cmd.key_addr;
-				meta_len = cmd.key_length;
-			} else {
-				memcpy(c.kv_delete.key, cmd.key, cmd.key_length);
-			}
-			break;
-		case nvme_cmd_kv_exist:
-			option = cpu_to_le32(cmd.cdw4);
-			/* validate key length */
-			if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-					cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-				cmd.result = KVS_ERR_VALUE;
-				status = -EINVAL;
-				goto exit;
-			}
-			c.kv_exist.key_len = cpu_to_le32(cmd.key_length -1);
-			c.kv_exist.option = (option & 0xff);	
-			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-				metadata = (void __user *)cmd.key_addr;
-				meta_len = cmd.key_length;
-			} else {
-				memcpy(c.kv_exist.key, cmd.key, cmd.key_length);
-			}
-			break;
-
-		case nvme_cmd_kv_iter_req:
-			option = cpu_to_le32(cmd.cdw4);
-			iter_handle = cpu_to_le32(cmd.cdw5);
-			c.kv_iter_req.iter_handle = iter_handle & 0xff;
-			c.kv_iter_req.option = option & 0xff;
-			c.kv_iter_req.iter_val = cpu_to_le32(cmd.cdw12);
-			c.kv_iter_req.iter_bitmask = cpu_to_le32(cmd.cdw13);
-			break;
-		case nvme_cmd_kv_iter_read:
-			option = cpu_to_le32(cmd.cdw4);
-			iter_handle = cpu_to_le32(cmd.cdw5);
-			c.kv_iter_read.iter_handle = iter_handle & 0xff;
-			c.kv_iter_read.option = option & 0xff;
-			c.kv_iter_read.value_len = cpu_to_le32((cmd.data_length >> 2));
-			break;
-		default:
-			cmd.result = KVS_ERR_IO;
-			status = -EINVAL;
-			goto exit;
-	}
-
-	status = __nvme_submit_kv_user_cmd(ns, &c, &cmd,
-			(void __user *)(uintptr_t)cmd.data_addr, cmd.data_length, metadata, meta_len,
-			&cmd.result, &cmd.status, timeout, aio);
-exit:
-	if (!aio) {
-		if (put_user(cmd.result, &ucmd->result))
-			return -EFAULT;
-		if (put_user(cmd.status, &ucmd->status))
-			return -EFAULT;
-	}
-	return status;
-}
-
-static int nvme_ioctl(struct block_device *bdev, fmode_t mode, unsigned int cmd,
-							unsigned long arg)
-{
-	struct nvme_ns *ns = bdev->bd_disk->private_data;
-
-	switch (cmd) {
-	case NVME_IOCTL_ID:
-		force_successful_syscall_return();
-		return ns->ns_id;
-	case NVME_IOCTL_ADMIN_CMD:
-		return nvme_user_admin_cmd(ns->dev, (void __user *)arg);
-	case NVME_IOCTL_SUBMIT_IO:
-		return nvme_submit_io(ns, (void __user *)arg);
-	case SG_GET_VERSION_NUM:
-		return nvme_sg_get_version_num((void __user *)arg);
-	case SG_IO:
-		return nvme_sg_io(ns, (void __user *)arg);
-	/*
-	 * kv device ioctl.   
-	 */
-	case NVME_IOCTL_AIO_CMD:
-		return nvme_user_kv_cmd(ns, (void __user *)arg, true);
-	case NVME_IOCTL_IO_KV_CMD:
-		return nvme_user_kv_cmd(ns, (void __user *)arg, false);
-	case NVME_IOCTL_SET_AIOCTX:
-		return nvme_set_aioctx((void __user *)arg);
-	case NVME_IOCTL_DEL_AIOCTX:
-		return nvme_del_aioctx((void __user *)arg);
-	case NVME_IOCTL_GET_AIOEVENT:
-		return nvme_get_aioevents((void __user *)arg);
-	default:
-		return -ENOTTY;
-	}
-}
-
-#ifdef CONFIG_COMPAT
-static int nvme_compat_ioctl(struct block_device *bdev, fmode_t mode,
-					unsigned int cmd, unsigned long arg)
-{
-	struct nvme_ns *ns = bdev->bd_disk->private_data;
-
-	switch (cmd) {
-	case SG_IO:
-		return nvme_sg_io32(ns, arg);
-	}
-	return nvme_ioctl(bdev, mode, cmd, arg);
-}
-#else
-#define nvme_compat_ioctl	NULL
-#endif
-
-static int nvme_open(struct block_device *bdev, fmode_t mode)
-{
-	struct nvme_ns *ns = bdev->bd_disk->private_data;
-	struct nvme_dev *dev = ns->dev;
-
-	kref_get(&dev->kref);
-	return 0;
-}
-
-static void nvme_free_dev(struct kref *kref);
-
-static void nvme_release(struct gendisk *disk, fmode_t mode)
-{
-	struct nvme_ns *ns = disk->private_data;
-	struct nvme_dev *dev = ns->dev;
-
-	kref_put(&dev->kref, nvme_free_dev);
-}
-
-static int nvme_getgeo(struct block_device *bd, struct hd_geometry *geo)
-{
-	/* some standard values */
-	geo->heads = 1 << 6;
-	geo->sectors = 1 << 5;
-	geo->cylinders = get_capacity(bd->bd_disk) >> 11;
-	return 0;
-}
-
-static const struct block_device_operations nvme_fops = {
-	.owner		= THIS_MODULE,
-	.ioctl		= nvme_ioctl,
-	.compat_ioctl	= nvme_compat_ioctl,
-	.open		= nvme_open,
-	.release	= nvme_release,
-	.getgeo		= nvme_getgeo,
-};
-
-static void nvme_resubmit_iods(struct nvme_queue *nvmeq)
-{
-	struct nvme_iod *iod, *next;
-
-	list_for_each_entry_safe(iod, next, &nvmeq->iod_bio, node) {
-		if (unlikely(nvme_submit_iod(nvmeq, iod)))
-			break;
-		list_del(&iod->node);
-		if (bio_list_empty(&nvmeq->sq_cong) &&
-						list_empty(&nvmeq->iod_bio))
-			remove_wait_queue(&nvmeq->sq_full,
-						&nvmeq->sq_cong_wait);
-	}
-}
-
-static void nvme_resubmit_bios(struct nvme_queue *nvmeq)
-{
-	while (bio_list_peek(&nvmeq->sq_cong)) {
-		struct bio *bio = bio_list_pop(&nvmeq->sq_cong);
-		struct nvme_ns *ns = bio->bi_bdev->bd_disk->private_data;
-
-		if (bio_list_empty(&nvmeq->sq_cong) &&
-						list_empty(&nvmeq->iod_bio))
-			remove_wait_queue(&nvmeq->sq_full,
-							&nvmeq->sq_cong_wait);
-		if (nvme_submit_bio_queue(nvmeq, ns, bio)) {
-			if (!waitqueue_active(&nvmeq->sq_full))
-				add_wait_queue(&nvmeq->sq_full,
-							&nvmeq->sq_cong_wait);
-			bio_list_add_head(&nvmeq->sq_cong, bio);
-			break;
-		}
-	}
-}
-
-static int nvme_kthread(void *data)
-{
-	struct nvme_dev *dev, *next;
-
-	while (!kthread_should_stop()) {
-		set_current_state(TASK_INTERRUPTIBLE);
-		spin_lock(&dev_list_lock);
-		list_for_each_entry_safe(dev, next, &dev_list, node) {
-			int i;
-			if (readl(&dev->bar->csts) & NVME_CSTS_CFS &&
-							dev->initialized) {
-				if (work_busy(&dev->reset_work))
-					continue;
-				list_del_init(&dev->node);
-				dev_warn(&dev->pci_dev->dev,
-					"Failed status, reset controller\n");
-				PREPARE_WORK(&dev->reset_work,
-							nvme_reset_failed_dev);
-				queue_work(nvme_workq, &dev->reset_work);
-				continue;
-			}
-			rcu_read_lock();
-			for (i = 0; i < dev->queue_count; i++) {
-				struct nvme_queue *nvmeq =
-						rcu_dereference(dev->queues[i]);
-				if (!nvmeq)
-					continue;
-				spin_lock_irq(&nvmeq->q_lock);
-				if (nvmeq->q_suspended)
-					goto unlock;
-				nvme_process_cq(nvmeq);
-				nvme_cancel_ios(nvmeq, true);
-				nvme_resubmit_bios(nvmeq);
-				nvme_resubmit_iods(nvmeq);
- unlock:
-				spin_unlock_irq(&nvmeq->q_lock);
-			}
-			rcu_read_unlock();
-		}
-		spin_unlock(&dev_list_lock);
-		schedule_timeout(round_jiffies_relative(HZ));
-	}
-	return 0;
-}
-
-static void nvme_config_discard(struct nvme_ns *ns)
-{
-	u32 logical_block_size = queue_logical_block_size(ns->queue);
-	ns->queue->limits.discard_zeroes_data = 0;
-	ns->queue->limits.discard_alignment = logical_block_size;
-	ns->queue->limits.discard_granularity = logical_block_size;
-	ns->queue->limits.max_discard_sectors = 0xffffffff;
-	queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, ns->queue);
-}
-
-static struct nvme_ns *nvme_alloc_ns(struct nvme_dev *dev, unsigned nsid,
-			struct nvme_id_ns *id, struct nvme_lba_range_type *rt)
-{
-	struct nvme_ns *ns;
-	struct gendisk *disk;
-	int lbaf;
-
-	if (rt->attributes & NVME_LBART_ATTRIB_HIDE)
-		return NULL;
-
-	ns = kzalloc(sizeof(*ns), GFP_KERNEL);
-	if (!ns)
-		return NULL;
-	ns->queue = blk_alloc_queue(GFP_KERNEL);
-	if (!ns->queue)
-		goto out_free_ns;
-	ns->queue->queue_flags = QUEUE_FLAG_DEFAULT;
-	queue_flag_clear_unlocked(QUEUE_FLAG_STACKABLE, ns->queue);
-	queue_flag_set_unlocked(QUEUE_FLAG_NOMERGES, ns->queue);
-	queue_flag_set_unlocked(QUEUE_FLAG_NONROT, ns->queue);
-	queue_flag_clear_unlocked(QUEUE_FLAG_ADD_RANDOM, ns->queue);
-	blk_queue_make_request(ns->queue, nvme_make_request);
-	ns->dev = dev;
-	ns->queue->queuedata = ns;
-
-	disk = alloc_disk(0);
-	if (!disk)
-		goto out_free_queue;
-	ns->ns_id = nsid;
-	ns->disk = disk;
-	lbaf = id->flbas & 0xf;
-	ns->lba_shift = id->lbaf[lbaf].ds;
-	ns->ms = le16_to_cpu(id->lbaf[lbaf].ms);
-	blk_queue_logical_block_size(ns->queue, 1 << ns->lba_shift);
-	if (dev->max_hw_sectors)
-		blk_queue_max_hw_sectors(ns->queue, dev->max_hw_sectors);
-	if (dev->vwc & NVME_CTRL_VWC_PRESENT)
-		blk_queue_flush(ns->queue, REQ_FLUSH | REQ_FUA);
-
-	disk->major = nvme_major;
-	disk->first_minor = 0;
-	disk->fops = &nvme_fops;
-	disk->private_data = ns;
-	disk->queue = ns->queue;
-	disk->driverfs_dev = &dev->pci_dev->dev;
-	disk->flags = GENHD_FL_EXT_DEVT;
-	sprintf(disk->disk_name, "nvme%dn%d", dev->instance, nsid);
-	set_capacity(disk, le64_to_cpup(&id->nsze) << (ns->lba_shift - 9));
-
-	if (dev->oncs & NVME_CTRL_ONCS_DSM)
-		nvme_config_discard(ns);
-
-	return ns;
-
- out_free_queue:
-	blk_cleanup_queue(ns->queue);
- out_free_ns:
-	kfree(ns);
-	return NULL;
-}
-
-static int nvme_find_closest_node(int node)
-{
-	int n, val, min_val = INT_MAX, best_node = node;
-
-	for_each_online_node(n) {
-		if (n == node)
-			continue;
-		val = node_distance(node, n);
-		if (val < min_val) {
-			min_val = val;
-			best_node = n;
-		}
-	}
-	return best_node;
-}
-
-static void nvme_set_queue_cpus(cpumask_t *qmask, struct nvme_queue *nvmeq,
-								int count)
-{
-	int cpu;
-	for_each_cpu(cpu, qmask) {
-		if (cpumask_weight(nvmeq->cpu_mask) >= count)
-			break;
-		if (!cpumask_test_and_set_cpu(cpu, nvmeq->cpu_mask))
-			*per_cpu_ptr(nvmeq->dev->io_queue, cpu) = nvmeq->qid;
-	}
-}
-
-static void nvme_add_cpus(cpumask_t *mask, const cpumask_t *unassigned_cpus,
-	const cpumask_t *new_mask, struct nvme_queue *nvmeq, int cpus_per_queue)
-{
-	int next_cpu;
-	for_each_cpu(next_cpu, new_mask) {
-		cpumask_or(mask, mask, get_cpu_mask(next_cpu));
-		cpumask_or(mask, mask, topology_thread_cpumask(next_cpu));
-		cpumask_and(mask, mask, unassigned_cpus);
-		nvme_set_queue_cpus(mask, nvmeq, cpus_per_queue);
-	}
-}
-
-static void nvme_create_io_queues(struct nvme_dev *dev)
-{
-	unsigned i, max;
-
-	max = min(dev->max_qid, num_online_cpus());
-	for (i = dev->queue_count; i <= max; i++)
-		if (!nvme_alloc_queue(dev, i, dev->q_depth, i - 1))
-			break;
-
-	max = min(dev->queue_count - 1, num_online_cpus());
-	for (i = dev->online_queues; i <= max; i++)
-		if (nvme_create_queue(raw_nvmeq(dev, i), i))
-			break;
-}
-
-/*
- * If there are fewer queues than online cpus, this will try to optimally
- * assign a queue to multiple cpus by grouping cpus that are "close" together:
- * thread siblings, core, socket, closest node, then whatever else is
- * available.
- */
-static void nvme_assign_io_queues(struct nvme_dev *dev)
-{
-	unsigned cpu, cpus_per_queue, queues, remainder, i;
-	cpumask_var_t unassigned_cpus;
-
-	nvme_create_io_queues(dev);
-
-	queues = min(dev->online_queues - 1, num_online_cpus());
-	if (!queues)
-		return;
-
-	cpus_per_queue = num_online_cpus() / queues;
-	remainder = queues - (num_online_cpus() - queues * cpus_per_queue);
-
-	if (!alloc_cpumask_var(&unassigned_cpus, GFP_KERNEL))
-		return;
-
-	cpumask_copy(unassigned_cpus, cpu_online_mask);
-	cpu = cpumask_first(unassigned_cpus);
-	for (i = 1; i <= queues; i++) {
-		struct nvme_queue *nvmeq = lock_nvmeq(dev, i);
-		cpumask_t mask;
-
-		cpumask_clear(nvmeq->cpu_mask);
-		if (!cpumask_weight(unassigned_cpus)) {
-			unlock_nvmeq(nvmeq);
-			break;
-		}
-
-		mask = *get_cpu_mask(cpu);
-		nvme_set_queue_cpus(&mask, nvmeq, cpus_per_queue);
-		if (cpus_weight(mask) < cpus_per_queue)
-			nvme_add_cpus(&mask, unassigned_cpus,
-				topology_thread_cpumask(cpu),
-				nvmeq, cpus_per_queue);
-		if (cpus_weight(mask) < cpus_per_queue)
-			nvme_add_cpus(&mask, unassigned_cpus,
-				topology_core_cpumask(cpu),
-				nvmeq, cpus_per_queue);
-		if (cpus_weight(mask) < cpus_per_queue)
-			nvme_add_cpus(&mask, unassigned_cpus,
-				cpumask_of_node(cpu_to_node(cpu)),
-				nvmeq, cpus_per_queue);
-		if (cpus_weight(mask) < cpus_per_queue)
-			nvme_add_cpus(&mask, unassigned_cpus,
-				cpumask_of_node(
-					nvme_find_closest_node(
-						cpu_to_node(cpu))),
-				nvmeq, cpus_per_queue);
-		if (cpus_weight(mask) < cpus_per_queue)
-			nvme_add_cpus(&mask, unassigned_cpus,
-				unassigned_cpus,
-				nvmeq, cpus_per_queue);
-
-		WARN(cpumask_weight(nvmeq->cpu_mask) != cpus_per_queue,
-			"nvme%d qid:%d mis-matched queue-to-cpu assignment\n",
-			dev->instance, i);
-
-		irq_set_affinity_hint(dev->entry[nvmeq->cq_vector].vector,
-							nvmeq->cpu_mask);
-		cpumask_andnot(unassigned_cpus, unassigned_cpus,
-						nvmeq->cpu_mask);
-		cpu = cpumask_next(cpu, unassigned_cpus);
-		if (remainder && !--remainder)
-			cpus_per_queue++;
-		unlock_nvmeq(nvmeq);
-	}
-	WARN(cpumask_weight(unassigned_cpus), "nvme%d unassigned online cpus\n",
-								dev->instance);
-	i = 0;
-	cpumask_andnot(unassigned_cpus, cpu_possible_mask, cpu_online_mask);
-	for_each_cpu(cpu, unassigned_cpus)
-		*per_cpu_ptr(dev->io_queue, cpu) = (i++ % queues) + 1;
-	free_cpumask_var(unassigned_cpus);
-}
-
-static int set_queue_count(struct nvme_dev *dev, int count)
-{
-	int status;
-	u32 result;
-	u32 q_count = (count - 1) | ((count - 1) << 16);
-
-	status = nvme_set_features(dev, NVME_FEAT_NUM_QUEUES, q_count, 0,
-								&result);
-	if (status < 0)
-		return status;
-	if (status > 0) {
-		dev_err(&dev->pci_dev->dev, "Could not set queue count (%d)\n",
-									status);
-		return -EBUSY;
-	}
-	return min(result & 0xffff, result >> 16) + 1;
-}
-
-static size_t db_bar_size(struct nvme_dev *dev, unsigned nr_io_queues)
-{
-	return 4096 + ((nr_io_queues + 1) * 8 * dev->db_stride);
-}
-
-static void nvme_cpu_workfn(struct work_struct *work)
-{
-	struct nvme_dev *dev = container_of(work, struct nvme_dev, cpu_work);
-	if (dev->initialized)
-		nvme_assign_io_queues(dev);
-}
-
-static int nvme_cpu_notify(struct notifier_block *self,
-				unsigned long action, void *hcpu)
-{
-	struct nvme_dev *dev;
-
-	switch (action) {
-	case CPU_ONLINE:
-	case CPU_DEAD:
-		spin_lock(&dev_list_lock);
-		list_for_each_entry(dev, &dev_list, node)
-			schedule_work(&dev->cpu_work);
-		spin_unlock(&dev_list_lock);
-		break;
-	}
-	return NOTIFY_OK;
-}
-
-static int nvme_setup_io_queues(struct nvme_dev *dev)
-{
-	struct nvme_queue *adminq = raw_nvmeq(dev, 0);
-	struct pci_dev *pdev = dev->pci_dev;
-	int result, i, vecs, nr_io_queues, size;
-
-	nr_io_queues = num_possible_cpus();
-	result = set_queue_count(dev, nr_io_queues);
-	if (result < 0)
-		return result;
-	if (result < nr_io_queues)
-		nr_io_queues = result;
-
-	size = db_bar_size(dev, nr_io_queues);
-	if (size > 8192) {
-		iounmap(dev->bar);
-		do {
-			dev->bar = ioremap(pci_resource_start(pdev, 0), size);
-			if (dev->bar)
-				break;
-			if (!--nr_io_queues)
-				return -ENOMEM;
-			size = db_bar_size(dev, nr_io_queues);
-		} while (1);
-		dev->dbs = ((void __iomem *)dev->bar) + 4096;
-		adminq->q_db = dev->dbs;
-	}
-
-	/* Deregister the admin queue's interrupt */
-	free_irq(dev->entry[0].vector, adminq);
-
-	vecs = nr_io_queues;
-	for (i = 0; i < vecs; i++)
-		dev->entry[i].entry = i;
-	for (;;) {
-		result = pci_enable_msix(pdev, dev->entry, vecs);
-		if (result <= 0)
-			break;
-		vecs = result;
-	}
-
-	if (result < 0) {
-		vecs = nr_io_queues;
-		if (vecs > 32)
-			vecs = 32;
-		for (;;) {
-			result = pci_enable_msi_block(pdev, vecs);
-			if (result == 0) {
-				for (i = 0; i < vecs; i++)
-					dev->entry[i].vector = i + pdev->irq;
-				break;
-			} else if (result < 0) {
-				vecs = 1;
-				break;
-			}
-			vecs = result;
-		}
-	}
-
-	/*
-	 * Should investigate if there's a performance win from allocating
-	 * more queues than interrupt vectors; it might allow the submission
-	 * path to scale better, even if the receive path is limited by the
-	 * number of interrupts.
-	 */
-	nr_io_queues = vecs;
-	dev->max_qid = nr_io_queues;
-
-	result = queue_request_irq(dev, adminq, adminq->irqname);
-	if (result) {
-		adminq->q_suspended = 1;
-		goto free_queues;
-	}
-
-	/* Free previously allocated queues that are no longer usable */
-	nvme_free_queues(dev, nr_io_queues + 1);
-	nvme_assign_io_queues(dev);
-
-	return 0;
-
- free_queues:
-	nvme_free_queues(dev, 1);
-	return result;
-}
-
-/*
- * Return: error value if an error occurred setting up the queues or calling
- * Identify Device.  0 if these succeeded, even if adding some of the
- * namespaces failed.  At the moment, these failures are silent.  TBD which
- * failures should be reported.
- */
-static int nvme_dev_add(struct nvme_dev *dev)
-{
-	struct pci_dev *pdev = dev->pci_dev;
-	int res;
-	unsigned nn, i;
-	struct nvme_ns *ns;
-	struct nvme_id_ctrl *ctrl;
-	struct nvme_id_ns *id_ns;
-	void *mem;
-	dma_addr_t dma_addr;
-	int shift = NVME_CAP_MPSMIN(readq(&dev->bar->cap)) + 12;
-
-	mem = dma_alloc_coherent(&pdev->dev, 8192, &dma_addr, GFP_KERNEL);
-	if (!mem)
-		return -ENOMEM;
-
-	res = nvme_identify(dev, 0, 1, dma_addr);
-	if (res) {
-		dev_err(&pdev->dev, "Identify Controller failed (%d)\n", res);
-		res = -EIO;
-		goto out;
-	}
-
-	ctrl = mem;
-	nn = le32_to_cpup(&ctrl->nn);
-	dev->oncs = le16_to_cpup(&ctrl->oncs);
-	dev->abort_limit = ctrl->acl + 1;
-	dev->vwc = ctrl->vwc;
-	memcpy(dev->serial, ctrl->sn, sizeof(ctrl->sn));
-	memcpy(dev->model, ctrl->mn, sizeof(ctrl->mn));
-	memcpy(dev->firmware_rev, ctrl->fr, sizeof(ctrl->fr));
-	if (ctrl->mdts)
-		dev->max_hw_sectors = 1 << (ctrl->mdts + shift - 9);
-	if ((pdev->vendor == PCI_VENDOR_ID_INTEL) &&
-			(pdev->device == 0x0953) && ctrl->vs[3])
-		dev->stripe_size = 1 << (ctrl->vs[3] + shift);
-
-	id_ns = mem;
-	for (i = 1; i <= nn; i++) {
-		res = nvme_identify(dev, i, 0, dma_addr);
-		if (res)
-			continue;
-
-		if (id_ns->ncap == 0)
-			continue;
-
-		res = nvme_get_features(dev, NVME_FEAT_LBA_RANGE, i,
-							dma_addr + 4096, NULL);
-		if (res)
-			memset(mem + 4096, 0, 4096);
-
-		ns = nvme_alloc_ns(dev, i, mem, mem + 4096);
-		if (ns)
-			list_add_tail(&ns->list, &dev->namespaces);
-	}
-	list_for_each_entry(ns, &dev->namespaces, list)
-		add_disk(ns->disk);
-	res = 0;
-
- out:
-	dma_free_coherent(&dev->pci_dev->dev, 8192, mem, dma_addr);
-	return res;
-}
-
-static int nvme_dev_map(struct nvme_dev *dev)
-{
-	u64 cap;
-	int bars, result = -ENOMEM;
-	struct pci_dev *pdev = dev->pci_dev;
-
-	if (pci_enable_device_mem(pdev))
-		return result;
-
-	dev->entry[0].vector = pdev->irq;
-	pci_set_master(pdev);
-	bars = pci_select_bars(pdev, IORESOURCE_MEM);
-	if (pci_request_selected_regions(pdev, bars, "nvme"))
-		goto disable_pci;
-
-	if (dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64)) &&
-	    dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32)))
-		goto disable;
-
-	dev->bar = ioremap(pci_resource_start(pdev, 0), 8192);
-	if (!dev->bar)
-		goto disable;
-	if (readl(&dev->bar->csts) == -1) {
-		result = -ENODEV;
-		goto unmap;
-	}
-	cap = readq(&dev->bar->cap);
-	dev->q_depth = min_t(int, NVME_CAP_MQES(cap) + 1, NVME_Q_DEPTH);
-	dev->db_stride = 1 << NVME_CAP_STRIDE(cap);
-	dev->dbs = ((void __iomem *)dev->bar) + 4096;
-
-	return 0;
-
- unmap:
-	iounmap(dev->bar);
-	dev->bar = NULL;
- disable:
-	pci_release_regions(pdev);
- disable_pci:
-	pci_disable_device(pdev);
-	return result;
-}
-
-static void nvme_dev_unmap(struct nvme_dev *dev)
-{
-	if (dev->pci_dev->msi_enabled)
-		pci_disable_msi(dev->pci_dev);
-	else if (dev->pci_dev->msix_enabled)
-		pci_disable_msix(dev->pci_dev);
-
-	if (dev->bar) {
-		iounmap(dev->bar);
-		dev->bar = NULL;
-		pci_release_regions(dev->pci_dev);
-	}
-
-	if (pci_is_enabled(dev->pci_dev))
-		pci_disable_device(dev->pci_dev);
-}
-
-struct nvme_delq_ctx {
-	struct task_struct *waiter;
-	struct kthread_worker* worker;
-	atomic_t refcount;
-};
-
-static void nvme_wait_dq(struct nvme_delq_ctx *dq, struct nvme_dev *dev)
-{
-	dq->waiter = current;
-	mb();
-
-	for (;;) {
-		set_current_state(TASK_KILLABLE);
-		if (!atomic_read(&dq->refcount))
-			break;
-		if (!schedule_timeout(ADMIN_TIMEOUT) ||
-					fatal_signal_pending(current)) {
-			set_current_state(TASK_RUNNING);
-
-			nvme_disable_ctrl(dev, readq(&dev->bar->cap));
-			nvme_disable_queue(dev, 0);
-
-			send_sig(SIGKILL, dq->worker->task, 1);
-			flush_kthread_worker(dq->worker);
-			return;
-		}
-	}
-	set_current_state(TASK_RUNNING);
-}
-
-static void nvme_put_dq(struct nvme_delq_ctx *dq)
-{
-	atomic_dec(&dq->refcount);
-	if (dq->waiter)
-		wake_up_process(dq->waiter);
-}
-
-static struct nvme_delq_ctx *nvme_get_dq(struct nvme_delq_ctx *dq)
-{
-	atomic_inc(&dq->refcount);
-	return dq;
-}
-
-static void nvme_del_queue_end(struct nvme_queue *nvmeq)
-{
-	struct nvme_delq_ctx *dq = nvmeq->cmdinfo.ctx;
-
-	nvme_clear_queue(nvmeq);
-	nvme_put_dq(dq);
-}
-
-static int adapter_async_del_queue(struct nvme_queue *nvmeq, u8 opcode,
-						kthread_work_func_t fn)
-{
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.delete_queue.opcode = opcode;
-	c.delete_queue.qid = cpu_to_le16(nvmeq->qid);
-
-	init_kthread_work(&nvmeq->cmdinfo.work, fn);
-	return nvme_submit_admin_cmd_async(nvmeq->dev, &c, &nvmeq->cmdinfo);
-}
-
-static void nvme_del_cq_work_handler(struct kthread_work *work)
-{
-	struct nvme_queue *nvmeq = container_of(work, struct nvme_queue,
-							cmdinfo.work);
-	nvme_del_queue_end(nvmeq);
-}
-
-static int nvme_delete_cq(struct nvme_queue *nvmeq)
-{
-	return adapter_async_del_queue(nvmeq, nvme_admin_delete_cq,
-						nvme_del_cq_work_handler);
-}
-
-static void nvme_del_sq_work_handler(struct kthread_work *work)
-{
-	struct nvme_queue *nvmeq = container_of(work, struct nvme_queue,
-							cmdinfo.work);
-	int status = nvmeq->cmdinfo.status;
-
-	if (!status)
-		status = nvme_delete_cq(nvmeq);
-	if (status)
-		nvme_del_queue_end(nvmeq);
-}
-
-static int nvme_delete_sq(struct nvme_queue *nvmeq)
-{
-	return adapter_async_del_queue(nvmeq, nvme_admin_delete_sq,
-						nvme_del_sq_work_handler);
-}
-
-static void nvme_del_queue_start(struct kthread_work *work)
-{
-	struct nvme_queue *nvmeq = container_of(work, struct nvme_queue,
-							cmdinfo.work);
-	allow_signal(SIGKILL);
-	if (nvme_delete_sq(nvmeq))
-		nvme_del_queue_end(nvmeq);
-}
-
-static void nvme_disable_io_queues(struct nvme_dev *dev)
-{
-	int i;
-	DEFINE_KTHREAD_WORKER_ONSTACK(worker);
-	struct nvme_delq_ctx dq;
-	struct task_struct *kworker_task = kthread_run(kthread_worker_fn,
-					&worker, "nvme%d", dev->instance);
-
-	if (IS_ERR(kworker_task)) {
-		dev_err(&dev->pci_dev->dev,
-			"Failed to create queue del task\n");
-		for (i = dev->queue_count - 1; i > 0; i--)
-			nvme_disable_queue(dev, i);
-		return;
-	}
-
-	dq.waiter = NULL;
-	atomic_set(&dq.refcount, 0);
-	dq.worker = &worker;
-	for (i = dev->queue_count - 1; i > 0; i--) {
-		struct nvme_queue *nvmeq = raw_nvmeq(dev, i);
-
-		if (nvme_suspend_queue(nvmeq))
-			continue;
-		nvmeq->cmdinfo.ctx = nvme_get_dq(&dq);
-		nvmeq->cmdinfo.worker = dq.worker;
-		init_kthread_work(&nvmeq->cmdinfo.work, nvme_del_queue_start);
-		queue_kthread_work(dq.worker, &nvmeq->cmdinfo.work);
-	}
-	nvme_wait_dq(&dq, dev);
-	kthread_stop(kworker_task);
-}
-
-/*
-* Remove the node from the device list and check
-* for whether or not we need to stop the nvme_thread.
-*/
-static void nvme_dev_list_remove(struct nvme_dev *dev)
-{
-	struct task_struct *tmp = NULL;
-
-	spin_lock(&dev_list_lock);
-	list_del_init(&dev->node);
-	if (list_empty(&dev_list) && !IS_ERR_OR_NULL(nvme_thread)) {
-		tmp = nvme_thread;
-		nvme_thread = NULL;
-	}
-	spin_unlock(&dev_list_lock);
-
-	if (tmp)
-		kthread_stop(tmp);
-}
-
-static void nvme_dev_shutdown(struct nvme_dev *dev)
-{
-	int i;
-
-	dev->initialized = 0;
-	nvme_dev_list_remove(dev);
-
-	if (!dev->bar || (dev->bar && readl(&dev->bar->csts) == -1)) {
-		for (i = dev->queue_count - 1; i >= 0; i--) {
-			struct nvme_queue *nvmeq = raw_nvmeq(dev, i);
-			nvme_suspend_queue(nvmeq);
-			nvme_clear_queue(nvmeq);
-		}
-	} else {
-		nvme_disable_io_queues(dev);
-		nvme_shutdown_ctrl(dev);
-		nvme_disable_queue(dev, 0);
-	}
-	nvme_dev_unmap(dev);
-}
-
-static void nvme_dev_remove(struct nvme_dev *dev)
-{
-	struct nvme_ns *ns;
-
-	list_for_each_entry(ns, &dev->namespaces, list) {
-		if (ns->disk->flags & GENHD_FL_UP)
-			del_gendisk(ns->disk);
-		if (!blk_queue_dying(ns->queue))
-			blk_cleanup_queue(ns->queue);
-	}
-}
-
-static int nvme_setup_prp_pools(struct nvme_dev *dev)
-{
-	struct device *dmadev = &dev->pci_dev->dev;
-	dev->prp_page_pool = dma_pool_create("prp list page", dmadev,
-						PAGE_SIZE, PAGE_SIZE, 0);
-	if (!dev->prp_page_pool)
-		return -ENOMEM;
-
-	/* Optimisation for I/Os between 4k and 128k */
-	dev->prp_small_pool = dma_pool_create("prp list 256", dmadev,
-						256, 256, 0);
-	if (!dev->prp_small_pool) {
-		dma_pool_destroy(dev->prp_page_pool);
-		return -ENOMEM;
-	}
-	return 0;
-}
-
-static void nvme_release_prp_pools(struct nvme_dev *dev)
-{
-	dma_pool_destroy(dev->prp_page_pool);
-	dma_pool_destroy(dev->prp_small_pool);
-}
-
-static DEFINE_IDA(nvme_instance_ida);
-
-static int nvme_set_instance(struct nvme_dev *dev)
-{
-	int instance, error;
-
-	do {
-		if (!ida_pre_get(&nvme_instance_ida, GFP_KERNEL))
-			return -ENODEV;
-
-		spin_lock(&dev_list_lock);
-		error = ida_get_new(&nvme_instance_ida, &instance);
-		spin_unlock(&dev_list_lock);
-	} while (error == -EAGAIN);
-
-	if (error)
-		return -ENODEV;
-
-	dev->instance = instance;
-	return 0;
-}
-
-static void nvme_release_instance(struct nvme_dev *dev)
-{
-	spin_lock(&dev_list_lock);
-	ida_remove(&nvme_instance_ida, dev->instance);
-	spin_unlock(&dev_list_lock);
-}
-
-static void nvme_free_namespaces(struct nvme_dev *dev)
-{
-	struct nvme_ns *ns, *next;
-
-	list_for_each_entry_safe(ns, next, &dev->namespaces, list) {
-		list_del(&ns->list);
-		put_disk(ns->disk);
-		kfree(ns);
-	}
-}
-
-static void nvme_free_dev(struct kref *kref)
-{
-	struct nvme_dev *dev = container_of(kref, struct nvme_dev, kref);
-
-	nvme_free_namespaces(dev);
-	free_percpu(dev->io_queue);
-	kfree(dev->queues);
-	kfree(dev->entry);
-	kfree(dev);
-}
-
-static int nvme_dev_open(struct inode *inode, struct file *f)
-{
-	struct nvme_dev *dev = container_of(f->private_data, struct nvme_dev,
-								miscdev);
-	kref_get(&dev->kref);
-	f->private_data = dev;
-	return 0;
-}
-
-static int nvme_dev_release(struct inode *inode, struct file *f)
-{
-	struct nvme_dev *dev = f->private_data;
-	kref_put(&dev->kref, nvme_free_dev);
-	return 0;
-}
-
-static long nvme_dev_ioctl(struct file *f, unsigned int cmd, unsigned long arg)
-{
-	struct nvme_dev *dev = f->private_data;
-	switch (cmd) {
-	case NVME_IOCTL_ADMIN_CMD:
-		return nvme_user_admin_cmd(dev, (void __user *)arg);
-	default:
-		return -ENOTTY;
-	}
-}
-
-static const struct file_operations nvme_dev_fops = {
-	.owner		= THIS_MODULE,
-	.open		= nvme_dev_open,
-	.release	= nvme_dev_release,
-	.unlocked_ioctl	= nvme_dev_ioctl,
-	.compat_ioctl	= nvme_dev_ioctl,
-};
-
-static int nvme_dev_start(struct nvme_dev *dev)
-{
-	int result;
-	bool start_thread = false;
-
-	result = nvme_dev_map(dev);
-	if (result)
-		return result;
-
-	result = nvme_configure_admin_queue(dev);
-	if (result)
-		goto unmap;
-
-	spin_lock(&dev_list_lock);
-	if (list_empty(&dev_list) && IS_ERR_OR_NULL(nvme_thread)) {
-		start_thread = true;
-		nvme_thread = NULL;
-	}
-	list_add(&dev->node, &dev_list);
-	spin_unlock(&dev_list_lock);
-
-	if (start_thread) {
-		nvme_thread = kthread_run(nvme_kthread, NULL, "nvme");
-		wake_up(&nvme_kthread_wait);
-	} else
-		wait_event_killable(nvme_kthread_wait, nvme_thread);
-
-	if (IS_ERR_OR_NULL(nvme_thread)) {
-		result = nvme_thread ? PTR_ERR(nvme_thread) : -EINTR;
-		goto disable;
-	}
-
-	result = nvme_setup_io_queues(dev);
-	if (result && result != -EBUSY)
-		goto disable;
-
-	return result;
-
- disable:
-	nvme_disable_queue(dev, 0);
-	nvme_dev_list_remove(dev);
- unmap:
-	nvme_dev_unmap(dev);
-	return result;
-}
-
-static int nvme_remove_dead_ctrl(void *arg)
-{
-	struct nvme_dev *dev = (struct nvme_dev *)arg;
-	struct pci_dev *pdev = dev->pci_dev;
-
-	if (pci_get_drvdata(pdev))
-		pci_stop_and_remove_bus_device(pdev);
-	kref_put(&dev->kref, nvme_free_dev);
-	return 0;
-}
-
-static void nvme_remove_disks(struct work_struct *ws)
-{
-	struct nvme_dev *dev = container_of(ws, struct nvme_dev, reset_work);
-
-	nvme_dev_remove(dev);
-	nvme_free_queues(dev, 1);
-}
-
-static int nvme_dev_resume(struct nvme_dev *dev)
-{
-	int ret;
-
-	ret = nvme_dev_start(dev);
-	if (ret && ret != -EBUSY)
-		return ret;
-	if (ret == -EBUSY) {
-		spin_lock(&dev_list_lock);
-		PREPARE_WORK(&dev->reset_work, nvme_remove_disks);
-		queue_work(nvme_workq, &dev->reset_work);
-		spin_unlock(&dev_list_lock);
-	}
-	dev->initialized = 1;
-	return 0;
-}
-
-static void nvme_dev_reset(struct nvme_dev *dev)
-{
-	nvme_dev_shutdown(dev);
-	if (nvme_dev_resume(dev)) {
-		dev_err(&dev->pci_dev->dev, "Device failed to resume\n");
-		kref_get(&dev->kref);
-		if (IS_ERR(kthread_run(nvme_remove_dead_ctrl, dev, "nvme%d",
-							dev->instance))) {
-			dev_err(&dev->pci_dev->dev,
-				"Failed to start controller remove task\n");
-			kref_put(&dev->kref, nvme_free_dev);
-		}
-	}
-}
-
-static void nvme_reset_failed_dev(struct work_struct *ws)
-{
-	struct nvme_dev *dev = container_of(ws, struct nvme_dev, reset_work);
-	nvme_dev_reset(dev);
-}
-
-static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
-{
-	int result = -ENOMEM;
-	struct nvme_dev *dev;
-
-	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
-	if (!dev)
-		return -ENOMEM;
-	dev->entry = kcalloc(num_possible_cpus(), sizeof(*dev->entry),
-								GFP_KERNEL);
-	if (!dev->entry)
-		goto free;
-	dev->queues = kcalloc(num_possible_cpus() + 1, sizeof(void *),
-								GFP_KERNEL);
-	if (!dev->queues)
-		goto free;
-	dev->io_queue = alloc_percpu(unsigned short);
-	if (!dev->io_queue)
-		goto free;
-
-	INIT_LIST_HEAD(&dev->namespaces);
-	INIT_WORK(&dev->reset_work, nvme_reset_failed_dev);
-	INIT_WORK(&dev->cpu_work, nvme_cpu_workfn);
-	dev->pci_dev = pdev;
-	pci_set_drvdata(pdev, dev);
-	result = nvme_set_instance(dev);
-	if (result)
-		goto free;
-
-	result = nvme_setup_prp_pools(dev);
-	if (result)
-		goto release;
-
-	kref_init(&dev->kref);
-	result = nvme_dev_start(dev);
-	if (result) {
-		if (result == -EBUSY)
-			goto create_cdev;
-		goto release_pools;
-	}
-
-	result = nvme_dev_add(dev);
-	if (result)
-		goto shutdown;
-
- create_cdev:
-	scnprintf(dev->name, sizeof(dev->name), "nvme%d", dev->instance);
-	dev->miscdev.minor = MISC_DYNAMIC_MINOR;
-	dev->miscdev.parent = &pdev->dev;
-	dev->miscdev.name = dev->name;
-	dev->miscdev.fops = &nvme_dev_fops;
-	result = misc_register(&dev->miscdev);
-	if (result)
-		goto remove;
-
-	dev->initialized = 1;
-	return 0;
-
- remove:
-	nvme_dev_remove(dev);
-	nvme_free_namespaces(dev);
- shutdown:
-	nvme_dev_shutdown(dev);
- release_pools:
-	nvme_free_queues(dev, 0);
-	nvme_release_prp_pools(dev);
- release:
-	nvme_release_instance(dev);
- free:
-	free_percpu(dev->io_queue);
-	kfree(dev->queues);
-	kfree(dev->entry);
-	kfree(dev);
-	return result;
-}
-
-static void nvme_shutdown(struct pci_dev *pdev)
-{
-	struct nvme_dev *dev = pci_get_drvdata(pdev);
-	nvme_dev_shutdown(dev);
-}
-
-static void nvme_remove(struct pci_dev *pdev)
-{
-	struct nvme_dev *dev = pci_get_drvdata(pdev);
-
-	spin_lock(&dev_list_lock);
-	list_del_init(&dev->node);
-	spin_unlock(&dev_list_lock);
-
-	pci_set_drvdata(pdev, NULL);
-	flush_work(&dev->reset_work);
-	flush_work(&dev->cpu_work);
-	misc_deregister(&dev->miscdev);
-	nvme_dev_remove(dev);
-	nvme_dev_shutdown(dev);
-	nvme_free_queues(dev, 0);
-	rcu_barrier();
-	nvme_release_instance(dev);
-	nvme_release_prp_pools(dev);
-	kref_put(&dev->kref, nvme_free_dev);
-}
-
-/* These functions are yet to be implemented */
-#define nvme_error_detected NULL
-#define nvme_dump_registers NULL
-#define nvme_link_reset NULL
-#define nvme_slot_reset NULL
-#define nvme_error_resume NULL
-
-static int nvme_suspend(struct device *dev)
-{
-	struct pci_dev *pdev = to_pci_dev(dev);
-	struct nvme_dev *ndev = pci_get_drvdata(pdev);
-
-	nvme_dev_shutdown(ndev);
-	return 0;
-}
-
-static int nvme_resume(struct device *dev)
-{
-	struct pci_dev *pdev = to_pci_dev(dev);
-	struct nvme_dev *ndev = pci_get_drvdata(pdev);
-
-	if (nvme_dev_resume(ndev) && !work_busy(&ndev->reset_work)) {
-		PREPARE_WORK(&ndev->reset_work, nvme_reset_failed_dev);
-		queue_work(nvme_workq, &ndev->reset_work);
-	}
-	return 0;
-}
-
-static SIMPLE_DEV_PM_OPS(nvme_dev_pm_ops, nvme_suspend, nvme_resume);
-
-static const struct pci_error_handlers nvme_err_handler = {
-	.error_detected	= nvme_error_detected,
-	.mmio_enabled	= nvme_dump_registers,
-	.link_reset	= nvme_link_reset,
-	.slot_reset	= nvme_slot_reset,
-	.resume		= nvme_error_resume,
-};
-
-/* Move to pci_ids.h later */
-#define PCI_CLASS_STORAGE_EXPRESS	0x010802
-
-static const struct pci_device_id nvme_id_table[] = {
-	{ PCI_DEVICE_CLASS(PCI_CLASS_STORAGE_EXPRESS, 0xffffff) },
-	{ 0, }
-};
-MODULE_DEVICE_TABLE(pci, nvme_id_table);
-
-static struct pci_driver nvme_driver = {
-	.name		= "nvme",
-	.id_table	= nvme_id_table,
-	.probe		= nvme_probe,
-	.remove		= nvme_remove,
-	.shutdown	= nvme_shutdown,
-	.driver		= {
-		.pm	= &nvme_dev_pm_ops,
-	},
-	.err_handler	= &nvme_err_handler,
-};
-
-static int __init nvme_init(void)
-{
-	int result;
-
-	init_waitqueue_head(&nvme_kthread_wait);
-
-	nvme_workq = create_singlethread_workqueue("nvme");
-	if (!nvme_workq)
-		return -ENOMEM;
-
-	result = register_blkdev(nvme_major, "nvme");
-	if (result < 0)
-		goto kill_workq;
-	else if (result > 0)
-		nvme_major = result;
-
-	nvme_nb.notifier_call = &nvme_cpu_notify;
-	result = register_hotcpu_notifier(&nvme_nb);
-	if (result)
-		goto unregister_blkdev;
-
-	result = pci_register_driver(&nvme_driver);
-	if (result)
-		goto unregister_hotcpu;
-
-	result = aio_service_init();
-	if (result)
-		goto unregister_pcidev;
-	return 0;
-
-unregister_pcidev:
-	pci_unregister_driver(&nvme_driver);
-unregister_hotcpu:
-	unregister_hotcpu_notifier(&nvme_nb);
-unregister_blkdev:
-	unregister_blkdev(nvme_major, "nvme");
-kill_workq:
-	destroy_workqueue(nvme_workq);
-	return result;
-}
-
-static void __exit nvme_exit(void)
-{
-	pci_unregister_driver(&nvme_driver);
-	unregister_hotcpu_notifier(&nvme_nb);
-	unregister_blkdev(nvme_major, "nvme");
-	destroy_workqueue(nvme_workq);
-	BUG_ON(nvme_thread && !IS_ERR(nvme_thread));
-	_nvme_check_size();
-	aio_service_exit();
-}
-
-MODULE_AUTHOR("Matthew Wilcox <willy@linux.intel.com>");
-MODULE_LICENSE("GPL");
-MODULE_VERSION("0.9");
-module_init(nvme_init);
-module_exit(nvme_exit);
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/nvme-scsi.c b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/nvme-scsi.c
deleted file mode 100644
index c72a08b..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/nvme-scsi.c
+++ /dev/null
@@ -1,3168 +0,0 @@
-/*
- * NVM Express device driver
- * Copyright (c) 2011-2014, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- */
-
-/*
- * Refer to the SCSI-NVMe Translation spec for details on how
- * each command is translated.
- */
-
-#include <linux/bio.h>
-#include <linux/bitops.h>
-#include <linux/blkdev.h>
-#include <linux/compat.h>
-#include <linux/delay.h>
-#include <linux/errno.h>
-#include <linux/fs.h>
-#include <linux/genhd.h>
-#include <linux/idr.h>
-#include <linux/init.h>
-#include <linux/interrupt.h>
-#include <linux/io.h>
-#include <linux/kdev_t.h>
-#include <linux/kthread.h>
-#include <linux/kernel.h>
-#include <linux/mm.h>
-#include <linux/module.h>
-#include <linux/moduleparam.h>
-#include <linux/pci.h>
-#include <linux/poison.h>
-#include <linux/sched.h>
-#include <linux/slab.h>
-#include <linux/types.h>
-#include <scsi/sg.h>
-#include <scsi/scsi.h>
-#include "nvme.h"
-
-
-static int sg_version_num = 30534;	/* 2 digits for each component */
-
-#define SNTI_TRANSLATION_SUCCESS			0
-#define SNTI_INTERNAL_ERROR				1
-
-/* VPD Page Codes */
-#define VPD_SUPPORTED_PAGES				0x00
-#define VPD_SERIAL_NUMBER				0x80
-#define VPD_DEVICE_IDENTIFIERS				0x83
-#define VPD_EXTENDED_INQUIRY				0x86
-#define VPD_BLOCK_DEV_CHARACTERISTICS			0xB1
-
-/* CDB offsets */
-#define REPORT_LUNS_CDB_ALLOC_LENGTH_OFFSET		6
-#define REPORT_LUNS_SR_OFFSET				2
-#define READ_CAP_16_CDB_ALLOC_LENGTH_OFFSET		10
-#define REQUEST_SENSE_CDB_ALLOC_LENGTH_OFFSET		4
-#define REQUEST_SENSE_DESC_OFFSET			1
-#define REQUEST_SENSE_DESC_MASK				0x01
-#define DESCRIPTOR_FORMAT_SENSE_DATA_TYPE		1
-#define INQUIRY_EVPD_BYTE_OFFSET			1
-#define INQUIRY_PAGE_CODE_BYTE_OFFSET			2
-#define INQUIRY_EVPD_BIT_MASK				1
-#define INQUIRY_CDB_ALLOCATION_LENGTH_OFFSET		3
-#define START_STOP_UNIT_CDB_IMMED_OFFSET		1
-#define START_STOP_UNIT_CDB_IMMED_MASK			0x1
-#define START_STOP_UNIT_CDB_POWER_COND_MOD_OFFSET	3
-#define START_STOP_UNIT_CDB_POWER_COND_MOD_MASK		0xF
-#define START_STOP_UNIT_CDB_POWER_COND_OFFSET		4
-#define START_STOP_UNIT_CDB_POWER_COND_MASK		0xF0
-#define START_STOP_UNIT_CDB_NO_FLUSH_OFFSET		4
-#define START_STOP_UNIT_CDB_NO_FLUSH_MASK		0x4
-#define START_STOP_UNIT_CDB_START_OFFSET		4
-#define START_STOP_UNIT_CDB_START_MASK			0x1
-#define WRITE_BUFFER_CDB_MODE_OFFSET			1
-#define WRITE_BUFFER_CDB_MODE_MASK			0x1F
-#define WRITE_BUFFER_CDB_BUFFER_ID_OFFSET		2
-#define WRITE_BUFFER_CDB_BUFFER_OFFSET_OFFSET		3
-#define WRITE_BUFFER_CDB_PARM_LIST_LENGTH_OFFSET	6
-#define FORMAT_UNIT_CDB_FORMAT_PROT_INFO_OFFSET		1
-#define FORMAT_UNIT_CDB_FORMAT_PROT_INFO_MASK		0xC0
-#define FORMAT_UNIT_CDB_FORMAT_PROT_INFO_SHIFT		6
-#define FORMAT_UNIT_CDB_LONG_LIST_OFFSET		1
-#define FORMAT_UNIT_CDB_LONG_LIST_MASK			0x20
-#define FORMAT_UNIT_CDB_FORMAT_DATA_OFFSET		1
-#define FORMAT_UNIT_CDB_FORMAT_DATA_MASK		0x10
-#define FORMAT_UNIT_SHORT_PARM_LIST_LEN			4
-#define FORMAT_UNIT_LONG_PARM_LIST_LEN			8
-#define FORMAT_UNIT_PROT_INT_OFFSET			3
-#define FORMAT_UNIT_PROT_FIELD_USAGE_OFFSET		0
-#define FORMAT_UNIT_PROT_FIELD_USAGE_MASK		0x07
-#define UNMAP_CDB_PARAM_LIST_LENGTH_OFFSET		7
-
-/* Misc. defines */
-#define NIBBLE_SHIFT					4
-#define FIXED_SENSE_DATA				0x70
-#define DESC_FORMAT_SENSE_DATA				0x72
-#define FIXED_SENSE_DATA_ADD_LENGTH			10
-#define LUN_ENTRY_SIZE					8
-#define LUN_DATA_HEADER_SIZE				8
-#define ALL_LUNS_RETURNED				0x02
-#define ALL_WELL_KNOWN_LUNS_RETURNED			0x01
-#define RESTRICTED_LUNS_RETURNED			0x00
-#define NVME_POWER_STATE_START_VALID			0x00
-#define NVME_POWER_STATE_ACTIVE				0x01
-#define NVME_POWER_STATE_IDLE				0x02
-#define NVME_POWER_STATE_STANDBY			0x03
-#define NVME_POWER_STATE_LU_CONTROL			0x07
-#define POWER_STATE_0					0
-#define POWER_STATE_1					1
-#define POWER_STATE_2					2
-#define POWER_STATE_3					3
-#define DOWNLOAD_SAVE_ACTIVATE				0x05
-#define DOWNLOAD_SAVE_DEFER_ACTIVATE			0x0E
-#define ACTIVATE_DEFERRED_MICROCODE			0x0F
-#define FORMAT_UNIT_IMMED_MASK				0x2
-#define FORMAT_UNIT_IMMED_OFFSET			1
-#define KELVIN_TEMP_FACTOR				273
-#define FIXED_FMT_SENSE_DATA_SIZE			18
-#define DESC_FMT_SENSE_DATA_SIZE			8
-
-/* SCSI/NVMe defines and bit masks */
-#define INQ_STANDARD_INQUIRY_PAGE			0x00
-#define INQ_SUPPORTED_VPD_PAGES_PAGE			0x00
-#define INQ_UNIT_SERIAL_NUMBER_PAGE			0x80
-#define INQ_DEVICE_IDENTIFICATION_PAGE			0x83
-#define INQ_EXTENDED_INQUIRY_DATA_PAGE			0x86
-#define INQ_BDEV_CHARACTERISTICS_PAGE			0xB1
-#define INQ_SERIAL_NUMBER_LENGTH			0x14
-#define INQ_NUM_SUPPORTED_VPD_PAGES			5
-#define VERSION_SPC_4					0x06
-#define ACA_UNSUPPORTED					0
-#define STANDARD_INQUIRY_LENGTH				36
-#define ADDITIONAL_STD_INQ_LENGTH			31
-#define EXTENDED_INQUIRY_DATA_PAGE_LENGTH		0x3C
-#define RESERVED_FIELD					0
-
-/* SCSI READ/WRITE Defines */
-#define IO_CDB_WP_MASK					0xE0
-#define IO_CDB_WP_SHIFT					5
-#define IO_CDB_FUA_MASK					0x8
-#define IO_6_CDB_LBA_OFFSET				0
-#define IO_6_CDB_LBA_MASK				0x001FFFFF
-#define IO_6_CDB_TX_LEN_OFFSET				4
-#define IO_6_DEFAULT_TX_LEN				256
-#define IO_10_CDB_LBA_OFFSET				2
-#define IO_10_CDB_TX_LEN_OFFSET				7
-#define IO_10_CDB_WP_OFFSET				1
-#define IO_10_CDB_FUA_OFFSET				1
-#define IO_12_CDB_LBA_OFFSET				2
-#define IO_12_CDB_TX_LEN_OFFSET				6
-#define IO_12_CDB_WP_OFFSET				1
-#define IO_12_CDB_FUA_OFFSET				1
-#define IO_16_CDB_FUA_OFFSET				1
-#define IO_16_CDB_WP_OFFSET				1
-#define IO_16_CDB_LBA_OFFSET				2
-#define IO_16_CDB_TX_LEN_OFFSET				10
-
-/* Mode Sense/Select defines */
-#define MODE_PAGE_INFO_EXCEP				0x1C
-#define MODE_PAGE_CACHING				0x08
-#define MODE_PAGE_CONTROL				0x0A
-#define MODE_PAGE_POWER_CONDITION			0x1A
-#define MODE_PAGE_RETURN_ALL				0x3F
-#define MODE_PAGE_BLK_DES_LEN				0x08
-#define MODE_PAGE_LLBAA_BLK_DES_LEN			0x10
-#define MODE_PAGE_CACHING_LEN				0x14
-#define MODE_PAGE_CONTROL_LEN				0x0C
-#define MODE_PAGE_POW_CND_LEN				0x28
-#define MODE_PAGE_INF_EXC_LEN				0x0C
-#define MODE_PAGE_ALL_LEN				0x54
-#define MODE_SENSE6_MPH_SIZE				4
-#define MODE_SENSE6_ALLOC_LEN_OFFSET			4
-#define MODE_SENSE_PAGE_CONTROL_OFFSET			2
-#define MODE_SENSE_PAGE_CONTROL_MASK			0xC0
-#define MODE_SENSE_PAGE_CODE_OFFSET			2
-#define MODE_SENSE_PAGE_CODE_MASK			0x3F
-#define MODE_SENSE_LLBAA_OFFSET				1
-#define MODE_SENSE_LLBAA_MASK				0x10
-#define MODE_SENSE_LLBAA_SHIFT				4
-#define MODE_SENSE_DBD_OFFSET				1
-#define MODE_SENSE_DBD_MASK				8
-#define MODE_SENSE_DBD_SHIFT				3
-#define MODE_SENSE10_MPH_SIZE				8
-#define MODE_SENSE10_ALLOC_LEN_OFFSET			7
-#define MODE_SELECT_CDB_PAGE_FORMAT_OFFSET		1
-#define MODE_SELECT_CDB_SAVE_PAGES_OFFSET		1
-#define MODE_SELECT_6_CDB_PARAM_LIST_LENGTH_OFFSET	4
-#define MODE_SELECT_10_CDB_PARAM_LIST_LENGTH_OFFSET	7
-#define MODE_SELECT_CDB_PAGE_FORMAT_MASK		0x10
-#define MODE_SELECT_CDB_SAVE_PAGES_MASK			0x1
-#define MODE_SELECT_6_BD_OFFSET				3
-#define MODE_SELECT_10_BD_OFFSET			6
-#define MODE_SELECT_10_LLBAA_OFFSET			4
-#define MODE_SELECT_10_LLBAA_MASK			1
-#define MODE_SELECT_6_MPH_SIZE				4
-#define MODE_SELECT_10_MPH_SIZE				8
-#define CACHING_MODE_PAGE_WCE_MASK			0x04
-#define MODE_SENSE_BLK_DESC_ENABLED			0
-#define MODE_SENSE_BLK_DESC_COUNT			1
-#define MODE_SELECT_PAGE_CODE_MASK			0x3F
-#define SHORT_DESC_BLOCK				8
-#define LONG_DESC_BLOCK					16
-#define MODE_PAGE_POW_CND_LEN_FIELD			0x26
-#define MODE_PAGE_INF_EXC_LEN_FIELD			0x0A
-#define MODE_PAGE_CACHING_LEN_FIELD			0x12
-#define MODE_PAGE_CONTROL_LEN_FIELD			0x0A
-#define MODE_SENSE_PC_CURRENT_VALUES			0
-
-/* Log Sense defines */
-#define LOG_PAGE_SUPPORTED_LOG_PAGES_PAGE		0x00
-#define LOG_PAGE_SUPPORTED_LOG_PAGES_LENGTH		0x07
-#define LOG_PAGE_INFORMATIONAL_EXCEPTIONS_PAGE		0x2F
-#define LOG_PAGE_TEMPERATURE_PAGE			0x0D
-#define LOG_SENSE_CDB_SP_OFFSET				1
-#define LOG_SENSE_CDB_SP_NOT_ENABLED			0
-#define LOG_SENSE_CDB_PC_OFFSET				2
-#define LOG_SENSE_CDB_PC_MASK				0xC0
-#define LOG_SENSE_CDB_PC_SHIFT				6
-#define LOG_SENSE_CDB_PC_CUMULATIVE_VALUES		1
-#define LOG_SENSE_CDB_PAGE_CODE_MASK			0x3F
-#define LOG_SENSE_CDB_ALLOC_LENGTH_OFFSET		7
-#define REMAINING_INFO_EXCP_PAGE_LENGTH			0x8
-#define LOG_INFO_EXCP_PAGE_LENGTH			0xC
-#define REMAINING_TEMP_PAGE_LENGTH			0xC
-#define LOG_TEMP_PAGE_LENGTH				0x10
-#define LOG_TEMP_UNKNOWN				0xFF
-#define SUPPORTED_LOG_PAGES_PAGE_LENGTH			0x3
-
-/* Read Capacity defines */
-#define READ_CAP_10_RESP_SIZE				8
-#define READ_CAP_16_RESP_SIZE				32
-
-/* NVMe Namespace and Command Defines */
-#define BYTES_TO_DWORDS					4
-#define NVME_MAX_FIRMWARE_SLOT				7
-
-/* Report LUNs defines */
-#define REPORT_LUNS_FIRST_LUN_OFFSET			8
-
-/* SCSI ADDITIONAL SENSE Codes */
-
-#define SCSI_ASC_NO_SENSE				0x00
-#define SCSI_ASC_PERIPHERAL_DEV_WRITE_FAULT		0x03
-#define SCSI_ASC_LUN_NOT_READY				0x04
-#define SCSI_ASC_WARNING				0x0B
-#define SCSI_ASC_LOG_BLOCK_GUARD_CHECK_FAILED		0x10
-#define SCSI_ASC_LOG_BLOCK_APPTAG_CHECK_FAILED		0x10
-#define SCSI_ASC_LOG_BLOCK_REFTAG_CHECK_FAILED		0x10
-#define SCSI_ASC_UNRECOVERED_READ_ERROR			0x11
-#define SCSI_ASC_MISCOMPARE_DURING_VERIFY		0x1D
-#define SCSI_ASC_ACCESS_DENIED_INVALID_LUN_ID		0x20
-#define SCSI_ASC_ILLEGAL_COMMAND			0x20
-#define SCSI_ASC_ILLEGAL_BLOCK				0x21
-#define SCSI_ASC_INVALID_CDB				0x24
-#define SCSI_ASC_INVALID_LUN				0x25
-#define SCSI_ASC_INVALID_PARAMETER			0x26
-#define SCSI_ASC_FORMAT_COMMAND_FAILED			0x31
-#define SCSI_ASC_INTERNAL_TARGET_FAILURE		0x44
-
-/* SCSI ADDITIONAL SENSE Code Qualifiers */
-
-#define SCSI_ASCQ_CAUSE_NOT_REPORTABLE			0x00
-#define SCSI_ASCQ_FORMAT_COMMAND_FAILED			0x01
-#define SCSI_ASCQ_LOG_BLOCK_GUARD_CHECK_FAILED		0x01
-#define SCSI_ASCQ_LOG_BLOCK_APPTAG_CHECK_FAILED		0x02
-#define SCSI_ASCQ_LOG_BLOCK_REFTAG_CHECK_FAILED		0x03
-#define SCSI_ASCQ_FORMAT_IN_PROGRESS			0x04
-#define SCSI_ASCQ_POWER_LOSS_EXPECTED			0x08
-#define SCSI_ASCQ_INVALID_LUN_ID			0x09
-
-/**
- * DEVICE_SPECIFIC_PARAMETER in mode parameter header (see sbc2r16) to
- * enable DPOFUA support type 0x10 value.
- */
-#define DEVICE_SPECIFIC_PARAMETER			0
-#define VPD_ID_DESCRIPTOR_LENGTH sizeof(VPD_IDENTIFICATION_DESCRIPTOR)
-
-/* MACROs to extract information from CDBs */
-
-#define GET_OPCODE(cdb)		cdb[0]
-
-#define GET_U8_FROM_CDB(cdb, index) (cdb[index] << 0)
-
-#define GET_U16_FROM_CDB(cdb, index) ((cdb[index] << 8) | (cdb[index + 1] << 0))
-
-#define GET_U24_FROM_CDB(cdb, index) ((cdb[index] << 16) | \
-(cdb[index + 1] <<  8) | \
-(cdb[index + 2] <<  0))
-
-#define GET_U32_FROM_CDB(cdb, index) ((cdb[index] << 24) | \
-(cdb[index + 1] << 16) | \
-(cdb[index + 2] <<  8) | \
-(cdb[index + 3] <<  0))
-
-#define GET_U64_FROM_CDB(cdb, index) ((((u64)cdb[index]) << 56) | \
-(((u64)cdb[index + 1]) << 48) | \
-(((u64)cdb[index + 2]) << 40) | \
-(((u64)cdb[index + 3]) << 32) | \
-(((u64)cdb[index + 4]) << 24) | \
-(((u64)cdb[index + 5]) << 16) | \
-(((u64)cdb[index + 6]) <<  8) | \
-(((u64)cdb[index + 7]) <<  0))
-
-/* Inquiry Helper Macros */
-#define GET_INQ_EVPD_BIT(cdb) \
-((GET_U8_FROM_CDB(cdb, INQUIRY_EVPD_BYTE_OFFSET) &		\
-INQUIRY_EVPD_BIT_MASK) ? 1 : 0)
-
-#define GET_INQ_PAGE_CODE(cdb)					\
-(GET_U8_FROM_CDB(cdb, INQUIRY_PAGE_CODE_BYTE_OFFSET))
-
-#define GET_INQ_ALLOC_LENGTH(cdb)				\
-(GET_U16_FROM_CDB(cdb, INQUIRY_CDB_ALLOCATION_LENGTH_OFFSET))
-
-/* Report LUNs Helper Macros */
-#define GET_REPORT_LUNS_ALLOC_LENGTH(cdb)			\
-(GET_U32_FROM_CDB(cdb, REPORT_LUNS_CDB_ALLOC_LENGTH_OFFSET))
-
-/* Read Capacity Helper Macros */
-#define GET_READ_CAP_16_ALLOC_LENGTH(cdb)			\
-(GET_U32_FROM_CDB(cdb, READ_CAP_16_CDB_ALLOC_LENGTH_OFFSET))
-
-#define IS_READ_CAP_16(cdb)					\
-((cdb[0] == SERVICE_ACTION_IN && cdb[1] == SAI_READ_CAPACITY_16) ? 1 : 0)
-
-/* Request Sense Helper Macros */
-#define GET_REQUEST_SENSE_ALLOC_LENGTH(cdb)			\
-(GET_U8_FROM_CDB(cdb, REQUEST_SENSE_CDB_ALLOC_LENGTH_OFFSET))
-
-/* Mode Sense Helper Macros */
-#define GET_MODE_SENSE_DBD(cdb)					\
-((GET_U8_FROM_CDB(cdb, MODE_SENSE_DBD_OFFSET) & MODE_SENSE_DBD_MASK) >>	\
-MODE_SENSE_DBD_SHIFT)
-
-#define GET_MODE_SENSE_LLBAA(cdb)				\
-((GET_U8_FROM_CDB(cdb, MODE_SENSE_LLBAA_OFFSET) &		\
-MODE_SENSE_LLBAA_MASK) >> MODE_SENSE_LLBAA_SHIFT)
-
-#define GET_MODE_SENSE_MPH_SIZE(cdb10)				\
-(cdb10 ? MODE_SENSE10_MPH_SIZE : MODE_SENSE6_MPH_SIZE)
-
-
-/* Struct to gather data that needs to be extracted from a SCSI CDB.
-   Not conforming to any particular CDB variant, but compatible with all. */
-
-struct nvme_trans_io_cdb {
-	u8 fua;
-	u8 prot_info;
-	u64 lba;
-	u32 xfer_len;
-};
-
-
-/* Internal Helper Functions */
-
-
-/* Copy data to userspace memory */
-
-static int nvme_trans_copy_to_user(struct sg_io_hdr *hdr, void *from,
-								unsigned long n)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	unsigned long not_copied;
-	int i;
-	void *index = from;
-	size_t remaining = n;
-	size_t xfer_len;
-
-	if (hdr->iovec_count > 0) {
-		struct sg_iovec sgl;
-
-		for (i = 0; i < hdr->iovec_count; i++) {
-			not_copied = copy_from_user(&sgl, hdr->dxferp +
-						i * sizeof(struct sg_iovec),
-						sizeof(struct sg_iovec));
-			if (not_copied)
-				return -EFAULT;
-			xfer_len = min(remaining, sgl.iov_len);
-			not_copied = copy_to_user(sgl.iov_base, index,
-								xfer_len);
-			if (not_copied) {
-				res = -EFAULT;
-				break;
-			}
-			index += xfer_len;
-			remaining -= xfer_len;
-			if (remaining == 0)
-				break;
-		}
-		return res;
-	}
-	not_copied = copy_to_user(hdr->dxferp, from, n);
-	if (not_copied)
-		res = -EFAULT;
-	return res;
-}
-
-/* Copy data from userspace memory */
-
-static int nvme_trans_copy_from_user(struct sg_io_hdr *hdr, void *to,
-								unsigned long n)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	unsigned long not_copied;
-	int i;
-	void *index = to;
-	size_t remaining = n;
-	size_t xfer_len;
-
-	if (hdr->iovec_count > 0) {
-		struct sg_iovec sgl;
-
-		for (i = 0; i < hdr->iovec_count; i++) {
-			not_copied = copy_from_user(&sgl, hdr->dxferp +
-						i * sizeof(struct sg_iovec),
-						sizeof(struct sg_iovec));
-			if (not_copied)
-				return -EFAULT;
-			xfer_len = min(remaining, sgl.iov_len);
-			not_copied = copy_from_user(index, sgl.iov_base,
-								xfer_len);
-			if (not_copied) {
-				res = -EFAULT;
-				break;
-			}
-			index += xfer_len;
-			remaining -= xfer_len;
-			if (remaining == 0)
-				break;
-		}
-		return res;
-	}
-
-	not_copied = copy_from_user(to, hdr->dxferp, n);
-	if (not_copied)
-		res = -EFAULT;
-	return res;
-}
-
-/* Status/Sense Buffer Writeback */
-
-static int nvme_trans_completion(struct sg_io_hdr *hdr, u8 status, u8 sense_key,
-				 u8 asc, u8 ascq)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 xfer_len;
-	u8 resp[DESC_FMT_SENSE_DATA_SIZE];
-
-	if (scsi_status_is_good(status)) {
-		hdr->status = SAM_STAT_GOOD;
-		hdr->masked_status = GOOD;
-		hdr->host_status = DID_OK;
-		hdr->driver_status = DRIVER_OK;
-		hdr->sb_len_wr = 0;
-	} else {
-		hdr->status = status;
-		hdr->masked_status = status >> 1;
-		hdr->host_status = DID_OK;
-		hdr->driver_status = DRIVER_OK;
-
-		memset(resp, 0, DESC_FMT_SENSE_DATA_SIZE);
-		resp[0] = DESC_FORMAT_SENSE_DATA;
-		resp[1] = sense_key;
-		resp[2] = asc;
-		resp[3] = ascq;
-
-		xfer_len = min_t(u8, hdr->mx_sb_len, DESC_FMT_SENSE_DATA_SIZE);
-		hdr->sb_len_wr = xfer_len;
-		if (copy_to_user(hdr->sbp, resp, xfer_len) > 0)
-			res = -EFAULT;
-	}
-
-	return res;
-}
-
-static int nvme_trans_status_code(struct sg_io_hdr *hdr, int nvme_sc)
-{
-	u8 status, sense_key, asc, ascq;
-	int res = SNTI_TRANSLATION_SUCCESS;
-
-	/* For non-nvme (Linux) errors, simply return the error code */
-	if (nvme_sc < 0)
-		return nvme_sc;
-
-	/* Mask DNR, More, and reserved fields */
-	nvme_sc &= 0x7FF;
-
-	switch (nvme_sc) {
-	/* Generic Command Status */
-	case NVME_SC_SUCCESS:
-		status = SAM_STAT_GOOD;
-		sense_key = NO_SENSE;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_INVALID_OPCODE:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_ILLEGAL_COMMAND;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_INVALID_FIELD:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_INVALID_CDB;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_DATA_XFER_ERROR:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_POWER_LOSS:
-		status = SAM_STAT_TASK_ABORTED;
-		sense_key = ABORTED_COMMAND;
-		asc = SCSI_ASC_WARNING;
-		ascq = SCSI_ASCQ_POWER_LOSS_EXPECTED;
-		break;
-	case NVME_SC_INTERNAL:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = HARDWARE_ERROR;
-		asc = SCSI_ASC_INTERNAL_TARGET_FAILURE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_ABORT_REQ:
-		status = SAM_STAT_TASK_ABORTED;
-		sense_key = ABORTED_COMMAND;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_ABORT_QUEUE:
-		status = SAM_STAT_TASK_ABORTED;
-		sense_key = ABORTED_COMMAND;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_FUSED_FAIL:
-		status = SAM_STAT_TASK_ABORTED;
-		sense_key = ABORTED_COMMAND;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_FUSED_MISSING:
-		status = SAM_STAT_TASK_ABORTED;
-		sense_key = ABORTED_COMMAND;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_INVALID_NS:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_ACCESS_DENIED_INVALID_LUN_ID;
-		ascq = SCSI_ASCQ_INVALID_LUN_ID;
-		break;
-	case NVME_SC_LBA_RANGE:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_ILLEGAL_BLOCK;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_CAP_EXCEEDED:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_NS_NOT_READY:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = NOT_READY;
-		asc = SCSI_ASC_LUN_NOT_READY;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-
-	/* Command Specific Status */
-	case NVME_SC_INVALID_FORMAT:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_FORMAT_COMMAND_FAILED;
-		ascq = SCSI_ASCQ_FORMAT_COMMAND_FAILED;
-		break;
-	case NVME_SC_BAD_ATTRIBUTES:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_INVALID_CDB;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-
-	/* Media Errors */
-	case NVME_SC_WRITE_FAULT:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_PERIPHERAL_DEV_WRITE_FAULT;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_READ_ERROR:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_UNRECOVERED_READ_ERROR;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_GUARD_CHECK:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_LOG_BLOCK_GUARD_CHECK_FAILED;
-		ascq = SCSI_ASCQ_LOG_BLOCK_GUARD_CHECK_FAILED;
-		break;
-	case NVME_SC_APPTAG_CHECK:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_LOG_BLOCK_APPTAG_CHECK_FAILED;
-		ascq = SCSI_ASCQ_LOG_BLOCK_APPTAG_CHECK_FAILED;
-		break;
-	case NVME_SC_REFTAG_CHECK:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MEDIUM_ERROR;
-		asc = SCSI_ASC_LOG_BLOCK_REFTAG_CHECK_FAILED;
-		ascq = SCSI_ASCQ_LOG_BLOCK_REFTAG_CHECK_FAILED;
-		break;
-	case NVME_SC_COMPARE_FAILED:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = MISCOMPARE;
-		asc = SCSI_ASC_MISCOMPARE_DURING_VERIFY;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	case NVME_SC_ACCESS_DENIED:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_ACCESS_DENIED_INVALID_LUN_ID;
-		ascq = SCSI_ASCQ_INVALID_LUN_ID;
-		break;
-
-	/* Unspecified/Default */
-	case NVME_SC_CMDID_CONFLICT:
-	case NVME_SC_CMD_SEQ_ERROR:
-	case NVME_SC_CQ_INVALID:
-	case NVME_SC_QID_INVALID:
-	case NVME_SC_QUEUE_SIZE:
-	case NVME_SC_ABORT_LIMIT:
-	case NVME_SC_ABORT_MISSING:
-	case NVME_SC_ASYNC_LIMIT:
-	case NVME_SC_FIRMWARE_SLOT:
-	case NVME_SC_FIRMWARE_IMAGE:
-	case NVME_SC_INVALID_VECTOR:
-	case NVME_SC_INVALID_LOG_PAGE:
-	default:
-		status = SAM_STAT_CHECK_CONDITION;
-		sense_key = ILLEGAL_REQUEST;
-		asc = SCSI_ASC_NO_SENSE;
-		ascq = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		break;
-	}
-
-	res = nvme_trans_completion(hdr, status, sense_key, asc, ascq);
-
-	return res;
-}
-
-/* INQUIRY Helper Functions */
-
-static int nvme_trans_standard_inquiry_page(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *inq_response,
-					int alloc_len)
-{
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ns *id_ns;
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	int xfer_len;
-	u8 resp_data_format = 0x02;
-	u8 protect;
-	u8 cmdque = 0x01 << 1;
-	u8 fw_offset = sizeof(dev->firmware_rev);
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-				&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out_dma;
-	}
-
-	/* nvme ns identify - use DPS value for PROTECT field */
-	nvme_sc = nvme_identify(dev, ns->ns_id, 0, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	/*
-	 * If nvme_sc was -ve, res will be -ve here.
-	 * If nvme_sc was +ve, the status would bace been translated, and res
-	 *  can only be 0 or -ve.
-	 *    - If 0 && nvme_sc > 0, then go into next if where res gets nvme_sc
-	 *    - If -ve, return because its a Linux error.
-	 */
-	if (res)
-		goto out_free;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_free;
-	}
-	id_ns = mem;
-	(id_ns->dps) ? (protect = 0x01) : (protect = 0);
-
-	memset(inq_response, 0, STANDARD_INQUIRY_LENGTH);
-	inq_response[2] = VERSION_SPC_4;
-	inq_response[3] = resp_data_format;	/*normaca=0 | hisup=0 */
-	inq_response[4] = ADDITIONAL_STD_INQ_LENGTH;
-	inq_response[5] = protect;	/* sccs=0 | acc=0 | tpgs=0 | pc3=0 */
-	inq_response[7] = cmdque;	/* wbus16=0 | sync=0 | vs=0 */
-	strncpy(&inq_response[8], "NVMe    ", 8);
-	strncpy(&inq_response[16], dev->model, 16);
-
-	while (dev->firmware_rev[fw_offset - 1] == ' ' && fw_offset > 4)
-		fw_offset--;
-	fw_offset -= 4;
-	strncpy(&inq_response[32], dev->firmware_rev + fw_offset, 4);
-
-	xfer_len = min(alloc_len, STANDARD_INQUIRY_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
-
- out_free:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
- out_dma:
-	return res;
-}
-
-static int nvme_trans_supported_vpd_pages(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *inq_response,
-					int alloc_len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-
-	memset(inq_response, 0, STANDARD_INQUIRY_LENGTH);
-	inq_response[1] = INQ_SUPPORTED_VPD_PAGES_PAGE;   /* Page Code */
-	inq_response[3] = INQ_NUM_SUPPORTED_VPD_PAGES;    /* Page Length */
-	inq_response[4] = INQ_SUPPORTED_VPD_PAGES_PAGE;
-	inq_response[5] = INQ_UNIT_SERIAL_NUMBER_PAGE;
-	inq_response[6] = INQ_DEVICE_IDENTIFICATION_PAGE;
-	inq_response[7] = INQ_EXTENDED_INQUIRY_DATA_PAGE;
-	inq_response[8] = INQ_BDEV_CHARACTERISTICS_PAGE;
-
-	xfer_len = min(alloc_len, STANDARD_INQUIRY_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
-
-	return res;
-}
-
-static int nvme_trans_unit_serial_page(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *inq_response,
-					int alloc_len)
-{
-	struct nvme_dev *dev = ns->dev;
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-
-	memset(inq_response, 0, STANDARD_INQUIRY_LENGTH);
-	inq_response[1] = INQ_UNIT_SERIAL_NUMBER_PAGE; /* Page Code */
-	inq_response[3] = INQ_SERIAL_NUMBER_LENGTH;    /* Page Length */
-	strncpy(&inq_response[4], dev->serial, INQ_SERIAL_NUMBER_LENGTH);
-
-	xfer_len = min(alloc_len, STANDARD_INQUIRY_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
-
-	return res;
-}
-
-static int nvme_trans_device_id_page(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					u8 *inq_response, int alloc_len)
-{
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ctrl *id_ctrl;
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	u8 ieee[4];
-	int xfer_len;
-	__be32 tmp_id = cpu_to_be32(ns->ns_id);
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-					&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out_dma;
-	}
-
-	/* nvme controller identify */
-	nvme_sc = nvme_identify(dev, 0, 1, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_free;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_free;
-	}
-	id_ctrl = mem;
-
-	/* Since SCSI tried to save 4 bits... [SPC-4(r34) Table 591] */
-	ieee[0] = id_ctrl->ieee[0] << 4;
-	ieee[1] = id_ctrl->ieee[0] >> 4 | id_ctrl->ieee[1] << 4;
-	ieee[2] = id_ctrl->ieee[1] >> 4 | id_ctrl->ieee[2] << 4;
-	ieee[3] = id_ctrl->ieee[2] >> 4;
-
-	memset(inq_response, 0, STANDARD_INQUIRY_LENGTH);
-	inq_response[1] = INQ_DEVICE_IDENTIFICATION_PAGE;    /* Page Code */
-	inq_response[3] = 20;      /* Page Length */
-	/* Designation Descriptor start */
-	inq_response[4] = 0x01;    /* Proto ID=0h | Code set=1h */
-	inq_response[5] = 0x03;    /* PIV=0b | Asso=00b | Designator Type=3h */
-	inq_response[6] = 0x00;    /* Rsvd */
-	inq_response[7] = 16;      /* Designator Length */
-	/* Designator start */
-	inq_response[8] = 0x60 | ieee[3]; /* NAA=6h | IEEE ID MSB, High nibble*/
-	inq_response[9] = ieee[2];        /* IEEE ID */
-	inq_response[10] = ieee[1];       /* IEEE ID */
-	inq_response[11] = ieee[0];       /* IEEE ID| Vendor Specific ID... */
-	inq_response[12] = (dev->pci_dev->vendor & 0xFF00) >> 8;
-	inq_response[13] = (dev->pci_dev->vendor & 0x00FF);
-	inq_response[14] = dev->serial[0];
-	inq_response[15] = dev->serial[1];
-	inq_response[16] = dev->model[0];
-	inq_response[17] = dev->model[1];
-	memcpy(&inq_response[18], &tmp_id, sizeof(u32));
-	/* Last 2 bytes are zero */
-
-	xfer_len = min(alloc_len, STANDARD_INQUIRY_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
-
- out_free:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
- out_dma:
-	return res;
-}
-
-static int nvme_trans_ext_inq_page(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					int alloc_len)
-{
-	u8 *inq_response;
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ctrl *id_ctrl;
-	struct nvme_id_ns *id_ns;
-	int xfer_len;
-	u8 microcode = 0x80;
-	u8 spt;
-	u8 spt_lut[8] = {0, 0, 2, 1, 4, 6, 5, 7};
-	u8 grd_chk, app_chk, ref_chk, protect;
-	u8 uask_sup = 0x20;
-	u8 v_sup;
-	u8 luiclr = 0x01;
-
-	inq_response = kmalloc(EXTENDED_INQUIRY_DATA_PAGE_LENGTH, GFP_KERNEL);
-	if (inq_response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-							&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out_dma;
-	}
-
-	/* nvme ns identify */
-	nvme_sc = nvme_identify(dev, ns->ns_id, 0, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_free;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_free;
-	}
-	id_ns = mem;
-	spt = spt_lut[(id_ns->dpc) & 0x07] << 3;
-	(id_ns->dps) ? (protect = 0x01) : (protect = 0);
-	grd_chk = protect << 2;
-	app_chk = protect << 1;
-	ref_chk = protect;
-
-	/* nvme controller identify */
-	nvme_sc = nvme_identify(dev, 0, 1, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_free;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_free;
-	}
-	id_ctrl = mem;
-	v_sup = id_ctrl->vwc;
-
-	memset(inq_response, 0, EXTENDED_INQUIRY_DATA_PAGE_LENGTH);
-	inq_response[1] = INQ_EXTENDED_INQUIRY_DATA_PAGE;    /* Page Code */
-	inq_response[2] = 0x00;    /* Page Length MSB */
-	inq_response[3] = 0x3C;    /* Page Length LSB */
-	inq_response[4] = microcode | spt | grd_chk | app_chk | ref_chk;
-	inq_response[5] = uask_sup;
-	inq_response[6] = v_sup;
-	inq_response[7] = luiclr;
-	inq_response[8] = 0;
-	inq_response[9] = 0;
-
-	xfer_len = min(alloc_len, EXTENDED_INQUIRY_DATA_PAGE_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
-
- out_free:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
- out_dma:
-	kfree(inq_response);
- out_mem:
-	return res;
-}
-
-static int nvme_trans_bdev_char_page(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					int alloc_len)
-{
-	u8 *inq_response;
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-
-	inq_response = kzalloc(EXTENDED_INQUIRY_DATA_PAGE_LENGTH, GFP_KERNEL);
-	if (inq_response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	inq_response[1] = INQ_BDEV_CHARACTERISTICS_PAGE;    /* Page Code */
-	inq_response[2] = 0x00;    /* Page Length MSB */
-	inq_response[3] = 0x3C;    /* Page Length LSB */
-	inq_response[4] = 0x00;    /* Medium Rotation Rate MSB */
-	inq_response[5] = 0x01;    /* Medium Rotation Rate LSB */
-	inq_response[6] = 0x00;    /* Form Factor */
-
-	xfer_len = min(alloc_len, EXTENDED_INQUIRY_DATA_PAGE_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, inq_response, xfer_len);
-
-	kfree(inq_response);
- out_mem:
-	return res;
-}
-
-/* LOG SENSE Helper Functions */
-
-static int nvme_trans_log_supp_pages(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					int alloc_len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-	u8 *log_response;
-
-	log_response = kzalloc(LOG_PAGE_SUPPORTED_LOG_PAGES_LENGTH, GFP_KERNEL);
-	if (log_response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	log_response[0] = LOG_PAGE_SUPPORTED_LOG_PAGES_PAGE;
-	/* Subpage=0x00, Page Length MSB=0 */
-	log_response[3] = SUPPORTED_LOG_PAGES_PAGE_LENGTH;
-	log_response[4] = LOG_PAGE_SUPPORTED_LOG_PAGES_PAGE;
-	log_response[5] = LOG_PAGE_INFORMATIONAL_EXCEPTIONS_PAGE;
-	log_response[6] = LOG_PAGE_TEMPERATURE_PAGE;
-
-	xfer_len = min(alloc_len, LOG_PAGE_SUPPORTED_LOG_PAGES_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, log_response, xfer_len);
-
-	kfree(log_response);
- out_mem:
-	return res;
-}
-
-static int nvme_trans_log_info_exceptions(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, int alloc_len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-	u8 *log_response;
-	struct nvme_command c;
-	struct nvme_dev *dev = ns->dev;
-	struct nvme_smart_log *smart_log;
-	dma_addr_t dma_addr;
-	void *mem;
-	u8 temp_c;
-	u16 temp_k;
-
-	log_response = kzalloc(LOG_INFO_EXCP_PAGE_LENGTH, GFP_KERNEL);
-	if (log_response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev,
-					sizeof(struct nvme_smart_log),
-					&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out_dma;
-	}
-
-	/* Get SMART Log Page */
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = nvme_admin_get_log_page;
-	c.common.nsid = cpu_to_le32(0xFFFFFFFF);
-	c.common.prp1 = cpu_to_le64(dma_addr);
-	c.common.cdw10[0] = cpu_to_le32((((sizeof(struct nvme_smart_log) /
-			BYTES_TO_DWORDS) - 1) << 16) | NVME_LOG_SMART);
-	res = nvme_submit_admin_cmd(dev, &c, NULL);
-	if (res != NVME_SC_SUCCESS) {
-		temp_c = LOG_TEMP_UNKNOWN;
-	} else {
-		smart_log = mem;
-		temp_k = (smart_log->temperature[1] << 8) +
-				(smart_log->temperature[0]);
-		temp_c = temp_k - KELVIN_TEMP_FACTOR;
-	}
-
-	log_response[0] = LOG_PAGE_INFORMATIONAL_EXCEPTIONS_PAGE;
-	/* Subpage=0x00, Page Length MSB=0 */
-	log_response[3] = REMAINING_INFO_EXCP_PAGE_LENGTH;
-	/* Informational Exceptions Log Parameter 1 Start */
-	/* Parameter Code=0x0000 bytes 4,5 */
-	log_response[6] = 0x23; /* DU=0, TSD=1, ETC=0, TMC=0, FMT_AND_LNK=11b */
-	log_response[7] = 0x04; /* PARAMETER LENGTH */
-	/* Add sense Code and qualifier = 0x00 each */
-	/* Use Temperature from NVMe Get Log Page, convert to C from K */
-	log_response[10] = temp_c;
-
-	xfer_len = min(alloc_len, LOG_INFO_EXCP_PAGE_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, log_response, xfer_len);
-
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_smart_log),
-			  mem, dma_addr);
- out_dma:
-	kfree(log_response);
- out_mem:
-	return res;
-}
-
-static int nvme_trans_log_temperature(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					int alloc_len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-	u8 *log_response;
-	struct nvme_command c;
-	struct nvme_dev *dev = ns->dev;
-	struct nvme_smart_log *smart_log;
-	dma_addr_t dma_addr;
-	void *mem;
-	u32 feature_resp;
-	u8 temp_c_cur, temp_c_thresh;
-	u16 temp_k;
-
-	log_response = kzalloc(LOG_TEMP_PAGE_LENGTH, GFP_KERNEL);
-	if (log_response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev,
-					sizeof(struct nvme_smart_log),
-					&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out_dma;
-	}
-
-	/* Get SMART Log Page */
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = nvme_admin_get_log_page;
-	c.common.nsid = cpu_to_le32(0xFFFFFFFF);
-	c.common.prp1 = cpu_to_le64(dma_addr);
-	c.common.cdw10[0] = cpu_to_le32((((sizeof(struct nvme_smart_log) /
-			BYTES_TO_DWORDS) - 1) << 16) | NVME_LOG_SMART);
-	res = nvme_submit_admin_cmd(dev, &c, NULL);
-	if (res != NVME_SC_SUCCESS) {
-		temp_c_cur = LOG_TEMP_UNKNOWN;
-	} else {
-		smart_log = mem;
-		temp_k = (smart_log->temperature[1] << 8) +
-				(smart_log->temperature[0]);
-		temp_c_cur = temp_k - KELVIN_TEMP_FACTOR;
-	}
-
-	/* Get Features for Temp Threshold */
-	res = nvme_get_features(dev, NVME_FEAT_TEMP_THRESH, 0, 0,
-								&feature_resp);
-	if (res != NVME_SC_SUCCESS)
-		temp_c_thresh = LOG_TEMP_UNKNOWN;
-	else
-		temp_c_thresh = (feature_resp & 0xFFFF) - KELVIN_TEMP_FACTOR;
-
-	log_response[0] = LOG_PAGE_TEMPERATURE_PAGE;
-	/* Subpage=0x00, Page Length MSB=0 */
-	log_response[3] = REMAINING_TEMP_PAGE_LENGTH;
-	/* Temperature Log Parameter 1 (Temperature) Start */
-	/* Parameter Code = 0x0000 */
-	log_response[6] = 0x01;		/* Format and Linking = 01b */
-	log_response[7] = 0x02;		/* Parameter Length */
-	/* Use Temperature from NVMe Get Log Page, convert to C from K */
-	log_response[9] = temp_c_cur;
-	/* Temperature Log Parameter 2 (Reference Temperature) Start */
-	log_response[11] = 0x01;	/* Parameter Code = 0x0001 */
-	log_response[12] = 0x01;	/* Format and Linking = 01b */
-	log_response[13] = 0x02;	/* Parameter Length */
-	/* Use Temperature Thresh from NVMe Get Log Page, convert to C from K */
-	log_response[15] = temp_c_thresh;
-
-	xfer_len = min(alloc_len, LOG_TEMP_PAGE_LENGTH);
-	res = nvme_trans_copy_to_user(hdr, log_response, xfer_len);
-
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_smart_log),
-			  mem, dma_addr);
- out_dma:
-	kfree(log_response);
- out_mem:
-	return res;
-}
-
-/* MODE SENSE Helper Functions */
-
-static int nvme_trans_fill_mode_parm_hdr(u8 *resp, int len, u8 cdb10, u8 llbaa,
-					u16 mode_data_length, u16 blk_desc_len)
-{
-	/* Quick check to make sure I don't stomp on my own memory... */
-	if ((cdb10 && len < 8) || (!cdb10 && len < 4))
-		return SNTI_INTERNAL_ERROR;
-
-	if (cdb10) {
-		resp[0] = (mode_data_length & 0xFF00) >> 8;
-		resp[1] = (mode_data_length & 0x00FF);
-		/* resp[2] and [3] are zero */
-		resp[4] = llbaa;
-		resp[5] = RESERVED_FIELD;
-		resp[6] = (blk_desc_len & 0xFF00) >> 8;
-		resp[7] = (blk_desc_len & 0x00FF);
-	} else {
-		resp[0] = (mode_data_length & 0x00FF);
-		/* resp[1] and [2] are zero */
-		resp[3] = (blk_desc_len & 0x00FF);
-	}
-
-	return SNTI_TRANSLATION_SUCCESS;
-}
-
-static int nvme_trans_fill_blk_desc(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-				    u8 *resp, int len, u8 llbaa)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ns *id_ns;
-	u8 flbas;
-	u32 lba_length;
-
-	if (llbaa == 0 && len < MODE_PAGE_BLK_DES_LEN)
-		return SNTI_INTERNAL_ERROR;
-	else if (llbaa > 0 && len < MODE_PAGE_LLBAA_BLK_DES_LEN)
-		return SNTI_INTERNAL_ERROR;
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-							&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-
-	/* nvme ns identify */
-	nvme_sc = nvme_identify(dev, ns->ns_id, 0, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_dma;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_dma;
-	}
-	id_ns = mem;
-	flbas = (id_ns->flbas) & 0x0F;
-	lba_length = (1 << (id_ns->lbaf[flbas].ds));
-
-	if (llbaa == 0) {
-		__be32 tmp_cap = cpu_to_be32(le64_to_cpu(id_ns->ncap));
-		/* Byte 4 is reserved */
-		__be32 tmp_len = cpu_to_be32(lba_length & 0x00FFFFFF);
-
-		memcpy(resp, &tmp_cap, sizeof(u32));
-		memcpy(&resp[4], &tmp_len, sizeof(u32));
-	} else {
-		__be64 tmp_cap = cpu_to_be64(le64_to_cpu(id_ns->ncap));
-		__be32 tmp_len = cpu_to_be32(lba_length);
-
-		memcpy(resp, &tmp_cap, sizeof(u64));
-		/* Bytes 8, 9, 10, 11 are reserved */
-		memcpy(&resp[12], &tmp_len, sizeof(u32));
-	}
-
- out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
- out:
-	return res;
-}
-
-static int nvme_trans_fill_control_page(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *resp,
-					int len)
-{
-	if (len < MODE_PAGE_CONTROL_LEN)
-		return SNTI_INTERNAL_ERROR;
-
-	resp[0] = MODE_PAGE_CONTROL;
-	resp[1] = MODE_PAGE_CONTROL_LEN_FIELD;
-	resp[2] = 0x0E;		/* TST=000b, TMF_ONLY=0, DPICZ=1,
-				 * D_SENSE=1, GLTSD=1, RLEC=0 */
-	resp[3] = 0x12;		/* Q_ALGO_MODIFIER=1h, NUAR=0, QERR=01b */
-	/* Byte 4:  VS=0, RAC=0, UA_INT=0, SWP=0 */
-	resp[5] = 0x40;		/* ATO=0, TAS=1, ATMPE=0, RWWP=0, AUTOLOAD=0 */
-	/* resp[6] and [7] are obsolete, thus zero */
-	resp[8] = 0xFF;		/* Busy timeout period = 0xffff */
-	resp[9] = 0xFF;
-	/* Bytes 10,11: Extended selftest completion time = 0x0000 */
-
-	return SNTI_TRANSLATION_SUCCESS;
-}
-
-static int nvme_trans_fill_caching_page(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr,
-					u8 *resp, int len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	u32 feature_resp;
-	u8 vwc;
-
-	if (len < MODE_PAGE_CACHING_LEN)
-		return SNTI_INTERNAL_ERROR;
-
-	nvme_sc = nvme_get_features(dev, NVME_FEAT_VOLATILE_WC, 0, 0,
-								&feature_resp);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out;
-	}
-	vwc = feature_resp & 0x00000001;
-
-	resp[0] = MODE_PAGE_CACHING;
-	resp[1] = MODE_PAGE_CACHING_LEN_FIELD;
-	resp[2] = vwc << 2;
-
- out:
-	return res;
-}
-
-static int nvme_trans_fill_pow_cnd_page(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *resp,
-					int len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-
-	if (len < MODE_PAGE_POW_CND_LEN)
-		return SNTI_INTERNAL_ERROR;
-
-	resp[0] = MODE_PAGE_POWER_CONDITION;
-	resp[1] = MODE_PAGE_POW_CND_LEN_FIELD;
-	/* All other bytes are zero */
-
-	return res;
-}
-
-static int nvme_trans_fill_inf_exc_page(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *resp,
-					int len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-
-	if (len < MODE_PAGE_INF_EXC_LEN)
-		return SNTI_INTERNAL_ERROR;
-
-	resp[0] = MODE_PAGE_INFO_EXCEP;
-	resp[1] = MODE_PAGE_INF_EXC_LEN_FIELD;
-	resp[2] = 0x88;
-	/* All other bytes are zero */
-
-	return res;
-}
-
-static int nvme_trans_fill_all_pages(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-				     u8 *resp, int len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u16 mode_pages_offset_1 = 0;
-	u16 mode_pages_offset_2, mode_pages_offset_3, mode_pages_offset_4;
-
-	mode_pages_offset_2 = mode_pages_offset_1 + MODE_PAGE_CACHING_LEN;
-	mode_pages_offset_3 = mode_pages_offset_2 + MODE_PAGE_CONTROL_LEN;
-	mode_pages_offset_4 = mode_pages_offset_3 + MODE_PAGE_POW_CND_LEN;
-
-	res = nvme_trans_fill_caching_page(ns, hdr, &resp[mode_pages_offset_1],
-					MODE_PAGE_CACHING_LEN);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-	res = nvme_trans_fill_control_page(ns, hdr, &resp[mode_pages_offset_2],
-					MODE_PAGE_CONTROL_LEN);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-	res = nvme_trans_fill_pow_cnd_page(ns, hdr, &resp[mode_pages_offset_3],
-					MODE_PAGE_POW_CND_LEN);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-	res = nvme_trans_fill_inf_exc_page(ns, hdr, &resp[mode_pages_offset_4],
-					MODE_PAGE_INF_EXC_LEN);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-
- out:
-	return res;
-}
-
-static inline int nvme_trans_get_blk_desc_len(u8 dbd, u8 llbaa)
-{
-	if (dbd == MODE_SENSE_BLK_DESC_ENABLED) {
-		/* SPC-4: len = 8 x Num_of_descriptors if llbaa = 0, 16x if 1 */
-		return 8 * (llbaa + 1) * MODE_SENSE_BLK_DESC_COUNT;
-	} else {
-		return 0;
-	}
-}
-
-static int nvme_trans_mode_page_create(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *cmd,
-					u16 alloc_len, u8 cdb10,
-					int (*mode_page_fill_func)
-					(struct nvme_ns *,
-					struct sg_io_hdr *hdr, u8 *, int),
-					u16 mode_pages_tot_len)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int xfer_len;
-	u8 *response;
-	u8 dbd, llbaa;
-	u16 resp_size;
-	int mph_size;
-	u16 mode_pages_offset_1;
-	u16 blk_desc_len, blk_desc_offset, mode_data_length;
-
-	dbd = GET_MODE_SENSE_DBD(cmd);
-	llbaa = GET_MODE_SENSE_LLBAA(cmd);
-	mph_size = GET_MODE_SENSE_MPH_SIZE(cdb10);
-	blk_desc_len = nvme_trans_get_blk_desc_len(dbd, llbaa);
-
-	resp_size = mph_size + blk_desc_len + mode_pages_tot_len;
-	/* Refer spc4r34 Table 440 for calculation of Mode data Length field */
-	mode_data_length = 3 + (3 * cdb10) + blk_desc_len + mode_pages_tot_len;
-
-	blk_desc_offset = mph_size;
-	mode_pages_offset_1 = blk_desc_offset + blk_desc_len;
-
-	response = kzalloc(resp_size, GFP_KERNEL);
-	if (response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	res = nvme_trans_fill_mode_parm_hdr(&response[0], mph_size, cdb10,
-					llbaa, mode_data_length, blk_desc_len);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out_free;
-	if (blk_desc_len > 0) {
-		res = nvme_trans_fill_blk_desc(ns, hdr,
-					       &response[blk_desc_offset],
-					       blk_desc_len, llbaa);
-		if (res != SNTI_TRANSLATION_SUCCESS)
-			goto out_free;
-	}
-	res = mode_page_fill_func(ns, hdr, &response[mode_pages_offset_1],
-					mode_pages_tot_len);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out_free;
-
-	xfer_len = min(alloc_len, resp_size);
-	res = nvme_trans_copy_to_user(hdr, response, xfer_len);
-
- out_free:
-	kfree(response);
- out_mem:
-	return res;
-}
-
-/* Read Capacity Helper Functions */
-
-static void nvme_trans_fill_read_cap(u8 *response, struct nvme_id_ns *id_ns,
-								u8 cdb16)
-{
-	u8 flbas;
-	u32 lba_length;
-	u64 rlba;
-	u8 prot_en;
-	u8 p_type_lut[4] = {0, 0, 1, 2};
-	__be64 tmp_rlba;
-	__be32 tmp_rlba_32;
-	__be32 tmp_len;
-
-	flbas = (id_ns->flbas) & 0x0F;
-	lba_length = (1 << (id_ns->lbaf[flbas].ds));
-	rlba = le64_to_cpup(&id_ns->nsze) - 1;
-	(id_ns->dps) ? (prot_en = 0x01) : (prot_en = 0);
-
-	if (!cdb16) {
-		if (rlba > 0xFFFFFFFF)
-			rlba = 0xFFFFFFFF;
-		tmp_rlba_32 = cpu_to_be32(rlba);
-		tmp_len = cpu_to_be32(lba_length);
-		memcpy(response, &tmp_rlba_32, sizeof(u32));
-		memcpy(&response[4], &tmp_len, sizeof(u32));
-	} else {
-		tmp_rlba = cpu_to_be64(rlba);
-		tmp_len = cpu_to_be32(lba_length);
-		memcpy(response, &tmp_rlba, sizeof(u64));
-		memcpy(&response[8], &tmp_len, sizeof(u32));
-		response[12] = (p_type_lut[id_ns->dps & 0x3] << 1) | prot_en;
-		/* P_I_Exponent = 0x0 | LBPPBE = 0x0 */
-		/* LBPME = 0 | LBPRZ = 0 | LALBA = 0x00 */
-		/* Bytes 16-31 - Reserved */
-	}
-}
-
-/* Start Stop Unit Helper Functions */
-
-static int nvme_trans_power_state(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-						u8 pc, u8 pcmod, u8 start)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ctrl *id_ctrl;
-	int lowest_pow_st;	/* max npss = lowest power consumption */
-	unsigned ps_desired = 0;
-
-	/* NVMe Controller Identify */
-	mem = dma_alloc_coherent(&dev->pci_dev->dev,
-				sizeof(struct nvme_id_ctrl),
-				&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-	nvme_sc = nvme_identify(dev, 0, 1, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_dma;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_dma;
-	}
-	id_ctrl = mem;
-	lowest_pow_st = max(POWER_STATE_0, (int)(id_ctrl->npss - 1));
-
-	switch (pc) {
-	case NVME_POWER_STATE_START_VALID:
-		/* Action unspecified if POWER CONDITION MODIFIER != 0 */
-		if (pcmod == 0 && start == 0x1)
-			ps_desired = POWER_STATE_0;
-		if (pcmod == 0 && start == 0x0)
-			ps_desired = lowest_pow_st;
-		break;
-	case NVME_POWER_STATE_ACTIVE:
-		/* Action unspecified if POWER CONDITION MODIFIER != 0 */
-		if (pcmod == 0)
-			ps_desired = POWER_STATE_0;
-		break;
-	case NVME_POWER_STATE_IDLE:
-		/* Action unspecified if POWER CONDITION MODIFIER != [0,1,2] */
-		if (pcmod == 0x0)
-			ps_desired = POWER_STATE_1;
-		else if (pcmod == 0x1)
-			ps_desired = POWER_STATE_2;
-		else if (pcmod == 0x2)
-			ps_desired = POWER_STATE_3;
-		break;
-	case NVME_POWER_STATE_STANDBY:
-		/* Action unspecified if POWER CONDITION MODIFIER != [0,1] */
-		if (pcmod == 0x0)
-			ps_desired = max(POWER_STATE_0, (lowest_pow_st - 2));
-		else if (pcmod == 0x1)
-			ps_desired = max(POWER_STATE_0, (lowest_pow_st - 1));
-		break;
-	case NVME_POWER_STATE_LU_CONTROL:
-	default:
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-				ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-				SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		break;
-	}
-	nvme_sc = nvme_set_features(dev, NVME_FEAT_POWER_MGMT, ps_desired, 0,
-				    NULL);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_dma;
-	if (nvme_sc)
-		res = nvme_sc;
- out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ctrl), mem,
-			  dma_addr);
- out:
-	return res;
-}
-
-/* Write Buffer Helper Functions */
-/* Also using this for Format Unit with hdr passed as NULL, and buffer_id, 0 */
-
-static int nvme_trans_send_fw_cmd(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					u8 opcode, u32 tot_len, u32 offset,
-					u8 buffer_id)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	struct nvme_command c;
-	struct nvme_iod *iod = NULL;
-	unsigned length;
-
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = opcode;
-	if (opcode == nvme_admin_download_fw) {
-		if (hdr->iovec_count > 0) {
-			/* Assuming SGL is not allowed for this command */
-			res = nvme_trans_completion(hdr,
-						SAM_STAT_CHECK_CONDITION,
-						ILLEGAL_REQUEST,
-						SCSI_ASC_INVALID_CDB,
-						SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-			goto out;
-		}
-		iod = nvme_map_user_pages(dev, DMA_TO_DEVICE,
-				(unsigned long)hdr->dxferp, tot_len);
-		if (IS_ERR(iod)) {
-			res = PTR_ERR(iod);
-			goto out;
-		}
-		length = nvme_setup_prps(dev, iod, tot_len, GFP_KERNEL);
-		if (length != tot_len) {
-			res = -ENOMEM;
-			goto out_unmap;
-		}
-
-		c.dlfw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
-		c.dlfw.prp2 = cpu_to_le64(iod->first_dma);
-		c.dlfw.numd = cpu_to_le32((tot_len/BYTES_TO_DWORDS) - 1);
-		c.dlfw.offset = cpu_to_le32(offset/BYTES_TO_DWORDS);
-	} else if (opcode == nvme_admin_activate_fw) {
-		u32 cdw10 = buffer_id | NVME_FWACT_REPL_ACTV;
-		c.common.cdw10[0] = cpu_to_le32(cdw10);
-	}
-
-	nvme_sc = nvme_submit_admin_cmd(dev, &c, NULL);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_unmap;
-	if (nvme_sc)
-		res = nvme_sc;
-
- out_unmap:
-	if (opcode == nvme_admin_download_fw) {
-		nvme_unmap_user_pages(dev, DMA_TO_DEVICE, iod);
-		nvme_free_iod(dev, iod);
-	}
- out:
-	return res;
-}
-
-/* Mode Select Helper Functions */
-
-static inline void nvme_trans_modesel_get_bd_len(u8 *parm_list, u8 cdb10,
-						u16 *bd_len, u8 *llbaa)
-{
-	if (cdb10) {
-		/* 10 Byte CDB */
-		*bd_len = (parm_list[MODE_SELECT_10_BD_OFFSET] << 8) +
-			parm_list[MODE_SELECT_10_BD_OFFSET + 1];
-		*llbaa = parm_list[MODE_SELECT_10_LLBAA_OFFSET] &&
-				MODE_SELECT_10_LLBAA_MASK;
-	} else {
-		/* 6 Byte CDB */
-		*bd_len = parm_list[MODE_SELECT_6_BD_OFFSET];
-	}
-}
-
-static void nvme_trans_modesel_save_bd(struct nvme_ns *ns, u8 *parm_list,
-					u16 idx, u16 bd_len, u8 llbaa)
-{
-	u16 bd_num;
-
-	bd_num = bd_len / ((llbaa == 0) ?
-			SHORT_DESC_BLOCK : LONG_DESC_BLOCK);
-	/* Store block descriptor info if a FORMAT UNIT comes later */
-	/* TODO Saving 1st BD info; what to do if multiple BD received? */
-	if (llbaa == 0) {
-		/* Standard Block Descriptor - spc4r34 7.5.5.1 */
-		ns->mode_select_num_blocks =
-				(parm_list[idx + 1] << 16) +
-				(parm_list[idx + 2] << 8) +
-				(parm_list[idx + 3]);
-
-		ns->mode_select_block_len =
-				(parm_list[idx + 5] << 16) +
-				(parm_list[idx + 6] << 8) +
-				(parm_list[idx + 7]);
-	} else {
-		/* Long LBA Block Descriptor - sbc3r27 6.4.2.3 */
-		ns->mode_select_num_blocks =
-				(((u64)parm_list[idx + 0]) << 56) +
-				(((u64)parm_list[idx + 1]) << 48) +
-				(((u64)parm_list[idx + 2]) << 40) +
-				(((u64)parm_list[idx + 3]) << 32) +
-				(((u64)parm_list[idx + 4]) << 24) +
-				(((u64)parm_list[idx + 5]) << 16) +
-				(((u64)parm_list[idx + 6]) << 8) +
-				((u64)parm_list[idx + 7]);
-
-		ns->mode_select_block_len =
-				(parm_list[idx + 12] << 24) +
-				(parm_list[idx + 13] << 16) +
-				(parm_list[idx + 14] << 8) +
-				(parm_list[idx + 15]);
-	}
-}
-
-static int nvme_trans_modesel_get_mp(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					u8 *mode_page, u8 page_code)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	unsigned dword11;
-
-	switch (page_code) {
-	case MODE_PAGE_CACHING:
-		dword11 = ((mode_page[2] & CACHING_MODE_PAGE_WCE_MASK) ? 1 : 0);
-		nvme_sc = nvme_set_features(dev, NVME_FEAT_VOLATILE_WC, dword11,
-					    0, NULL);
-		res = nvme_trans_status_code(hdr, nvme_sc);
-		if (res)
-			break;
-		if (nvme_sc) {
-			res = nvme_sc;
-			break;
-		}
-		break;
-	case MODE_PAGE_CONTROL:
-		break;
-	case MODE_PAGE_POWER_CONDITION:
-		/* Verify the OS is not trying to set timers */
-		if ((mode_page[2] & 0x01) != 0 || (mode_page[3] & 0x0F) != 0) {
-			res = nvme_trans_completion(hdr,
-						SAM_STAT_CHECK_CONDITION,
-						ILLEGAL_REQUEST,
-						SCSI_ASC_INVALID_PARAMETER,
-						SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-			if (!res)
-				res = SNTI_INTERNAL_ERROR;
-			break;
-		}
-		break;
-	default:
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		if (!res)
-			res = SNTI_INTERNAL_ERROR;
-		break;
-	}
-
-	return res;
-}
-
-static int nvme_trans_modesel_data(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-					u8 *cmd, u16 parm_list_len, u8 pf,
-					u8 sp, u8 cdb10)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 *parm_list;
-	u16 bd_len;
-	u8 llbaa = 0;
-	u16 index, saved_index;
-	u8 page_code;
-	u16 mp_size;
-
-	/* Get parm list from data-in/out buffer */
-	parm_list = kmalloc(parm_list_len, GFP_KERNEL);
-	if (parm_list == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-
-	res = nvme_trans_copy_from_user(hdr, parm_list, parm_list_len);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out_mem;
-
-	nvme_trans_modesel_get_bd_len(parm_list, cdb10, &bd_len, &llbaa);
-	index = (cdb10) ? (MODE_SELECT_10_MPH_SIZE) : (MODE_SELECT_6_MPH_SIZE);
-
-	if (bd_len != 0) {
-		/* Block Descriptors present, parse */
-		nvme_trans_modesel_save_bd(ns, parm_list, index, bd_len, llbaa);
-		index += bd_len;
-	}
-	saved_index = index;
-
-	/* Multiple mode pages may be present; iterate through all */
-	/* In 1st Iteration, don't do NVME Command, only check for CDB errors */
-	do {
-		page_code = parm_list[index] & MODE_SELECT_PAGE_CODE_MASK;
-		mp_size = parm_list[index + 1] + 2;
-		if ((page_code != MODE_PAGE_CACHING) &&
-		    (page_code != MODE_PAGE_CONTROL) &&
-		    (page_code != MODE_PAGE_POWER_CONDITION)) {
-			res = nvme_trans_completion(hdr,
-						SAM_STAT_CHECK_CONDITION,
-						ILLEGAL_REQUEST,
-						SCSI_ASC_INVALID_CDB,
-						SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-			goto out_mem;
-		}
-		index += mp_size;
-	} while (index < parm_list_len);
-
-	/* In 2nd Iteration, do the NVME Commands */
-	index = saved_index;
-	do {
-		page_code = parm_list[index] & MODE_SELECT_PAGE_CODE_MASK;
-		mp_size = parm_list[index + 1] + 2;
-		res = nvme_trans_modesel_get_mp(ns, hdr, &parm_list[index],
-								page_code);
-		if (res != SNTI_TRANSLATION_SUCCESS)
-			break;
-		index += mp_size;
-	} while (index < parm_list_len);
-
- out_mem:
-	kfree(parm_list);
- out:
-	return res;
-}
-
-/* Format Unit Helper Functions */
-
-static int nvme_trans_fmt_set_blk_size_count(struct nvme_ns *ns,
-					     struct sg_io_hdr *hdr)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ns *id_ns;
-	u8 flbas;
-
-	/*
-	 * SCSI Expects a MODE SELECT would have been issued prior to
-	 * a FORMAT UNIT, and the block size and number would be used
-	 * from the block descriptor in it. If a MODE SELECT had not
-	 * been issued, FORMAT shall use the current values for both.
-	 */
-
-	if (ns->mode_select_num_blocks == 0 || ns->mode_select_block_len == 0) {
-		mem = dma_alloc_coherent(&dev->pci_dev->dev,
-			sizeof(struct nvme_id_ns), &dma_addr, GFP_KERNEL);
-		if (mem == NULL) {
-			res = -ENOMEM;
-			goto out;
-		}
-		/* nvme ns identify */
-		nvme_sc = nvme_identify(dev, ns->ns_id, 0, dma_addr);
-		res = nvme_trans_status_code(hdr, nvme_sc);
-		if (res)
-			goto out_dma;
-		if (nvme_sc) {
-			res = nvme_sc;
-			goto out_dma;
-		}
-		id_ns = mem;
-
-		if (ns->mode_select_num_blocks == 0)
-			ns->mode_select_num_blocks = le64_to_cpu(id_ns->ncap);
-		if (ns->mode_select_block_len == 0) {
-			flbas = (id_ns->flbas) & 0x0F;
-			ns->mode_select_block_len =
-						(1 << (id_ns->lbaf[flbas].ds));
-		}
- out_dma:
-		dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-				  mem, dma_addr);
-	}
- out:
-	return res;
-}
-
-static int nvme_trans_fmt_get_parm_header(struct sg_io_hdr *hdr, u8 len,
-					u8 format_prot_info, u8 *nvme_pf_code)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 *parm_list;
-	u8 pf_usage, pf_code;
-
-	parm_list = kmalloc(len, GFP_KERNEL);
-	if (parm_list == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-	res = nvme_trans_copy_from_user(hdr, parm_list, len);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out_mem;
-
-	if ((parm_list[FORMAT_UNIT_IMMED_OFFSET] &
-				FORMAT_UNIT_IMMED_MASK) != 0) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out_mem;
-	}
-
-	if (len == FORMAT_UNIT_LONG_PARM_LIST_LEN &&
-	    (parm_list[FORMAT_UNIT_PROT_INT_OFFSET] & 0x0F) != 0) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out_mem;
-	}
-	pf_usage = parm_list[FORMAT_UNIT_PROT_FIELD_USAGE_OFFSET] &
-			FORMAT_UNIT_PROT_FIELD_USAGE_MASK;
-	pf_code = (pf_usage << 2) | format_prot_info;
-	switch (pf_code) {
-	case 0:
-		*nvme_pf_code = 0;
-		break;
-	case 2:
-		*nvme_pf_code = 1;
-		break;
-	case 3:
-		*nvme_pf_code = 2;
-		break;
-	case 7:
-		*nvme_pf_code = 3;
-		break;
-	default:
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		break;
-	}
-
- out_mem:
-	kfree(parm_list);
- out:
-	return res;
-}
-
-static int nvme_trans_fmt_send_cmd(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-				   u8 prot_info)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ns *id_ns;
-	u8 i;
-	u8 flbas, nlbaf;
-	u8 selected_lbaf = 0xFF;
-	u32 cdw10 = 0;
-	struct nvme_command c;
-
-	/* Loop thru LBAF's in id_ns to match reqd lbaf, put in cdw10 */
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-							&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-	/* nvme ns identify */
-	nvme_sc = nvme_identify(dev, ns->ns_id, 0, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_dma;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_dma;
-	}
-	id_ns = mem;
-	flbas = (id_ns->flbas) & 0x0F;
-	nlbaf = id_ns->nlbaf;
-
-	for (i = 0; i < nlbaf; i++) {
-		if (ns->mode_select_block_len == (1 << (id_ns->lbaf[i].ds))) {
-			selected_lbaf = i;
-			break;
-		}
-	}
-	if (selected_lbaf > 0x0F) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-				ILLEGAL_REQUEST, SCSI_ASC_INVALID_PARAMETER,
-				SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-	}
-	if (ns->mode_select_num_blocks != le64_to_cpu(id_ns->ncap)) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-				ILLEGAL_REQUEST, SCSI_ASC_INVALID_PARAMETER,
-				SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-	}
-
-	cdw10 |= prot_info << 5;
-	cdw10 |= selected_lbaf & 0x0F;
-	memset(&c, 0, sizeof(c));
-	c.format.opcode = nvme_admin_format_nvm;
-	c.format.nsid = cpu_to_le32(ns->ns_id);
-	c.format.cdw10 = cpu_to_le32(cdw10);
-
-	nvme_sc = nvme_submit_admin_cmd(dev, &c, NULL);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_dma;
-	if (nvme_sc)
-		res = nvme_sc;
-
- out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
- out:
-	return res;
-}
-
-/* Read/Write Helper Functions */
-
-static inline void nvme_trans_get_io_cdb6(u8 *cmd,
-					struct nvme_trans_io_cdb *cdb_info)
-{
-	cdb_info->fua = 0;
-	cdb_info->prot_info = 0;
-	cdb_info->lba = GET_U32_FROM_CDB(cmd, IO_6_CDB_LBA_OFFSET) &
-					IO_6_CDB_LBA_MASK;
-	cdb_info->xfer_len = GET_U8_FROM_CDB(cmd, IO_6_CDB_TX_LEN_OFFSET);
-
-	/* sbc3r27 sec 5.32 - TRANSFER LEN of 0 implies a 256 Block transfer */
-	if (cdb_info->xfer_len == 0)
-		cdb_info->xfer_len = IO_6_DEFAULT_TX_LEN;
-}
-
-static inline void nvme_trans_get_io_cdb10(u8 *cmd,
-					struct nvme_trans_io_cdb *cdb_info)
-{
-	cdb_info->fua = GET_U8_FROM_CDB(cmd, IO_10_CDB_FUA_OFFSET) &
-					IO_CDB_FUA_MASK;
-	cdb_info->prot_info = GET_U8_FROM_CDB(cmd, IO_10_CDB_WP_OFFSET) &
-					IO_CDB_WP_MASK >> IO_CDB_WP_SHIFT;
-	cdb_info->lba = GET_U32_FROM_CDB(cmd, IO_10_CDB_LBA_OFFSET);
-	cdb_info->xfer_len = GET_U16_FROM_CDB(cmd, IO_10_CDB_TX_LEN_OFFSET);
-}
-
-static inline void nvme_trans_get_io_cdb12(u8 *cmd,
-					struct nvme_trans_io_cdb *cdb_info)
-{
-	cdb_info->fua = GET_U8_FROM_CDB(cmd, IO_12_CDB_FUA_OFFSET) &
-					IO_CDB_FUA_MASK;
-	cdb_info->prot_info = GET_U8_FROM_CDB(cmd, IO_12_CDB_WP_OFFSET) &
-					IO_CDB_WP_MASK >> IO_CDB_WP_SHIFT;
-	cdb_info->lba = GET_U32_FROM_CDB(cmd, IO_12_CDB_LBA_OFFSET);
-	cdb_info->xfer_len = GET_U32_FROM_CDB(cmd, IO_12_CDB_TX_LEN_OFFSET);
-}
-
-static inline void nvme_trans_get_io_cdb16(u8 *cmd,
-					struct nvme_trans_io_cdb *cdb_info)
-{
-	cdb_info->fua = GET_U8_FROM_CDB(cmd, IO_16_CDB_FUA_OFFSET) &
-					IO_CDB_FUA_MASK;
-	cdb_info->prot_info = GET_U8_FROM_CDB(cmd, IO_16_CDB_WP_OFFSET) &
-					IO_CDB_WP_MASK >> IO_CDB_WP_SHIFT;
-	cdb_info->lba = GET_U64_FROM_CDB(cmd, IO_16_CDB_LBA_OFFSET);
-	cdb_info->xfer_len = GET_U32_FROM_CDB(cmd, IO_16_CDB_TX_LEN_OFFSET);
-}
-
-static inline u32 nvme_trans_io_get_num_cmds(struct sg_io_hdr *hdr,
-					struct nvme_trans_io_cdb *cdb_info,
-					u32 max_blocks)
-{
-	/* If using iovecs, send one nvme command per vector */
-	if (hdr->iovec_count > 0)
-		return hdr->iovec_count;
-	else if (cdb_info->xfer_len > max_blocks)
-		return ((cdb_info->xfer_len - 1) / max_blocks) + 1;
-	else
-		return 1;
-}
-
-static u16 nvme_trans_io_get_control(struct nvme_ns *ns,
-					struct nvme_trans_io_cdb *cdb_info)
-{
-	u16 control = 0;
-
-	/* When Protection information support is added, implement here */
-
-	if (cdb_info->fua > 0)
-		control |= NVME_RW_FUA;
-
-	return control;
-}
-
-static int nvme_trans_do_nvme_io(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-				struct nvme_trans_io_cdb *cdb_info, u8 is_write)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_dev *dev = ns->dev;
-	u32 num_cmds;
-	struct nvme_iod *iod;
-	u64 unit_len;
-	u64 unit_num_blocks;	/* Number of blocks to xfer in each nvme cmd */
-	u32 retcode;
-	u32 i = 0;
-	u64 nvme_offset = 0;
-	void __user *next_mapping_addr;
-	struct nvme_command c;
-	u8 opcode = (is_write ? nvme_cmd_write : nvme_cmd_read);
-	u16 control;
-	u32 max_blocks = queue_max_hw_sectors(ns->queue);
-
-	num_cmds = nvme_trans_io_get_num_cmds(hdr, cdb_info, max_blocks);
-
-	/*
-	 * This loop handles two cases.
-	 * First, when an SGL is used in the form of an iovec list:
-	 *   - Use iov_base as the next mapping address for the nvme command_id
-	 *   - Use iov_len as the data transfer length for the command.
-	 * Second, when we have a single buffer
-	 *   - If larger than max_blocks, split into chunks, offset
-	 *        each nvme command accordingly.
-	 */
-	for (i = 0; i < num_cmds; i++) {
-		memset(&c, 0, sizeof(c));
-		if (hdr->iovec_count > 0) {
-			struct sg_iovec sgl;
-
-			retcode = copy_from_user(&sgl, hdr->dxferp +
-					i * sizeof(struct sg_iovec),
-					sizeof(struct sg_iovec));
-			if (retcode)
-				return -EFAULT;
-			unit_len = sgl.iov_len;
-			unit_num_blocks = unit_len >> ns->lba_shift;
-			next_mapping_addr = sgl.iov_base;
-		} else {
-			unit_num_blocks = min((u64)max_blocks,
-					(cdb_info->xfer_len - nvme_offset));
-			unit_len = unit_num_blocks << ns->lba_shift;
-			next_mapping_addr = hdr->dxferp +
-					((1 << ns->lba_shift) * nvme_offset);
-		}
-
-		c.rw.opcode = opcode;
-		c.rw.nsid = cpu_to_le32(ns->ns_id);
-		c.rw.slba = cpu_to_le64(cdb_info->lba + nvme_offset);
-		c.rw.length = cpu_to_le16(unit_num_blocks - 1);
-		control = nvme_trans_io_get_control(ns, cdb_info);
-		c.rw.control = cpu_to_le16(control);
-
-		iod = nvme_map_user_pages(dev,
-			(is_write) ? DMA_TO_DEVICE : DMA_FROM_DEVICE,
-			(unsigned long)next_mapping_addr, unit_len);
-		if (IS_ERR(iod)) {
-			res = PTR_ERR(iod);
-			goto out;
-		}
-		retcode = nvme_setup_prps(dev, iod, unit_len, GFP_KERNEL);
-		if (retcode != unit_len) {
-			nvme_unmap_user_pages(dev,
-				(is_write) ? DMA_TO_DEVICE : DMA_FROM_DEVICE,
-				iod);
-			nvme_free_iod(dev, iod);
-			res = -ENOMEM;
-			goto out;
-		}
-		c.rw.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
-		c.rw.prp2 = cpu_to_le64(iod->first_dma);
-
-		nvme_offset += unit_num_blocks;
-
-		nvme_sc = nvme_submit_io_cmd(dev, &c, NULL);
-		if (nvme_sc != NVME_SC_SUCCESS) {
-			nvme_unmap_user_pages(dev,
-				(is_write) ? DMA_TO_DEVICE : DMA_FROM_DEVICE,
-				iod);
-			nvme_free_iod(dev, iod);
-			res = nvme_trans_status_code(hdr, nvme_sc);
-			goto out;
-		}
-		nvme_unmap_user_pages(dev,
-				(is_write) ? DMA_TO_DEVICE : DMA_FROM_DEVICE,
-				iod);
-		nvme_free_iod(dev, iod);
-	}
-	res = nvme_trans_status_code(hdr, NVME_SC_SUCCESS);
-
- out:
-	return res;
-}
-
-
-/* SCSI Command Translation Functions */
-
-static int nvme_trans_io(struct nvme_ns *ns, struct sg_io_hdr *hdr, u8 is_write,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	struct nvme_trans_io_cdb cdb_info;
-	u8 opcode = cmd[0];
-	u64 xfer_bytes;
-	u64 sum_iov_len = 0;
-	struct sg_iovec sgl;
-	int i;
-	size_t not_copied;
-
-	/* Extract Fields from CDB */
-	switch (opcode) {
-	case WRITE_6:
-	case READ_6:
-		nvme_trans_get_io_cdb6(cmd, &cdb_info);
-		break;
-	case WRITE_10:
-	case READ_10:
-		nvme_trans_get_io_cdb10(cmd, &cdb_info);
-		break;
-	case WRITE_12:
-	case READ_12:
-		nvme_trans_get_io_cdb12(cmd, &cdb_info);
-		break;
-	case WRITE_16:
-	case READ_16:
-		nvme_trans_get_io_cdb16(cmd, &cdb_info);
-		break;
-	default:
-		/* Will never really reach here */
-		res = SNTI_INTERNAL_ERROR;
-		goto out;
-	}
-
-	/* Calculate total length of transfer (in bytes) */
-	if (hdr->iovec_count > 0) {
-		for (i = 0; i < hdr->iovec_count; i++) {
-			not_copied = copy_from_user(&sgl, hdr->dxferp +
-						i * sizeof(struct sg_iovec),
-						sizeof(struct sg_iovec));
-			if (not_copied)
-				return -EFAULT;
-			sum_iov_len += sgl.iov_len;
-			/* IO vector sizes should be multiples of block size */
-			if (sgl.iov_len % (1 << ns->lba_shift) != 0) {
-				res = nvme_trans_completion(hdr,
-						SAM_STAT_CHECK_CONDITION,
-						ILLEGAL_REQUEST,
-						SCSI_ASC_INVALID_PARAMETER,
-						SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-				goto out;
-			}
-		}
-	} else {
-		sum_iov_len = hdr->dxfer_len;
-	}
-
-	/* As Per sg ioctl howto, if the lengths differ, use the lower one */
-	xfer_bytes = min(((u64)hdr->dxfer_len), sum_iov_len);
-
-	/* If block count and actual data buffer size dont match, error out */
-	if (xfer_bytes != (cdb_info.xfer_len << ns->lba_shift)) {
-		res = -EINVAL;
-		goto out;
-	}
-
-	/* Check for 0 length transfer - it is not illegal */
-	if (cdb_info.xfer_len == 0)
-		goto out;
-
-	/* Send NVMe IO Command(s) */
-	res = nvme_trans_do_nvme_io(ns, hdr, &cdb_info, is_write);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-
- out:
-	return res;
-}
-
-static int nvme_trans_inquiry(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 evpd;
-	u8 page_code;
-	int alloc_len;
-	u8 *inq_response;
-
-	evpd = GET_INQ_EVPD_BIT(cmd);
-	page_code = GET_INQ_PAGE_CODE(cmd);
-	alloc_len = GET_INQ_ALLOC_LENGTH(cmd);
-
-	inq_response = kmalloc(STANDARD_INQUIRY_LENGTH, GFP_KERNEL);
-	if (inq_response == NULL) {
-		res = -ENOMEM;
-		goto out_mem;
-	}
-
-	if (evpd == 0) {
-		if (page_code == INQ_STANDARD_INQUIRY_PAGE) {
-			res = nvme_trans_standard_inquiry_page(ns, hdr,
-						inq_response, alloc_len);
-		} else {
-			res = nvme_trans_completion(hdr,
-						SAM_STAT_CHECK_CONDITION,
-						ILLEGAL_REQUEST,
-						SCSI_ASC_INVALID_CDB,
-						SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		}
-	} else {
-		switch (page_code) {
-		case VPD_SUPPORTED_PAGES:
-			res = nvme_trans_supported_vpd_pages(ns, hdr,
-						inq_response, alloc_len);
-			break;
-		case VPD_SERIAL_NUMBER:
-			res = nvme_trans_unit_serial_page(ns, hdr, inq_response,
-								alloc_len);
-			break;
-		case VPD_DEVICE_IDENTIFIERS:
-			res = nvme_trans_device_id_page(ns, hdr, inq_response,
-								alloc_len);
-			break;
-		case VPD_EXTENDED_INQUIRY:
-			res = nvme_trans_ext_inq_page(ns, hdr, alloc_len);
-			break;
-		case VPD_BLOCK_DEV_CHARACTERISTICS:
-			res = nvme_trans_bdev_char_page(ns, hdr, alloc_len);
-			break;
-		default:
-			res = nvme_trans_completion(hdr,
-						SAM_STAT_CHECK_CONDITION,
-						ILLEGAL_REQUEST,
-						SCSI_ASC_INVALID_CDB,
-						SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-			break;
-		}
-	}
-	kfree(inq_response);
- out_mem:
-	return res;
-}
-
-static int nvme_trans_log_sense(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u16 alloc_len;
-	u8 sp;
-	u8 pc;
-	u8 page_code;
-
-	sp = GET_U8_FROM_CDB(cmd, LOG_SENSE_CDB_SP_OFFSET);
-	if (sp != LOG_SENSE_CDB_SP_NOT_ENABLED) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	}
-	pc = GET_U8_FROM_CDB(cmd, LOG_SENSE_CDB_PC_OFFSET);
-	page_code = pc & LOG_SENSE_CDB_PAGE_CODE_MASK;
-	pc = (pc & LOG_SENSE_CDB_PC_MASK) >> LOG_SENSE_CDB_PC_SHIFT;
-	if (pc != LOG_SENSE_CDB_PC_CUMULATIVE_VALUES) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	}
-	alloc_len = GET_U16_FROM_CDB(cmd, LOG_SENSE_CDB_ALLOC_LENGTH_OFFSET);
-	switch (page_code) {
-	case LOG_PAGE_SUPPORTED_LOG_PAGES_PAGE:
-		res = nvme_trans_log_supp_pages(ns, hdr, alloc_len);
-		break;
-	case LOG_PAGE_INFORMATIONAL_EXCEPTIONS_PAGE:
-		res = nvme_trans_log_info_exceptions(ns, hdr, alloc_len);
-		break;
-	case LOG_PAGE_TEMPERATURE_PAGE:
-		res = nvme_trans_log_temperature(ns, hdr, alloc_len);
-		break;
-	default:
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		break;
-	}
-
- out:
-	return res;
-}
-
-static int nvme_trans_mode_select(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 cdb10 = 0;
-	u16 parm_list_len;
-	u8 page_format;
-	u8 save_pages;
-
-	page_format = GET_U8_FROM_CDB(cmd, MODE_SELECT_CDB_PAGE_FORMAT_OFFSET);
-	page_format &= MODE_SELECT_CDB_PAGE_FORMAT_MASK;
-
-	save_pages = GET_U8_FROM_CDB(cmd, MODE_SELECT_CDB_SAVE_PAGES_OFFSET);
-	save_pages &= MODE_SELECT_CDB_SAVE_PAGES_MASK;
-
-	if (GET_OPCODE(cmd) == MODE_SELECT) {
-		parm_list_len = GET_U8_FROM_CDB(cmd,
-				MODE_SELECT_6_CDB_PARAM_LIST_LENGTH_OFFSET);
-	} else {
-		parm_list_len = GET_U16_FROM_CDB(cmd,
-				MODE_SELECT_10_CDB_PARAM_LIST_LENGTH_OFFSET);
-		cdb10 = 1;
-	}
-
-	if (parm_list_len != 0) {
-		/*
-		 * According to SPC-4 r24, a paramter list length field of 0
-		 * shall not be considered an error
-		 */
-		res = nvme_trans_modesel_data(ns, hdr, cmd, parm_list_len,
-						page_format, save_pages, cdb10);
-	}
-
-	return res;
-}
-
-static int nvme_trans_mode_sense(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u16 alloc_len;
-	u8 cdb10 = 0;
-	u8 page_code;
-	u8 pc;
-
-	if (GET_OPCODE(cmd) == MODE_SENSE) {
-		alloc_len = GET_U8_FROM_CDB(cmd, MODE_SENSE6_ALLOC_LEN_OFFSET);
-	} else {
-		alloc_len = GET_U16_FROM_CDB(cmd,
-						MODE_SENSE10_ALLOC_LEN_OFFSET);
-		cdb10 = 1;
-	}
-
-	pc = GET_U8_FROM_CDB(cmd, MODE_SENSE_PAGE_CONTROL_OFFSET) &
-						MODE_SENSE_PAGE_CONTROL_MASK;
-	if (pc != MODE_SENSE_PC_CURRENT_VALUES) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	}
-
-	page_code = GET_U8_FROM_CDB(cmd, MODE_SENSE_PAGE_CODE_OFFSET) &
-					MODE_SENSE_PAGE_CODE_MASK;
-	switch (page_code) {
-	case MODE_PAGE_CACHING:
-		res = nvme_trans_mode_page_create(ns, hdr, cmd, alloc_len,
-						cdb10,
-						&nvme_trans_fill_caching_page,
-						MODE_PAGE_CACHING_LEN);
-		break;
-	case MODE_PAGE_CONTROL:
-		res = nvme_trans_mode_page_create(ns, hdr, cmd, alloc_len,
-						cdb10,
-						&nvme_trans_fill_control_page,
-						MODE_PAGE_CONTROL_LEN);
-		break;
-	case MODE_PAGE_POWER_CONDITION:
-		res = nvme_trans_mode_page_create(ns, hdr, cmd, alloc_len,
-						cdb10,
-						&nvme_trans_fill_pow_cnd_page,
-						MODE_PAGE_POW_CND_LEN);
-		break;
-	case MODE_PAGE_INFO_EXCEP:
-		res = nvme_trans_mode_page_create(ns, hdr, cmd, alloc_len,
-						cdb10,
-						&nvme_trans_fill_inf_exc_page,
-						MODE_PAGE_INF_EXC_LEN);
-		break;
-	case MODE_PAGE_RETURN_ALL:
-		res = nvme_trans_mode_page_create(ns, hdr, cmd, alloc_len,
-						cdb10,
-						&nvme_trans_fill_all_pages,
-						MODE_PAGE_ALL_LEN);
-		break;
-	default:
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		break;
-	}
-
- out:
-	return res;
-}
-
-static int nvme_trans_read_capacity(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	u32 alloc_len = READ_CAP_10_RESP_SIZE;
-	u32 resp_size = READ_CAP_10_RESP_SIZE;
-	u32 xfer_len;
-	u8 cdb16;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ns *id_ns;
-	u8 *response;
-
-	cdb16 = IS_READ_CAP_16(cmd);
-	if (cdb16) {
-		alloc_len = GET_READ_CAP_16_ALLOC_LENGTH(cmd);
-		resp_size = READ_CAP_16_RESP_SIZE;
-	}
-
-	mem = dma_alloc_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns),
-							&dma_addr, GFP_KERNEL);
-	if (mem == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-	/* nvme ns identify */
-	nvme_sc = nvme_identify(dev, ns->ns_id, 0, dma_addr);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out_dma;
-	if (nvme_sc) {
-		res = nvme_sc;
-		goto out_dma;
-	}
-	id_ns = mem;
-
-	response = kzalloc(resp_size, GFP_KERNEL);
-	if (response == NULL) {
-		res = -ENOMEM;
-		goto out_dma;
-	}
-	nvme_trans_fill_read_cap(response, id_ns, cdb16);
-
-	xfer_len = min(alloc_len, resp_size);
-	res = nvme_trans_copy_to_user(hdr, response, xfer_len);
-
-	kfree(response);
- out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ns), mem,
-			  dma_addr);
- out:
-	return res;
-}
-
-static int nvme_trans_report_luns(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	u32 alloc_len, xfer_len, resp_size;
-	u8 select_report;
-	u8 *response;
-	struct nvme_dev *dev = ns->dev;
-	dma_addr_t dma_addr;
-	void *mem;
-	struct nvme_id_ctrl *id_ctrl;
-	u32 ll_length, lun_id;
-	u8 lun_id_offset = REPORT_LUNS_FIRST_LUN_OFFSET;
-	__be32 tmp_len;
-
-	alloc_len = GET_REPORT_LUNS_ALLOC_LENGTH(cmd);
-	select_report = GET_U8_FROM_CDB(cmd, REPORT_LUNS_SR_OFFSET);
-
-	if ((select_report != ALL_LUNS_RETURNED) &&
-	    (select_report != ALL_WELL_KNOWN_LUNS_RETURNED) &&
-	    (select_report != RESTRICTED_LUNS_RETURNED)) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	} else {
-		/* NVMe Controller Identify */
-		mem = dma_alloc_coherent(&dev->pci_dev->dev,
-					sizeof(struct nvme_id_ctrl),
-					&dma_addr, GFP_KERNEL);
-		if (mem == NULL) {
-			res = -ENOMEM;
-			goto out;
-		}
-		nvme_sc = nvme_identify(dev, 0, 1, dma_addr);
-		res = nvme_trans_status_code(hdr, nvme_sc);
-		if (res)
-			goto out_dma;
-		if (nvme_sc) {
-			res = nvme_sc;
-			goto out_dma;
-		}
-		id_ctrl = mem;
-		ll_length = le32_to_cpu(id_ctrl->nn) * LUN_ENTRY_SIZE;
-		resp_size = ll_length + LUN_DATA_HEADER_SIZE;
-
-		if (alloc_len < resp_size) {
-			res = nvme_trans_completion(hdr,
-					SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-			goto out_dma;
-		}
-
-		response = kzalloc(resp_size, GFP_KERNEL);
-		if (response == NULL) {
-			res = -ENOMEM;
-			goto out_dma;
-		}
-
-		/* The first LUN ID will always be 0 per the SAM spec */
-		for (lun_id = 0; lun_id < le32_to_cpu(id_ctrl->nn); lun_id++) {
-			/*
-			 * Set the LUN Id and then increment to the next LUN
-			 * location in the parameter data.
-			 */
-			__be64 tmp_id = cpu_to_be64(lun_id);
-			memcpy(&response[lun_id_offset], &tmp_id, sizeof(u64));
-			lun_id_offset += LUN_ENTRY_SIZE;
-		}
-		tmp_len = cpu_to_be32(ll_length);
-		memcpy(response, &tmp_len, sizeof(u32));
-	}
-
-	xfer_len = min(alloc_len, resp_size);
-	res = nvme_trans_copy_to_user(hdr, response, xfer_len);
-
-	kfree(response);
- out_dma:
-	dma_free_coherent(&dev->pci_dev->dev, sizeof(struct nvme_id_ctrl), mem,
-			  dma_addr);
- out:
-	return res;
-}
-
-static int nvme_trans_request_sense(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 alloc_len, xfer_len, resp_size;
-	u8 desc_format;
-	u8 *response;
-
-	alloc_len = GET_REQUEST_SENSE_ALLOC_LENGTH(cmd);
-	desc_format = GET_U8_FROM_CDB(cmd, REQUEST_SENSE_DESC_OFFSET);
-	desc_format &= REQUEST_SENSE_DESC_MASK;
-
-	resp_size = ((desc_format) ? (DESC_FMT_SENSE_DATA_SIZE) :
-					(FIXED_FMT_SENSE_DATA_SIZE));
-	response = kzalloc(resp_size, GFP_KERNEL);
-	if (response == NULL) {
-		res = -ENOMEM;
-		goto out;
-	}
-
-	if (desc_format == DESCRIPTOR_FORMAT_SENSE_DATA_TYPE) {
-		/* Descriptor Format Sense Data */
-		response[0] = DESC_FORMAT_SENSE_DATA;
-		response[1] = NO_SENSE;
-		/* TODO How is LOW POWER CONDITION ON handled? (byte 2) */
-		response[2] = SCSI_ASC_NO_SENSE;
-		response[3] = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		/* SDAT_OVFL = 0 | Additional Sense Length = 0 */
-	} else {
-		/* Fixed Format Sense Data */
-		response[0] = FIXED_SENSE_DATA;
-		/* Byte 1 = Obsolete */
-		response[2] = NO_SENSE; /* FM, EOM, ILI, SDAT_OVFL = 0 */
-		/* Bytes 3-6 - Information - set to zero */
-		response[7] = FIXED_SENSE_DATA_ADD_LENGTH;
-		/* Bytes 8-11 - Cmd Specific Information - set to zero */
-		response[12] = SCSI_ASC_NO_SENSE;
-		response[13] = SCSI_ASCQ_CAUSE_NOT_REPORTABLE;
-		/* Byte 14 = Field Replaceable Unit Code = 0 */
-		/* Bytes 15-17 - SKSV=0; Sense Key Specific = 0 */
-	}
-
-	xfer_len = min(alloc_len, resp_size);
-	res = nvme_trans_copy_to_user(hdr, response, xfer_len);
-
-	kfree(response);
- out:
-	return res;
-}
-
-static int nvme_trans_security_protocol(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr,
-					u8 *cmd)
-{
-	return nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-				ILLEGAL_REQUEST, SCSI_ASC_ILLEGAL_COMMAND,
-				SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-}
-
-static int nvme_trans_start_stop(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_command c;
-	u8 immed, pcmod, pc, no_flush, start;
-
-	immed = GET_U8_FROM_CDB(cmd, START_STOP_UNIT_CDB_IMMED_OFFSET);
-	pcmod = GET_U8_FROM_CDB(cmd, START_STOP_UNIT_CDB_POWER_COND_MOD_OFFSET);
-	pc = GET_U8_FROM_CDB(cmd, START_STOP_UNIT_CDB_POWER_COND_OFFSET);
-	no_flush = GET_U8_FROM_CDB(cmd, START_STOP_UNIT_CDB_NO_FLUSH_OFFSET);
-	start = GET_U8_FROM_CDB(cmd, START_STOP_UNIT_CDB_START_OFFSET);
-
-	immed &= START_STOP_UNIT_CDB_IMMED_MASK;
-	pcmod &= START_STOP_UNIT_CDB_POWER_COND_MOD_MASK;
-	pc = (pc & START_STOP_UNIT_CDB_POWER_COND_MASK) >> NIBBLE_SHIFT;
-	no_flush &= START_STOP_UNIT_CDB_NO_FLUSH_MASK;
-	start &= START_STOP_UNIT_CDB_START_MASK;
-
-	if (immed != 0) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-	} else {
-		if (no_flush == 0) {
-			/* Issue NVME FLUSH command prior to START STOP UNIT */
-			memset(&c, 0, sizeof(c));
-			c.common.opcode = nvme_cmd_flush;
-			c.common.nsid = cpu_to_le32(ns->ns_id);
-
-			nvme_sc = nvme_submit_io_cmd(ns->dev, &c, NULL);
-			res = nvme_trans_status_code(hdr, nvme_sc);
-			if (res)
-				goto out;
-			if (nvme_sc) {
-				res = nvme_sc;
-				goto out;
-			}
-		}
-		/* Setup the expected power state transition */
-		res = nvme_trans_power_state(ns, hdr, pc, pcmod, start);
-	}
-
- out:
-	return res;
-}
-
-static int nvme_trans_synchronize_cache(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr, u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	int nvme_sc;
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = nvme_cmd_flush;
-	c.common.nsid = cpu_to_le32(ns->ns_id);
-
-	nvme_sc = nvme_submit_io_cmd(ns->dev, &c, NULL);
-
-	res = nvme_trans_status_code(hdr, nvme_sc);
-	if (res)
-		goto out;
-	if (nvme_sc)
-		res = nvme_sc;
-
- out:
-	return res;
-}
-
-static int nvme_trans_format_unit(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u8 parm_hdr_len = 0;
-	u8 nvme_pf_code = 0;
-	u8 format_prot_info, long_list, format_data;
-
-	format_prot_info = GET_U8_FROM_CDB(cmd,
-				FORMAT_UNIT_CDB_FORMAT_PROT_INFO_OFFSET);
-	long_list = GET_U8_FROM_CDB(cmd, FORMAT_UNIT_CDB_LONG_LIST_OFFSET);
-	format_data = GET_U8_FROM_CDB(cmd, FORMAT_UNIT_CDB_FORMAT_DATA_OFFSET);
-
-	format_prot_info = (format_prot_info &
-				FORMAT_UNIT_CDB_FORMAT_PROT_INFO_MASK) >>
-				FORMAT_UNIT_CDB_FORMAT_PROT_INFO_SHIFT;
-	long_list &= FORMAT_UNIT_CDB_LONG_LIST_MASK;
-	format_data &= FORMAT_UNIT_CDB_FORMAT_DATA_MASK;
-
-	if (format_data != 0) {
-		if (format_prot_info != 0) {
-			if (long_list == 0)
-				parm_hdr_len = FORMAT_UNIT_SHORT_PARM_LIST_LEN;
-			else
-				parm_hdr_len = FORMAT_UNIT_LONG_PARM_LIST_LEN;
-		}
-	} else if (format_data == 0 && format_prot_info != 0) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	}
-
-	/* Get parm header from data-in/out buffer */
-	/*
-	 * According to the translation spec, the only fields in the parameter
-	 * list we are concerned with are in the header. So allocate only that.
-	 */
-	if (parm_hdr_len > 0) {
-		res = nvme_trans_fmt_get_parm_header(hdr, parm_hdr_len,
-					format_prot_info, &nvme_pf_code);
-		if (res != SNTI_TRANSLATION_SUCCESS)
-			goto out;
-	}
-
-	/* Attempt to activate any previously downloaded firmware image */
-	res = nvme_trans_send_fw_cmd(ns, hdr, nvme_admin_activate_fw, 0, 0, 0);
-
-	/* Determine Block size and count and send format command */
-	res = nvme_trans_fmt_set_blk_size_count(ns, hdr);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-
-	res = nvme_trans_fmt_send_cmd(ns, hdr, nvme_pf_code);
-
- out:
-	return res;
-}
-
-static int nvme_trans_test_unit_ready(struct nvme_ns *ns,
-					struct sg_io_hdr *hdr,
-					u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	struct nvme_dev *dev = ns->dev;
-
-	if (!(readl(&dev->bar->csts) & NVME_CSTS_RDY))
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					    NOT_READY, SCSI_ASC_LUN_NOT_READY,
-					    SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-	else
-		res = nvme_trans_completion(hdr, SAM_STAT_GOOD, NO_SENSE, 0, 0);
-
-	return res;
-}
-
-static int nvme_trans_write_buffer(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	int res = SNTI_TRANSLATION_SUCCESS;
-	u32 buffer_offset, parm_list_length;
-	u8 buffer_id, mode;
-
-	parm_list_length =
-		GET_U24_FROM_CDB(cmd, WRITE_BUFFER_CDB_PARM_LIST_LENGTH_OFFSET);
-	if (parm_list_length % BYTES_TO_DWORDS != 0) {
-		/* NVMe expects Firmware file to be a whole number of DWORDS */
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	}
-	buffer_id = GET_U8_FROM_CDB(cmd, WRITE_BUFFER_CDB_BUFFER_ID_OFFSET);
-	if (buffer_id > NVME_MAX_FIRMWARE_SLOT) {
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		goto out;
-	}
-	mode = GET_U8_FROM_CDB(cmd, WRITE_BUFFER_CDB_MODE_OFFSET) &
-						WRITE_BUFFER_CDB_MODE_MASK;
-	buffer_offset =
-		GET_U24_FROM_CDB(cmd, WRITE_BUFFER_CDB_BUFFER_OFFSET_OFFSET);
-
-	switch (mode) {
-	case DOWNLOAD_SAVE_ACTIVATE:
-		res = nvme_trans_send_fw_cmd(ns, hdr, nvme_admin_download_fw,
-						parm_list_length, buffer_offset,
-						buffer_id);
-		if (res != SNTI_TRANSLATION_SUCCESS)
-			goto out;
-		res = nvme_trans_send_fw_cmd(ns, hdr, nvme_admin_activate_fw,
-						parm_list_length, buffer_offset,
-						buffer_id);
-		break;
-	case DOWNLOAD_SAVE_DEFER_ACTIVATE:
-		res = nvme_trans_send_fw_cmd(ns, hdr, nvme_admin_download_fw,
-						parm_list_length, buffer_offset,
-						buffer_id);
-		break;
-	case ACTIVATE_DEFERRED_MICROCODE:
-		res = nvme_trans_send_fw_cmd(ns, hdr, nvme_admin_activate_fw,
-						parm_list_length, buffer_offset,
-						buffer_id);
-		break;
-	default:
-		res = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-					ILLEGAL_REQUEST, SCSI_ASC_INVALID_CDB,
-					SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		break;
-	}
-
- out:
-	return res;
-}
-
-struct scsi_unmap_blk_desc {
-	__be64	slba;
-	__be32	nlb;
-	u32	resv;
-};
-
-struct scsi_unmap_parm_list {
-	__be16	unmap_data_len;
-	__be16	unmap_blk_desc_data_len;
-	u32	resv;
-	struct scsi_unmap_blk_desc desc[0];
-};
-
-static int nvme_trans_unmap(struct nvme_ns *ns, struct sg_io_hdr *hdr,
-							u8 *cmd)
-{
-	struct nvme_dev *dev = ns->dev;
-	struct scsi_unmap_parm_list *plist;
-	struct nvme_dsm_range *range;
-	struct nvme_command c;
-	int i, nvme_sc, res = -ENOMEM;
-	u16 ndesc, list_len;
-	dma_addr_t dma_addr;
-
-	list_len = GET_U16_FROM_CDB(cmd, UNMAP_CDB_PARAM_LIST_LENGTH_OFFSET);
-	if (!list_len)
-		return -EINVAL;
-
-	plist = kmalloc(list_len, GFP_KERNEL);
-	if (!plist)
-		return -ENOMEM;
-
-	res = nvme_trans_copy_from_user(hdr, plist, list_len);
-	if (res != SNTI_TRANSLATION_SUCCESS)
-		goto out;
-
-	ndesc = be16_to_cpu(plist->unmap_blk_desc_data_len) >> 4;
-	if (!ndesc || ndesc > 256) {
-		res = -EINVAL;
-		goto out;
-	}
-
-	range = dma_alloc_coherent(&dev->pci_dev->dev, ndesc * sizeof(*range),
-							&dma_addr, GFP_KERNEL);
-	if (!range)
-		goto out;
-
-	for (i = 0; i < ndesc; i++) {
-		range[i].nlb = cpu_to_le32(be32_to_cpu(plist->desc[i].nlb));
-		range[i].slba = cpu_to_le64(be64_to_cpu(plist->desc[i].slba));
-		range[i].cattr = 0;
-	}
-
-	memset(&c, 0, sizeof(c));
-	c.dsm.opcode = nvme_cmd_dsm;
-	c.dsm.nsid = cpu_to_le32(ns->ns_id);
-	c.dsm.prp1 = cpu_to_le64(dma_addr);
-	c.dsm.nr = cpu_to_le32(ndesc - 1);
-	c.dsm.attributes = cpu_to_le32(NVME_DSMGMT_AD);
-
-	nvme_sc = nvme_submit_io_cmd(dev, &c, NULL);
-	res = nvme_trans_status_code(hdr, nvme_sc);
-
-	dma_free_coherent(&dev->pci_dev->dev, ndesc * sizeof(*range),
-							range, dma_addr);
- out:
-	kfree(plist);
-	return res;
-}
-
-static int nvme_scsi_translate(struct nvme_ns *ns, struct sg_io_hdr *hdr)
-{
-	u8 cmd[BLK_MAX_CDB];
-	int retcode;
-	unsigned int opcode;
-
-	if (hdr->cmdp == NULL)
-		return -EMSGSIZE;
-	if (copy_from_user(cmd, hdr->cmdp, hdr->cmd_len))
-		return -EFAULT;
-
-	opcode = cmd[0];
-
-	switch (opcode) {
-	case READ_6:
-	case READ_10:
-	case READ_12:
-	case READ_16:
-		retcode = nvme_trans_io(ns, hdr, 0, cmd);
-		break;
-	case WRITE_6:
-	case WRITE_10:
-	case WRITE_12:
-	case WRITE_16:
-		retcode = nvme_trans_io(ns, hdr, 1, cmd);
-		break;
-	case INQUIRY:
-		retcode = nvme_trans_inquiry(ns, hdr, cmd);
-		break;
-	case LOG_SENSE:
-		retcode = nvme_trans_log_sense(ns, hdr, cmd);
-		break;
-	case MODE_SELECT:
-	case MODE_SELECT_10:
-		retcode = nvme_trans_mode_select(ns, hdr, cmd);
-		break;
-	case MODE_SENSE:
-	case MODE_SENSE_10:
-		retcode = nvme_trans_mode_sense(ns, hdr, cmd);
-		break;
-	case READ_CAPACITY:
-		retcode = nvme_trans_read_capacity(ns, hdr, cmd);
-		break;
-	case SERVICE_ACTION_IN:
-		if (IS_READ_CAP_16(cmd))
-			retcode = nvme_trans_read_capacity(ns, hdr, cmd);
-		else
-			goto out;
-		break;
-	case REPORT_LUNS:
-		retcode = nvme_trans_report_luns(ns, hdr, cmd);
-		break;
-	case REQUEST_SENSE:
-		retcode = nvme_trans_request_sense(ns, hdr, cmd);
-		break;
-	case SECURITY_PROTOCOL_IN:
-	case SECURITY_PROTOCOL_OUT:
-		retcode = nvme_trans_security_protocol(ns, hdr, cmd);
-		break;
-	case START_STOP:
-		retcode = nvme_trans_start_stop(ns, hdr, cmd);
-		break;
-	case SYNCHRONIZE_CACHE:
-		retcode = nvme_trans_synchronize_cache(ns, hdr, cmd);
-		break;
-	case FORMAT_UNIT:
-		retcode = nvme_trans_format_unit(ns, hdr, cmd);
-		break;
-	case TEST_UNIT_READY:
-		retcode = nvme_trans_test_unit_ready(ns, hdr, cmd);
-		break;
-	case WRITE_BUFFER:
-		retcode = nvme_trans_write_buffer(ns, hdr, cmd);
-		break;
-	case UNMAP:
-		retcode = nvme_trans_unmap(ns, hdr, cmd);
-		break;
-	default:
- out:
-		retcode = nvme_trans_completion(hdr, SAM_STAT_CHECK_CONDITION,
-				ILLEGAL_REQUEST, SCSI_ASC_ILLEGAL_COMMAND,
-				SCSI_ASCQ_CAUSE_NOT_REPORTABLE);
-		break;
-	}
-	return retcode;
-}
-
-int nvme_sg_io(struct nvme_ns *ns, struct sg_io_hdr __user *u_hdr)
-{
-	struct sg_io_hdr hdr;
-	int retcode;
-
-	if (!capable(CAP_SYS_ADMIN))
-		return -EACCES;
-	if (copy_from_user(&hdr, u_hdr, sizeof(hdr)))
-		return -EFAULT;
-	if (hdr.interface_id != 'S')
-		return -EINVAL;
-	if (hdr.cmd_len > BLK_MAX_CDB)
-		return -EINVAL;
-
-	retcode = nvme_scsi_translate(ns, &hdr);
-	if (retcode < 0)
-		return retcode;
-	if (retcode > 0)
-		retcode = SNTI_TRANSLATION_SUCCESS;
-	if (copy_to_user(u_hdr, &hdr, sizeof(sg_io_hdr_t)) > 0)
-		return -EFAULT;
-
-	return retcode;
-}
-
-#ifdef CONFIG_COMPAT
-typedef struct sg_io_hdr32 {
-	compat_int_t interface_id;	/* [i] 'S' for SCSI generic (required) */
-	compat_int_t dxfer_direction;	/* [i] data transfer direction  */
-	unsigned char cmd_len;		/* [i] SCSI command length ( <= 16 bytes) */
-	unsigned char mx_sb_len;		/* [i] max length to write to sbp */
-	unsigned short iovec_count;	/* [i] 0 implies no scatter gather */
-	compat_uint_t dxfer_len;		/* [i] byte count of data transfer */
-	compat_uint_t dxferp;		/* [i], [*io] points to data transfer memory
-					      or scatter gather list */
-	compat_uptr_t cmdp;		/* [i], [*i] points to command to perform */
-	compat_uptr_t sbp;		/* [i], [*o] points to sense_buffer memory */
-	compat_uint_t timeout;		/* [i] MAX_UINT->no timeout (unit: millisec) */
-	compat_uint_t flags;		/* [i] 0 -> default, see SG_FLAG... */
-	compat_int_t pack_id;		/* [i->o] unused internally (normally) */
-	compat_uptr_t usr_ptr;		/* [i->o] unused internally */
-	unsigned char status;		/* [o] scsi status */
-	unsigned char masked_status;	/* [o] shifted, masked scsi status */
-	unsigned char msg_status;		/* [o] messaging level data (optional) */
-	unsigned char sb_len_wr;		/* [o] byte count actually written to sbp */
-	unsigned short host_status;	/* [o] errors from host adapter */
-	unsigned short driver_status;	/* [o] errors from software driver */
-	compat_int_t resid;		/* [o] dxfer_len - actual_transferred */
-	compat_uint_t duration;		/* [o] time taken by cmd (unit: millisec) */
-	compat_uint_t info;		/* [o] auxiliary information */
-} sg_io_hdr32_t;  /* 64 bytes long (on sparc32) */
-
-typedef struct sg_iovec32 {
-	compat_uint_t iov_base;
-	compat_uint_t iov_len;
-} sg_iovec32_t;
-
-static int sg_build_iovec(sg_io_hdr_t __user *sgio, void __user *dxferp, u16 iovec_count)
-{
-	sg_iovec_t __user *iov = (sg_iovec_t __user *) (sgio + 1);
-	sg_iovec32_t __user *iov32 = dxferp;
-	int i;
-
-	for (i = 0; i < iovec_count; i++) {
-		u32 base, len;
-
-		if (get_user(base, &iov32[i].iov_base) ||
-		    get_user(len, &iov32[i].iov_len) ||
-		    put_user(compat_ptr(base), &iov[i].iov_base) ||
-		    put_user(len, &iov[i].iov_len))
-			return -EFAULT;
-	}
-
-	if (put_user(iov, &sgio->dxferp))
-		return -EFAULT;
-	return 0;
-}
-
-int nvme_sg_io32(struct nvme_ns *ns, unsigned long arg)
-{
-	sg_io_hdr32_t __user *sgio32 = (sg_io_hdr32_t __user *)arg;
-	sg_io_hdr_t __user *sgio;
-	u16 iovec_count;
-	u32 data;
-	void __user *dxferp;
-	int err;
-	int interface_id;
-
-	if (get_user(interface_id, &sgio32->interface_id))
-		return -EFAULT;
-	if (interface_id != 'S')
-		return -EINVAL;
-
-	if (get_user(iovec_count, &sgio32->iovec_count))
-		return -EFAULT;
-
-	{
-		void __user *top = compat_alloc_user_space(0);
-		void __user *new = compat_alloc_user_space(sizeof(sg_io_hdr_t) +
-				       (iovec_count * sizeof(sg_iovec_t)));
-		if (new > top)
-			return -EINVAL;
-
-		sgio = new;
-	}
-
-	/* Ok, now construct.  */
-	if (copy_in_user(&sgio->interface_id, &sgio32->interface_id,
-			 (2 * sizeof(int)) +
-			 (2 * sizeof(unsigned char)) +
-			 (1 * sizeof(unsigned short)) +
-			 (1 * sizeof(unsigned int))))
-		return -EFAULT;
-
-	if (get_user(data, &sgio32->dxferp))
-		return -EFAULT;
-	dxferp = compat_ptr(data);
-	if (iovec_count) {
-		if (sg_build_iovec(sgio, dxferp, iovec_count))
-			return -EFAULT;
-	} else {
-		if (put_user(dxferp, &sgio->dxferp))
-			return -EFAULT;
-	}
-
-	{
-		unsigned char __user *cmdp;
-		unsigned char __user *sbp;
-
-		if (get_user(data, &sgio32->cmdp))
-			return -EFAULT;
-		cmdp = compat_ptr(data);
-
-		if (get_user(data, &sgio32->sbp))
-			return -EFAULT;
-		sbp = compat_ptr(data);
-
-		if (put_user(cmdp, &sgio->cmdp) ||
-		    put_user(sbp, &sgio->sbp))
-			return -EFAULT;
-	}
-
-	if (copy_in_user(&sgio->timeout, &sgio32->timeout,
-			 3 * sizeof(int)))
-		return -EFAULT;
-
-	if (get_user(data, &sgio32->usr_ptr))
-		return -EFAULT;
-	if (put_user(compat_ptr(data), &sgio->usr_ptr))
-		return -EFAULT;
-
-	err = nvme_sg_io(ns, sgio);
-	if (err >= 0) {
-		void __user *datap;
-
-		if (copy_in_user(&sgio32->pack_id, &sgio->pack_id,
-				 sizeof(int)) ||
-		    get_user(datap, &sgio->usr_ptr) ||
-		    put_user((u32)(unsigned long)datap,
-			     &sgio32->usr_ptr) ||
-		    copy_in_user(&sgio32->status, &sgio->status,
-				 (4 * sizeof(unsigned char)) +
-				 (2 * sizeof(unsigned short)) +
-				 (3 * sizeof(int))))
-			err = -EFAULT;
-	}
-
-	return err;
-}
-#endif
-
-int nvme_sg_get_version_num(int __user *ip)
-{
-	return put_user(sg_version_num, ip);
-}
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/nvme.h b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/nvme.h
deleted file mode 100644
index 061826f..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/nvme.h
+++ /dev/null
@@ -1,173 +0,0 @@
-/*
- * Definitions for the NVM Express interface
- * Copyright (c) 2011-2014, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- */
-
-#ifndef _LINUX_NVME_H
-#define _LINUX_NVME_H
-
-#include <linux/pci.h>
-#include <linux/miscdevice.h>
-#include <linux/kref.h>
-#include "uapi_linux_nvme.h"
-
-struct nvme_bar {
-	__u64			cap;	/* Controller Capabilities */
-	__u32			vs;	/* Version */
-	__u32			intms;	/* Interrupt Mask Set */
-	__u32			intmc;	/* Interrupt Mask Clear */
-	__u32			cc;	/* Controller Configuration */
-	__u32			rsvd1;	/* Reserved */
-	__u32			csts;	/* Controller Status */
-	__u32			rsvd2;	/* Reserved */
-	__u32			aqa;	/* Admin Queue Attributes */
-	__u64			asq;	/* Admin SQ Base Address */
-	__u64			acq;	/* Admin CQ Base Address */
-};
-
-#define NVME_CAP_MQES(cap)	((cap) & 0xffff)
-#define NVME_CAP_TIMEOUT(cap)	(((cap) >> 24) & 0xff)
-#define NVME_CAP_STRIDE(cap)	(((cap) >> 32) & 0xf)
-#define NVME_CAP_MPSMIN(cap)	(((cap) >> 48) & 0xf)
-
-enum {
-	NVME_CC_ENABLE		= 1 << 0,
-	NVME_CC_CSS_NVM		= 0 << 4,
-	NVME_CC_MPS_SHIFT	= 7,
-	NVME_CC_ARB_RR		= 0 << 11,
-	NVME_CC_ARB_WRRU	= 1 << 11,
-	NVME_CC_ARB_VS		= 7 << 11,
-	NVME_CC_SHN_NONE	= 0 << 14,
-	NVME_CC_SHN_NORMAL	= 1 << 14,
-	NVME_CC_SHN_ABRUPT	= 2 << 14,
-	NVME_CC_SHN_MASK	= 3 << 14,
-	NVME_CC_IOSQES		= 6 << 16,
-	NVME_CC_IOCQES		= 4 << 20,
-	NVME_CSTS_RDY		= 1 << 0,
-	NVME_CSTS_CFS		= 1 << 1,
-	NVME_CSTS_SHST_NORMAL	= 0 << 2,
-	NVME_CSTS_SHST_OCCUR	= 1 << 2,
-	NVME_CSTS_SHST_CMPLT	= 2 << 2,
-	NVME_CSTS_SHST_MASK	= 3 << 2,
-};
-
-#define NVME_VS(major, minor)	(major << 16 | minor)
-
-extern unsigned char nvme_io_timeout;
-#define NVME_IO_TIMEOUT	(nvme_io_timeout * HZ)
-
-/*
- * Represents an NVM Express device.  Each nvme_dev is a PCI function.
- */
-struct nvme_dev {
-	struct list_head node;
-	struct nvme_queue __rcu **queues;
-	unsigned short __percpu *io_queue;
-	u32 __iomem *dbs;
-	struct pci_dev *pci_dev;
-	struct dma_pool *prp_page_pool;
-	struct dma_pool *prp_small_pool;
-	int instance;
-	unsigned queue_count;
-	unsigned online_queues;
-	unsigned max_qid;
-	int q_depth;
-	u32 db_stride;
-	u32 ctrl_config;
-	struct msix_entry *entry;
-	struct nvme_bar __iomem *bar;
-	struct list_head namespaces;
-	struct kref kref;
-	struct miscdevice miscdev;
-	struct work_struct reset_work;
-	struct work_struct cpu_work;
-	char name[12];
-	char serial[20];
-	char model[40];
-	char firmware_rev[8];
-	u32 max_hw_sectors;
-	u32 stripe_size;
-	u16 oncs;
-	u16 abort_limit;
-	u8 vwc;
-	u8 initialized;
-};
-
-/*
- * An NVM Express namespace is equivalent to a SCSI LUN
- */
-struct nvme_ns {
-	struct list_head list;
-
-	struct nvme_dev *dev;
-	struct request_queue *queue;
-	struct gendisk *disk;
-
-	unsigned ns_id;
-	int lba_shift;
-	int ms;
-	u64 mode_select_num_blocks;
-	u32 mode_select_block_len;
-};
-
-/*
- * The nvme_iod describes the data in an I/O, including the list of PRP
- * entries.  You can't see it in this data structure because C doesn't let
- * me express that.  Use nvme_alloc_iod to ensure there's enough space
- * allocated to store the PRP list.
- */
-struct nvme_iod {
-	void *private;		/* For the use of the submitter of the I/O */
-	int npages;		/* In the PRP list. 0 means small pool in use */
-	int offset;		/* Of PRP list */
-	int nents;		/* Used in scatterlist */
-	int length;		/* Of data, in bytes */
-	unsigned long start_time;
-	dma_addr_t first_dma;
-	struct list_head node;
-	struct scatterlist sg[0];
-};
-
-static inline u64 nvme_block_nr(struct nvme_ns *ns, sector_t sector)
-{
-	return (sector >> (ns->lba_shift - 9));
-}
-
-/**
- * nvme_free_iod - frees an nvme_iod
- * @dev: The device that the I/O was submitted to
- * @iod: The memory to free
- */
-void nvme_free_iod(struct nvme_dev *dev, struct nvme_iod *iod);
-
-int nvme_setup_prps(struct nvme_dev *, struct nvme_iod *, int , gfp_t);
-struct nvme_iod *nvme_map_user_pages(struct nvme_dev *dev, int write,
-				unsigned long addr, unsigned length);
-void nvme_unmap_user_pages(struct nvme_dev *dev, int write,
-			struct nvme_iod *iod);
-int nvme_submit_io_cmd(struct nvme_dev *, struct nvme_command *, u32 *);
-int nvme_submit_admin_cmd(struct nvme_dev *, struct nvme_command *,
-							u32 *result);
-int nvme_identify(struct nvme_dev *, unsigned nsid, unsigned cns,
-							dma_addr_t dma_addr);
-int nvme_get_features(struct nvme_dev *dev, unsigned fid, unsigned nsid,
-			dma_addr_t dma_addr, u32 *result);
-int nvme_set_features(struct nvme_dev *dev, unsigned fid, unsigned dword11,
-			dma_addr_t dma_addr, u32 *result);
-
-struct sg_io_hdr;
-
-int nvme_sg_io(struct nvme_ns *ns, struct sg_io_hdr __user *u_hdr);
-int nvme_sg_io32(struct nvme_ns *ns, unsigned long arg);
-int nvme_sg_get_version_num(int __user *ip);
-
-#endif /* _LINUX_NVME_H */
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/uapi_linux_nvme.h b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/uapi_linux_nvme.h
deleted file mode 100644
index 3fbd281..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-229-centos-7_1/uapi_linux_nvme.h
+++ /dev/null
@@ -1,798 +0,0 @@
-/*
- * Definitions for the NVM Express interface
- * Copyright (c) 2011-2014, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- */
-
-#ifndef _UAPI_LINUX_NVME_H
-#define _UAPI_LINUX_NVME_H
-
-#include <linux/types.h>
-
-#define KSID_SUPPORT
-struct nvme_id_power_state {
-	__le16			max_power;	/* centiwatts */
-	__u8			rsvd2;
-	__u8			flags;
-	__le32			entry_lat;	/* microseconds */
-	__le32			exit_lat;	/* microseconds */
-	__u8			read_tput;
-	__u8			read_lat;
-	__u8			write_tput;
-	__u8			write_lat;
-	__le16			idle_power;
-	__u8			idle_scale;
-	__u8			rsvd19;
-	__le16			active_power;
-	__u8			active_work_scale;
-	__u8			rsvd23[9];
-};
-
-enum {
-	NVME_PS_FLAGS_MAX_POWER_SCALE	= 1 << 0,
-	NVME_PS_FLAGS_NON_OP_STATE	= 1 << 1,
-};
-
-struct nvme_id_ctrl {
-	__le16			vid;
-	__le16			ssvid;
-	char			sn[20];
-	char			mn[40];
-	char			fr[8];
-	__u8			rab;
-	__u8			ieee[3];
-	__u8			mic;
-	__u8			mdts;
-	__u16			cntlid;
-	__u32			ver;
-	__u8			rsvd84[172];
-	__le16			oacs;
-	__u8			acl;
-	__u8			aerl;
-	__u8			frmw;
-	__u8			lpa;
-	__u8			elpe;
-	__u8			npss;
-	__u8			avscc;
-	__u8			apsta;
-	__le16			wctemp;
-	__le16			cctemp;
-	__u8			rsvd270[242];
-	__u8			sqes;
-	__u8			cqes;
-	__u8			rsvd514[2];
-	__le32			nn;
-	__le16			oncs;
-	__le16			fuses;
-	__u8			fna;
-	__u8			vwc;
-	__le16			awun;
-	__le16			awupf;
-	__u8			nvscc;
-	__u8			rsvd531;
-	__le16			acwu;
-	__u8			rsvd534[2];
-	__le32			sgls;
-	__u8			rsvd540[1508];
-	struct nvme_id_power_state	psd[32];
-	__u8			vs[1024];
-};
-
-enum {
-	NVME_CTRL_ONCS_COMPARE			= 1 << 0,
-	NVME_CTRL_ONCS_WRITE_UNCORRECTABLE	= 1 << 1,
-	NVME_CTRL_ONCS_DSM			= 1 << 2,
-	NVME_CTRL_VWC_PRESENT			= 1 << 0,
-};
-
-struct nvme_lbaf {
-	__le16			ms;
-	__u8			ds;
-	__u8			rp;
-};
-
-struct nvme_id_ns {
-	__le64			nsze;
-	__le64			ncap;
-	__le64			nuse;
-	__u8			nsfeat;
-	__u8			nlbaf;
-	__u8			flbas;
-	__u8			mc;
-	__u8			dpc;
-	__u8			dps;
-	__u8			nmic;
-	__u8			rescap;
-	__u8			fpi;
-	__u8			rsvd33;
-	__le16			nawun;
-	__le16			nawupf;
-	__le16			nacwu;
-	__u8			rsvd40[80];
-	__u8			eui64[8];
-	struct nvme_lbaf	lbaf[16];
-	__u8			rsvd192[192];
-	__u8			vs[3712];
-};
-
-enum {
-	NVME_NS_FEAT_THIN	= 1 << 0,
-	NVME_LBAF_RP_BEST	= 0,
-	NVME_LBAF_RP_BETTER	= 1,
-	NVME_LBAF_RP_GOOD	= 2,
-	NVME_LBAF_RP_DEGRADED	= 3,
-};
-
-struct nvme_smart_log {
-	__u8			critical_warning;
-	__u8			temperature[2];
-	__u8			avail_spare;
-	__u8			spare_thresh;
-	__u8			percent_used;
-	__u8			rsvd6[26];
-	__u8			data_units_read[16];
-	__u8			data_units_written[16];
-	__u8			host_reads[16];
-	__u8			host_writes[16];
-	__u8			ctrl_busy_time[16];
-	__u8			power_cycles[16];
-	__u8			power_on_hours[16];
-	__u8			unsafe_shutdowns[16];
-	__u8			media_errors[16];
-	__u8			num_err_log_entries[16];
-	__le32			warning_temp_time;
-	__le32			critical_comp_time;
-	__le16			temp_sensor[8];
-	__u8			rsvd216[296];
-};
-
-enum {
-	NVME_SMART_CRIT_SPARE		= 1 << 0,
-	NVME_SMART_CRIT_TEMPERATURE	= 1 << 1,
-	NVME_SMART_CRIT_RELIABILITY	= 1 << 2,
-	NVME_SMART_CRIT_MEDIA		= 1 << 3,
-	NVME_SMART_CRIT_VOLATILE_MEMORY	= 1 << 4,
-};
-
-struct nvme_lba_range_type {
-	__u8			type;
-	__u8			attributes;
-	__u8			rsvd2[14];
-	__u64			slba;
-	__u64			nlb;
-	__u8			guid[16];
-	__u8			rsvd48[16];
-};
-
-enum {
-	NVME_LBART_TYPE_FS	= 0x01,
-	NVME_LBART_TYPE_RAID	= 0x02,
-	NVME_LBART_TYPE_CACHE	= 0x03,
-	NVME_LBART_TYPE_SWAP	= 0x04,
-
-	NVME_LBART_ATTRIB_TEMP	= 1 << 0,
-	NVME_LBART_ATTRIB_HIDE	= 1 << 1,
-};
-
-/* I/O commands */
-
-enum nvme_opcode {
-	nvme_cmd_flush		= 0x00,
-	nvme_cmd_write		= 0x01,
-	nvme_cmd_read		= 0x02,
-	nvme_cmd_write_uncor	= 0x04,
-	nvme_cmd_compare	= 0x05,
-	nvme_cmd_dsm		= 0x09,
-	nvme_cmd_kv_store	= 0x81,
-	nvme_cmd_kv_append	= 0x83,
-	nvme_cmd_kv_retrieve	= 0x90,
-	nvme_cmd_kv_delete	= 0xA1,
-	nvme_cmd_kv_iter_req	= 0xB1,
-	nvme_cmd_kv_iter_read	= 0xB2, 
-	nvme_cmd_kv_exist	= 0xB3,
-};
-
-#define is_kv_append_cmd(opcode)	((opcode) == nvme_cmd_kv_append)
-#define is_kv_store_cmd(opcode)	((opcode) == nvme_cmd_kv_store)
-#define is_kv_retrieve_cmd(opcode)	((opcode) == nvme_cmd_kv_retrieve)
-#define is_kv_delete_cmd(opcode)	((opcode) == nvme_cmd_kv_delete)
-#define is_kv_iter_req_cmd(opcode)	((opcode) == nvme_cmd_kv_iter_req)
-#define is_kv_iter_read_cmd(opcode)	((opcode) == nvme_cmd_kv_iter_read)
-#define is_kv_exist_cmd(opcode)	((opcode) == nvme_cmd_kv_exist)
-#define is_kv_cmd(opcode)	(is_kv_store_cmd(opcode) || is_kv_append_cmd(opcode) ||\
-		is_kv_retrieve_cmd(opcode) || is_kv_delete_cmd(opcode) ||\
-		is_kv_iter_req_cmd(opcode) || is_kv_iter_read_cmd(opcode) ||\
-		is_kv_exist_cmd(opcode))
-
-#define kv_nvme_is_write(opcode) (is_kv_store_cmd(opcode) || is_kv_append_cmd(opcode))
-
-struct nvme_common_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__le32			cdw2[2];
-	__le64			metadata;
-	__le64			prp1;
-	__le64			prp2;
-	__le32			cdw10[6];
-};
-
-struct nvme_rw_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2;
-	__le64			metadata;
-	__le64			prp1;
-	__le64			prp2;
-	__le64			slba;
-	__le16			length;
-	__le16			control;
-	__le32			dsmgmt;
-	__le32			reftag;
-	__le16			apptag;
-	__le16			appmask;
-};
-
-struct nvme_kv_store_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd;
-	__le32			offset;
-	__u32			rsvd2;
-	__le64			prp1;
-	__le64			prp2;/* value dptr prp1,2 */
-	__le32			value_len; /* size in word */
-	__u8			key_len; /* 0 ~ 255 (key len -1) */
-	__u8			option;
-	__u8            invalid_byte:2;
-	__u8            rsvd3:6;
-	__u8            rsvd4;
-	union {
-		struct {
-			char	key[16];
-		};
-		struct {
-			__le64	key_prp;
-			__le64	key_prp2;
-		};
-	};
-};
-
-struct nvme_kv_append_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd;
-	__le32			offset;
-	__u32			rsvd2;
-	__le64			prp1;
-	__le64			prp2;/* value dptr prp1,2 */
-	__le32			value_len; /* size in word */
-	__u8			key_len; /* 0 ~ 255 (key len -1) */
-	__u8			option;
-	__u8            invalid_byte:2;
-	__u8            rsvd3:6;
-	__u8            rsvd4;
-	union {
-		struct {
-			char	key[16];
-		};
-		struct {
-			__le64	key_prp;
-			__le64	key_prp2;
-		};
-	};
-};
-
-struct nvme_kv_retrieve_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd;
-	__le32			offset;
-	__u32			rsvd2;
-	__le64			prp1;
-	__le64			prp2;/* value dptr prp1,2 */
-	__le32			value_len; /* size in word */
-	__u8			key_len; /* 0 ~ 255 (key len -1) */
-	__u8			option;
-	__u16			rsvd3;
-	union {
-		struct {
-			char	key[16];
-		};
-		struct {
-			__le64	key_prp;
-			__le64	key_prp2;
-		};
-	};
-};
-
-struct nvme_kv_delete_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd;
-	__le32			offset;
-	__u32			rsvd2;
-	__u64			rsvd3[2];
-	__le32			value_len; /* should be zero*/
-	__u8            key_len; /* 0 ~ 255 (key len -1) */
-	__u8            option;
-	__u16           rsvd4;
-	union {
-		struct {
-			char	key[16];
-		};
-		struct {
-			__le64	key_prp;
-			__le64	key_prp2;
-		};
-	};
-};
-
-struct nvme_kv_exist_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd;
-	__le32			offset;
-	__u32			rsvd2;
-	__u64			rsvd3[2];
-	__le32			value_len; /* should be zero*/
-	__u8            key_len; /* 0 ~ 255 (key len -1) */
-	__u8            option;
-	__u16           rsvd4;
-	union {
-		struct {
-			char	key[16];
-		};
-		struct {
-			__le64	key_prp;
-			__le64	key_prp2;
-		};
-	};
-};
-
-
-struct nvme_kv_iter_req_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd[4];
-	__le32			zero; /* should be zero*/
-	__u8            iter_handle;
-	__u8            option;
-	__u16           rsvd2;
-	__le32          iter_val;
-	__le32          iter_bitmask;
-	__u64           rsvd3;
-};
-
-
-struct nvme_kv_iter_read_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd[2];
-	__le64			prp1;
-	__le64			prp2;/* value dptr prp1,2 */
-	__le32			value_len; /* size in word */
-	__u8            iter_handle; /* 0 ~ 255 (key len -1) */
-	__u8            option;
-	__u16           rsvd2;
-	__u64           rsvd3[2];
-};
-
-enum {
-	NVME_RW_LR			= 1 << 15,
-	NVME_RW_FUA			= 1 << 14,
-	NVME_RW_DSM_FREQ_UNSPEC		= 0,
-	NVME_RW_DSM_FREQ_TYPICAL	= 1,
-	NVME_RW_DSM_FREQ_RARE		= 2,
-	NVME_RW_DSM_FREQ_READS		= 3,
-	NVME_RW_DSM_FREQ_WRITES		= 4,
-	NVME_RW_DSM_FREQ_RW		= 5,
-	NVME_RW_DSM_FREQ_ONCE		= 6,
-	NVME_RW_DSM_FREQ_PREFETCH	= 7,
-	NVME_RW_DSM_FREQ_TEMP		= 8,
-	NVME_RW_DSM_LATENCY_NONE	= 0 << 4,
-	NVME_RW_DSM_LATENCY_IDLE	= 1 << 4,
-	NVME_RW_DSM_LATENCY_NORM	= 2 << 4,
-	NVME_RW_DSM_LATENCY_LOW		= 3 << 4,
-	NVME_RW_DSM_SEQ_REQ		= 1 << 6,
-	NVME_RW_DSM_COMPRESSED		= 1 << 7,
-};
-
-struct nvme_dsm_cmd {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[2];
-	__le64			prp1;
-	__le64			prp2;
-	__le32			nr;
-	__le32			attributes;
-	__u32			rsvd12[4];
-};
-
-enum {
-	NVME_DSMGMT_IDR		= 1 << 0,
-	NVME_DSMGMT_IDW		= 1 << 1,
-	NVME_DSMGMT_AD		= 1 << 2,
-};
-
-struct nvme_dsm_range {
-	__le32			cattr;
-	__le32			nlb;
-	__le64			slba;
-};
-
-/* Admin commands */
-
-enum nvme_admin_opcode {
-	nvme_admin_delete_sq		= 0x00,
-	nvme_admin_create_sq		= 0x01,
-	nvme_admin_get_log_page		= 0x02,
-	nvme_admin_delete_cq		= 0x04,
-	nvme_admin_create_cq		= 0x05,
-	nvme_admin_identify		= 0x06,
-	nvme_admin_abort_cmd		= 0x08,
-	nvme_admin_set_features		= 0x09,
-	nvme_admin_get_features		= 0x0a,
-	nvme_admin_async_event		= 0x0c,
-	nvme_admin_activate_fw		= 0x10,
-	nvme_admin_download_fw		= 0x11,
-	nvme_admin_format_nvm		= 0x80,
-	nvme_admin_security_send	= 0x81,
-	nvme_admin_security_recv	= 0x82,
-};
-
-enum {
-	NVME_QUEUE_PHYS_CONTIG	= (1 << 0),
-	NVME_CQ_IRQ_ENABLED	= (1 << 1),
-	NVME_SQ_PRIO_URGENT	= (0 << 1),
-	NVME_SQ_PRIO_HIGH	= (1 << 1),
-	NVME_SQ_PRIO_MEDIUM	= (2 << 1),
-	NVME_SQ_PRIO_LOW	= (3 << 1),
-	NVME_FEAT_ARBITRATION	= 0x01,
-	NVME_FEAT_POWER_MGMT	= 0x02,
-	NVME_FEAT_LBA_RANGE	= 0x03,
-	NVME_FEAT_TEMP_THRESH	= 0x04,
-	NVME_FEAT_ERR_RECOVERY	= 0x05,
-	NVME_FEAT_VOLATILE_WC	= 0x06,
-	NVME_FEAT_NUM_QUEUES	= 0x07,
-	NVME_FEAT_IRQ_COALESCE	= 0x08,
-	NVME_FEAT_IRQ_CONFIG	= 0x09,
-	NVME_FEAT_WRITE_ATOMIC	= 0x0a,
-	NVME_FEAT_ASYNC_EVENT	= 0x0b,
-	NVME_FEAT_SW_PROGRESS	= 0x0c,
-	NVME_LOG_ERROR		= 0x01,
-	NVME_LOG_SMART		= 0x02,
-	NVME_LOG_FW_SLOT	= 0x03,
-	NVME_LOG_RESERVATION	= 0x80,
-	NVME_FWACT_REPL		= (0 << 3),
-	NVME_FWACT_REPL_ACTV	= (1 << 3),
-	NVME_FWACT_ACTV		= (2 << 3),
-};
-
-struct nvme_identify {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[2];
-	__le64			prp1;
-	__le64			prp2;
-	__le32			cns;
-	__u32			rsvd11[5];
-};
-
-struct nvme_features {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[2];
-	__le64			prp1;
-	__le64			prp2;
-	__le32			fid;
-	__le32			dword11;
-	__u32			rsvd12[4];
-};
-
-struct nvme_create_cq {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[5];
-	__le64			prp1;
-	__u64			rsvd8;
-	__le16			cqid;
-	__le16			qsize;
-	__le16			cq_flags;
-	__le16			irq_vector;
-	__u32			rsvd12[4];
-};
-
-struct nvme_create_sq {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[5];
-	__le64			prp1;
-	__u64			rsvd8;
-	__le16			sqid;
-	__le16			qsize;
-	__le16			sq_flags;
-	__le16			cqid;
-	__u32			rsvd12[4];
-};
-
-struct nvme_delete_queue {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[9];
-	__le16			qid;
-	__u16			rsvd10;
-	__u32			rsvd11[5];
-};
-
-struct nvme_abort_cmd {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[9];
-	__le16			sqid;
-	__le16			cid;
-	__u32			rsvd11[5];
-};
-
-struct nvme_download_firmware {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[5];
-	__le64			prp1;
-	__le64			prp2;
-	__le32			numd;
-	__le32			offset;
-	__u32			rsvd12[4];
-};
-
-struct nvme_format_cmd {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[4];
-	__le32			cdw10;
-	__u32			rsvd11[5];
-};
-
-struct nvme_command {
-	union {
-		struct nvme_common_command common;
-		struct nvme_rw_command rw;
-		struct nvme_identify identify;
-		struct nvme_features features;
-		struct nvme_create_cq create_cq;
-		struct nvme_create_sq create_sq;
-		struct nvme_delete_queue delete_queue;
-		struct nvme_download_firmware dlfw;
-		struct nvme_format_cmd format;
-		struct nvme_dsm_cmd dsm;
-		struct nvme_abort_cmd abort;
-		struct nvme_kv_store_command kv_store;
-		struct nvme_kv_append_command kv_append;
-		struct nvme_kv_retrieve_command kv_retrieve;
-		struct nvme_kv_delete_command kv_delete;
-		struct nvme_kv_iter_req_command kv_iter_req;
-		struct nvme_kv_iter_read_command kv_iter_read;
-		struct nvme_kv_exist_command kv_exist;
-	};
-};
-
-enum {
-	NVME_SC_SUCCESS			= 0x0,
-	NVME_SC_INVALID_OPCODE		= 0x1,
-	NVME_SC_INVALID_FIELD		= 0x2,
-	NVME_SC_CMDID_CONFLICT		= 0x3,
-	NVME_SC_DATA_XFER_ERROR		= 0x4,
-	NVME_SC_POWER_LOSS		= 0x5,
-	NVME_SC_INTERNAL		= 0x6,
-	NVME_SC_ABORT_REQ		= 0x7,
-	NVME_SC_ABORT_QUEUE		= 0x8,
-	NVME_SC_FUSED_FAIL		= 0x9,
-	NVME_SC_FUSED_MISSING		= 0xa,
-	NVME_SC_INVALID_NS		= 0xb,
-	NVME_SC_CMD_SEQ_ERROR		= 0xc,
-	NVME_SC_LBA_RANGE		= 0x80,
-	NVME_SC_CAP_EXCEEDED		= 0x81,
-	NVME_SC_NS_NOT_READY		= 0x82,
-	NVME_SC_CQ_INVALID		= 0x100,
-	NVME_SC_QID_INVALID		= 0x101,
-	NVME_SC_QUEUE_SIZE		= 0x102,
-	NVME_SC_ABORT_LIMIT		= 0x103,
-	NVME_SC_ABORT_MISSING		= 0x104,
-	NVME_SC_ASYNC_LIMIT		= 0x105,
-	NVME_SC_FIRMWARE_SLOT		= 0x106,
-	NVME_SC_FIRMWARE_IMAGE		= 0x107,
-	NVME_SC_INVALID_VECTOR		= 0x108,
-	NVME_SC_INVALID_LOG_PAGE	= 0x109,
-	NVME_SC_INVALID_FORMAT		= 0x10a,
-	NVME_SC_BAD_ATTRIBUTES		= 0x180,
-	NVME_SC_WRITE_FAULT		= 0x280,
-	NVME_SC_READ_ERROR		= 0x281,
-	NVME_SC_GUARD_CHECK		= 0x282,
-	NVME_SC_APPTAG_CHECK		= 0x283,
-	NVME_SC_REFTAG_CHECK		= 0x284,
-	NVME_SC_COMPARE_FAILED		= 0x285,
-	NVME_SC_ACCESS_DENIED		= 0x286,
-	NVME_SC_DNR			= 0x4000,
-};
-
-struct nvme_completion {
-	__le32	result;		/* Used by admin commands to return data */
-	__u32	rsvd;
-	__le16	sq_head;	/* how much of this queue may be reclaimed */
-	__le16	sq_id;		/* submission queue that generated this entry */
-	__u16	command_id;	/* of the command which completed */
-	__le16	status;		/* did the command fail, and if so, why? */
-};
-
-struct nvme_user_io {
-	__u8	opcode;
-	__u8	flags;
-	__u16	control;
-	__u16	nblocks;
-	__u16	rsvd;
-	__u64	metadata;
-	__u64	addr;
-	__u64	slba;
-	__u32	dsmgmt;
-	__u32	reftag;
-	__u16	apptag;
-	__u16	appmask;
-};
-
-struct nvme_admin_cmd {
-	__u8	opcode;
-	__u8	flags;
-	__u16	rsvd1;
-	__u32	nsid;
-	__u32	cdw2;
-	__u32	cdw3;
-	__u64	metadata;
-	__u64	addr;
-	__u32	metadata_len;
-	__u32	data_len;
-	__u32	cdw10;
-	__u32	cdw11;
-	__u32	cdw12;
-	__u32	cdw13;
-	__u32	cdw14;
-	__u32	cdw15;
-	__u32	timeout_ms;
-	__u32	result;
-};
-
-#define NVME_IOCTL_ID		_IO('N', 0x40)
-#define NVME_IOCTL_ADMIN_CMD	_IOWR('N', 0x41, struct nvme_admin_cmd)
-#define NVME_IOCTL_SUBMIT_IO	_IOW('N', 0x42, struct nvme_user_io)
-
-#define KVCMD_INLINE_KEY_MAX	(16)
-#define KVCMD_MAX_KEY_SIZE		(255)
-#define KVCMD_MIN_KEY_SIZE		(4)
-
-#define	KVS_SUCCESS		0
-#define KVS_ERR_ALIGNMENT	(-1)
-#define KVS_ERR_CAPAPCITY	(-2)
-#define KVS_ERR_CLOSE	(-3)
-#define KVS_ERR_CONT_EXIST	(-4)
-#define KVS_ERR_CONT_NAME	(-5)
-#define KVS_ERR_CONT_NOT_EXIST	(-6)
-#define KVS_ERR_DEVICE_NOT_EXIST (-7)
-#define KVS_ERR_GROUP	(-8)
-#define KVS_ERR_INDEX	(-9)
-#define KVS_ERR_IO	(-10)
-#define KVS_ERR_KEY	(-11)
-#define KVS_ERR_KEY_TYPE	(-12)
-#define KVS_ERR_MEMORY	(-13)
-#define KVS_ERR_NULL_INPUT	(-14)
-#define KVS_ERR_OFFSET	(-15)
-#define KVS_ERR_OPEN	(-16)
-#define KVS_ERR_OPTION_NOT_SUPPORTED	(-17)
-#define KVS_ERR_PERMISSION	(-18)
-#define KVS_ERR_SPACE	(-19)
-#define KVS_ERR_TIMEOUT	(-20)
-#define KVS_ERR_TUPLE_EXIST	(-21)
-#define KVS_ERR_TUPLE_NOT_EXIST	(-22)
-#define KVS_ERR_VALUE	(-23)
-
-/*
- * Extended definition for kv device
- */
-struct nvme_passthru_kv_cmd {
-	__u8	opcode;
-	__u8	flags;
-	__u16	rsvd1;
-	__u32	nsid;
-	__u32	cdw2;
-	__u32	cdw3;
-	__u32	cdw4;
-	__u32	cdw5;
-	__u64	data_addr;
-	__u32	data_length;
-	__u32	key_length;
-	__u32	cdw10;
-	__u32	cdw11;
-	union {
-		struct {
-			__u64 key_addr;
-			__u32 rsvd5;
-			__u32 rsvd6;
-		};
-		__u8 key[16];
-		struct {
-			__u32 cdw12;
-			__u32 cdw13;
-			__u32 cdw14;
-			__u32 cdw15;
-		};
-	};
-	__u32	timeout_ms;
-	__u32	result;
-	__u32	status;
-	__u32	ctxid;
-	__u64	reqid;
-};
-
-struct nvme_aioctx {
-	__u32	ctxid;
-	__u32	eventfd;
-};
-
-
-struct nvme_aioevent {
-	__u64	reqid;
-	__u32	ctxid;
-	__u32	result;
-	__u16	status;
-};
-
-#define MAX_AIO_EVENTS	128
-struct nvme_aioevents {
-	__u16	nr;
-	__u32   ctxid;
-	struct nvme_aioevent events[MAX_AIO_EVENTS];
-};
-
-#define NVME_IOCTL_AIO_CMD		_IOWR('N', 0x47, struct nvme_passthru_kv_cmd)
-#define NVME_IOCTL_SET_AIOCTX	_IOWR('N', 0x48, struct nvme_aioctx)
-#define NVME_IOCTL_DEL_AIOCTX	_IOWR('N', 0x49, struct nvme_aioctx)
-#define NVME_IOCTL_GET_AIOEVENT	_IOWR('N', 0x50, struct nvme_aioevents)
-#define NVME_IOCTL_IO_KV_CMD	_IOWR('N', 0x51, struct nvme_passthru_kv_cmd)
-
-#endif /* _UAPI_LINUX_NVME_H */
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-693-centos-7_4/core.c b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-693-centos-7_4/core.c
index 9d2cb7b..da00466 100644
--- a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-693-centos-7_4/core.c
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-693-centos-7_4/core.c
@@ -3160,6 +3160,11 @@ int __init nvme_core_init(void)
 	else if (result > 0)
 		nvme_char_major = result;
 
+	nvme_class = class_create(THIS_MODULE, "nvme");
+	if (IS_ERR(nvme_class)) {
+		result = PTR_ERR(nvme_class);
+		goto unregister_chrdev;
+	}
 #if 1
 	result = aio_service_init();
 	if (result)
@@ -3168,18 +3173,10 @@ int __init nvme_core_init(void)
 	result = aio_worker_init();
 	if (result)
 		goto unregister_aio_service;
-
 #endif
-	nvme_class = class_create(THIS_MODULE, "nvme");
-	if (IS_ERR(nvme_class)) {
-		result = PTR_ERR(nvme_class);
-		goto unregister_aio_worker;
-	}
 
 	return 0;
 #if 1
-unregister_aio_worker:
-	aio_worker_exit();
 unregister_aio_service:
 	aio_service_exit();
 #endif
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-693-centos-7_4/linux_nvme_ioctl.h b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-693-centos-7_4/linux_nvme_ioctl.h
index 8edc08e..359f29e 100644
--- a/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-693-centos-7_4/linux_nvme_ioctl.h
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v3.10.0-693-centos-7_4/linux_nvme_ioctl.h
@@ -88,15 +88,28 @@ struct nvme_passthru_kv_cmd {
 	__u32	cdw3;
 	__u32	cdw4;
 	__u32	cdw5;
-	__u64	data_addr;
-	__u32	data_length;
-	__u32	key_length;
+	struct {
+		__u64	data_addr;
+		union {
+			struct {
+				__u32	data_length;
+				__u8	rsvd2;//Not used
+				__u8	list_key_offset;//List key offset
+				__u16	list_max_keys;//List max keys
+			};
+			struct {
+				__u8    length[3];//Length in 24 bit
+				__u8    rkey[4];//RDMA remote key
+				__u8    type;//SGL Type
+			};//Keyed SGL second half
+		};
+	};//dptr
 	__u32	cdw10;
 	__u32	cdw11;
 	union {
 		struct {
 			__u64 key_addr;
-			__u32 rsvd5;
+			__u32 key_length;
 			__u32 rsvd6;
 		};
 		__u8 key[16];
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.13.15-041315-ubuntu-16_04/core.c b/PDK/driver/PCIe/kernel_driver/kernel_v4.13.15-041315-ubuntu-16_04/core.c
index 30716b1..e7f9a5b 100644
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.13.15-041315-ubuntu-16_04/core.c
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v4.13.15-041315-ubuntu-16_04/core.c
@@ -1486,7 +1486,7 @@ int __nvme_submit_kv_user_cmd(struct request_queue *q, struct nvme_command *cmd,
 	} else {
 		blk_execute_rq(req->q, disk, req, 0);
         if (nvme_req(req)->flags & NVME_REQ_CANCELLED)
-            ret = -EINTR;
+            ret = EINTR;
         else
 		    ret = nvme_req(req)->status;
 
@@ -1584,18 +1584,18 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
 //    }
 
 	switch(cmd.opcode) {
-        case nvme_cmd_kv_store:
+		case nvme_cmd_kv_store:
         case nvme_cmd_kv_append:
             option = cpu_to_le32(cmd.cdw4);
-            c.kv_store.offset = cpu_to_le32(cmd.cdw5);
-            /* validate key length */
+			c.kv_store.offset = cpu_to_le32(cmd.cdw5);
+			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
                     cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            c.kv_store.key_len = cpu_to_le32(cmd.key_length -1); /* key len -1 */
+			c.kv_store.key_len = cpu_to_le32(cmd.key_length -1); /* key len -1 */
             c.kv_store.option = (option & 0xff);
             /* set value size */
             if (cmd.data_length % 4) {
@@ -1605,90 +1605,90 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
                 c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2));
             }
 
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user*)cmd.key_addr;
-                meta_len = cmd.key_length;
-            } else {
-                memcpy(c.kv_store.key, cmd.key, cmd.key_length);
-            }
-            break;
-        case nvme_cmd_kv_retrieve:
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user*)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_store.key, cmd.key, cmd.key_length);
+			}
+		break;
+		case nvme_cmd_kv_retrieve:
             option = cpu_to_le32(cmd.cdw4);
-            c.kv_retrieve.offset = cpu_to_le32(cmd.cdw5);
-            /* validate key length */
+			c.kv_retrieve.offset = cpu_to_le32(cmd.cdw5);
+			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
                     cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            c.kv_retrieve.key_len = cpu_to_le32(cmd.key_length -1); /* key len - 1 */
+			c.kv_retrieve.key_len = cpu_to_le32(cmd.key_length -1); /* key len - 1 */
             c.kv_retrieve.option = (option & 0xff);
-            c.kv_retrieve.value_len = cpu_to_le32((cmd.data_length >> 2));
+			c.kv_retrieve.value_len = cpu_to_le32((cmd.data_length >> 2));
 
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user*)cmd.key_addr;
-                meta_len = cmd.key_length;
-            } else {
-                memcpy(c.kv_retrieve.key, cmd.key, cmd.key_length);
-            }
-            break;
-        case nvme_cmd_kv_delete:
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user*)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_retrieve.key, cmd.key, cmd.key_length);
+			}
+		break;
+		case nvme_cmd_kv_delete:
             option = cpu_to_le32(cmd.cdw4);
-            /* validate key length */
+			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
                     cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            c.kv_delete.key_len = cpu_to_le32(cmd.key_length -1);
-            c.kv_delete.option = (option & 0xff);	
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user *)cmd.key_addr;
-                meta_len = cmd.key_length;
-            } else {
-                memcpy(c.kv_delete.key, cmd.key, cmd.key_length);
-            }
-            break;
-        case nvme_cmd_kv_exist:
+			c.kv_delete.key_len = cpu_to_le32(cmd.key_length -1);
+		    c.kv_delete.option = (option & 0xff);	
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user *)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_delete.key, cmd.key, cmd.key_length);
+			}
+			break;
+		case nvme_cmd_kv_exist:
             option = cpu_to_le32(cmd.cdw4);
-            /* validate key length */
+			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
                     cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            c.kv_exist.key_len = cpu_to_le32(cmd.key_length -1);
-            c.kv_exist.option = (option & 0xff);	
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user *)cmd.key_addr;
-                meta_len = cmd.key_length;
-            } else {
-                memcpy(c.kv_exist.key, cmd.key, cmd.key_length);
-            }
-            break;
-        case nvme_cmd_kv_iter_req:
+			c.kv_exist.key_len = cpu_to_le32(cmd.key_length -1);
+		    c.kv_exist.option = (option & 0xff);	
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user *)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_exist.key, cmd.key, cmd.key_length);
+			}
+			break;
+		case nvme_cmd_kv_iter_req:
             option = cpu_to_le32(cmd.cdw4);
             iter_handle = cpu_to_le32(cmd.cdw5);
             c.kv_iter_req.iter_handle = iter_handle & 0xff;
             c.kv_iter_req.option = option & 0xff;
             c.kv_iter_req.iter_val = cpu_to_le32(cmd.cdw12);
             c.kv_iter_req.iter_bitmask = cpu_to_le32(cmd.cdw13);
-            break;
-        case nvme_cmd_kv_iter_read:
+			break;
+		case nvme_cmd_kv_iter_read:
             option = cpu_to_le32(cmd.cdw4);
             iter_handle = cpu_to_le32(cmd.cdw5);
             c.kv_iter_read.iter_handle = iter_handle & 0xff;
             c.kv_iter_read.option = option & 0xff;
-            c.kv_iter_read.value_len = cpu_to_le32((cmd.data_length >> 2));
-        break;
-        default:
-            cmd.result = KVS_ERR_IO;
-            status = -EINVAL;
-            goto exit;
-}
+			c.kv_iter_read.value_len = cpu_to_le32((cmd.data_length >> 2));
+		break;
+		default:
+				cmd.result = KVS_ERR_IO;
+				status = -EINVAL;
+				goto exit;
+	}
 
 //    if (cmd.data_addr) {
 //        u32 *c_data = c.common.cdw2;
@@ -3874,6 +3874,11 @@ int __init nvme_core_init(void)
 	else if (result > 0)
 		nvme_char_major = result;
 
+	nvme_class = class_create(THIS_MODULE, "nvme");
+	if (IS_ERR(nvme_class)) {
+		result = PTR_ERR(nvme_class);
+		goto unregister_chrdev;
+	}
 #if 1
     result = aio_service_init();
     if (result)
@@ -3881,20 +3886,10 @@ int __init nvme_core_init(void)
     
     result = aio_worker_init();
     if (result)
-		goto destroy_aio_service;
+		goto unregister_chrdev;
 #endif
-
-	nvme_class = class_create(THIS_MODULE, "nvme");
-	if (IS_ERR(nvme_class)) {
-		result = PTR_ERR(nvme_class);
-		goto destroy_aio_worker;
-	}
 	return 0;
 
-destroy_aio_worker:
-	aio_worker_exit();
-destroy_aio_service:
-	aio_service_exit();
 unregister_chrdev:
 	__unregister_chrdev(nvme_char_major, 0, NVME_MINORS, "nvme");
 destroy_wq:
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.13.15-041315-ubuntu-16_04/linux_nvme.h b/PDK/driver/PCIe/kernel_driver/kernel_v4.13.15-041315-ubuntu-16_04/linux_nvme.h
index f7d117c..3225788 100644
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.13.15-041315-ubuntu-16_04/linux_nvme.h
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v4.13.15-041315-ubuntu-16_04/linux_nvme.h
@@ -1171,9 +1171,9 @@ struct nvme_command {
 		struct nvme_kv_store_command kv_store;
 		struct nvme_kv_retrieve_command kv_retrieve;
 		struct nvme_kv_delete_command kv_delete;
-		struct nvme_kv_append_command kv_append;
-		struct nvme_kv_iter_req_command kv_iter_req;
-		struct nvme_kv_iter_read_command kv_iter_read;
+        struct nvme_kv_append_command kv_append;
+        struct nvme_kv_iter_req_command kv_iter_req;
+        struct nvme_kv_iter_read_command kv_iter_read;
 		struct nvme_kv_exist_command kv_exist;
 #endif
 	};
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/Kconfig b/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/Kconfig
deleted file mode 100644
index 8f845de..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/Kconfig
+++ /dev/null
@@ -1,50 +0,0 @@
-config NVME_CORE
-	tristate
-
-config BLK_DEV_NVME
-	tristate "NVM Express block device"
-	depends on PCI && BLOCK
-	select NVME_CORE
-	---help---
-	  The NVM Express driver is for solid state drives directly
-	  connected to the PCI or PCI Express bus.  If you know you
-	  don't have one of these, it is safe to answer N.
-
-	  To compile this driver as a module, choose M here: the
-	  module will be called nvme.
-
-config NVME_FABRICS
-	tristate
-
-config NVME_RDMA
-	tristate "NVM Express over Fabrics RDMA host driver"
-	depends on INFINIBAND && INFINIBAND_ADDR_TRANS && BLOCK
-	select NVME_CORE
-	select NVME_FABRICS
-	select SG_POOL
-	help
-	  This provides support for the NVMe over Fabrics protocol using
-	  the RDMA (Infiniband, RoCE, iWarp) transport.  This allows you
-	  to use remote block devices exported using the NVMe protocol set.
-
-	  To configure a NVMe over Fabrics controller use the nvme-cli tool
-	  from https://github.com/linux-nvme/nvme-cli.
-
-	  If unsure, say N.
-
-config NVME_FC
-	tristate "NVM Express over Fabrics FC host driver"
-	depends on BLOCK
-	depends on HAS_DMA
-	select NVME_CORE
-	select NVME_FABRICS
-	select SG_POOL
-	help
-	  This provides support for the NVMe over Fabrics protocol using
-	  the FC transport.  This allows you to use remote block devices
-	  exported using the NVMe protocol set.
-
-	  To configure a NVMe over Fabrics controller use the nvme-cli tool
-	  from https://github.com/linux-nvme/nvme-cli.
-
-	  If unsure, say N.
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/Makefile b/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/Makefile
deleted file mode 100644
index 3040253..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/Makefile
+++ /dev/null
@@ -1,22 +0,0 @@
-# SPDX-License-Identifier: GPL-2.0
-obj-$(CONFIG_NVME_CORE)			+= nvme-core.o
-obj-$(CONFIG_BLK_DEV_NVME)		+= nvme.o
-obj-$(CONFIG_NVME_FABRICS)		+= nvme-fabrics.o
-obj-$(CONFIG_NVME_RDMA)			+= nvme-rdma.o
-obj-$(CONFIG_NVME_FC)			+= nvme-fc.o
-
-nvme-core-y				:= core.o
-nvme-core-$(CONFIG_NVM)			+= lightnvm.o
-
-nvme-y					+= pci.o
-
-nvme-fabrics-y				+= fabrics.o
-
-nvme-rdma-y				+= rdma.o
-
-nvme-fc-y				+= fc.o
-
-all:
-	        make -C /lib/modules/`uname -r`/build M=`pwd` modules
-clean:
-	        make -C /lib/modules/`uname -r`/build M=`pwd` clean
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/core.c b/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/core.c
deleted file mode 100644
index 1b164df..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/core.c
+++ /dev/null
@@ -1,3958 +0,0 @@
-/*
- * NVM Express device driver
- * Copyright (c) 2011-2014, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- */
-
-#include <linux/blkdev.h>
-#include <linux/blk-mq.h>
-#include <linux/delay.h>
-#include <linux/errno.h>
-#include <linux/hdreg.h>
-#include <linux/kernel.h>
-#include <linux/module.h>
-#include <linux/list_sort.h>
-#include <linux/slab.h>
-#include <linux/types.h>
-#include <linux/pr.h>
-#include <linux/ptrace.h>
-#include <linux/t10-pi.h>
-#include <linux/pm_qos.h>
-#include <asm/unaligned.h>
-
-#include <linux/file.h>
-#include <linux/eventfd.h>
-#include <linux/kthread.h>
-
-#include "linux_nvme_ioctl.h"
-#include "nvme.h"
-#include "fabrics.h"
-
-#define NVME_MINORS		(1U << MINORBITS)
-
-unsigned char admin_timeout = 60;
-module_param(admin_timeout, byte, 0644);
-MODULE_PARM_DESC(admin_timeout, "timeout in seconds for admin commands");
-EXPORT_SYMBOL_GPL(admin_timeout);
-
-unsigned char nvme_io_timeout = 30;
-module_param_named(io_timeout, nvme_io_timeout, byte, 0644);
-MODULE_PARM_DESC(io_timeout, "timeout in seconds for I/O");
-EXPORT_SYMBOL_GPL(nvme_io_timeout);
-
-static unsigned char shutdown_timeout = 5;
-module_param(shutdown_timeout, byte, 0644);
-MODULE_PARM_DESC(shutdown_timeout, "timeout in seconds for controller shutdown");
-
-static u8 nvme_max_retries = 5;
-module_param_named(max_retries, nvme_max_retries, byte, 0644);
-MODULE_PARM_DESC(max_retries, "max number of retries a command may have");
-
-static int nvme_char_major;
-module_param(nvme_char_major, int, 0);
-
-static unsigned long default_ps_max_latency_us = 100000;
-module_param(default_ps_max_latency_us, ulong, 0644);
-MODULE_PARM_DESC(default_ps_max_latency_us,
-		 "max power saving latency for new devices; use PM QOS to change per device");
-
-static bool force_apst;
-module_param(force_apst, bool, 0644);
-MODULE_PARM_DESC(force_apst, "allow APST for newly enumerated devices even if quirked off");
-
-static bool streams;
-module_param(streams, bool, 0644);
-MODULE_PARM_DESC(streams, "turn on support for Streams write directives");
-
-struct workqueue_struct *nvme_wq;
-EXPORT_SYMBOL_GPL(nvme_wq);
-
-static LIST_HEAD(nvme_ctrl_list);
-static DEFINE_SPINLOCK(dev_list_lock);
-
-static struct class *nvme_class;
-
-//AIO data structure
-static struct	kmem_cache *kaioctx_cachep = 0;
-static struct kmem_cache *kaiocb_cachep = 0;
-static mempool_t *kaioctx_mempool = 0;
-static mempool_t *kaiocb_mempool = 0;
-
-static __u32 aio_context_id;
-
-#define AIOCTX_MAX 1024
-#define AIOCB_MAX (1024 * 64)
-
-static __u64 debug_completed = 0;
-static int debug_outstanding = 0;
-
-struct nvme_kaioctx
-{
-	struct nvme_aioctx uctx;
-	struct eventfd_ctx *eventctx;
-	struct list_head kaiocb_list;
-	spinlock_t kaioctx_spinlock;
-	struct kref ref;
-};
-
-static struct nvme_kaioctx **g_kaioctx_tb = NULL;
-static spinlock_t g_kaioctx_tb_spinlock;
-
-struct aio_user_ctx {
-	int nents;
-	int len;
-	struct page ** pages;
-	struct scatterlist *sg;
-	char data[1];
-};
-
-struct nvme_kaiocb {
-	struct list_head aiocb_list;
-	struct nvme_aioevent event;
-	int opcode;
-	struct nvme_command cmd;
-	struct gendisk *disk;
-	unsigned long start_time;
-	struct scatterlist meta_sg;
-	bool need_to_copy;
-	bool use_meta;
-	void *kv_data;
-	void *meta;
-	struct aio_user_ctx *user_ctx;
-	struct aio_user_ctx *kernel_ctx;
-	struct request *req;
-};
-
-/* context for aio worker.*/
-struct aio_worker_ctx{
-	int cpu;
-	spinlock_t kaiocb_spinlock;
-	struct list_head kaiocb_list;
-	wait_queue_head_t aio_waitqueue;
-};
-
-/* percpu aio worker context pointer */
-struct aio_worker_ctx * __percpu aio_w_ctx;
-/* percpu aio worker pointer */
-struct task_struct ** __percpu aio_worker;
-
-
-static void remove_kaioctx(struct nvme_kaioctx * ctx)
-{
-	struct nvme_kaiocb *tmpcb;
-	struct list_head *pos, *q;
-	unsigned long flags;
-	if (ctx) {
-		spin_lock_irqsave(&ctx->kaioctx_spinlock, flags);
-		list_for_each_safe(pos, q, &ctx->kaiocb_list) {
-			tmpcb = list_entry(pos, struct nvme_kaiocb, aiocb_list);
-			list_del(pos);
-			mempool_free(tmpcb, kaiocb_mempool);
-		}
-		spin_unlock_irqrestore(&ctx->kaioctx_spinlock, flags);
-		eventfd_ctx_put(ctx->eventctx);
-		mempool_free(ctx, kaioctx_mempool);
-	}
-}
-
-static void cleanup_kaioctx(struct kref *kref) {
-	struct nvme_kaioctx *ctx = container_of(kref, struct nvme_kaioctx, ref);
-	remove_kaioctx(ctx);
-}
-
-static void ref_kaioctx(struct nvme_kaioctx *ctx) {
-	kref_get(&ctx->ref);
-}
-
-static void deref_kaioctx(struct nvme_kaioctx *ctx) {
-	kref_put(&ctx->ref, cleanup_kaioctx);
-}
-
-/* destroy mempools */
-static void destroy_aio_mempool(void)
-{
-	int i = 0;
-	if (g_kaioctx_tb) {
-		for (i = 0; i < AIOCTX_MAX; ++i) {
-			if (g_kaioctx_tb[i]) {
-				remove_kaioctx(g_kaioctx_tb[i]);
-				g_kaioctx_tb[i] = NULL;
-			}
-		}
-		kfree(g_kaioctx_tb);
-		g_kaioctx_tb = NULL;
-	}
-	if (kaiocb_mempool)
-		mempool_destroy(kaiocb_mempool);
-	if (kaioctx_mempool)
-		mempool_destroy(kaioctx_mempool);
-	if (kaiocb_cachep)
-		kmem_cache_destroy(kaioctx_cachep);
-	if (kaioctx_cachep)
-		kmem_cache_destroy(kaioctx_cachep);
-}
-
-/* prepare basic data structures
- * to support aio context and requests
- */
-static int aio_service_init(void)
-{
-	g_kaioctx_tb = (struct nvme_kaioctx**)kmalloc(sizeof(struct nvme_kaioctx*) * AIOCTX_MAX, GFP_KERNEL);
-	if (!g_kaioctx_tb)
-		goto fail;
-	memset(g_kaioctx_tb, 0, sizeof(struct nvme_kaioctx*) * AIOCTX_MAX);
-
-	// slab allocator and memory pool
-	kaioctx_cachep = kmem_cache_create("nvme_kaioctx", sizeof(struct nvme_kaioctx), 0, 0, NULL);
-	if (!kaioctx_cachep)
-		goto fail;
-	kaiocb_cachep = kmem_cache_create("nvme_kaiocb", sizeof(struct nvme_kaiocb), 0, 0, NULL);
-	if (!kaiocb_cachep)
-		goto fail;
-
-	kaiocb_mempool = mempool_create_slab_pool(AIOCB_MAX, kaiocb_cachep);
-	if (!kaiocb_mempool)
-		goto fail;
-	kaioctx_mempool = mempool_create_slab_pool(AIOCTX_MAX, kaioctx_cachep);
-	if (!kaioctx_mempool)
-		goto fail;
-
-	// context id 0 is reserved for normal I/O operations (synchronous)
-	aio_context_id = 1;
-	spin_lock_init(&g_kaioctx_tb_spinlock);
-	printk(KERN_DEBUG "nvme-aio:initialized\n");
-	return 0;
-
-fail:
-	destroy_aio_mempool();
-	return -ENOMEM;
-}
-
-/* release memory before exit */
-static int aio_service_exit(void)
-{
-	destroy_aio_mempool();
-	printk(KERN_DEBUG "nvme-aio: unloaded\n");
-	return 0;
-}
-
-static struct nvme_kaioctx* find_kaioctx(__u32 ctxid) {
-	struct nvme_kaioctx *tmp = NULL;
-	tmp = g_kaioctx_tb[ctxid];
-	if (tmp) ref_kaioctx(tmp);
-	return tmp;
-}
-
-/* find an aio context with a given id */
-static int set_aio_event(__u32 ctxid, struct nvme_kaiocb *kaiocb)
-{
-	struct nvme_kaioctx *tmp;
-	unsigned long flags;
-	tmp = find_kaioctx(ctxid);
-	if (tmp) {
-		spin_lock_irqsave(&tmp->kaioctx_spinlock, flags);
-		list_add_tail(&kaiocb->aiocb_list, &tmp->kaiocb_list);
-		spin_unlock_irqrestore(&tmp->kaioctx_spinlock, flags);
-		eventfd_signal(tmp->eventctx, 1);
-		deref_kaioctx(tmp);
-		return 0;
-	}
-
-	return -1;
-}
-
-/*
- * delete an aio context
- * it will release any resources allocated for this context
- * */
-static int nvme_del_aioctx(struct nvme_aioctx __user *uctx)
-{
-	struct nvme_kaioctx ctx;
-	unsigned long flags;
-	if (copy_from_user(&ctx, uctx, sizeof(struct nvme_aioctx)))
-		return -EFAULT;
-
-	spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
-	if (g_kaioctx_tb[ctx.uctx.ctxid]) {
-		deref_kaioctx(g_kaioctx_tb[ctx.uctx.ctxid]);
-		g_kaioctx_tb[ctx.uctx.ctxid] = NULL;
-	}
-	spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-	return 0;
-}
-
-/*
- * set up an aio context
- * allocate a new context with given parameters and prepare an eventfd_context
- */
-static int nvme_set_aioctx(struct nvme_aioctx __user *uctx)
-{
-	struct nvme_kaioctx* ctx;
-	struct fd efile;
-	struct eventfd_ctx* eventfd_ctx = NULL;
-	unsigned long flags;
-	int ret = 0;
-	int i = 0;
-
-	if (!capable(CAP_SYS_ADMIN))
-		return -EACCES;
-
-	ctx = mempool_alloc(kaioctx_mempool, GFP_NOIO);
-	if (!ctx)
-		return -ENOMEM;
-
-	if (copy_from_user(ctx, uctx, sizeof(struct nvme_aioctx)))
-		return -EFAULT;
-
-	efile = fdget(ctx->uctx.eventfd);
-	if (!efile.file) {
-		pr_err("nvme_set_aioctx: failed to get efile for efd %d.\n", ctx->uctx.eventfd);
-		ret = -EBADF;
-		goto exit;
-	}
-
-	eventfd_ctx = eventfd_ctx_fileget(efile.file);
-	if (IS_ERR(eventfd_ctx)) {
-		pr_err("nvme_set_aioctx: failed to get eventfd_ctx for efd %d.\n", ctx->uctx.eventfd);
-		ret = PTR_ERR(eventfd_ctx);
-		goto put_efile;
-	}
-
-	// set context id
-	spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
-	if (g_kaioctx_tb[aio_context_id]) {
-		for (i = 0; i < AIOCTX_MAX; ++i) {
-			if (g_kaioctx_tb[i] == NULL) {
-				aio_context_id = i;
-				break;
-			}
-		}
-		if (i >= AIOCTX_MAX) {
-			spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-			pr_err("nvme_set_aioctx: too many aioctx open.\n");
-			ret = -EMFILE;
-			goto put_event_fd;
-		}
-	}
-	spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-	ctx->uctx.ctxid = aio_context_id++;
-	if (aio_context_id == AIOCTX_MAX)
-		aio_context_id = 0;
-	ctx->eventctx = eventfd_ctx;
-	spin_lock_init(&ctx->kaioctx_spinlock);
-	INIT_LIST_HEAD(&ctx->kaiocb_list);
-	kref_init(&ctx->ref);
-	spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
-	g_kaioctx_tb[ctx->uctx.ctxid] = ctx;
-	spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-
-	if (copy_to_user(&uctx->ctxid, &ctx->uctx.ctxid, sizeof(ctx->uctx.ctxid))) {
-		pr_err("nvme_set_aioctx: failed to copy context id %d to user.\n", ctx->uctx.ctxid);
-		ret = -EINVAL;
-		goto cleanup;
-	}
-	eventfd_ctx = NULL;
-	debug_outstanding = 0;;
-	debug_completed = 0;
-	fdput(efile);
-	return 0;
-cleanup:
-	spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
-	g_kaioctx_tb[ctx->uctx.ctxid - 1] = NULL;
-	spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
-	mempool_free(ctx, kaiocb_mempool);
-put_event_fd:
-	eventfd_ctx_put(eventfd_ctx);
-put_efile:
-	fdput(efile);
-exit:
-	return ret;
-}
-
-/*
- * get an aiocb which represents a single I/O request
- * */
-static struct nvme_kaiocb* get_aiocb(__u64 reqid)
-{
-	struct nvme_kaiocb* req;
-	req = mempool_alloc(kaiocb_mempool, GFP_NOIO);
-	if (!req) return 0;
-	memset(req, 0, sizeof(*req));
-	INIT_LIST_HEAD(&req->aiocb_list);
-	req->event.reqid = reqid;
-	return req;
-}
-
-/*
- * returns the completed events to user
- * */
-static int nvme_get_ioevents(struct nvme_aioevents __user *uevents)
-{
-	struct list_head *pos, *q;
-	struct nvme_kaiocb *tmp;
-	struct nvme_kaioctx *tmp_ctx;
-	unsigned long flags;
-	LIST_HEAD(tmp_head);
-	__u16 count = 0;
-	__u16 nr = 0;
-	__u32 ctxid = 0;
-
-	if (!capable(CAP_SYS_ADMIN))
-		return -EACCES;
-
-	if (get_user(nr, &uevents->nr) < 0)
-		return -EINVAL;
-
-	if (get_user(ctxid, &uevents->ctxid) < 0)
-		return -EINVAL;
-
-	tmp_ctx = find_kaioctx(ctxid);
-	if (tmp_ctx) {
-		spin_lock_irqsave(&tmp_ctx->kaioctx_spinlock, flags);
-		list_for_each_safe(pos, q, &tmp_ctx->kaiocb_list) {
-			list_del_init(pos);
-			list_add(pos, &tmp_head);
-			++count;
-			if (nr == count) break;
-		}
-		spin_unlock_irqrestore(&tmp_ctx->kaioctx_spinlock, flags);
-		deref_kaioctx(tmp_ctx);
-		count = 0;
-		list_for_each_safe(pos, q, &tmp_head) {
-			list_del(pos);
-			tmp = list_entry(pos, struct nvme_kaiocb, aiocb_list);
-			copy_to_user(&uevents->events[count], &tmp->event, sizeof(struct nvme_aioevent));
-			mempool_free(tmp, kaiocb_mempool);
-			++count;
-		}
-	}
-	if (put_user(count, &uevents->nr) < 0)
-		return -EINVAL;
-
-	return 0;
-}
-
-static void kv_complete_aio_fn(struct nvme_kaiocb* cmdinfo)
-{
-	struct request* req = cmdinfo->req;
-	int i = 0;
-	blk_mq_free_request(req);
-	if (cmdinfo->need_to_copy) {
-		if ((is_kv_retrieve_cmd(cmdinfo->opcode) && !cmdinfo->event.status) ||
-				(is_kv_iter_read_cmd(cmdinfo->opcode) && (!cmdinfo->event.status ||
-										((cmdinfo->event.status & 0xff) == 0x93)))) {
-			/* unaligned user buffer, copy back to user if this request is for read */
-			sg_copy_from_buffer(cmdinfo->user_ctx->sg, cmdinfo->user_ctx->nents,
-					cmdinfo->kv_data, cmdinfo->user_ctx->len);
-		}
-		for (i = 0; i < cmdinfo->kernel_ctx->nents; ++i)
-			put_page(sg_page(&cmdinfo->kernel_ctx->sg[i]));
-	}
-
-	if (cmdinfo->user_ctx) {
-		if (is_kv_store_cmd(cmdinfo->opcode) || is_kv_append_cmd(cmdinfo->opcode))
-			generic_end_io_acct(req->q, WRITE, &cmdinfo->disk->part0, cmdinfo->start_time);
-		else
-			generic_end_io_acct(req->q, READ, &cmdinfo->disk->part0, cmdinfo->start_time);
-		for (i = 0; i < cmdinfo->user_ctx->nents; ++i)
-			put_page(sg_page(&cmdinfo->user_ctx->sg[i]));
-	}
-
-	if (cmdinfo->use_meta)
-		put_page(sg_page(&cmdinfo->meta_sg));
-
-	if (cmdinfo->need_to_copy) {
-		kfree(cmdinfo->kernel_ctx);
-		kfree(cmdinfo->kv_data);
-	}
-
-	if (cmdinfo->user_ctx) kfree(cmdinfo->user_ctx);
-	if (cmdinfo->meta) kfree(cmdinfo->meta);
-
-	if (set_aio_event(cmdinfo->event.ctxid, cmdinfo))
-		mempool_free(cmdinfo, kaiocb_mempool);
-}
-
-static void wake_up_aio_worker(struct aio_worker_ctx *aio_ctx)
-{
-	wake_up(&aio_ctx->aio_waitqueue);
-}
-
-static void insert_aiocb_to_worker(struct nvme_kaiocb *aiocb)
-{
-	struct aio_worker_ctx *ctx = NULL;
-	unsigned long flags;
-	int cpu = smp_processor_id();
-	INIT_LIST_HEAD(&aiocb->aiocb_list);
-
-	ctx = per_cpu_ptr(aio_w_ctx, cpu);
-	spin_lock_irqsave(&ctx->kaiocb_spinlock, flags);
-	list_add_tail(&aiocb->aiocb_list, &ctx->kaiocb_list);
-	spin_unlock_irqrestore(&ctx->kaiocb_spinlock, flags);
-	wake_up_aio_worker(ctx);
-}
-
-static void kv_async_completion(struct request *req, blk_status_t status)
-{
-	struct nvme_kaiocb *aiocb = req->end_io_data;
-
-	aiocb->req = req;
-	aiocb->event.result = le32_to_cpu(nvme_req(req)->result.u32);
-	aiocb->event.status = le16_to_cpu(nvme_req(req)->status);
-
-	insert_aiocb_to_worker(aiocb);
-}
-
-static int kvaio_percpu_worker_fn(void *arg)
-{
-	struct aio_worker_ctx *ctx = (struct aio_worker_ctx*)arg;
-	struct list_head *pos, *next;
-	unsigned long flags;
-	LIST_HEAD(tmp_list);
-	pr_err("start aio worker %u\n", ctx->cpu);
-	while (!kthread_should_stop() || !list_empty(&ctx->kaiocb_list)) {
-		if (list_empty(&ctx->kaiocb_list)) {
-			wait_event_interruptible_timeout(ctx->aio_waitqueue,
-					!list_empty(&ctx->kaiocb_list), HZ/10);
-			continue;
-		}
-		INIT_LIST_HEAD(&tmp_list);
-		spin_lock_irqsave(&ctx->kaiocb_spinlock, flags);
-		list_splice(&ctx->kaiocb_list, &tmp_list);
-		INIT_LIST_HEAD(&ctx->kaiocb_list);
-		spin_unlock_irqrestore(&ctx->kaiocb_spinlock, flags);
-		if (!list_empty(&tmp_list)) {
-			list_for_each_safe(pos, next, &tmp_list) {
-				struct nvme_kaiocb *aiocb = list_entry(pos, struct nvme_kaiocb, aiocb_list);
-				list_del_init(pos);
-				kv_complete_aio_fn(aiocb);
-			}
-		}
-	}
-	return 0;
-}
-
-static int aio_worker_init(void)
-{
-	struct task_struct **p = NULL;
-	struct aio_worker_ctx *ctx = NULL;
-	int i = 0;
-
-	aio_worker = alloc_percpu(struct task_struct *);
-	if (!aio_worker) {
-		pr_err("fail to alloc percpu worker task_struct!\n");
-		return -ENOMEM;
-	}
-
-	aio_w_ctx = alloc_percpu(struct aio_worker_ctx);
-	if (!aio_w_ctx) {
-		pr_err("fail to alloc percpu aio context!\n");
-		goto out_free;
-	}
-
-	for_each_online_cpu(i) {
-		ctx = per_cpu_ptr(aio_w_ctx, i);
-		ctx->cpu = i;
-		spin_lock_init(&ctx->kaiocb_spinlock);
-		INIT_LIST_HEAD(&ctx->kaiocb_list);
-		init_waitqueue_head(&ctx->aio_waitqueue);
-		p = per_cpu_ptr(aio_worker, i);
-		*p = kthread_create_on_node(kvaio_percpu_worker_fn, ctx, cpu_to_node(i), "aio_completion_worker/%u", i);
-		if (!(*p))
-			goto reset_pthread;
-		kthread_bind(*p, i);
-		wake_up_process(*p);
-	}
-	return 0;
-
-reset_pthread:
-	for_each_online_cpu(i) {
-		p = per_cpu_ptr(aio_worker, i);
-		if (*p) kthread_stop(*p);
-	}
-out_free:
-	if (aio_worker) free_percpu(aio_worker);
-	return -ENOMEM;
-}
-
-static void aio_worker_exit(void) {
-	struct task_struct **p = NULL;
-	int i = 0;
-	for_each_online_cpu(i) {
-		p = per_cpu_ptr(aio_worker, i);
-		if (*p) kthread_stop(*p);
-	}
-	free_percpu(aio_worker);
-	free_percpu(aio_w_ctx);
-}
-
-static __le32 nvme_get_log_dw10(u8 lid, size_t size)
-{
-	return cpu_to_le32((((size / 4) - 1) << 16) | lid);
-}
-
-int nvme_reset_ctrl(struct nvme_ctrl *ctrl)
-{
-	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING))
-		return -EBUSY;
-	if (!queue_work(nvme_wq, &ctrl->reset_work))
-		return -EBUSY;
-	return 0;
-}
-EXPORT_SYMBOL_GPL(nvme_reset_ctrl);
-
-static int nvme_reset_ctrl_sync(struct nvme_ctrl *ctrl)
-{
-	int ret;
-
-	ret = nvme_reset_ctrl(ctrl);
-	if (!ret)
-		flush_work(&ctrl->reset_work);
-	return ret;
-}
-
-static blk_status_t nvme_error_status(struct request *req)
-{
-	switch (nvme_req(req)->status & 0x7ff) {
-	case NVME_SC_SUCCESS:
-		return BLK_STS_OK;
-	case NVME_SC_CAP_EXCEEDED:
-		return BLK_STS_NOSPC;
-	case NVME_SC_ONCS_NOT_SUPPORTED:
-		return BLK_STS_NOTSUPP;
-	case NVME_SC_WRITE_FAULT:
-	case NVME_SC_READ_ERROR:
-	case NVME_SC_UNWRITTEN_BLOCK:
-	case NVME_SC_ACCESS_DENIED:
-	case NVME_SC_READ_ONLY:
-		return BLK_STS_MEDIUM;
-	case NVME_SC_GUARD_CHECK:
-	case NVME_SC_APPTAG_CHECK:
-	case NVME_SC_REFTAG_CHECK:
-	case NVME_SC_INVALID_PI:
-		return BLK_STS_PROTECTION;
-	case NVME_SC_RESERVATION_CONFLICT:
-		return BLK_STS_NEXUS;
-	default:
-		return BLK_STS_IOERR;
-	}
-}
-
-static inline bool nvme_req_needs_retry(struct request *req)
-{
-	if (blk_noretry_request(req))
-		return false;
-	if (nvme_req(req)->status & NVME_SC_DNR)
-		return false;
-	if (nvme_req(req)->retries >= nvme_max_retries)
-		return false;
-	return true;
-}
-
-void nvme_complete_rq(struct request *req)
-{
-	if (unlikely(nvme_req(req)->status && nvme_req_needs_retry(req))) {
-		nvme_req(req)->retries++;
-		blk_mq_requeue_request(req, true);
-		return;
-	}
-
-	blk_mq_end_request(req, nvme_error_status(req));
-}
-EXPORT_SYMBOL_GPL(nvme_complete_rq);
-
-void nvme_cancel_request(struct request *req, void *data, bool reserved)
-{
-	int status;
-
-	if (!blk_mq_request_started(req))
-		return;
-
-	dev_dbg_ratelimited(((struct nvme_ctrl *) data)->device,
-				"Cancelling I/O %d", req->tag);
-
-	status = NVME_SC_ABORT_REQ;
-	if (blk_queue_dying(req->q))
-		status |= NVME_SC_DNR;
-	nvme_req(req)->status = status;
-	blk_mq_complete_request(req);
-
-}
-EXPORT_SYMBOL_GPL(nvme_cancel_request);
-
-bool nvme_change_ctrl_state(struct nvme_ctrl *ctrl,
-		enum nvme_ctrl_state new_state)
-{
-	enum nvme_ctrl_state old_state;
-	unsigned long flags;
-	bool changed = false;
-
-	spin_lock_irqsave(&ctrl->lock, flags);
-
-	old_state = ctrl->state;
-	switch (new_state) {
-	case NVME_CTRL_LIVE:
-		switch (old_state) {
-		case NVME_CTRL_NEW:
-		case NVME_CTRL_RESETTING:
-		case NVME_CTRL_RECONNECTING:
-			changed = true;
-			/* FALLTHRU */
-		default:
-			break;
-		}
-		break;
-	case NVME_CTRL_RESETTING:
-		switch (old_state) {
-		case NVME_CTRL_NEW:
-		case NVME_CTRL_LIVE:
-			changed = true;
-			/* FALLTHRU */
-		default:
-			break;
-		}
-		break;
-	case NVME_CTRL_RECONNECTING:
-		switch (old_state) {
-		case NVME_CTRL_LIVE:
-			changed = true;
-			/* FALLTHRU */
-		default:
-			break;
-		}
-		break;
-	case NVME_CTRL_DELETING:
-		switch (old_state) {
-		case NVME_CTRL_LIVE:
-		case NVME_CTRL_RESETTING:
-		case NVME_CTRL_RECONNECTING:
-			changed = true;
-			/* FALLTHRU */
-		default:
-			break;
-		}
-		break;
-	case NVME_CTRL_DEAD:
-		switch (old_state) {
-		case NVME_CTRL_DELETING:
-			changed = true;
-			/* FALLTHRU */
-		default:
-			break;
-		}
-		break;
-	default:
-		break;
-	}
-
-	if (changed)
-		ctrl->state = new_state;
-
-	spin_unlock_irqrestore(&ctrl->lock, flags);
-
-	return changed;
-}
-EXPORT_SYMBOL_GPL(nvme_change_ctrl_state);
-
-static void nvme_free_ns(struct kref *kref)
-{
-	struct nvme_ns *ns = container_of(kref, struct nvme_ns, kref);
-
-	if (ns->ndev)
-		nvme_nvm_unregister(ns);
-
-	if (ns->disk) {
-		spin_lock(&dev_list_lock);
-		ns->disk->private_data = NULL;
-		spin_unlock(&dev_list_lock);
-	}
-
-	put_disk(ns->disk);
-	ida_simple_remove(&ns->ctrl->ns_ida, ns->instance);
-	nvme_put_ctrl(ns->ctrl);
-	kfree(ns);
-}
-
-static void nvme_put_ns(struct nvme_ns *ns)
-{
-	kref_put(&ns->kref, nvme_free_ns);
-}
-
-static struct nvme_ns *nvme_get_ns_from_disk(struct gendisk *disk)
-{
-	struct nvme_ns *ns;
-
-	spin_lock(&dev_list_lock);
-	ns = disk->private_data;
-	if (ns) {
-		if (!kref_get_unless_zero(&ns->kref))
-			goto fail;
-		if (!try_module_get(ns->ctrl->ops->module))
-			goto fail_put_ns;
-	}
-	spin_unlock(&dev_list_lock);
-
-	return ns;
-
-fail_put_ns:
-	kref_put(&ns->kref, nvme_free_ns);
-fail:
-	spin_unlock(&dev_list_lock);
-	return NULL;
-}
-
-struct request *nvme_alloc_request(struct request_queue *q,
-		struct nvme_command *cmd, unsigned int flags, int qid)
-{
-	unsigned op = nvme_is_write(cmd) ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN;
-	struct request *req;
-
-	if (qid == NVME_QID_ANY) {
-		req = blk_mq_alloc_request(q, op, flags);
-	} else {
-		req = blk_mq_alloc_request_hctx(q, op, flags,
-				qid ? qid - 1 : 0);
-	}
-	if (IS_ERR(req))
-		return req;
-
-	req->cmd_flags |= REQ_FAILFAST_DRIVER;
-	nvme_req(req)->cmd = cmd;
-
-	return req;
-}
-EXPORT_SYMBOL_GPL(nvme_alloc_request);
-
-static int nvme_toggle_streams(struct nvme_ctrl *ctrl, bool enable)
-{
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-
-	c.directive.opcode = nvme_admin_directive_send;
-	c.directive.nsid = cpu_to_le32(NVME_NSID_ALL);
-	c.directive.doper = NVME_DIR_SND_ID_OP_ENABLE;
-	c.directive.dtype = NVME_DIR_IDENTIFY;
-	c.directive.tdtype = NVME_DIR_STREAMS;
-	c.directive.endir = enable ? NVME_DIR_ENDIR : 0;
-
-	return nvme_submit_sync_cmd(ctrl->admin_q, &c, NULL, 0);
-}
-
-static int nvme_disable_streams(struct nvme_ctrl *ctrl)
-{
-	return nvme_toggle_streams(ctrl, false);
-}
-
-static int nvme_enable_streams(struct nvme_ctrl *ctrl)
-{
-	return nvme_toggle_streams(ctrl, true);
-}
-
-static int nvme_get_stream_params(struct nvme_ctrl *ctrl,
-				  struct streams_directive_params *s, u32 nsid)
-{
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	memset(s, 0, sizeof(*s));
-
-	c.directive.opcode = nvme_admin_directive_recv;
-	c.directive.nsid = cpu_to_le32(nsid);
-	c.directive.numd = cpu_to_le32((sizeof(*s) >> 2) - 1);
-	c.directive.doper = NVME_DIR_RCV_ST_OP_PARAM;
-	c.directive.dtype = NVME_DIR_STREAMS;
-
-	return nvme_submit_sync_cmd(ctrl->admin_q, &c, s, sizeof(*s));
-}
-
-static int nvme_configure_directives(struct nvme_ctrl *ctrl)
-{
-	struct streams_directive_params s;
-	int ret;
-
-	if (!(ctrl->oacs & NVME_CTRL_OACS_DIRECTIVES))
-		return 0;
-	if (!streams)
-		return 0;
-
-	ret = nvme_enable_streams(ctrl);
-	if (ret)
-		return ret;
-
-	ret = nvme_get_stream_params(ctrl, &s, NVME_NSID_ALL);
-	if (ret)
-		return ret;
-
-	ctrl->nssa = le16_to_cpu(s.nssa);
-	if (ctrl->nssa < BLK_MAX_WRITE_HINTS - 1) {
-		dev_info(ctrl->device, "too few streams (%u) available\n",
-					ctrl->nssa);
-		nvme_disable_streams(ctrl);
-		return 0;
-	}
-
-	ctrl->nr_streams = min_t(unsigned, ctrl->nssa, BLK_MAX_WRITE_HINTS - 1);
-	dev_info(ctrl->device, "Using %u streams\n", ctrl->nr_streams);
-	return 0;
-}
-
-/*
- * Check if 'req' has a write hint associated with it. If it does, assign
- * a valid namespace stream to the write.
- */
-static void nvme_assign_write_stream(struct nvme_ctrl *ctrl,
-				     struct request *req, u16 *control,
-				     u32 *dsmgmt)
-{
-	enum rw_hint streamid = req->write_hint;
-
-	if (streamid == WRITE_LIFE_NOT_SET || streamid == WRITE_LIFE_NONE)
-		streamid = 0;
-	else {
-		streamid--;
-		if (WARN_ON_ONCE(streamid > ctrl->nr_streams))
-			return;
-
-		*control |= NVME_RW_DTYPE_STREAMS;
-		*dsmgmt |= streamid << 16;
-	}
-
-	if (streamid < ARRAY_SIZE(req->q->write_hints))
-		req->q->write_hints[streamid] += blk_rq_bytes(req) >> 9;
-}
-
-static inline void nvme_setup_flush(struct nvme_ns *ns,
-		struct nvme_command *cmnd)
-{
-	memset(cmnd, 0, sizeof(*cmnd));
-	cmnd->common.opcode = nvme_cmd_flush;
-	cmnd->common.nsid = cpu_to_le32(ns->ns_id);
-}
-
-static blk_status_t nvme_setup_discard(struct nvme_ns *ns, struct request *req,
-		struct nvme_command *cmnd)
-{
-	unsigned short segments = blk_rq_nr_discard_segments(req), n = 0;
-	struct nvme_dsm_range *range;
-	struct bio *bio;
-
-	range = kmalloc_array(segments, sizeof(*range), GFP_ATOMIC);
-	if (!range)
-		return BLK_STS_RESOURCE;
-
-	__rq_for_each_bio(bio, req) {
-		u64 slba = nvme_block_nr(ns, bio->bi_iter.bi_sector);
-		u32 nlb = bio->bi_iter.bi_size >> ns->lba_shift;
-
-		range[n].cattr = cpu_to_le32(0);
-		range[n].nlb = cpu_to_le32(nlb);
-		range[n].slba = cpu_to_le64(slba);
-		n++;
-	}
-
-	if (WARN_ON_ONCE(n != segments)) {
-		kfree(range);
-		return BLK_STS_IOERR;
-	}
-
-	memset(cmnd, 0, sizeof(*cmnd));
-	cmnd->dsm.opcode = nvme_cmd_dsm;
-	cmnd->dsm.nsid = cpu_to_le32(ns->ns_id);
-	cmnd->dsm.nr = cpu_to_le32(segments - 1);
-	cmnd->dsm.attributes = cpu_to_le32(NVME_DSMGMT_AD);
-
-	req->special_vec.bv_page = virt_to_page(range);
-	req->special_vec.bv_offset = offset_in_page(range);
-	req->special_vec.bv_len = sizeof(*range) * segments;
-	req->rq_flags |= RQF_SPECIAL_PAYLOAD;
-
-	return BLK_STS_OK;
-}
-
-static inline blk_status_t nvme_setup_rw(struct nvme_ns *ns,
-		struct request *req, struct nvme_command *cmnd)
-{
-	struct nvme_ctrl *ctrl = ns->ctrl;
-	u16 control = 0;
-	u32 dsmgmt = 0;
-
-	/*
-	 * If formated with metadata, require the block layer provide a buffer
-	 * unless this namespace is formated such that the metadata can be
-	 * stripped/generated by the controller with PRACT=1.
-	 */
-	if (ns && ns->ms &&
-	    (!ns->pi_type || ns->ms != sizeof(struct t10_pi_tuple)) &&
-	    !blk_integrity_rq(req) && !blk_rq_is_passthrough(req))
-		return BLK_STS_NOTSUPP;
-
-	if (req->cmd_flags & REQ_FUA)
-		control |= NVME_RW_FUA;
-	if (req->cmd_flags & (REQ_FAILFAST_DEV | REQ_RAHEAD))
-		control |= NVME_RW_LR;
-
-	if (req->cmd_flags & REQ_RAHEAD)
-		dsmgmt |= NVME_RW_DSM_FREQ_PREFETCH;
-
-	memset(cmnd, 0, sizeof(*cmnd));
-	cmnd->rw.opcode = (rq_data_dir(req) ? nvme_cmd_write : nvme_cmd_read);
-	cmnd->rw.nsid = cpu_to_le32(ns->ns_id);
-	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
-	cmnd->rw.length = cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);
-
-	if (req_op(req) == REQ_OP_WRITE && ctrl->nr_streams)
-		nvme_assign_write_stream(ctrl, req, &control, &dsmgmt);
-
-	if (ns->ms) {
-		switch (ns->pi_type) {
-		case NVME_NS_DPS_PI_TYPE3:
-			control |= NVME_RW_PRINFO_PRCHK_GUARD;
-			break;
-		case NVME_NS_DPS_PI_TYPE1:
-		case NVME_NS_DPS_PI_TYPE2:
-			control |= NVME_RW_PRINFO_PRCHK_GUARD |
-					NVME_RW_PRINFO_PRCHK_REF;
-			cmnd->rw.reftag = cpu_to_le32(
-					nvme_block_nr(ns, blk_rq_pos(req)));
-			break;
-		}
-		if (!blk_integrity_rq(req))
-			control |= NVME_RW_PRINFO_PRACT;
-	}
-
-	cmnd->rw.control = cpu_to_le16(control);
-	cmnd->rw.dsmgmt = cpu_to_le32(dsmgmt);
-	return 0;
-}
-
-blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
-		struct nvme_command *cmd)
-{
-	blk_status_t ret = BLK_STS_OK;
-
-	if (!(req->rq_flags & RQF_DONTPREP)) {
-		nvme_req(req)->retries = 0;
-		nvme_req(req)->flags = 0;
-		req->rq_flags |= RQF_DONTPREP;
-	}
-
-	switch (req_op(req)) {
-	case REQ_OP_DRV_IN:
-	case REQ_OP_DRV_OUT:
-		memcpy(cmd, nvme_req(req)->cmd, sizeof(*cmd));
-		break;
-	case REQ_OP_FLUSH:
-		nvme_setup_flush(ns, cmd);
-		break;
-	case REQ_OP_WRITE_ZEROES:
-		/* currently only aliased to deallocate for a few ctrls: */
-	case REQ_OP_DISCARD:
-		ret = nvme_setup_discard(ns, req, cmd);
-		break;
-	case REQ_OP_READ:
-	case REQ_OP_WRITE:
-		ret = nvme_setup_rw(ns, req, cmd);
-		break;
-	default:
-		WARN_ON_ONCE(1);
-		return BLK_STS_IOERR;
-	}
-
-	cmd->common.command_id = req->tag;
-	return ret;
-}
-EXPORT_SYMBOL_GPL(nvme_setup_cmd);
-
-/*
- * Returns 0 on success.  If the result is negative, it's a Linux error code;
- * if the result is positive, it's an NVM Express status code
- */
-int __nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
-		union nvme_result *result, void *buffer, unsigned bufflen,
-		unsigned timeout, int qid, int at_head, int flags)
-{
-	struct request *req;
-	int ret;
-
-	req = nvme_alloc_request(q, cmd, flags, qid);
-	if (IS_ERR(req))
-		return PTR_ERR(req);
-
-	req->timeout = timeout ? timeout : ADMIN_TIMEOUT;
-
-	if (buffer && bufflen) {
-		ret = blk_rq_map_kern(q, req, buffer, bufflen, GFP_KERNEL);
-		if (ret)
-			goto out;
-	}
-
-	blk_execute_rq(req->q, NULL, req, at_head);
-	if (result)
-		*result = nvme_req(req)->result;
-	if (nvme_req(req)->flags & NVME_REQ_CANCELLED)
-		ret = -EINTR;
-	else
-		ret = nvme_req(req)->status;
- out:
-	blk_mq_free_request(req);
-	return ret;
-}
-EXPORT_SYMBOL_GPL(__nvme_submit_sync_cmd);
-
-int nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
-		void *buffer, unsigned bufflen)
-{
-	return __nvme_submit_sync_cmd(q, cmd, NULL, buffer, bufflen, 0,
-			NVME_QID_ANY, 0, 0);
-}
-EXPORT_SYMBOL_GPL(nvme_submit_sync_cmd);
-
-static void *nvme_add_user_metadata(struct bio *bio, void __user *ubuf,
-		unsigned len, u32 seed, bool write)
-{
-	struct bio_integrity_payload *bip;
-	int ret = -ENOMEM;
-	void *buf;
-
-	buf = kmalloc(len, GFP_KERNEL);
-	if (!buf)
-		goto out;
-
-	ret = -EFAULT;
-	if (write && copy_from_user(buf, ubuf, len))
-		goto out_free_meta;
-
-	bip = bio_integrity_alloc(bio, GFP_KERNEL, 1);
-	if (IS_ERR(bip)) {
-		ret = PTR_ERR(bip);
-		goto out_free_meta;
-	}
-
-	bip->bip_iter.bi_size = len;
-	bip->bip_iter.bi_sector = seed;
-	ret = bio_integrity_add_page(bio, virt_to_page(buf), len,
-			offset_in_page(buf));
-	if (ret == len)
-		return buf;
-	ret = -ENOMEM;
-out_free_meta:
-	kfree(buf);
-out:
-	return ERR_PTR(ret);
-}
-
-static int nvme_submit_user_cmd(struct request_queue *q,
-		struct nvme_command *cmd, void __user *ubuffer,
-		unsigned bufflen, void __user *meta_buffer, unsigned meta_len,
-		u32 meta_seed, u32 *result, unsigned timeout)
-{
-	bool write = nvme_is_write(cmd);
-	struct nvme_ns *ns = q->queuedata;
-	struct gendisk *disk = ns ? ns->disk : NULL;
-	struct request *req;
-	struct bio *bio = NULL;
-	void *meta = NULL;
-	int ret;
-
-	req = nvme_alloc_request(q, cmd, 0, NVME_QID_ANY);
-	if (IS_ERR(req))
-		return PTR_ERR(req);
-
-	req->timeout = timeout ? timeout : ADMIN_TIMEOUT;
-
-	if (ubuffer && bufflen) {
-		ret = blk_rq_map_user(q, req, NULL, ubuffer, bufflen,
-				GFP_KERNEL);
-		if (ret)
-			goto out;
-		bio = req->bio;
-		bio->bi_disk = disk;
-		if (disk && meta_buffer && meta_len) {
-			meta = nvme_add_user_metadata(bio, meta_buffer, meta_len,
-					meta_seed, write);
-			if (IS_ERR(meta)) {
-				ret = PTR_ERR(meta);
-				goto out_unmap;
-			}
-			req->cmd_flags |= REQ_INTEGRITY;
-		}
-	}
-
-	blk_execute_rq(req->q, disk, req, 0);
-	if (nvme_req(req)->flags & NVME_REQ_CANCELLED)
-		ret = -EINTR;
-	else
-		ret = nvme_req(req)->status;
-	if (result)
-		*result = le32_to_cpu(nvme_req(req)->result.u32);
-	if (meta && !ret && !write) {
-		if (copy_to_user(meta_buffer, meta, meta_len))
-			ret = -EFAULT;
-	}
-	kfree(meta);
- out_unmap:
-	if (bio)
-		blk_rq_unmap_user(bio);
- out:
-	blk_mq_free_request(req);
-	return ret;
-}
-
-static void nvme_keep_alive_end_io(struct request *rq, blk_status_t status)
-{
-	struct nvme_ctrl *ctrl = rq->end_io_data;
-
-	blk_mq_free_request(rq);
-
-	if (status) {
-		dev_err(ctrl->device,
-			"failed nvme_keep_alive_end_io error=%d\n",
-				status);
-		return;
-	}
-
-	schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
-}
-
-static int nvme_keep_alive(struct nvme_ctrl *ctrl)
-{
-	struct nvme_command c;
-	struct request *rq;
-
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = nvme_admin_keep_alive;
-
-	rq = nvme_alloc_request(ctrl->admin_q, &c, BLK_MQ_REQ_RESERVED,
-			NVME_QID_ANY);
-	if (IS_ERR(rq))
-		return PTR_ERR(rq);
-
-	rq->timeout = ctrl->kato * HZ;
-	rq->end_io_data = ctrl;
-
-	blk_execute_rq_nowait(rq->q, NULL, rq, 0, nvme_keep_alive_end_io);
-
-	return 0;
-}
-
-/*
- * There was two types of operation need to map single continueous physical continues physical address.
- * 1. kv_exist and kv_iterate's buffer.
- * 2. kv_store, kv_retrieve, and kv_delete's key buffer.
- * Note.
- * - check it's address 4byte word algined. If not malloc and copy data.
- */
-#define KV_QUEUE_DMA_ALIGNMENT (0x03)
-
-static bool check_add_for_single_cont_phyaddress(void __user *address, unsigned length, struct request_queue *q)
-{
-	unsigned offset = 0;
-	unsigned count = 0;
-
-	offset = offset_in_page(address);
-	count = DIV_ROUND_UP(offset + length, PAGE_SIZE);
-	if ((count > 1) || ((unsigned long)address & KV_QUEUE_DMA_ALIGNMENT)) {
-		/* address does not aligned as 4 bytes or addr needs more than one page */
-		return false;
-	}
-	return true;
-}
-
-static int user_addr_npages(int offset, int size)
-{
-	unsigned count = DIV_ROUND_UP(offset + size, PAGE_SIZE);
-	return count;
-}
-
-static struct aio_user_ctx *get_aio_user_ctx(void __user *addr, unsigned len, bool b_kernel)
-{
-	int offset = offset_in_page(addr);
-	int datalen = len;
-	int num_page = user_addr_npages(offset, len);
-	int size = 0;
-	struct aio_user_ctx *user_ctx = NULL;
-	int mapped_pages = 0;
-	int i = 0;
-	size = sizeof(struct aio_user_ctx) + sizeof(__le64*) * num_page
-		+ sizeof(struct scatterlist) * num_page - 1;
-	/* need to keep user address to map to copy when complete request */
-	user_ctx = (struct aio_user_ctx*)kmalloc(size, GFP_KERNEL);
-	if (!user_ctx)
-		return NULL;
-
-	user_ctx->nents = 0;
-	user_ctx->pages = (struct page**)user_ctx->data;
-	user_ctx->sg = (struct scatterlist*)(user_ctx->data + sizeof(__le64*) * num_page);
-	if (b_kernel) {
-		struct page* page = NULL;
-		char *src_data = addr;
-		for ( i = 0; i < num_page; ++i) {
-			page = virt_to_page(src_data);
-			get_page(page);
-			user_ctx->pages[i] = page;
-			src_data += PAGE_SIZE;
-		}
-	} else {
-		mapped_pages = get_user_pages_fast((unsigned long)addr, num_page, 0,
-				user_ctx->pages);
-		if (mapped_pages != num_page) {
-			user_ctx->nents = mapped_pages;
-			goto exit;
-		}
-	}
-	user_ctx->nents = num_page;
-	user_ctx->len = datalen;
-	sg_init_table(user_ctx->sg, num_page);
-	for (i = 0; i < num_page; ++i) {
-		sg_set_page(&user_ctx->sg[i], user_ctx->pages[i],
-				min_t(unsigned, datalen, PAGE_SIZE - offset), offset);
-		datalen -= (PAGE_SIZE - offset);
-		offset = 0;
-	}
-	sg_mark_end(&user_ctx->sg[i - 1]);
-	return user_ctx;
-exit:
-	if (user_ctx) {
-		for (i = 0; i < user_ctx->nents; ++i)
-			put_page(user_ctx->pages[i]);
-		kfree(user_ctx);
-	}
-	return NULL;
-}
-
-int __nvme_submit_kv_user_cmd(struct request_queue *q, struct nvme_command *cmd,
-		struct nvme_passthru_kv_cmd *pthr_cmd,
-		void __user *ubuffer, unsigned bufflen,
-		void __user *meta_buffer, unsigned meta_len, u32 meta_seed,
-		u32 *result, u32 *status, unsigned timeout, bool aio)
-{
-	struct nvme_ns *ns = q->queuedata;
-	struct gendisk *disk = ns ? ns->disk : NULL;
-	struct request *req;
-	int ret = 0;
-	struct nvme_kaiocb *aiocb = NULL;
-	struct aio_user_ctx *user_ctx = NULL;
-	struct aio_user_ctx *kernel_ctx = NULL;
-	struct scatterlist *meta_sg_ptr;
-	struct scatterlist meta_sg;
-	struct page *p_page = NULL;
-	struct nvme_io_param *param = NULL;
-	char *kv_data = NULL;
-	char *kv_meta = NULL;
-	bool need_to_copy = false;
-	int i = 0, offset = 0;
-	unsigned len = 0;
-	unsigned long start_time = jiffies;
-
-	if (!disk)
-		return -EFAULT;
-
-	if (aio) {
-		aiocb = get_aiocb(pthr_cmd->reqid);
-		if (!aiocb) {
-			ret = -ENOMEM;
-			goto out_end;
-		}
-		aiocb->cmd = *cmd;
-		aiocb->disk = disk;
-		cmd = &aiocb->cmd;
-	}
-
-	req = nvme_alloc_request(q, cmd, 0, NVME_QID_ANY);
-	if (IS_ERR(req)) {
-		if (aiocb) mempool_free(aiocb, kaiocb_mempool);
-		return PTR_ERR(req);
-	}
-
-	req->timeout = timeout ? timeout : ADMIN_TIMEOUT;
-	param = nvme_io_param(req);
-
-	param->kv_data_sg_ptr = NULL;
-	param->kv_meta_sg_ptr = NULL;
-	param->kv_data_nents = 0;
-	param->kv_data_len = 0;
-
-	if (ubuffer && bufflen) {
-		if ((unsigned long)ubuffer & KV_QUEUE_DMA_ALIGNMENT) {
-			need_to_copy = true;
-			len = DIV_ROUND_UP(bufflen, PAGE_SIZE) * PAGE_SIZE;
-			kv_data = kmalloc(len, GFP_KERNEL);
-			if (kv_data == NULL) {
-				ret = -ENOMEM;
-				goto out_req_end;
-			}
-		}
-		user_ctx = get_aio_user_ctx(ubuffer, bufflen, false);
-		if (need_to_copy) {
-			kernel_ctx = get_aio_user_ctx(kv_data, bufflen, true);
-			if (kernel_ctx) {
-				if (is_kv_store_cmd(cmd->common.opcode) || is_kv_append_cmd(cmd->common.opcode))
-					sg_copy_to_buffer(user_ctx->sg, user_ctx->nents, kv_data, user_ctx->len);
-			}
-		} else {
-			kernel_ctx = user_ctx;
-		}
-		if (user_ctx == NULL || kernel_ctx == NULL) {
-			ret = -ENOMEM;
-			goto out_unmap;
-		}
-		param->kv_data_sg_ptr = kernel_ctx->sg;
-		param->kv_data_nents = kernel_ctx->nents;
-		param->kv_data_len = kernel_ctx->len;
-		if (aio) {
-			aiocb->need_to_copy = need_to_copy;
-			aiocb->kv_data = kv_data;
-			aiocb->kernel_ctx = kernel_ctx;
-			aiocb->user_ctx = user_ctx;
-			aiocb->start_time = start_time;
-		}
-		if (is_kv_store_cmd(cmd->common.opcode) || is_kv_append_cmd(cmd->common.opcode)) {
-			generic_start_io_acct(q, WRITE, (bufflen >> 9 ? bufflen >> 9 : 1), &disk->part0);
-		} else {
-			generic_start_io_acct(q, READ, (bufflen >> 9 ? bufflen >> 9 : 1), &disk->part0);
-		}
-	}
-
-	if (meta_buffer || meta_len) {
-		if (check_add_for_single_cont_phyaddress(meta_buffer, meta_len, q)) {
-			ret = get_user_pages_fast((unsigned long)meta_buffer, 1, 0, &p_page);
-			if (ret != 1) {
-				ret = -ENOMEM;
-				goto out_unmap;
-			}
-			offset = offset_in_page(meta_buffer);
-		} else {
-			len = DIV_ROUND_UP(meta_len, 256) * 256;
-			kv_meta = kmalloc(len, GFP_KERNEL);
-			if (!kv_meta) {
-				ret = -ENOMEM;
-				goto out_unmap;
-			}
-			if (copy_from_user(kv_meta, meta_buffer, meta_len)) {
-				ret = -ENOMEM;
-				goto out_free_meta;
-			}
-			offset = offset_in_page(kv_meta);
-			p_page = virt_to_page(kv_meta);
-			get_page(p_page);
-		}
-		if (aio) {
-			aiocb->use_meta = true;
-			aiocb->meta = kv_meta;
-			meta_sg_ptr = &aiocb->meta_sg;
-		} else {
-			meta_sg_ptr = &meta_sg;     /* sync io */
-		}
-
-		sg_init_table(meta_sg_ptr, 1);
-		sg_set_page(meta_sg_ptr, p_page, meta_len, offset);
-		sg_mark_end(meta_sg_ptr);
-		param->kv_meta_sg_ptr = meta_sg_ptr;
-	} else {
-		param->kv_meta_sg_ptr = NULL;
-	}
-
-	if (aio) {
-		aiocb->event.ctxid = pthr_cmd->ctxid;
-		aiocb->event.reqid = pthr_cmd->reqid;
-		aiocb->opcode = cmd->common.opcode;
-		req->end_io_data = aiocb;
-		blk_execute_rq_nowait(req->q, disk, req, 0, kv_async_completion);
-		return 0;
-	} else {
-		blk_execute_rq(req->q, disk, req, 0);
-		if (nvme_req(req)->flags & NVME_REQ_CANCELLED)
-			ret = -EINTR;
-		else
-			ret = nvme_req(req)->status;
-
-		if (result)
-			*result = le32_to_cpu(nvme_req(req)->result.u32);
-		if (status)
-			*status = le16_to_cpu(nvme_req(req)->status);
-	}
-
-	if (need_to_copy) {
-		if ((is_kv_retrieve_cmd(cmd->common.opcode) && !ret) ||
-					(is_kv_iter_read_cmd(cmd->common.opcode) && (!ret || ((le16_to_cpu(nvme_req(req)->status) & 0xff) == 0x93)))) {
-			sg_copy_from_buffer(user_ctx->sg, user_ctx->nents, kv_data, user_ctx->len);
-		}
-	}
-out_free_meta:
-	if (p_page) put_page(p_page);
-	if (kv_meta) kfree(kv_meta);
-out_unmap:
-	if (user_ctx) {
-		for (i = 0; i < user_ctx->nents; ++i)
-			put_page(sg_page(&user_ctx->sg[i]));
-		kfree(user_ctx);
-	}
-	if (need_to_copy) {
-		if (kernel_ctx) {
-			for (i = 0; i < kernel_ctx->nents; ++i)
-				put_page(sg_page(&kernel_ctx->sg[i]));
-			kfree(kernel_ctx);
-		}
-		if (kv_data) kfree(kv_data);
-	}
-out_req_end:
-	if (aio && aiocb) mempool_free(aiocb, kaiocb_mempool);
-	blk_mq_free_request(req);
-
-	if (is_kv_store_cmd(cmd->common.opcode) || is_kv_append_cmd(cmd->common.opcode))
-		generic_end_io_acct(q, WRITE, &disk->part0, start_time);
-	else
-		generic_end_io_acct(q, READ, &disk->part0, start_time);
-out_end:
-	return ret;
-}
-
-static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
-		struct nvme_ns *ns,
-		struct nvme_passthru_kv_cmd __user *ucmd, bool aio)
-{
-	struct nvme_passthru_kv_cmd cmd;
-	struct nvme_command c;
-	unsigned timeout = 0;
-	int status;
-	void __user *metadata = NULL;
-	unsigned meta_len = 0;
-	unsigned option = 0;
-	unsigned iter_handle = 0;
-	if (!capable(CAP_SYS_ADMIN))
-		return -EACCES;
-	if (copy_from_user(&cmd, ucmd, sizeof(cmd)))
-		return -EFAULT;
-	if (cmd.flags)
-		return -EINVAL;
-
-	/* filter out non kv command */
-	if (!is_kv_cmd(cmd.opcode))
-		return -EINVAL;
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = cmd.opcode;
-	c.common.flags = cmd.flags;
-#ifdef KSID_SUPPORT
-	c.common.nsid = cmd.cdw3;
-#else
-	c.common.nsid = cpu_to_le32(cmd.nsid);
-#endif
-	if (cmd.timeout_ms)
-		timeout = msecs_to_jiffies(cmd.timeout_ms);
-
-	switch (cmd.opcode) {
-		case nvme_cmd_kv_store:
-		case nvme_cmd_kv_append:
-			option = cpu_to_le32(cmd.cdw4);
-			c.kv_store.offset = cpu_to_le32(cmd.cdw5);
-			/* validate key length */
-			if (cmd.key_len > KVCMD_MAX_KEY_SIZE ||
-					cmd.key_len < KVCMD_MIN_KEY_SIZE) {
-				cmd.result = KVS_ERR_VALUE;
-				status = -EINVAL;
-				goto exit;
-			}
-			c.kv_store.key_len = cpu_to_le32(cmd.key_len - 1);
-			c.kv_store.option = (option & 0xff);
-			/* set value size */
-			if (cmd.data_len % 4) {
-				c.kv_store.value_len = cpu_to_le32((cmd.data_len >> 2) + 1);
-				c.kv_store.invalid_byte = 4 - (cmd.data_len % 4);
-			} else {
-				c.kv_store.value_len = cpu_to_le32(cmd.data_len >> 2);
-			}
-
-			if (cmd.key_len > KVCMD_INLINE_KEY_MAX) {
-				metadata = (void __user*)cmd.key_addr;
-				meta_len = cmd.key_len;
-			} else {
-				memcpy(c.kv_store.key, cmd.key, cmd.key_len);
-			}
-			break;
-		case nvme_cmd_kv_retrieve:
-			option = cpu_to_le32(cmd.cdw4);
-			c.kv_retrieve.offset = cpu_to_le32(cmd.cdw5);
-			/* validate key length */
-			if (cmd.key_len > KVCMD_MAX_KEY_SIZE ||
-					cmd.key_len < KVCMD_MIN_KEY_SIZE) {
-				cmd.result = KVS_ERR_VALUE;
-				status = -EINVAL;
-				goto exit;
-			}
-			c.kv_retrieve.key_len = cpu_to_le32(cmd.key_len - 1);
-			c.kv_retrieve.option = option & 0xff;
-			c.kv_retrieve.value_len = cpu_to_le32(cmd.data_len >> 2);
-
-			if (cmd.key_len > KVCMD_INLINE_KEY_MAX) {
-				metadata = (void __user*)cmd.key_addr;
-				meta_len = cmd.key_len;
-			} else {
-				memcpy(c.kv_retrieve.key, cmd.key, cmd.key_len);
-			}
-			break;
-		case nvme_cmd_kv_delete:
-			option = cpu_to_le32(cmd.cdw4);
-			/* validate key length */
-			if (cmd.key_len > KVCMD_MAX_KEY_SIZE ||
-					cmd.key_len < KVCMD_MIN_KEY_SIZE) {
-				cmd.result = KVS_ERR_VALUE;
-				status = -EINVAL;
-				goto exit;
-			}
-			c.kv_delete.key_len = cpu_to_le32(cmd.key_len - 1);
-			c.kv_delete.option = option & 0xff;
-			if (cmd.key_len > KVCMD_INLINE_KEY_MAX) {
-				metadata = (void __user*)cmd.key_addr;
-				meta_len = cmd.key_len;
-			} else {
-				memcpy(c.kv_delete.key, cmd.key, cmd.key_len);
-			}
-			break;
-		case nvme_cmd_kv_exist:
-			option = cpu_to_le32(cmd.cdw4);
-			/* validate key length */
-			if (cmd.key_len > KVCMD_MAX_KEY_SIZE ||
-					cmd.key_len < KVCMD_MIN_KEY_SIZE) {
-				cmd.result = KVS_ERR_VALUE;
-				status = -EINVAL;
-				goto exit;
-			}
-			c.kv_exist.key_len = cpu_to_le32(cmd.key_len - 1);
-			c.kv_exist.option = option & 0xff;
-			if (cmd.key_len > KVCMD_INLINE_KEY_MAX) {
-				metadata = (void __user*)cmd.key_addr;
-				meta_len = cmd.key_len;
-			} else {
-				memcpy(c.kv_exist.key, cmd.key, cmd.key_len);
-			}
-			break;
-		case nvme_cmd_kv_iter_req:
-			option = cpu_to_le32(cmd.cdw4);
-			iter_handle = cpu_to_le32(cmd.cdw5);
-			c.kv_iter_req.iter_handle = iter_handle & 0xff;
-			c.kv_iter_req.option = option & 0xff;
-			c.kv_iter_req.iter_val = cpu_to_le32(cmd.cdw12);
-			c.kv_iter_req.iter_bitmask = cpu_to_le32(cmd.cdw13);
-			break;
-		case nvme_cmd_kv_iter_read:
-			option = cpu_to_le32(cmd.cdw4);
-			iter_handle = cpu_to_le32(cmd.cdw5);
-			c.kv_iter_read.iter_handle = iter_handle & 0xff;
-			c.kv_iter_read.option = option & 0xff;
-			c.kv_iter_read.value_len = cpu_to_le32(cmd.data_len >> 2);
-			break;
-		default:
-			cmd.result = KVS_ERR_IO;
-			status = -EINVAL;
-			goto exit;
-	}
-
-	status = __nvme_submit_kv_user_cmd(ns ? ns->queue : ctrl->admin_q, &c, &cmd,
-			(void __user*)(uintptr_t)cmd.data_addr, cmd.data_len, metadata, meta_len, 0,
-			&cmd.result, &cmd.status, timeout, aio);
-
-exit:
-	if (put_user(cmd.result, &ucmd->result))
-		return -EFAULT;
-	if (put_user(cmd.status, &ucmd->status))
-		return -EFAULT;
-	return status;
-}
-
-static void nvme_keep_alive_work(struct work_struct *work)
-{
-	struct nvme_ctrl *ctrl = container_of(to_delayed_work(work),
-			struct nvme_ctrl, ka_work);
-
-	if (nvme_keep_alive(ctrl)) {
-		/* allocation failure, reset the controller */
-		dev_err(ctrl->device, "keep-alive failed\n");
-		nvme_reset_ctrl(ctrl);
-		return;
-	}
-}
-
-void nvme_start_keep_alive(struct nvme_ctrl *ctrl)
-{
-	if (unlikely(ctrl->kato == 0))
-		return;
-
-	INIT_DELAYED_WORK(&ctrl->ka_work, nvme_keep_alive_work);
-	schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
-}
-EXPORT_SYMBOL_GPL(nvme_start_keep_alive);
-
-void nvme_stop_keep_alive(struct nvme_ctrl *ctrl)
-{
-	if (unlikely(ctrl->kato == 0))
-		return;
-
-	cancel_delayed_work_sync(&ctrl->ka_work);
-}
-EXPORT_SYMBOL_GPL(nvme_stop_keep_alive);
-
-static int nvme_identify_ctrl(struct nvme_ctrl *dev, struct nvme_id_ctrl **id)
-{
-	struct nvme_command c = { };
-	int error;
-
-	/* gcc-4.4.4 (at least) has issues with initializers and anon unions */
-	c.identify.opcode = nvme_admin_identify;
-	c.identify.cns = NVME_ID_CNS_CTRL;
-
-	*id = kmalloc(sizeof(struct nvme_id_ctrl), GFP_KERNEL);
-	if (!*id)
-		return -ENOMEM;
-
-	error = nvme_submit_sync_cmd(dev->admin_q, &c, *id,
-			sizeof(struct nvme_id_ctrl));
-	if (error)
-		kfree(*id);
-	return error;
-}
-
-static int nvme_identify_ns_descs(struct nvme_ctrl *ctrl, unsigned nsid,
-		u8 *eui64, u8 *nguid, uuid_t *uuid)
-{
-	struct nvme_command c = { };
-	int status;
-	void *data;
-	int pos;
-	int len;
-
-	c.identify.opcode = nvme_admin_identify;
-	c.identify.nsid = cpu_to_le32(nsid);
-	c.identify.cns = NVME_ID_CNS_NS_DESC_LIST;
-
-	data = kzalloc(NVME_IDENTIFY_DATA_SIZE, GFP_KERNEL);
-	if (!data)
-		return -ENOMEM;
-
-	status = nvme_submit_sync_cmd(ctrl->admin_q, &c, data,
-				      NVME_IDENTIFY_DATA_SIZE);
-	if (status)
-		goto free_data;
-
-	for (pos = 0; pos < NVME_IDENTIFY_DATA_SIZE; pos += len) {
-		struct nvme_ns_id_desc *cur = data + pos;
-
-		if (cur->nidl == 0)
-			break;
-
-		switch (cur->nidt) {
-		case NVME_NIDT_EUI64:
-			if (cur->nidl != NVME_NIDT_EUI64_LEN) {
-				dev_warn(ctrl->device,
-					 "ctrl returned bogus length: %d for NVME_NIDT_EUI64\n",
-					 cur->nidl);
-				goto free_data;
-			}
-			len = NVME_NIDT_EUI64_LEN;
-			memcpy(eui64, data + pos + sizeof(*cur), len);
-			break;
-		case NVME_NIDT_NGUID:
-			if (cur->nidl != NVME_NIDT_NGUID_LEN) {
-				dev_warn(ctrl->device,
-					 "ctrl returned bogus length: %d for NVME_NIDT_NGUID\n",
-					 cur->nidl);
-				goto free_data;
-			}
-			len = NVME_NIDT_NGUID_LEN;
-			memcpy(nguid, data + pos + sizeof(*cur), len);
-			break;
-		case NVME_NIDT_UUID:
-			if (cur->nidl != NVME_NIDT_UUID_LEN) {
-				dev_warn(ctrl->device,
-					 "ctrl returned bogus length: %d for NVME_NIDT_UUID\n",
-					 cur->nidl);
-				goto free_data;
-			}
-			len = NVME_NIDT_UUID_LEN;
-			uuid_copy(uuid, data + pos + sizeof(*cur));
-			break;
-		default:
-			/* Skip unnkown types */
-			len = cur->nidl;
-			break;
-		}
-
-		len += sizeof(*cur);
-	}
-free_data:
-	kfree(data);
-	return status;
-}
-
-static int nvme_identify_ns_list(struct nvme_ctrl *dev, unsigned nsid, __le32 *ns_list)
-{
-	struct nvme_command c = { };
-
-	c.identify.opcode = nvme_admin_identify;
-	c.identify.cns = NVME_ID_CNS_NS_ACTIVE_LIST;
-	c.identify.nsid = cpu_to_le32(nsid);
-	return nvme_submit_sync_cmd(dev->admin_q, &c, ns_list, 0x1000);
-}
-
-static struct nvme_id_ns *nvme_identify_ns(struct nvme_ctrl *ctrl,
-		unsigned nsid)
-{
-	struct nvme_id_ns *id;
-	struct nvme_command c = { };
-	int error;
-
-	/* gcc-4.4.4 (at least) has issues with initializers and anon unions */
-	c.identify.opcode = nvme_admin_identify;
-	c.identify.nsid = cpu_to_le32(nsid);
-	c.identify.cns = NVME_ID_CNS_NS;
-
-	id = kmalloc(sizeof(*id), GFP_KERNEL);
-	if (!id)
-		return NULL;
-
-	error = nvme_submit_sync_cmd(ctrl->admin_q, &c, id, sizeof(*id));
-	if (error) {
-		dev_warn(ctrl->device, "Identify namespace failed\n");
-		kfree(id);
-		return NULL;
-	}
-
-	return id;
-}
-
-static int nvme_set_features(struct nvme_ctrl *dev, unsigned fid, unsigned dword11,
-		      void *buffer, size_t buflen, u32 *result)
-{
-	struct nvme_command c;
-	union nvme_result res;
-	int ret;
-
-	memset(&c, 0, sizeof(c));
-	c.features.opcode = nvme_admin_set_features;
-	c.features.fid = cpu_to_le32(fid);
-	c.features.dword11 = cpu_to_le32(dword11);
-
-	ret = __nvme_submit_sync_cmd(dev->admin_q, &c, &res,
-			buffer, buflen, 0, NVME_QID_ANY, 0, 0);
-	if (ret >= 0 && result)
-		*result = le32_to_cpu(res.u32);
-	return ret;
-}
-
-int nvme_set_queue_count(struct nvme_ctrl *ctrl, int *count)
-{
-	u32 q_count = (*count - 1) | ((*count - 1) << 16);
-	u32 result;
-	int status, nr_io_queues;
-
-	status = nvme_set_features(ctrl, NVME_FEAT_NUM_QUEUES, q_count, NULL, 0,
-			&result);
-	if (status < 0)
-		return status;
-
-	/*
-	 * Degraded controllers might return an error when setting the queue
-	 * count.  We still want to be able to bring them online and offer
-	 * access to the admin queue, as that might be only way to fix them up.
-	 */
-	if (status > 0) {
-		dev_err(ctrl->device, "Could not set queue count (%d)\n", status);
-		*count = 0;
-	} else {
-		nr_io_queues = min(result & 0xffff, result >> 16) + 1;
-		*count = min(*count, nr_io_queues);
-	}
-
-	return 0;
-}
-EXPORT_SYMBOL_GPL(nvme_set_queue_count);
-
-static int nvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
-{
-	struct nvme_user_io io;
-	struct nvme_command c;
-	unsigned length, meta_len;
-	void __user *metadata;
-
-	if (copy_from_user(&io, uio, sizeof(io)))
-		return -EFAULT;
-	if (io.flags)
-		return -EINVAL;
-
-	switch (io.opcode) {
-	case nvme_cmd_write:
-	case nvme_cmd_read:
-	case nvme_cmd_compare:
-		break;
-	default:
-		return -EINVAL;
-	}
-
-	length = (io.nblocks + 1) << ns->lba_shift;
-	meta_len = (io.nblocks + 1) * ns->ms;
-	metadata = (void __user *)(uintptr_t)io.metadata;
-
-	if (ns->ext) {
-		length += meta_len;
-		meta_len = 0;
-	} else if (meta_len) {
-		if ((io.metadata & 3) || !io.metadata)
-			return -EINVAL;
-	}
-
-	memset(&c, 0, sizeof(c));
-	c.rw.opcode = io.opcode;
-	c.rw.flags = io.flags;
-	c.rw.nsid = cpu_to_le32(ns->ns_id);
-	c.rw.slba = cpu_to_le64(io.slba);
-	c.rw.length = cpu_to_le16(io.nblocks);
-	c.rw.control = cpu_to_le16(io.control);
-	c.rw.dsmgmt = cpu_to_le32(io.dsmgmt);
-	c.rw.reftag = cpu_to_le32(io.reftag);
-	c.rw.apptag = cpu_to_le16(io.apptag);
-	c.rw.appmask = cpu_to_le16(io.appmask);
-
-	return nvme_submit_user_cmd(ns->queue, &c,
-			(void __user *)(uintptr_t)io.addr, length,
-			metadata, meta_len, io.slba, NULL, 0);
-}
-
-static int nvme_user_cmd(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
-			struct nvme_passthru_cmd __user *ucmd)
-{
-	struct nvme_passthru_cmd cmd;
-	struct nvme_command c;
-	unsigned timeout = 0;
-	int status;
-
-	if (!capable(CAP_SYS_ADMIN))
-		return -EACCES;
-	if (copy_from_user(&cmd, ucmd, sizeof(cmd)))
-		return -EFAULT;
-	if (cmd.flags)
-		return -EINVAL;
-
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = cmd.opcode;
-	c.common.flags = cmd.flags;
-	c.common.nsid = cpu_to_le32(cmd.nsid);
-	c.common.cdw2[0] = cpu_to_le32(cmd.cdw2);
-	c.common.cdw2[1] = cpu_to_le32(cmd.cdw3);
-	c.common.cdw10[0] = cpu_to_le32(cmd.cdw10);
-	c.common.cdw10[1] = cpu_to_le32(cmd.cdw11);
-	c.common.cdw10[2] = cpu_to_le32(cmd.cdw12);
-	c.common.cdw10[3] = cpu_to_le32(cmd.cdw13);
-	c.common.cdw10[4] = cpu_to_le32(cmd.cdw14);
-	c.common.cdw10[5] = cpu_to_le32(cmd.cdw15);
-
-	if (cmd.timeout_ms)
-		timeout = msecs_to_jiffies(cmd.timeout_ms);
-
-	status = nvme_submit_user_cmd(ns ? ns->queue : ctrl->admin_q, &c,
-			(void __user *)(uintptr_t)cmd.addr, cmd.data_len,
-			(void __user *)(uintptr_t)cmd.metadata, cmd.metadata_len,
-			0, &cmd.result, timeout);
-	if (status >= 0) {
-		if (put_user(cmd.result, &ucmd->result))
-			return -EFAULT;
-	}
-
-	return status;
-}
-
-static int nvme_ioctl(struct block_device *bdev, fmode_t mode,
-		unsigned int cmd, unsigned long arg)
-{
-	struct nvme_ns *ns = bdev->bd_disk->private_data;
-
-	switch (cmd) {
-	case NVME_IOCTL_ID:
-		force_successful_syscall_return();
-		return ns->ns_id;
-	case NVME_IOCTL_ADMIN_CMD:
-		return nvme_user_cmd(ns->ctrl, NULL, (void __user *)arg);
-	case NVME_IOCTL_IO_CMD:
-		return nvme_user_cmd(ns->ctrl, ns, (void __user *)arg);
-	case NVME_IOCTL_SUBMIT_IO:
-		return nvme_submit_io(ns, (void __user *)arg);
-	case NVME_IOCTL_IO_KV_CMD:
-		return nvme_user_kv_cmd(ns->ctrl, ns, (void __user *)arg, false);
-	case NVME_IOCTL_AIO_CMD:
-		return nvme_user_kv_cmd(ns->ctrl, ns, (void __user*)arg, true);
-	case NVME_IOCTL_SET_AIOCTX:
-		return nvme_set_aioctx((void __user*)arg);
-	case NVME_IOCTL_DEL_AIOCTX:
-		return nvme_del_aioctx((void __user*)arg);
-	case NVME_IOCTL_GET_AIOEVENT:
-		return nvme_get_ioevents((void __user*)arg);
-	default:
-#ifdef CONFIG_NVM
-		if (ns->ndev)
-			return nvme_nvm_ioctl(ns, cmd, arg);
-#endif
-		if (is_sed_ioctl(cmd))
-			return sed_ioctl(ns->ctrl->opal_dev, cmd,
-					 (void __user *) arg);
-		return -ENOTTY;
-	}
-}
-
-#ifdef CONFIG_COMPAT
-static int nvme_compat_ioctl(struct block_device *bdev, fmode_t mode,
-			unsigned int cmd, unsigned long arg)
-{
-	return nvme_ioctl(bdev, mode, cmd, arg);
-}
-#else
-#define nvme_compat_ioctl	NULL
-#endif
-
-static int nvme_open(struct block_device *bdev, fmode_t mode)
-{
-	return nvme_get_ns_from_disk(bdev->bd_disk) ? 0 : -ENXIO;
-}
-
-static void nvme_release(struct gendisk *disk, fmode_t mode)
-{
-	struct nvme_ns *ns = disk->private_data;
-
-	module_put(ns->ctrl->ops->module);
-	nvme_put_ns(ns);
-}
-
-static int nvme_getgeo(struct block_device *bdev, struct hd_geometry *geo)
-{
-	/* some standard values */
-	geo->heads = 1 << 6;
-	geo->sectors = 1 << 5;
-	geo->cylinders = get_capacity(bdev->bd_disk) >> 11;
-	return 0;
-}
-
-#ifdef CONFIG_BLK_DEV_INTEGRITY
-static void nvme_prep_integrity(struct gendisk *disk, struct nvme_id_ns *id,
-		u16 bs)
-{
-	struct nvme_ns *ns = disk->private_data;
-	u16 old_ms = ns->ms;
-	u8 pi_type = 0;
-
-	ns->ms = le16_to_cpu(id->lbaf[id->flbas & NVME_NS_FLBAS_LBA_MASK].ms);
-	ns->ext = ns->ms && (id->flbas & NVME_NS_FLBAS_META_EXT);
-
-	/* PI implementation requires metadata equal t10 pi tuple size */
-	if (ns->ms == sizeof(struct t10_pi_tuple))
-		pi_type = id->dps & NVME_NS_DPS_PI_MASK;
-
-	if (blk_get_integrity(disk) &&
-	    (ns->pi_type != pi_type || ns->ms != old_ms ||
-	     bs != queue_logical_block_size(disk->queue) ||
-	     (ns->ms && ns->ext)))
-		blk_integrity_unregister(disk);
-
-	ns->pi_type = pi_type;
-}
-
-static void nvme_init_integrity(struct nvme_ns *ns)
-{
-	struct blk_integrity integrity;
-
-	memset(&integrity, 0, sizeof(integrity));
-	switch (ns->pi_type) {
-	case NVME_NS_DPS_PI_TYPE3:
-		integrity.profile = &t10_pi_type3_crc;
-		integrity.tag_size = sizeof(u16) + sizeof(u32);
-		integrity.flags |= BLK_INTEGRITY_DEVICE_CAPABLE;
-		break;
-	case NVME_NS_DPS_PI_TYPE1:
-	case NVME_NS_DPS_PI_TYPE2:
-		integrity.profile = &t10_pi_type1_crc;
-		integrity.tag_size = sizeof(u16);
-		integrity.flags |= BLK_INTEGRITY_DEVICE_CAPABLE;
-		break;
-	default:
-		integrity.profile = NULL;
-		break;
-	}
-	integrity.tuple_size = ns->ms;
-	blk_integrity_register(ns->disk, &integrity);
-	blk_queue_max_integrity_segments(ns->queue, 1);
-}
-#else
-static void nvme_prep_integrity(struct gendisk *disk, struct nvme_id_ns *id,
-		u16 bs)
-{
-}
-static void nvme_init_integrity(struct nvme_ns *ns)
-{
-}
-#endif /* CONFIG_BLK_DEV_INTEGRITY */
-
-static void nvme_set_chunk_size(struct nvme_ns *ns)
-{
-	u32 chunk_size = (((u32)ns->noiob) << (ns->lba_shift - 9));
-	blk_queue_chunk_sectors(ns->queue, rounddown_pow_of_two(chunk_size));
-}
-
-static void nvme_config_discard(struct nvme_ns *ns)
-{
-	struct nvme_ctrl *ctrl = ns->ctrl;
-	u32 logical_block_size = queue_logical_block_size(ns->queue);
-
-	BUILD_BUG_ON(PAGE_SIZE / sizeof(struct nvme_dsm_range) <
-			NVME_DSM_MAX_RANGES);
-
-	if (ctrl->nr_streams && ns->sws && ns->sgs) {
-		unsigned int sz = logical_block_size * ns->sws * ns->sgs;
-
-		ns->queue->limits.discard_alignment = sz;
-		ns->queue->limits.discard_granularity = sz;
-	} else {
-		ns->queue->limits.discard_alignment = logical_block_size;
-		ns->queue->limits.discard_granularity = logical_block_size;
-	}
-	blk_queue_max_discard_sectors(ns->queue, UINT_MAX);
-	blk_queue_max_discard_segments(ns->queue, NVME_DSM_MAX_RANGES);
-	queue_flag_set_unlocked(QUEUE_FLAG_DISCARD, ns->queue);
-
-	if (ctrl->quirks & NVME_QUIRK_DEALLOCATE_ZEROES)
-		blk_queue_max_write_zeroes_sectors(ns->queue, UINT_MAX);
-}
-
-static void nvme_report_ns_ids(struct nvme_ctrl *ctrl, unsigned int nsid,
-		struct nvme_id_ns *id, u8 *eui64, u8 *nguid, uuid_t *uuid)
-{
-	if (ctrl->vs >= NVME_VS(1, 1, 0))
-		memcpy(eui64, id->eui64, sizeof(id->eui64));
-	if (ctrl->vs >= NVME_VS(1, 2, 0))
-		memcpy(nguid, id->nguid, sizeof(id->nguid));
-	if (ctrl->vs >= NVME_VS(1, 3, 0)) {
-		 /* Don't treat error as fatal we potentially
-		  * already have a NGUID or EUI-64
-		  */
-		if (nvme_identify_ns_descs(ctrl, nsid, eui64, nguid, uuid))
-			dev_warn(ctrl->device,
-				 "%s: Identify Descriptors failed\n", __func__);
-	}
-}
-
-static void __nvme_revalidate_disk(struct gendisk *disk, struct nvme_id_ns *id)
-{
-	struct nvme_ns *ns = disk->private_data;
-	struct nvme_ctrl *ctrl = ns->ctrl;
-	u16 bs;
-
-	/*
-	 * If identify namespace failed, use default 512 byte block size so
-	 * block layer can use before failing read/write for 0 capacity.
-	 */
-	ns->lba_shift = id->lbaf[id->flbas & NVME_NS_FLBAS_LBA_MASK].ds;
-	if (ns->lba_shift == 0)
-		ns->lba_shift = 9;
-	bs = 1 << ns->lba_shift;
-	ns->noiob = le16_to_cpu(id->noiob);
-
-	blk_mq_freeze_queue(disk->queue);
-
-	if (ctrl->ops->flags & NVME_F_METADATA_SUPPORTED)
-		nvme_prep_integrity(disk, id, bs);
-	blk_queue_logical_block_size(ns->queue, bs);
-	if (ns->noiob)
-		nvme_set_chunk_size(ns);
-	if (ns->ms && !blk_get_integrity(disk) && !ns->ext)
-		nvme_init_integrity(ns);
-	if (ns->ms && !(ns->ms == 8 && ns->pi_type) && !blk_get_integrity(disk))
-		set_capacity(disk, 0);
-	else
-		set_capacity(disk, le64_to_cpup(&id->nsze) << (ns->lba_shift - 9));
-
-	if (ctrl->oncs & NVME_CTRL_ONCS_DSM)
-		nvme_config_discard(ns);
-	blk_mq_unfreeze_queue(disk->queue);
-}
-
-static int nvme_revalidate_disk(struct gendisk *disk)
-{
-	struct nvme_ns *ns = disk->private_data;
-	struct nvme_ctrl *ctrl = ns->ctrl;
-	struct nvme_id_ns *id;
-	u8 eui64[8] = { 0 }, nguid[16] = { 0 };
-	uuid_t uuid = uuid_null;
-	int ret = 0;
-
-	if (test_bit(NVME_NS_DEAD, &ns->flags)) {
-		set_capacity(disk, 0);
-		return -ENODEV;
-	}
-
-	id = nvme_identify_ns(ctrl, ns->ns_id);
-	if (!id)
-		return -ENODEV;
-
-	if (id->ncap == 0) {
-		ret = -ENODEV;
-		goto out;
-	}
-
-	__nvme_revalidate_disk(disk, id);
-	nvme_report_ns_ids(ctrl, ns->ns_id, id, eui64, nguid, &uuid);
-	if (!uuid_equal(&ns->uuid, &uuid) ||
-	    memcmp(&ns->nguid, &nguid, sizeof(ns->nguid)) ||
-	    memcmp(&ns->eui, &eui64, sizeof(ns->eui))) {
-		dev_err(ctrl->device,
-			"identifiers changed for nsid %d\n", ns->ns_id);
-		ret = -ENODEV;
-	}
-
-out:
-	kfree(id);
-	return ret;
-}
-
-static char nvme_pr_type(enum pr_type type)
-{
-	switch (type) {
-	case PR_WRITE_EXCLUSIVE:
-		return 1;
-	case PR_EXCLUSIVE_ACCESS:
-		return 2;
-	case PR_WRITE_EXCLUSIVE_REG_ONLY:
-		return 3;
-	case PR_EXCLUSIVE_ACCESS_REG_ONLY:
-		return 4;
-	case PR_WRITE_EXCLUSIVE_ALL_REGS:
-		return 5;
-	case PR_EXCLUSIVE_ACCESS_ALL_REGS:
-		return 6;
-	default:
-		return 0;
-	}
-};
-
-static int nvme_pr_command(struct block_device *bdev, u32 cdw10,
-				u64 key, u64 sa_key, u8 op)
-{
-	struct nvme_ns *ns = bdev->bd_disk->private_data;
-	struct nvme_command c;
-	u8 data[16] = { 0, };
-
-	put_unaligned_le64(key, &data[0]);
-	put_unaligned_le64(sa_key, &data[8]);
-
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = op;
-	c.common.nsid = cpu_to_le32(ns->ns_id);
-	c.common.cdw10[0] = cpu_to_le32(cdw10);
-
-	return nvme_submit_sync_cmd(ns->queue, &c, data, 16);
-}
-
-static int nvme_pr_register(struct block_device *bdev, u64 old,
-		u64 new, unsigned flags)
-{
-	u32 cdw10;
-
-	if (flags & ~PR_FL_IGNORE_KEY)
-		return -EOPNOTSUPP;
-
-	cdw10 = old ? 2 : 0;
-	cdw10 |= (flags & PR_FL_IGNORE_KEY) ? 1 << 3 : 0;
-	cdw10 |= (1 << 30) | (1 << 31); /* PTPL=1 */
-	return nvme_pr_command(bdev, cdw10, old, new, nvme_cmd_resv_register);
-}
-
-static int nvme_pr_reserve(struct block_device *bdev, u64 key,
-		enum pr_type type, unsigned flags)
-{
-	u32 cdw10;
-
-	if (flags & ~PR_FL_IGNORE_KEY)
-		return -EOPNOTSUPP;
-
-	cdw10 = nvme_pr_type(type) << 8;
-	cdw10 |= ((flags & PR_FL_IGNORE_KEY) ? 1 << 3 : 0);
-	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_acquire);
-}
-
-static int nvme_pr_preempt(struct block_device *bdev, u64 old, u64 new,
-		enum pr_type type, bool abort)
-{
-	u32 cdw10 = nvme_pr_type(type) << 8 | abort ? 2 : 1;
-	return nvme_pr_command(bdev, cdw10, old, new, nvme_cmd_resv_acquire);
-}
-
-static int nvme_pr_clear(struct block_device *bdev, u64 key)
-{
-	u32 cdw10 = 1 | (key ? 1 << 3 : 0);
-	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_register);
-}
-
-static int nvme_pr_release(struct block_device *bdev, u64 key, enum pr_type type)
-{
-	u32 cdw10 = nvme_pr_type(type) << 8 | key ? 1 << 3 : 0;
-	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_release);
-}
-
-static const struct pr_ops nvme_pr_ops = {
-	.pr_register	= nvme_pr_register,
-	.pr_reserve	= nvme_pr_reserve,
-	.pr_release	= nvme_pr_release,
-	.pr_preempt	= nvme_pr_preempt,
-	.pr_clear	= nvme_pr_clear,
-};
-
-#ifdef CONFIG_BLK_SED_OPAL
-int nvme_sec_submit(void *data, u16 spsp, u8 secp, void *buffer, size_t len,
-		bool send)
-{
-	struct nvme_ctrl *ctrl = data;
-	struct nvme_command cmd;
-
-	memset(&cmd, 0, sizeof(cmd));
-	if (send)
-		cmd.common.opcode = nvme_admin_security_send;
-	else
-		cmd.common.opcode = nvme_admin_security_recv;
-	cmd.common.nsid = 0;
-	cmd.common.cdw10[0] = cpu_to_le32(((u32)secp) << 24 | ((u32)spsp) << 8);
-	cmd.common.cdw10[1] = cpu_to_le32(len);
-
-	return __nvme_submit_sync_cmd(ctrl->admin_q, &cmd, NULL, buffer, len,
-				      ADMIN_TIMEOUT, NVME_QID_ANY, 1, 0);
-}
-EXPORT_SYMBOL_GPL(nvme_sec_submit);
-#endif /* CONFIG_BLK_SED_OPAL */
-
-static const struct block_device_operations nvme_fops = {
-	.owner		= THIS_MODULE,
-	.ioctl		= nvme_ioctl,
-	.compat_ioctl	= nvme_compat_ioctl,
-	.open		= nvme_open,
-	.release	= nvme_release,
-	.getgeo		= nvme_getgeo,
-	.revalidate_disk= nvme_revalidate_disk,
-	.pr_ops		= &nvme_pr_ops,
-};
-
-static int nvme_wait_ready(struct nvme_ctrl *ctrl, u64 cap, bool enabled)
-{
-	unsigned long timeout =
-		((NVME_CAP_TIMEOUT(cap) + 1) * HZ / 2) + jiffies;
-	u32 csts, bit = enabled ? NVME_CSTS_RDY : 0;
-	int ret;
-
-	while ((ret = ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts)) == 0) {
-		if (csts == ~0)
-			return -ENODEV;
-		if ((csts & NVME_CSTS_RDY) == bit)
-			break;
-
-		msleep(100);
-		if (fatal_signal_pending(current))
-			return -EINTR;
-		if (time_after(jiffies, timeout)) {
-			dev_err(ctrl->device,
-				"Device not ready; aborting %s\n", enabled ?
-						"initialisation" : "reset");
-			return -ENODEV;
-		}
-	}
-
-	return ret;
-}
-
-/*
- * If the device has been passed off to us in an enabled state, just clear
- * the enabled bit.  The spec says we should set the 'shutdown notification
- * bits', but doing so may cause the device to complete commands to the
- * admin queue ... and we don't know what memory that might be pointing at!
- */
-int nvme_disable_ctrl(struct nvme_ctrl *ctrl, u64 cap)
-{
-	int ret;
-
-	ctrl->ctrl_config &= ~NVME_CC_SHN_MASK;
-	ctrl->ctrl_config &= ~NVME_CC_ENABLE;
-
-	ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
-	if (ret)
-		return ret;
-
-	if (ctrl->quirks & NVME_QUIRK_DELAY_BEFORE_CHK_RDY)
-		msleep(NVME_QUIRK_DELAY_AMOUNT);
-
-	return nvme_wait_ready(ctrl, cap, false);
-}
-EXPORT_SYMBOL_GPL(nvme_disable_ctrl);
-
-int nvme_enable_ctrl(struct nvme_ctrl *ctrl, u64 cap)
-{
-	/*
-	 * Default to a 4K page size, with the intention to update this
-	 * path in the future to accomodate architectures with differing
-	 * kernel and IO page sizes.
-	 */
-	unsigned dev_page_min = NVME_CAP_MPSMIN(cap) + 12, page_shift = 12;
-	int ret;
-
-	if (page_shift < dev_page_min) {
-		dev_err(ctrl->device,
-			"Minimum device page size %u too large for host (%u)\n",
-			1 << dev_page_min, 1 << page_shift);
-		return -ENODEV;
-	}
-
-	ctrl->page_size = 1 << page_shift;
-
-	ctrl->ctrl_config = NVME_CC_CSS_NVM;
-	ctrl->ctrl_config |= (page_shift - 12) << NVME_CC_MPS_SHIFT;
-	ctrl->ctrl_config |= NVME_CC_AMS_RR | NVME_CC_SHN_NONE;
-	ctrl->ctrl_config |= NVME_CC_IOSQES | NVME_CC_IOCQES;
-	ctrl->ctrl_config |= NVME_CC_ENABLE;
-
-	ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
-	if (ret)
-		return ret;
-	return nvme_wait_ready(ctrl, cap, true);
-}
-EXPORT_SYMBOL_GPL(nvme_enable_ctrl);
-
-int nvme_shutdown_ctrl(struct nvme_ctrl *ctrl)
-{
-	unsigned long timeout = jiffies + (ctrl->shutdown_timeout * HZ);
-	u32 csts;
-	int ret;
-
-	ctrl->ctrl_config &= ~NVME_CC_SHN_MASK;
-	ctrl->ctrl_config |= NVME_CC_SHN_NORMAL;
-
-	ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
-	if (ret)
-		return ret;
-
-	while ((ret = ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts)) == 0) {
-		if ((csts & NVME_CSTS_SHST_MASK) == NVME_CSTS_SHST_CMPLT)
-			break;
-
-		msleep(100);
-		if (fatal_signal_pending(current))
-			return -EINTR;
-		if (time_after(jiffies, timeout)) {
-			dev_err(ctrl->device,
-				"Device shutdown incomplete; abort shutdown\n");
-			return -ENODEV;
-		}
-	}
-
-	return ret;
-}
-EXPORT_SYMBOL_GPL(nvme_shutdown_ctrl);
-
-static void nvme_set_queue_limits(struct nvme_ctrl *ctrl,
-		struct request_queue *q)
-{
-	bool vwc = false;
-
-	if (ctrl->max_hw_sectors) {
-		u32 max_segments =
-			(ctrl->max_hw_sectors / (ctrl->page_size >> 9)) + 1;
-
-		blk_queue_max_hw_sectors(q, ctrl->max_hw_sectors);
-		blk_queue_max_segments(q, min_t(u32, max_segments, USHRT_MAX));
-	}
-	if ((ctrl->quirks & NVME_QUIRK_STRIPE_SIZE) &&
-	    is_power_of_2(ctrl->max_hw_sectors))
-		blk_queue_chunk_sectors(q, ctrl->max_hw_sectors);
-	blk_queue_virt_boundary(q, ctrl->page_size - 1);
-	if (ctrl->vwc & NVME_CTRL_VWC_PRESENT)
-		vwc = true;
-	blk_queue_write_cache(q, vwc, vwc);
-}
-
-static int nvme_configure_timestamp(struct nvme_ctrl *ctrl)
-{
-	__le64 ts;
-	int ret;
-
-	if (!(ctrl->oncs & NVME_CTRL_ONCS_TIMESTAMP))
-		return 0;
-
-	ts = cpu_to_le64(ktime_to_ms(ktime_get_real()));
-	ret = nvme_set_features(ctrl, NVME_FEAT_TIMESTAMP, 0, &ts, sizeof(ts),
-			NULL);
-	if (ret)
-		dev_warn_once(ctrl->device,
-			"could not set timestamp (%d)\n", ret);
-	return ret;
-}
-
-static int nvme_configure_apst(struct nvme_ctrl *ctrl)
-{
-	/*
-	 * APST (Autonomous Power State Transition) lets us program a
-	 * table of power state transitions that the controller will
-	 * perform automatically.  We configure it with a simple
-	 * heuristic: we are willing to spend at most 2% of the time
-	 * transitioning between power states.  Therefore, when running
-	 * in any given state, we will enter the next lower-power
-	 * non-operational state after waiting 50 * (enlat + exlat)
-	 * microseconds, as long as that state's exit latency is under
-	 * the requested maximum latency.
-	 *
-	 * We will not autonomously enter any non-operational state for
-	 * which the total latency exceeds ps_max_latency_us.  Users
-	 * can set ps_max_latency_us to zero to turn off APST.
-	 */
-
-	unsigned apste;
-	struct nvme_feat_auto_pst *table;
-	u64 max_lat_us = 0;
-	int max_ps = -1;
-	int ret;
-
-	/*
-	 * If APST isn't supported or if we haven't been initialized yet,
-	 * then don't do anything.
-	 */
-	if (!ctrl->apsta)
-		return 0;
-
-	if (ctrl->npss > 31) {
-		dev_warn(ctrl->device, "NPSS is invalid; not using APST\n");
-		return 0;
-	}
-
-	table = kzalloc(sizeof(*table), GFP_KERNEL);
-	if (!table)
-		return 0;
-
-	if (!ctrl->apst_enabled || ctrl->ps_max_latency_us == 0) {
-		/* Turn off APST. */
-		apste = 0;
-		dev_dbg(ctrl->device, "APST disabled\n");
-	} else {
-		__le64 target = cpu_to_le64(0);
-		int state;
-
-		/*
-		 * Walk through all states from lowest- to highest-power.
-		 * According to the spec, lower-numbered states use more
-		 * power.  NPSS, despite the name, is the index of the
-		 * lowest-power state, not the number of states.
-		 */
-		for (state = (int)ctrl->npss; state >= 0; state--) {
-			u64 total_latency_us, exit_latency_us, transition_ms;
-
-			if (target)
-				table->entries[state] = target;
-
-			/*
-			 * Don't allow transitions to the deepest state
-			 * if it's quirked off.
-			 */
-			if (state == ctrl->npss &&
-			    (ctrl->quirks & NVME_QUIRK_NO_DEEPEST_PS))
-				continue;
-
-			/*
-			 * Is this state a useful non-operational state for
-			 * higher-power states to autonomously transition to?
-			 */
-			if (!(ctrl->psd[state].flags &
-			      NVME_PS_FLAGS_NON_OP_STATE))
-				continue;
-
-			exit_latency_us =
-				(u64)le32_to_cpu(ctrl->psd[state].exit_lat);
-			if (exit_latency_us > ctrl->ps_max_latency_us)
-				continue;
-
-			total_latency_us =
-				exit_latency_us +
-				le32_to_cpu(ctrl->psd[state].entry_lat);
-
-			/*
-			 * This state is good.  Use it as the APST idle
-			 * target for higher power states.
-			 */
-			transition_ms = total_latency_us + 19;
-			do_div(transition_ms, 20);
-			if (transition_ms > (1 << 24) - 1)
-				transition_ms = (1 << 24) - 1;
-
-			target = cpu_to_le64((state << 3) |
-					     (transition_ms << 8));
-
-			if (max_ps == -1)
-				max_ps = state;
-
-			if (total_latency_us > max_lat_us)
-				max_lat_us = total_latency_us;
-		}
-
-		apste = 1;
-
-		if (max_ps == -1) {
-			dev_dbg(ctrl->device, "APST enabled but no non-operational states are available\n");
-		} else {
-			dev_dbg(ctrl->device, "APST enabled: max PS = %d, max round-trip latency = %lluus, table = %*phN\n",
-				max_ps, max_lat_us, (int)sizeof(*table), table);
-		}
-	}
-
-	ret = nvme_set_features(ctrl, NVME_FEAT_AUTO_PST, apste,
-				table, sizeof(*table), NULL);
-	if (ret)
-		dev_err(ctrl->device, "failed to set APST feature (%d)\n", ret);
-
-	kfree(table);
-	return ret;
-}
-
-static void nvme_set_latency_tolerance(struct device *dev, s32 val)
-{
-	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
-	u64 latency;
-
-	switch (val) {
-	case PM_QOS_LATENCY_TOLERANCE_NO_CONSTRAINT:
-	case PM_QOS_LATENCY_ANY:
-		latency = U64_MAX;
-		break;
-
-	default:
-		latency = val;
-	}
-
-	if (ctrl->ps_max_latency_us != latency) {
-		ctrl->ps_max_latency_us = latency;
-		nvme_configure_apst(ctrl);
-	}
-}
-
-struct nvme_core_quirk_entry {
-	/*
-	 * NVMe model and firmware strings are padded with spaces.  For
-	 * simplicity, strings in the quirk table are padded with NULLs
-	 * instead.
-	 */
-	u16 vid;
-	const char *mn;
-	const char *fr;
-	unsigned long quirks;
-};
-
-static const struct nvme_core_quirk_entry core_quirks[] = {
-	{
-		/*
-		 * This Toshiba device seems to die using any APST states.  See:
-		 * https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1678184/comments/11
-		 */
-		.vid = 0x1179,
-		.mn = "THNSF5256GPUK TOSHIBA",
-		.quirks = NVME_QUIRK_NO_APST,
-	}
-};
-
-/* match is null-terminated but idstr is space-padded. */
-static bool string_matches(const char *idstr, const char *match, size_t len)
-{
-	size_t matchlen;
-
-	if (!match)
-		return true;
-
-	matchlen = strlen(match);
-	WARN_ON_ONCE(matchlen > len);
-
-	if (memcmp(idstr, match, matchlen))
-		return false;
-
-	for (; matchlen < len; matchlen++)
-		if (idstr[matchlen] != ' ')
-			return false;
-
-	return true;
-}
-
-static bool quirk_matches(const struct nvme_id_ctrl *id,
-			  const struct nvme_core_quirk_entry *q)
-{
-	return q->vid == le16_to_cpu(id->vid) &&
-		string_matches(id->mn, q->mn, sizeof(id->mn)) &&
-		string_matches(id->fr, q->fr, sizeof(id->fr));
-}
-
-static void nvme_init_subnqn(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
-{
-	size_t nqnlen;
-	int off;
-
-	nqnlen = strnlen(id->subnqn, NVMF_NQN_SIZE);
-	if (nqnlen > 0 && nqnlen < NVMF_NQN_SIZE) {
-		strcpy(ctrl->subnqn, id->subnqn);
-		return;
-	}
-
-	if (ctrl->vs >= NVME_VS(1, 2, 1))
-		dev_warn(ctrl->device, "missing or invalid SUBNQN field.\n");
-
-	/* Generate a "fake" NQN per Figure 254 in NVMe 1.3 + ECN 001 */
-	off = snprintf(ctrl->subnqn, NVMF_NQN_SIZE,
-			"nqn.2014.08.org.nvmexpress:%4x%4x",
-			le16_to_cpu(id->vid), le16_to_cpu(id->ssvid));
-	memcpy(ctrl->subnqn + off, id->sn, sizeof(id->sn));
-	off += sizeof(id->sn);
-	memcpy(ctrl->subnqn + off, id->mn, sizeof(id->mn));
-	off += sizeof(id->mn);
-	memset(ctrl->subnqn + off, 0, sizeof(ctrl->subnqn) - off);
-}
-
-/*
- * Initialize the cached copies of the Identify data and various controller
- * register in our nvme_ctrl structure.  This should be called as soon as
- * the admin queue is fully up and running.
- */
-int nvme_init_identify(struct nvme_ctrl *ctrl)
-{
-	struct nvme_id_ctrl *id;
-	u64 cap;
-	int ret, page_shift;
-	u32 max_hw_sectors;
-	bool prev_apst_enabled;
-
-	ret = ctrl->ops->reg_read32(ctrl, NVME_REG_VS, &ctrl->vs);
-	if (ret) {
-		dev_err(ctrl->device, "Reading VS failed (%d)\n", ret);
-		return ret;
-	}
-
-	ret = ctrl->ops->reg_read64(ctrl, NVME_REG_CAP, &cap);
-	if (ret) {
-		dev_err(ctrl->device, "Reading CAP failed (%d)\n", ret);
-		return ret;
-	}
-	page_shift = NVME_CAP_MPSMIN(cap) + 12;
-
-	if (ctrl->vs >= NVME_VS(1, 1, 0))
-		ctrl->subsystem = NVME_CAP_NSSRC(cap);
-
-	ret = nvme_identify_ctrl(ctrl, &id);
-	if (ret) {
-		dev_err(ctrl->device, "Identify Controller failed (%d)\n", ret);
-		return -EIO;
-	}
-
-	nvme_init_subnqn(ctrl, id);
-
-	if (!ctrl->identified) {
-		/*
-		 * Check for quirks.  Quirk can depend on firmware version,
-		 * so, in principle, the set of quirks present can change
-		 * across a reset.  As a possible future enhancement, we
-		 * could re-scan for quirks every time we reinitialize
-		 * the device, but we'd have to make sure that the driver
-		 * behaves intelligently if the quirks change.
-		 */
-
-		int i;
-
-		for (i = 0; i < ARRAY_SIZE(core_quirks); i++) {
-			if (quirk_matches(id, &core_quirks[i]))
-				ctrl->quirks |= core_quirks[i].quirks;
-		}
-	}
-
-	if (force_apst && (ctrl->quirks & NVME_QUIRK_NO_DEEPEST_PS)) {
-		dev_warn(ctrl->device, "forcibly allowing all power states due to nvme_core.force_apst -- use at your own risk\n");
-		ctrl->quirks &= ~NVME_QUIRK_NO_DEEPEST_PS;
-	}
-
-	ctrl->oacs = le16_to_cpu(id->oacs);
-	ctrl->vid = le16_to_cpu(id->vid);
-	ctrl->oncs = le16_to_cpup(&id->oncs);
-	atomic_set(&ctrl->abort_limit, id->acl + 1);
-	ctrl->vwc = id->vwc;
-	ctrl->cntlid = le16_to_cpup(&id->cntlid);
-	memcpy(ctrl->serial, id->sn, sizeof(id->sn));
-	memcpy(ctrl->model, id->mn, sizeof(id->mn));
-	memcpy(ctrl->firmware_rev, id->fr, sizeof(id->fr));
-	if (id->mdts)
-		max_hw_sectors = 1 << (id->mdts + page_shift - 9);
-	else
-		max_hw_sectors = UINT_MAX;
-	ctrl->max_hw_sectors =
-		min_not_zero(ctrl->max_hw_sectors, max_hw_sectors);
-
-	nvme_set_queue_limits(ctrl, ctrl->admin_q);
-	ctrl->sgls = le32_to_cpu(id->sgls);
-	ctrl->kas = le16_to_cpu(id->kas);
-
-	if (id->rtd3e) {
-		/* us -> s */
-		u32 transition_time = le32_to_cpu(id->rtd3e) / 1000000;
-
-		ctrl->shutdown_timeout = clamp_t(unsigned int, transition_time,
-						 shutdown_timeout, 60);
-
-		if (ctrl->shutdown_timeout != shutdown_timeout)
-			dev_warn(ctrl->device,
-				 "Shutdown timeout set to %u seconds\n",
-				 ctrl->shutdown_timeout);
-	} else
-		ctrl->shutdown_timeout = shutdown_timeout;
-
-	ctrl->npss = id->npss;
-	ctrl->apsta = id->apsta;
-	prev_apst_enabled = ctrl->apst_enabled;
-	if (ctrl->quirks & NVME_QUIRK_NO_APST) {
-		if (force_apst && id->apsta) {
-			dev_warn(ctrl->device, "forcibly allowing APST due to nvme_core.force_apst -- use at your own risk\n");
-			ctrl->apst_enabled = true;
-		} else {
-			ctrl->apst_enabled = false;
-		}
-	} else {
-		ctrl->apst_enabled = id->apsta;
-	}
-	memcpy(ctrl->psd, id->psd, sizeof(ctrl->psd));
-
-	if (ctrl->ops->flags & NVME_F_FABRICS) {
-		ctrl->icdoff = le16_to_cpu(id->icdoff);
-		ctrl->ioccsz = le32_to_cpu(id->ioccsz);
-		ctrl->iorcsz = le32_to_cpu(id->iorcsz);
-		ctrl->maxcmd = le16_to_cpu(id->maxcmd);
-
-		/*
-		 * In fabrics we need to verify the cntlid matches the
-		 * admin connect
-		 */
-		if (ctrl->cntlid != le16_to_cpu(id->cntlid)) {
-			ret = -EINVAL;
-			goto out_free;
-		}
-
-		if (!ctrl->opts->discovery_nqn && !ctrl->kas) {
-			dev_err(ctrl->device,
-				"keep-alive support is mandatory for fabrics\n");
-			ret = -EINVAL;
-			goto out_free;
-		}
-	} else {
-		ctrl->cntlid = le16_to_cpu(id->cntlid);
-		ctrl->hmpre = le32_to_cpu(id->hmpre);
-		ctrl->hmmin = le32_to_cpu(id->hmmin);
-		ctrl->hmminds = le32_to_cpu(id->hmminds);
-		ctrl->hmmaxd = le16_to_cpu(id->hmmaxd);
-	}
-
-	kfree(id);
-
-	if (ctrl->apst_enabled && !prev_apst_enabled)
-		dev_pm_qos_expose_latency_tolerance(ctrl->device);
-	else if (!ctrl->apst_enabled && prev_apst_enabled)
-		dev_pm_qos_hide_latency_tolerance(ctrl->device);
-
-	ret = nvme_configure_apst(ctrl);
-	if (ret < 0)
-		return ret;
-	
-	ret = nvme_configure_timestamp(ctrl);
-	if (ret < 0)
-		return ret;
-
-	ret = nvme_configure_directives(ctrl);
-	if (ret < 0)
-		return ret;
-
-	ctrl->identified = true;
-
-	return 0;
-
-out_free:
-	kfree(id);
-	return ret;
-}
-EXPORT_SYMBOL_GPL(nvme_init_identify);
-
-static int nvme_dev_open(struct inode *inode, struct file *file)
-{
-	struct nvme_ctrl *ctrl;
-	int instance = iminor(inode);
-	int ret = -ENODEV;
-
-	spin_lock(&dev_list_lock);
-	list_for_each_entry(ctrl, &nvme_ctrl_list, node) {
-		if (ctrl->instance != instance)
-			continue;
-
-		if (!ctrl->admin_q) {
-			ret = -EWOULDBLOCK;
-			break;
-		}
-		if (!kref_get_unless_zero(&ctrl->kref))
-			break;
-		file->private_data = ctrl;
-		ret = 0;
-		break;
-	}
-	spin_unlock(&dev_list_lock);
-
-	return ret;
-}
-
-static int nvme_dev_release(struct inode *inode, struct file *file)
-{
-	nvme_put_ctrl(file->private_data);
-	return 0;
-}
-
-static int nvme_dev_user_cmd(struct nvme_ctrl *ctrl, void __user *argp)
-{
-	struct nvme_ns *ns;
-	int ret;
-
-	mutex_lock(&ctrl->namespaces_mutex);
-	if (list_empty(&ctrl->namespaces)) {
-		ret = -ENOTTY;
-		goto out_unlock;
-	}
-
-	ns = list_first_entry(&ctrl->namespaces, struct nvme_ns, list);
-	if (ns != list_last_entry(&ctrl->namespaces, struct nvme_ns, list)) {
-		dev_warn(ctrl->device,
-			"NVME_IOCTL_IO_CMD not supported when multiple namespaces present!\n");
-		ret = -EINVAL;
-		goto out_unlock;
-	}
-
-	dev_warn(ctrl->device,
-		"using deprecated NVME_IOCTL_IO_CMD ioctl on the char device!\n");
-	kref_get(&ns->kref);
-	mutex_unlock(&ctrl->namespaces_mutex);
-
-	ret = nvme_user_cmd(ctrl, ns, argp);
-	nvme_put_ns(ns);
-	return ret;
-
-out_unlock:
-	mutex_unlock(&ctrl->namespaces_mutex);
-	return ret;
-}
-
-static long nvme_dev_ioctl(struct file *file, unsigned int cmd,
-		unsigned long arg)
-{
-	struct nvme_ctrl *ctrl = file->private_data;
-	void __user *argp = (void __user *)arg;
-
-	switch (cmd) {
-	case NVME_IOCTL_ADMIN_CMD:
-		return nvme_user_cmd(ctrl, NULL, argp);
-	case NVME_IOCTL_IO_CMD:
-		return nvme_dev_user_cmd(ctrl, argp);
-	case NVME_IOCTL_RESET:
-		dev_warn(ctrl->device, "resetting controller\n");
-		return nvme_reset_ctrl_sync(ctrl);
-	case NVME_IOCTL_SUBSYS_RESET:
-		return nvme_reset_subsystem(ctrl);
-	case NVME_IOCTL_RESCAN:
-		nvme_queue_scan(ctrl);
-		return 0;
-	default:
-		return -ENOTTY;
-	}
-}
-
-static const struct file_operations nvme_dev_fops = {
-	.owner		= THIS_MODULE,
-	.open		= nvme_dev_open,
-	.release	= nvme_dev_release,
-	.unlocked_ioctl	= nvme_dev_ioctl,
-	.compat_ioctl	= nvme_dev_ioctl,
-};
-
-static ssize_t nvme_sysfs_reset(struct device *dev,
-				struct device_attribute *attr, const char *buf,
-				size_t count)
-{
-	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
-	int ret;
-
-	ret = nvme_reset_ctrl_sync(ctrl);
-	if (ret < 0)
-		return ret;
-	return count;
-}
-static DEVICE_ATTR(reset_controller, S_IWUSR, NULL, nvme_sysfs_reset);
-
-static ssize_t nvme_sysfs_rescan(struct device *dev,
-				struct device_attribute *attr, const char *buf,
-				size_t count)
-{
-	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
-
-	nvme_queue_scan(ctrl);
-	return count;
-}
-static DEVICE_ATTR(rescan_controller, S_IWUSR, NULL, nvme_sysfs_rescan);
-
-static ssize_t wwid_show(struct device *dev, struct device_attribute *attr,
-								char *buf)
-{
-	struct nvme_ns *ns = nvme_get_ns_from_dev(dev);
-	struct nvme_ctrl *ctrl = ns->ctrl;
-	int serial_len = sizeof(ctrl->serial);
-	int model_len = sizeof(ctrl->model);
-
-	if (!uuid_is_null(&ns->uuid))
-		return sprintf(buf, "uuid.%pU\n", &ns->uuid);
-
-	if (memchr_inv(ns->nguid, 0, sizeof(ns->nguid)))
-		return sprintf(buf, "eui.%16phN\n", ns->nguid);
-
-	if (memchr_inv(ns->eui, 0, sizeof(ns->eui)))
-		return sprintf(buf, "eui.%8phN\n", ns->eui);
-
-	while (serial_len > 0 && (ctrl->serial[serial_len - 1] == ' ' ||
-				  ctrl->serial[serial_len - 1] == '\0'))
-		serial_len--;
-	while (model_len > 0 && (ctrl->model[model_len - 1] == ' ' ||
-				 ctrl->model[model_len - 1] == '\0'))
-		model_len--;
-
-	return sprintf(buf, "nvme.%04x-%*phN-%*phN-%08x\n", ctrl->vid,
-		serial_len, ctrl->serial, model_len, ctrl->model, ns->ns_id);
-}
-static DEVICE_ATTR(wwid, S_IRUGO, wwid_show, NULL);
-
-static ssize_t nguid_show(struct device *dev, struct device_attribute *attr,
-			  char *buf)
-{
-	struct nvme_ns *ns = nvme_get_ns_from_dev(dev);
-	return sprintf(buf, "%pU\n", ns->nguid);
-}
-static DEVICE_ATTR(nguid, S_IRUGO, nguid_show, NULL);
-
-static ssize_t uuid_show(struct device *dev, struct device_attribute *attr,
-								char *buf)
-{
-	struct nvme_ns *ns = nvme_get_ns_from_dev(dev);
-
-	/* For backward compatibility expose the NGUID to userspace if
-	 * we have no UUID set
-	 */
-	if (uuid_is_null(&ns->uuid)) {
-		printk_ratelimited(KERN_WARNING
-				   "No UUID available providing old NGUID\n");
-		return sprintf(buf, "%pU\n", ns->nguid);
-	}
-	return sprintf(buf, "%pU\n", &ns->uuid);
-}
-static DEVICE_ATTR(uuid, S_IRUGO, uuid_show, NULL);
-
-static ssize_t eui_show(struct device *dev, struct device_attribute *attr,
-								char *buf)
-{
-	struct nvme_ns *ns = nvme_get_ns_from_dev(dev);
-	return sprintf(buf, "%8phd\n", ns->eui);
-}
-static DEVICE_ATTR(eui, S_IRUGO, eui_show, NULL);
-
-static ssize_t nsid_show(struct device *dev, struct device_attribute *attr,
-								char *buf)
-{
-	struct nvme_ns *ns = nvme_get_ns_from_dev(dev);
-	return sprintf(buf, "%d\n", ns->ns_id);
-}
-static DEVICE_ATTR(nsid, S_IRUGO, nsid_show, NULL);
-
-static struct attribute *nvme_ns_attrs[] = {
-	&dev_attr_wwid.attr,
-	&dev_attr_uuid.attr,
-	&dev_attr_nguid.attr,
-	&dev_attr_eui.attr,
-	&dev_attr_nsid.attr,
-	NULL,
-};
-
-static umode_t nvme_ns_attrs_are_visible(struct kobject *kobj,
-		struct attribute *a, int n)
-{
-	struct device *dev = container_of(kobj, struct device, kobj);
-	struct nvme_ns *ns = nvme_get_ns_from_dev(dev);
-
-	if (a == &dev_attr_uuid.attr) {
-		if (uuid_is_null(&ns->uuid) &&
-		    !memchr_inv(ns->nguid, 0, sizeof(ns->nguid)))
-			return 0;
-	}
-	if (a == &dev_attr_nguid.attr) {
-		if (!memchr_inv(ns->nguid, 0, sizeof(ns->nguid)))
-			return 0;
-	}
-	if (a == &dev_attr_eui.attr) {
-		if (!memchr_inv(ns->eui, 0, sizeof(ns->eui)))
-			return 0;
-	}
-	return a->mode;
-}
-
-static const struct attribute_group nvme_ns_attr_group = {
-	.attrs		= nvme_ns_attrs,
-	.is_visible	= nvme_ns_attrs_are_visible,
-};
-
-#define nvme_show_str_function(field)						\
-static ssize_t  field##_show(struct device *dev,				\
-			    struct device_attribute *attr, char *buf)		\
-{										\
-        struct nvme_ctrl *ctrl = dev_get_drvdata(dev);				\
-        return sprintf(buf, "%.*s\n", (int)sizeof(ctrl->field), ctrl->field);	\
-}										\
-static DEVICE_ATTR(field, S_IRUGO, field##_show, NULL);
-
-#define nvme_show_int_function(field)						\
-static ssize_t  field##_show(struct device *dev,				\
-			    struct device_attribute *attr, char *buf)		\
-{										\
-        struct nvme_ctrl *ctrl = dev_get_drvdata(dev);				\
-        return sprintf(buf, "%d\n", ctrl->field);	\
-}										\
-static DEVICE_ATTR(field, S_IRUGO, field##_show, NULL);
-
-nvme_show_str_function(model);
-nvme_show_str_function(serial);
-nvme_show_str_function(firmware_rev);
-nvme_show_int_function(cntlid);
-
-static ssize_t nvme_sysfs_delete(struct device *dev,
-				struct device_attribute *attr, const char *buf,
-				size_t count)
-{
-	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
-
-	if (device_remove_file_self(dev, attr))
-		ctrl->ops->delete_ctrl(ctrl);
-	return count;
-}
-static DEVICE_ATTR(delete_controller, S_IWUSR, NULL, nvme_sysfs_delete);
-
-static ssize_t nvme_sysfs_show_transport(struct device *dev,
-					 struct device_attribute *attr,
-					 char *buf)
-{
-	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
-
-	return snprintf(buf, PAGE_SIZE, "%s\n", ctrl->ops->name);
-}
-static DEVICE_ATTR(transport, S_IRUGO, nvme_sysfs_show_transport, NULL);
-
-static ssize_t nvme_sysfs_show_state(struct device *dev,
-				     struct device_attribute *attr,
-				     char *buf)
-{
-	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
-	static const char *const state_name[] = {
-		[NVME_CTRL_NEW]		= "new",
-		[NVME_CTRL_LIVE]	= "live",
-		[NVME_CTRL_RESETTING]	= "resetting",
-		[NVME_CTRL_RECONNECTING]= "reconnecting",
-		[NVME_CTRL_DELETING]	= "deleting",
-		[NVME_CTRL_DEAD]	= "dead",
-	};
-
-	if ((unsigned)ctrl->state < ARRAY_SIZE(state_name) &&
-	    state_name[ctrl->state])
-		return sprintf(buf, "%s\n", state_name[ctrl->state]);
-
-	return sprintf(buf, "unknown state\n");
-}
-
-static DEVICE_ATTR(state, S_IRUGO, nvme_sysfs_show_state, NULL);
-
-static ssize_t nvme_sysfs_show_subsysnqn(struct device *dev,
-					 struct device_attribute *attr,
-					 char *buf)
-{
-	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
-
-	return snprintf(buf, PAGE_SIZE, "%s\n", ctrl->subnqn);
-}
-static DEVICE_ATTR(subsysnqn, S_IRUGO, nvme_sysfs_show_subsysnqn, NULL);
-
-static ssize_t nvme_sysfs_show_address(struct device *dev,
-					 struct device_attribute *attr,
-					 char *buf)
-{
-	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
-
-	return ctrl->ops->get_address(ctrl, buf, PAGE_SIZE);
-}
-static DEVICE_ATTR(address, S_IRUGO, nvme_sysfs_show_address, NULL);
-
-static struct attribute *nvme_dev_attrs[] = {
-	&dev_attr_reset_controller.attr,
-	&dev_attr_rescan_controller.attr,
-	&dev_attr_model.attr,
-	&dev_attr_serial.attr,
-	&dev_attr_firmware_rev.attr,
-	&dev_attr_cntlid.attr,
-	&dev_attr_delete_controller.attr,
-	&dev_attr_transport.attr,
-	&dev_attr_subsysnqn.attr,
-	&dev_attr_address.attr,
-	&dev_attr_state.attr,
-	NULL
-};
-
-static umode_t nvme_dev_attrs_are_visible(struct kobject *kobj,
-		struct attribute *a, int n)
-{
-	struct device *dev = container_of(kobj, struct device, kobj);
-	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
-
-	if (a == &dev_attr_delete_controller.attr && !ctrl->ops->delete_ctrl)
-		return 0;
-	if (a == &dev_attr_address.attr && !ctrl->ops->get_address)
-		return 0;
-
-	return a->mode;
-}
-
-static struct attribute_group nvme_dev_attrs_group = {
-	.attrs		= nvme_dev_attrs,
-	.is_visible	= nvme_dev_attrs_are_visible,
-};
-
-static const struct attribute_group *nvme_dev_attr_groups[] = {
-	&nvme_dev_attrs_group,
-	NULL,
-};
-
-static int ns_cmp(void *priv, struct list_head *a, struct list_head *b)
-{
-	struct nvme_ns *nsa = container_of(a, struct nvme_ns, list);
-	struct nvme_ns *nsb = container_of(b, struct nvme_ns, list);
-
-	return nsa->ns_id - nsb->ns_id;
-}
-
-static struct nvme_ns *nvme_find_get_ns(struct nvme_ctrl *ctrl, unsigned nsid)
-{
-	struct nvme_ns *ns, *ret = NULL;
-
-	mutex_lock(&ctrl->namespaces_mutex);
-	list_for_each_entry(ns, &ctrl->namespaces, list) {
-		if (ns->ns_id == nsid) {
-			if (!kref_get_unless_zero(&ns->kref))
-				continue;
-			ret = ns;
-			break;
-		}
-		if (ns->ns_id > nsid)
-			break;
-	}
-	mutex_unlock(&ctrl->namespaces_mutex);
-	return ret;
-}
-
-static int nvme_setup_streams_ns(struct nvme_ctrl *ctrl, struct nvme_ns *ns)
-{
-	struct streams_directive_params s;
-	int ret;
-
-	if (!ctrl->nr_streams)
-		return 0;
-
-	ret = nvme_get_stream_params(ctrl, &s, ns->ns_id);
-	if (ret)
-		return ret;
-
-	ns->sws = le32_to_cpu(s.sws);
-	ns->sgs = le16_to_cpu(s.sgs);
-
-	if (ns->sws) {
-		unsigned int bs = 1 << ns->lba_shift;
-
-		blk_queue_io_min(ns->queue, bs * ns->sws);
-		if (ns->sgs)
-			blk_queue_io_opt(ns->queue, bs * ns->sws * ns->sgs);
-	}
-
-	return 0;
-}
-
-static void nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
-{
-	struct nvme_ns *ns;
-	struct gendisk *disk;
-	struct nvme_id_ns *id;
-	char disk_name[DISK_NAME_LEN];
-	int node = dev_to_node(ctrl->dev);
-
-	ns = kzalloc_node(sizeof(*ns), GFP_KERNEL, node);
-	if (!ns)
-		return;
-
-	ns->instance = ida_simple_get(&ctrl->ns_ida, 1, 0, GFP_KERNEL);
-	if (ns->instance < 0)
-		goto out_free_ns;
-
-	ns->queue = blk_mq_init_queue(ctrl->tagset);
-	if (IS_ERR(ns->queue))
-		goto out_release_instance;
-	queue_flag_set_unlocked(QUEUE_FLAG_NONROT, ns->queue);
-	ns->queue->queuedata = ns;
-	ns->ctrl = ctrl;
-
-	kref_init(&ns->kref);
-	ns->ns_id = nsid;
-	ns->lba_shift = 9; /* set to a default value for 512 until disk is validated */
-
-	blk_queue_logical_block_size(ns->queue, 1 << ns->lba_shift);
-	nvme_set_queue_limits(ctrl, ns->queue);
-	nvme_setup_streams_ns(ctrl, ns);
-
-	sprintf(disk_name, "nvme%dn%d", ctrl->instance, ns->instance);
-
-	id = nvme_identify_ns(ctrl, nsid);
-	if (!id)
-		goto out_free_queue;
-
-	if (id->ncap == 0)
-		goto out_free_id;
-
-	nvme_report_ns_ids(ctrl, ns->ns_id, id, ns->eui, ns->nguid, &ns->uuid);
-
-	if ((ctrl->quirks & NVME_QUIRK_LIGHTNVM) && id->vs[0] == 0x1) {
-		if (nvme_nvm_register(ns, disk_name, node)) {
-			dev_warn(ctrl->device, "LightNVM init failure\n");
-			goto out_free_id;
-		}
-	}
-
-	disk = alloc_disk_node(0, node);
-	if (!disk)
-		goto out_free_id;
-
-	disk->fops = &nvme_fops;
-	disk->private_data = ns;
-	disk->queue = ns->queue;
-	disk->flags = GENHD_FL_EXT_DEVT;
-	memcpy(disk->disk_name, disk_name, DISK_NAME_LEN);
-	ns->disk = disk;
-
-	__nvme_revalidate_disk(disk, id);
-
-	mutex_lock(&ctrl->namespaces_mutex);
-	list_add_tail(&ns->list, &ctrl->namespaces);
-	mutex_unlock(&ctrl->namespaces_mutex);
-
-	kref_get(&ctrl->kref);
-
-	kfree(id);
-
-	device_add_disk(ctrl->device, ns->disk);
-	if (sysfs_create_group(&disk_to_dev(ns->disk)->kobj,
-					&nvme_ns_attr_group))
-		pr_warn("%s: failed to create sysfs group for identification\n",
-			ns->disk->disk_name);
-	if (ns->ndev && nvme_nvm_register_sysfs(ns))
-		pr_warn("%s: failed to register lightnvm sysfs group for identification\n",
-			ns->disk->disk_name);
-	return;
- out_free_id:
-	kfree(id);
- out_free_queue:
-	blk_cleanup_queue(ns->queue);
- out_release_instance:
-	ida_simple_remove(&ctrl->ns_ida, ns->instance);
- out_free_ns:
-	kfree(ns);
-}
-
-static void nvme_ns_remove(struct nvme_ns *ns)
-{
-	if (test_and_set_bit(NVME_NS_REMOVING, &ns->flags))
-		return;
-
-	if (ns->disk && ns->disk->flags & GENHD_FL_UP) {
-		if (blk_get_integrity(ns->disk))
-			blk_integrity_unregister(ns->disk);
-		sysfs_remove_group(&disk_to_dev(ns->disk)->kobj,
-					&nvme_ns_attr_group);
-		if (ns->ndev)
-			nvme_nvm_unregister_sysfs(ns);
-		del_gendisk(ns->disk);
-		blk_cleanup_queue(ns->queue);
-	}
-
-	mutex_lock(&ns->ctrl->namespaces_mutex);
-	list_del_init(&ns->list);
-	mutex_unlock(&ns->ctrl->namespaces_mutex);
-
-	nvme_put_ns(ns);
-}
-
-static void nvme_validate_ns(struct nvme_ctrl *ctrl, unsigned nsid)
-{
-	struct nvme_ns *ns;
-
-	ns = nvme_find_get_ns(ctrl, nsid);
-	if (ns) {
-		if (ns->disk && revalidate_disk(ns->disk))
-			nvme_ns_remove(ns);
-		nvme_put_ns(ns);
-	} else
-		nvme_alloc_ns(ctrl, nsid);
-}
-
-static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
-					unsigned nsid)
-{
-	struct nvme_ns *ns, *next;
-
-	list_for_each_entry_safe(ns, next, &ctrl->namespaces, list) {
-		if (ns->ns_id > nsid)
-			nvme_ns_remove(ns);
-	}
-}
-
-static int nvme_scan_ns_list(struct nvme_ctrl *ctrl, unsigned nn)
-{
-	struct nvme_ns *ns;
-	__le32 *ns_list;
-	unsigned i, j, nsid, prev = 0, num_lists = DIV_ROUND_UP(nn, 1024);
-	int ret = 0;
-
-	ns_list = kzalloc(0x1000, GFP_KERNEL);
-	if (!ns_list)
-		return -ENOMEM;
-
-	for (i = 0; i < num_lists; i++) {
-		ret = nvme_identify_ns_list(ctrl, prev, ns_list);
-		if (ret)
-			goto free;
-
-		for (j = 0; j < min(nn, 1024U); j++) {
-			nsid = le32_to_cpu(ns_list[j]);
-			if (!nsid)
-				goto out;
-
-			nvme_validate_ns(ctrl, nsid);
-
-			while (++prev < nsid) {
-				ns = nvme_find_get_ns(ctrl, prev);
-				if (ns) {
-					nvme_ns_remove(ns);
-					nvme_put_ns(ns);
-				}
-			}
-		}
-		nn -= j;
-	}
- out:
-	nvme_remove_invalid_namespaces(ctrl, prev);
- free:
-	kfree(ns_list);
-	return ret;
-}
-
-static void nvme_scan_ns_sequential(struct nvme_ctrl *ctrl, unsigned nn)
-{
-	unsigned i;
-
-	for (i = 1; i <= nn; i++)
-		nvme_validate_ns(ctrl, i);
-
-	nvme_remove_invalid_namespaces(ctrl, nn);
-}
-
-static void nvme_scan_work(struct work_struct *work)
-{
-	struct nvme_ctrl *ctrl =
-		container_of(work, struct nvme_ctrl, scan_work);
-	struct nvme_id_ctrl *id;
-	unsigned nn;
-
-	if (ctrl->state != NVME_CTRL_LIVE)
-		return;
-
-	if (nvme_identify_ctrl(ctrl, &id))
-		return;
-
-	nn = le32_to_cpu(id->nn);
-	if (ctrl->vs >= NVME_VS(1, 1, 0) &&
-	    !(ctrl->quirks & NVME_QUIRK_IDENTIFY_CNS)) {
-		if (!nvme_scan_ns_list(ctrl, nn))
-			goto done;
-	}
-	nvme_scan_ns_sequential(ctrl, nn);
- done:
-	mutex_lock(&ctrl->namespaces_mutex);
-	list_sort(NULL, &ctrl->namespaces, ns_cmp);
-	mutex_unlock(&ctrl->namespaces_mutex);
-	kfree(id);
-}
-
-void nvme_queue_scan(struct nvme_ctrl *ctrl)
-{
-	/*
-	 * Do not queue new scan work when a controller is reset during
-	 * removal.
-	 */
-	if (ctrl->state == NVME_CTRL_LIVE)
-		queue_work(nvme_wq, &ctrl->scan_work);
-}
-EXPORT_SYMBOL_GPL(nvme_queue_scan);
-
-/*
- * This function iterates the namespace list unlocked to allow recovery from
- * controller failure. It is up to the caller to ensure the namespace list is
- * not modified by scan work while this function is executing.
- */
-void nvme_remove_namespaces(struct nvme_ctrl *ctrl)
-{
-	struct nvme_ns *ns, *next;
-
-	/*
-	 * The dead states indicates the controller was not gracefully
-	 * disconnected. In that case, we won't be able to flush any data while
-	 * removing the namespaces' disks; fail all the queues now to avoid
-	 * potentially having to clean up the failed sync later.
-	 */
-	if (ctrl->state == NVME_CTRL_DEAD)
-		nvme_kill_queues(ctrl);
-
-	list_for_each_entry_safe(ns, next, &ctrl->namespaces, list)
-		nvme_ns_remove(ns);
-}
-EXPORT_SYMBOL_GPL(nvme_remove_namespaces);
-
-static void nvme_async_event_work(struct work_struct *work)
-{
-	struct nvme_ctrl *ctrl =
-		container_of(work, struct nvme_ctrl, async_event_work);
-
-	spin_lock_irq(&ctrl->lock);
-	while (ctrl->state == NVME_CTRL_LIVE && ctrl->event_limit > 0) {
-		int aer_idx = --ctrl->event_limit;
-
-		spin_unlock_irq(&ctrl->lock);
-		ctrl->ops->submit_async_event(ctrl, aer_idx);
-		spin_lock_irq(&ctrl->lock);
-	}
-	spin_unlock_irq(&ctrl->lock);
-}
-
-static bool nvme_ctrl_pp_status(struct nvme_ctrl *ctrl)
-{
-
-	u32 csts;
-
-	if (ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts))
-		return false;
-
-	if (csts == ~0)
-		return false;
-
-	return ((ctrl->ctrl_config & NVME_CC_ENABLE) && (csts & NVME_CSTS_PP));
-}
-
-static void nvme_get_fw_slot_info(struct nvme_ctrl *ctrl)
-{
-	struct nvme_command c = { };
-	struct nvme_fw_slot_info_log *log;
-
-	log = kmalloc(sizeof(*log), GFP_KERNEL);
-	if (!log)
-		return;
-
-	c.common.opcode = nvme_admin_get_log_page;
-	c.common.nsid = cpu_to_le32(NVME_NSID_ALL);
-	c.common.cdw10[0] = nvme_get_log_dw10(NVME_LOG_FW_SLOT, sizeof(*log));
-
-	if (!nvme_submit_sync_cmd(ctrl->admin_q, &c, log, sizeof(*log)))
-		dev_warn(ctrl->device,
-				"Get FW SLOT INFO log error\n");
-	kfree(log);
-}
-
-static void nvme_fw_act_work(struct work_struct *work)
-{
-	struct nvme_ctrl *ctrl = container_of(work,
-				struct nvme_ctrl, fw_act_work);
-	unsigned long fw_act_timeout;
-
-	if (ctrl->mtfa)
-		fw_act_timeout = jiffies +
-				msecs_to_jiffies(ctrl->mtfa * 100);
-	else
-		fw_act_timeout = jiffies +
-				msecs_to_jiffies(admin_timeout * 1000);
-
-	nvme_stop_queues(ctrl);
-	while (nvme_ctrl_pp_status(ctrl)) {
-		if (time_after(jiffies, fw_act_timeout)) {
-			dev_warn(ctrl->device,
-				"Fw activation timeout, reset controller\n");
-			nvme_reset_ctrl(ctrl);
-			break;
-		}
-		msleep(100);
-	}
-
-	if (ctrl->state != NVME_CTRL_LIVE)
-		return;
-
-	nvme_start_queues(ctrl);
-	/* read FW slot informationi to clear the AER*/
-	nvme_get_fw_slot_info(ctrl);
-}
-
-void nvme_complete_async_event(struct nvme_ctrl *ctrl, __le16 status,
-		union nvme_result *res)
-{
-	u32 result = le32_to_cpu(res->u32);
-	bool done = true;
-
-	switch (le16_to_cpu(status) >> 1) {
-	case NVME_SC_SUCCESS:
-		done = false;
-		/*FALLTHRU*/
-	case NVME_SC_ABORT_REQ:
-		++ctrl->event_limit;
-		if (ctrl->state == NVME_CTRL_LIVE)
-			queue_work(nvme_wq, &ctrl->async_event_work);
-		break;
-	default:
-		break;
-	}
-
-	if (done)
-		return;
-
-	switch (result & 0xff07) {
-	case NVME_AER_NOTICE_NS_CHANGED:
-		dev_info(ctrl->device, "rescanning\n");
-		nvme_queue_scan(ctrl);
-		break;
-	case NVME_AER_NOTICE_FW_ACT_STARTING:
-		queue_work(nvme_wq, &ctrl->fw_act_work);
-		break;
-	default:
-		dev_warn(ctrl->device, "async event result %08x\n", result);
-	}
-}
-EXPORT_SYMBOL_GPL(nvme_complete_async_event);
-
-void nvme_queue_async_events(struct nvme_ctrl *ctrl)
-{
-	ctrl->event_limit = NVME_NR_AERS;
-	queue_work(nvme_wq, &ctrl->async_event_work);
-}
-EXPORT_SYMBOL_GPL(nvme_queue_async_events);
-
-static DEFINE_IDA(nvme_instance_ida);
-
-static int nvme_set_instance(struct nvme_ctrl *ctrl)
-{
-	int instance, error;
-
-	do {
-		if (!ida_pre_get(&nvme_instance_ida, GFP_KERNEL))
-			return -ENODEV;
-
-		spin_lock(&dev_list_lock);
-		error = ida_get_new(&nvme_instance_ida, &instance);
-		spin_unlock(&dev_list_lock);
-	} while (error == -EAGAIN);
-
-	if (error)
-		return -ENODEV;
-
-	ctrl->instance = instance;
-	return 0;
-}
-
-static void nvme_release_instance(struct nvme_ctrl *ctrl)
-{
-	spin_lock(&dev_list_lock);
-	ida_remove(&nvme_instance_ida, ctrl->instance);
-	spin_unlock(&dev_list_lock);
-}
-
-void nvme_stop_ctrl(struct nvme_ctrl *ctrl)
-{
-	nvme_stop_keep_alive(ctrl);
-	flush_work(&ctrl->async_event_work);
-	flush_work(&ctrl->scan_work);
-	cancel_work_sync(&ctrl->fw_act_work);
-}
-EXPORT_SYMBOL_GPL(nvme_stop_ctrl);
-
-void nvme_start_ctrl(struct nvme_ctrl *ctrl)
-{
-	if (ctrl->kato)
-		nvme_start_keep_alive(ctrl);
-
-	if (ctrl->queue_count > 1) {
-		nvme_queue_scan(ctrl);
-		nvme_queue_async_events(ctrl);
-		nvme_start_queues(ctrl);
-	}
-}
-EXPORT_SYMBOL_GPL(nvme_start_ctrl);
-
-void nvme_uninit_ctrl(struct nvme_ctrl *ctrl)
-{
-	device_destroy(nvme_class, MKDEV(nvme_char_major, ctrl->instance));
-
-	spin_lock(&dev_list_lock);
-	list_del(&ctrl->node);
-	spin_unlock(&dev_list_lock);
-}
-EXPORT_SYMBOL_GPL(nvme_uninit_ctrl);
-
-static void nvme_free_ctrl(struct kref *kref)
-{
-	struct nvme_ctrl *ctrl = container_of(kref, struct nvme_ctrl, kref);
-
-	put_device(ctrl->device);
-	nvme_release_instance(ctrl);
-	ida_destroy(&ctrl->ns_ida);
-
-	ctrl->ops->free_ctrl(ctrl);
-}
-
-void nvme_put_ctrl(struct nvme_ctrl *ctrl)
-{
-	kref_put(&ctrl->kref, nvme_free_ctrl);
-}
-EXPORT_SYMBOL_GPL(nvme_put_ctrl);
-
-/*
- * Initialize a NVMe controller structures.  This needs to be called during
- * earliest initialization so that we have the initialized structured around
- * during probing.
- */
-int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
-		const struct nvme_ctrl_ops *ops, unsigned long quirks)
-{
-	int ret;
-
-	ctrl->state = NVME_CTRL_NEW;
-	spin_lock_init(&ctrl->lock);
-	INIT_LIST_HEAD(&ctrl->namespaces);
-	mutex_init(&ctrl->namespaces_mutex);
-	kref_init(&ctrl->kref);
-	ctrl->dev = dev;
-	ctrl->ops = ops;
-	ctrl->quirks = quirks;
-	INIT_WORK(&ctrl->scan_work, nvme_scan_work);
-	INIT_WORK(&ctrl->async_event_work, nvme_async_event_work);
-	INIT_WORK(&ctrl->fw_act_work, nvme_fw_act_work);
-
-	ret = nvme_set_instance(ctrl);
-	if (ret)
-		goto out;
-
-	ctrl->device = device_create_with_groups(nvme_class, ctrl->dev,
-				MKDEV(nvme_char_major, ctrl->instance),
-				ctrl, nvme_dev_attr_groups,
-				"nvme%d", ctrl->instance);
-	if (IS_ERR(ctrl->device)) {
-		ret = PTR_ERR(ctrl->device);
-		goto out_release_instance;
-	}
-	get_device(ctrl->device);
-	ida_init(&ctrl->ns_ida);
-
-	spin_lock(&dev_list_lock);
-	list_add_tail(&ctrl->node, &nvme_ctrl_list);
-	spin_unlock(&dev_list_lock);
-
-	/*
-	 * Initialize latency tolerance controls.  The sysfs files won't
-	 * be visible to userspace unless the device actually supports APST.
-	 */
-	ctrl->device->power.set_latency_tolerance = nvme_set_latency_tolerance;
-	dev_pm_qos_update_user_latency_tolerance(ctrl->device,
-		min(default_ps_max_latency_us, (unsigned long)S32_MAX));
-
-	return 0;
-out_release_instance:
-	nvme_release_instance(ctrl);
-out:
-	return ret;
-}
-EXPORT_SYMBOL_GPL(nvme_init_ctrl);
-
-/**
- * nvme_kill_queues(): Ends all namespace queues
- * @ctrl: the dead controller that needs to end
- *
- * Call this function when the driver determines it is unable to get the
- * controller in a state capable of servicing IO.
- */
-void nvme_kill_queues(struct nvme_ctrl *ctrl)
-{
-	struct nvme_ns *ns;
-
-	mutex_lock(&ctrl->namespaces_mutex);
-
-	/* Forcibly unquiesce queues to avoid blocking dispatch */
-	if (ctrl->admin_q)
-		blk_mq_unquiesce_queue(ctrl->admin_q);
-
-	list_for_each_entry(ns, &ctrl->namespaces, list) {
-		/*
-		 * Revalidating a dead namespace sets capacity to 0. This will
-		 * end buffered writers dirtying pages that can't be synced.
-		 */
-		if (!ns->disk || test_and_set_bit(NVME_NS_DEAD, &ns->flags))
-			continue;
-		revalidate_disk(ns->disk);
-		blk_set_queue_dying(ns->queue);
-
-		/* Forcibly unquiesce queues to avoid blocking dispatch */
-		blk_mq_unquiesce_queue(ns->queue);
-	}
-	mutex_unlock(&ctrl->namespaces_mutex);
-}
-EXPORT_SYMBOL_GPL(nvme_kill_queues);
-
-void nvme_unfreeze(struct nvme_ctrl *ctrl)
-{
-	struct nvme_ns *ns;
-
-	mutex_lock(&ctrl->namespaces_mutex);
-	list_for_each_entry(ns, &ctrl->namespaces, list)
-		blk_mq_unfreeze_queue(ns->queue);
-	mutex_unlock(&ctrl->namespaces_mutex);
-}
-EXPORT_SYMBOL_GPL(nvme_unfreeze);
-
-void nvme_wait_freeze_timeout(struct nvme_ctrl *ctrl, long timeout)
-{
-	struct nvme_ns *ns;
-
-	mutex_lock(&ctrl->namespaces_mutex);
-	list_for_each_entry(ns, &ctrl->namespaces, list) {
-		timeout = blk_mq_freeze_queue_wait_timeout(ns->queue, timeout);
-		if (timeout <= 0)
-			break;
-	}
-	mutex_unlock(&ctrl->namespaces_mutex);
-}
-EXPORT_SYMBOL_GPL(nvme_wait_freeze_timeout);
-
-void nvme_wait_freeze(struct nvme_ctrl *ctrl)
-{
-	struct nvme_ns *ns;
-
-	mutex_lock(&ctrl->namespaces_mutex);
-	list_for_each_entry(ns, &ctrl->namespaces, list)
-		blk_mq_freeze_queue_wait(ns->queue);
-	mutex_unlock(&ctrl->namespaces_mutex);
-}
-EXPORT_SYMBOL_GPL(nvme_wait_freeze);
-
-void nvme_start_freeze(struct nvme_ctrl *ctrl)
-{
-	struct nvme_ns *ns;
-
-	mutex_lock(&ctrl->namespaces_mutex);
-	list_for_each_entry(ns, &ctrl->namespaces, list)
-		blk_freeze_queue_start(ns->queue);
-	mutex_unlock(&ctrl->namespaces_mutex);
-}
-EXPORT_SYMBOL_GPL(nvme_start_freeze);
-
-void nvme_stop_queues(struct nvme_ctrl *ctrl)
-{
-	struct nvme_ns *ns;
-
-	mutex_lock(&ctrl->namespaces_mutex);
-	list_for_each_entry(ns, &ctrl->namespaces, list)
-		blk_mq_quiesce_queue(ns->queue);
-	mutex_unlock(&ctrl->namespaces_mutex);
-}
-EXPORT_SYMBOL_GPL(nvme_stop_queues);
-
-void nvme_start_queues(struct nvme_ctrl *ctrl)
-{
-	struct nvme_ns *ns;
-
-	mutex_lock(&ctrl->namespaces_mutex);
-	list_for_each_entry(ns, &ctrl->namespaces, list)
-		blk_mq_unquiesce_queue(ns->queue);
-	mutex_unlock(&ctrl->namespaces_mutex);
-}
-EXPORT_SYMBOL_GPL(nvme_start_queues);
-
-int __init nvme_core_init(void)
-{
-	int result;
-
-	nvme_wq = alloc_workqueue("nvme-wq",
-			WQ_UNBOUND | WQ_MEM_RECLAIM | WQ_SYSFS, 0);
-	if (!nvme_wq)
-		return -ENOMEM;
-
-	result = __register_chrdev(nvme_char_major, 0, NVME_MINORS, "nvme",
-							&nvme_dev_fops);
-	if (result < 0)
-		goto destroy_wq;
-	else if (result > 0)
-		nvme_char_major = result;
-
-	result = aio_service_init();
-	if (result)
-		goto unregister_chrdev;
-
-	result = aio_worker_init();
-	if (result)
-		goto destroy_aio_service;
-
-	nvme_class = class_create(THIS_MODULE, "nvme");
-	if (IS_ERR(nvme_class)) {
-		result = PTR_ERR(nvme_class);
-		goto destroy_aio_worker;
-	}
-
-	return 0;
-
-destroy_aio_worker:
-	aio_worker_exit();
-destroy_aio_service:
-	aio_service_exit();
-unregister_chrdev:
-	__unregister_chrdev(nvme_char_major, 0, NVME_MINORS, "nvme");
-destroy_wq:
-	destroy_workqueue(nvme_wq);
-	return result;
-}
-
-void nvme_core_exit(void)
-{
-	class_destroy(nvme_class);
-	__unregister_chrdev(nvme_char_major, 0, NVME_MINORS, "nvme");
-	destroy_workqueue(nvme_wq);
-	aio_worker_exit();
-	aio_service_exit();
-}
-
-MODULE_LICENSE("GPL");
-MODULE_VERSION("1.0");
-module_init(nvme_core_init);
-module_exit(nvme_core_exit);
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/fabrics.c b/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/fabrics.c
deleted file mode 100644
index 33d060c..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/fabrics.c
+++ /dev/null
@@ -1,1054 +0,0 @@
-/*
- * NVMe over Fabrics common host code.
- * Copyright (c) 2015-2016 HGST, a Western Digital Company.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- */
-#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
-#include <linux/init.h>
-#include <linux/miscdevice.h>
-#include <linux/module.h>
-#include <linux/mutex.h>
-#include <linux/parser.h>
-#include <linux/seq_file.h>
-#include "nvme.h"
-#include "fabrics.h"
-
-static LIST_HEAD(nvmf_transports);
-static DECLARE_RWSEM(nvmf_transports_rwsem);
-
-static LIST_HEAD(nvmf_hosts);
-static DEFINE_MUTEX(nvmf_hosts_mutex);
-
-static struct nvmf_host *nvmf_default_host;
-
-static struct nvmf_host *__nvmf_host_find(const char *hostnqn)
-{
-	struct nvmf_host *host;
-
-	list_for_each_entry(host, &nvmf_hosts, list) {
-		if (!strcmp(host->nqn, hostnqn))
-			return host;
-	}
-
-	return NULL;
-}
-
-static struct nvmf_host *nvmf_host_add(const char *hostnqn)
-{
-	struct nvmf_host *host;
-
-	mutex_lock(&nvmf_hosts_mutex);
-	host = __nvmf_host_find(hostnqn);
-	if (host) {
-		kref_get(&host->ref);
-		goto out_unlock;
-	}
-
-	host = kmalloc(sizeof(*host), GFP_KERNEL);
-	if (!host)
-		goto out_unlock;
-
-	kref_init(&host->ref);
-	memcpy(host->nqn, hostnqn, NVMF_NQN_SIZE);
-
-	list_add_tail(&host->list, &nvmf_hosts);
-out_unlock:
-	mutex_unlock(&nvmf_hosts_mutex);
-	return host;
-}
-
-static struct nvmf_host *nvmf_host_default(void)
-{
-	struct nvmf_host *host;
-
-	host = kmalloc(sizeof(*host), GFP_KERNEL);
-	if (!host)
-		return NULL;
-
-	kref_init(&host->ref);
-	uuid_gen(&host->id);
-	snprintf(host->nqn, NVMF_NQN_SIZE,
-		"nqn.2014-08.org.nvmexpress:uuid:%pUb", &host->id);
-
-	mutex_lock(&nvmf_hosts_mutex);
-	list_add_tail(&host->list, &nvmf_hosts);
-	mutex_unlock(&nvmf_hosts_mutex);
-
-	return host;
-}
-
-static void nvmf_host_destroy(struct kref *ref)
-{
-	struct nvmf_host *host = container_of(ref, struct nvmf_host, ref);
-
-	mutex_lock(&nvmf_hosts_mutex);
-	list_del(&host->list);
-	mutex_unlock(&nvmf_hosts_mutex);
-
-	kfree(host);
-}
-
-static void nvmf_host_put(struct nvmf_host *host)
-{
-	if (host)
-		kref_put(&host->ref, nvmf_host_destroy);
-}
-
-/**
- * nvmf_get_address() -  Get address/port
- * @ctrl:	Host NVMe controller instance which we got the address
- * @buf:	OUTPUT parameter that will contain the address/port
- * @size:	buffer size
- */
-int nvmf_get_address(struct nvme_ctrl *ctrl, char *buf, int size)
-{
-	int len = 0;
-
-	if (ctrl->opts->mask & NVMF_OPT_TRADDR)
-		len += snprintf(buf, size, "traddr=%s", ctrl->opts->traddr);
-	if (ctrl->opts->mask & NVMF_OPT_TRSVCID)
-		len += snprintf(buf + len, size - len, "%strsvcid=%s",
-				(len) ? "," : "", ctrl->opts->trsvcid);
-	if (ctrl->opts->mask & NVMF_OPT_HOST_TRADDR)
-		len += snprintf(buf + len, size - len, "%shost_traddr=%s",
-				(len) ? "," : "", ctrl->opts->host_traddr);
-	len += snprintf(buf + len, size - len, "\n");
-
-	return len;
-}
-EXPORT_SYMBOL_GPL(nvmf_get_address);
-
-/**
- * nvmf_reg_read32() -  NVMe Fabrics "Property Get" API function.
- * @ctrl:	Host NVMe controller instance maintaining the admin
- *		queue used to submit the property read command to
- *		the allocated NVMe controller resource on the target system.
- * @off:	Starting offset value of the targeted property
- *		register (see the fabrics section of the NVMe standard).
- * @val:	OUTPUT parameter that will contain the value of
- *		the property after a successful read.
- *
- * Used by the host system to retrieve a 32-bit capsule property value
- * from an NVMe controller on the target system.
- *
- * ("Capsule property" is an "PCIe register concept" applied to the
- * NVMe fabrics space.)
- *
- * Return:
- *	0: successful read
- *	> 0: NVMe error status code
- *	< 0: Linux errno error code
- */
-int nvmf_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val)
-{
-	struct nvme_command cmd;
-	union nvme_result res;
-	int ret;
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.prop_get.opcode = nvme_fabrics_command;
-	cmd.prop_get.fctype = nvme_fabrics_type_property_get;
-	cmd.prop_get.offset = cpu_to_le32(off);
-
-	ret = __nvme_submit_sync_cmd(ctrl->admin_q, &cmd, &res, NULL, 0, 0,
-			NVME_QID_ANY, 0, 0);
-
-	if (ret >= 0)
-		*val = le64_to_cpu(res.u64);
-	if (unlikely(ret != 0))
-		dev_err(ctrl->device,
-			"Property Get error: %d, offset %#x\n",
-			ret > 0 ? ret & ~NVME_SC_DNR : ret, off);
-
-	return ret;
-}
-EXPORT_SYMBOL_GPL(nvmf_reg_read32);
-
-/**
- * nvmf_reg_read64() -  NVMe Fabrics "Property Get" API function.
- * @ctrl:	Host NVMe controller instance maintaining the admin
- *		queue used to submit the property read command to
- *		the allocated controller resource on the target system.
- * @off:	Starting offset value of the targeted property
- *		register (see the fabrics section of the NVMe standard).
- * @val:	OUTPUT parameter that will contain the value of
- *		the property after a successful read.
- *
- * Used by the host system to retrieve a 64-bit capsule property value
- * from an NVMe controller on the target system.
- *
- * ("Capsule property" is an "PCIe register concept" applied to the
- * NVMe fabrics space.)
- *
- * Return:
- *	0: successful read
- *	> 0: NVMe error status code
- *	< 0: Linux errno error code
- */
-int nvmf_reg_read64(struct nvme_ctrl *ctrl, u32 off, u64 *val)
-{
-	struct nvme_command cmd;
-	union nvme_result res;
-	int ret;
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.prop_get.opcode = nvme_fabrics_command;
-	cmd.prop_get.fctype = nvme_fabrics_type_property_get;
-	cmd.prop_get.attrib = 1;
-	cmd.prop_get.offset = cpu_to_le32(off);
-
-	ret = __nvme_submit_sync_cmd(ctrl->admin_q, &cmd, &res, NULL, 0, 0,
-			NVME_QID_ANY, 0, 0);
-
-	if (ret >= 0)
-		*val = le64_to_cpu(res.u64);
-	if (unlikely(ret != 0))
-		dev_err(ctrl->device,
-			"Property Get error: %d, offset %#x\n",
-			ret > 0 ? ret & ~NVME_SC_DNR : ret, off);
-	return ret;
-}
-EXPORT_SYMBOL_GPL(nvmf_reg_read64);
-
-/**
- * nvmf_reg_write32() -  NVMe Fabrics "Property Write" API function.
- * @ctrl:	Host NVMe controller instance maintaining the admin
- *		queue used to submit the property read command to
- *		the allocated NVMe controller resource on the target system.
- * @off:	Starting offset value of the targeted property
- *		register (see the fabrics section of the NVMe standard).
- * @val:	Input parameter that contains the value to be
- *		written to the property.
- *
- * Used by the NVMe host system to write a 32-bit capsule property value
- * to an NVMe controller on the target system.
- *
- * ("Capsule property" is an "PCIe register concept" applied to the
- * NVMe fabrics space.)
- *
- * Return:
- *	0: successful write
- *	> 0: NVMe error status code
- *	< 0: Linux errno error code
- */
-int nvmf_reg_write32(struct nvme_ctrl *ctrl, u32 off, u32 val)
-{
-	struct nvme_command cmd;
-	int ret;
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.prop_set.opcode = nvme_fabrics_command;
-	cmd.prop_set.fctype = nvme_fabrics_type_property_set;
-	cmd.prop_set.attrib = 0;
-	cmd.prop_set.offset = cpu_to_le32(off);
-	cmd.prop_set.value = cpu_to_le64(val);
-
-	ret = __nvme_submit_sync_cmd(ctrl->admin_q, &cmd, NULL, NULL, 0, 0,
-			NVME_QID_ANY, 0, 0);
-	if (unlikely(ret))
-		dev_err(ctrl->device,
-			"Property Set error: %d, offset %#x\n",
-			ret > 0 ? ret & ~NVME_SC_DNR : ret, off);
-	return ret;
-}
-EXPORT_SYMBOL_GPL(nvmf_reg_write32);
-
-/**
- * nvmf_log_connect_error() - Error-parsing-diagnostic print
- * out function for connect() errors.
- *
- * @ctrl: the specific /dev/nvmeX device that had the error.
- *
- * @errval: Error code to be decoded in a more human-friendly
- *	    printout.
- *
- * @offset: For use with the NVMe error code NVME_SC_CONNECT_INVALID_PARAM.
- *
- * @cmd: This is the SQE portion of a submission capsule.
- *
- * @data: This is the "Data" portion of a submission capsule.
- */
-static void nvmf_log_connect_error(struct nvme_ctrl *ctrl,
-		int errval, int offset, struct nvme_command *cmd,
-		struct nvmf_connect_data *data)
-{
-	int err_sctype = errval & (~NVME_SC_DNR);
-
-	switch (err_sctype) {
-
-	case (NVME_SC_CONNECT_INVALID_PARAM):
-		if (offset >> 16) {
-			char *inv_data = "Connect Invalid Data Parameter";
-
-			switch (offset & 0xffff) {
-			case (offsetof(struct nvmf_connect_data, cntlid)):
-				dev_err(ctrl->device,
-					"%s, cntlid: %d\n",
-					inv_data, data->cntlid);
-				break;
-			case (offsetof(struct nvmf_connect_data, hostnqn)):
-				dev_err(ctrl->device,
-					"%s, hostnqn \"%s\"\n",
-					inv_data, data->hostnqn);
-				break;
-			case (offsetof(struct nvmf_connect_data, subsysnqn)):
-				dev_err(ctrl->device,
-					"%s, subsysnqn \"%s\"\n",
-					inv_data, data->subsysnqn);
-				break;
-			default:
-				dev_err(ctrl->device,
-					"%s, starting byte offset: %d\n",
-				       inv_data, offset & 0xffff);
-				break;
-			}
-		} else {
-			char *inv_sqe = "Connect Invalid SQE Parameter";
-
-			switch (offset) {
-			case (offsetof(struct nvmf_connect_command, qid)):
-				dev_err(ctrl->device,
-				       "%s, qid %d\n",
-					inv_sqe, cmd->connect.qid);
-				break;
-			default:
-				dev_err(ctrl->device,
-					"%s, starting byte offset: %d\n",
-					inv_sqe, offset);
-			}
-		}
-		break;
-
-	case NVME_SC_CONNECT_INVALID_HOST:
-		dev_err(ctrl->device,
-			"Connect for subsystem %s is not allowed, hostnqn: %s\n",
-			data->subsysnqn, data->hostnqn);
-		break;
-
-	case NVME_SC_CONNECT_CTRL_BUSY:
-		dev_err(ctrl->device,
-			"Connect command failed: controller is busy or not available\n");
-		break;
-
-	case NVME_SC_CONNECT_FORMAT:
-		dev_err(ctrl->device,
-			"Connect incompatible format: %d",
-			cmd->connect.recfmt);
-		break;
-
-	default:
-		dev_err(ctrl->device,
-			"Connect command failed, error wo/DNR bit: %d\n",
-			err_sctype);
-		break;
-	} /* switch (err_sctype) */
-}
-
-/**
- * nvmf_connect_admin_queue() - NVMe Fabrics Admin Queue "Connect"
- *				API function.
- * @ctrl:	Host nvme controller instance used to request
- *              a new NVMe controller allocation on the target
- *              system and  establish an NVMe Admin connection to
- *              that controller.
- *
- * This function enables an NVMe host device to request a new allocation of
- * an NVMe controller resource on a target system as well establish a
- * fabrics-protocol connection of the NVMe Admin queue between the
- * host system device and the allocated NVMe controller on the
- * target system via a NVMe Fabrics "Connect" command.
- *
- * Return:
- *	0: success
- *	> 0: NVMe error status code
- *	< 0: Linux errno error code
- *
- */
-int nvmf_connect_admin_queue(struct nvme_ctrl *ctrl)
-{
-	struct nvme_command cmd;
-	union nvme_result res;
-	struct nvmf_connect_data *data;
-	int ret;
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.connect.opcode = nvme_fabrics_command;
-	cmd.connect.fctype = nvme_fabrics_type_connect;
-	cmd.connect.qid = 0;
-	cmd.connect.sqsize = cpu_to_le16(NVME_AQ_DEPTH - 1);
-
-	/*
-	 * Set keep-alive timeout in seconds granularity (ms * 1000)
-	 * and add a grace period for controller kato enforcement
-	 */
-	cmd.connect.kato = ctrl->opts->discovery_nqn ? 0 :
-		cpu_to_le32((ctrl->kato + NVME_KATO_GRACE) * 1000);
-
-	data = kzalloc(sizeof(*data), GFP_KERNEL);
-	if (!data)
-		return -ENOMEM;
-
-	uuid_copy(&data->hostid, &ctrl->opts->host->id);
-	data->cntlid = cpu_to_le16(0xffff);
-	strncpy(data->subsysnqn, ctrl->opts->subsysnqn, NVMF_NQN_SIZE);
-	strncpy(data->hostnqn, ctrl->opts->host->nqn, NVMF_NQN_SIZE);
-
-	ret = __nvme_submit_sync_cmd(ctrl->admin_q, &cmd, &res,
-			data, sizeof(*data), 0, NVME_QID_ANY, 1,
-			BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT);
-	if (ret) {
-		nvmf_log_connect_error(ctrl, ret, le32_to_cpu(res.u32),
-				       &cmd, data);
-		goto out_free_data;
-	}
-
-	ctrl->cntlid = le16_to_cpu(res.u16);
-
-out_free_data:
-	kfree(data);
-	return ret;
-}
-EXPORT_SYMBOL_GPL(nvmf_connect_admin_queue);
-
-/**
- * nvmf_connect_io_queue() - NVMe Fabrics I/O Queue "Connect"
- *			     API function.
- * @ctrl:	Host nvme controller instance used to establish an
- *		NVMe I/O queue connection to the already allocated NVMe
- *		controller on the target system.
- * @qid:	NVMe I/O queue number for the new I/O connection between
- *		host and target (note qid == 0 is illegal as this is
- *		the Admin queue, per NVMe standard).
- *
- * This function issues a fabrics-protocol connection
- * of a NVMe I/O queue (via NVMe Fabrics "Connect" command)
- * between the host system device and the allocated NVMe controller
- * on the target system.
- *
- * Return:
- *	0: success
- *	> 0: NVMe error status code
- *	< 0: Linux errno error code
- */
-int nvmf_connect_io_queue(struct nvme_ctrl *ctrl, u16 qid)
-{
-	struct nvme_command cmd;
-	struct nvmf_connect_data *data;
-	union nvme_result res;
-	int ret;
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.connect.opcode = nvme_fabrics_command;
-	cmd.connect.fctype = nvme_fabrics_type_connect;
-	cmd.connect.qid = cpu_to_le16(qid);
-	cmd.connect.sqsize = cpu_to_le16(ctrl->sqsize);
-
-	data = kzalloc(sizeof(*data), GFP_KERNEL);
-	if (!data)
-		return -ENOMEM;
-
-	uuid_copy(&data->hostid, &ctrl->opts->host->id);
-	data->cntlid = cpu_to_le16(ctrl->cntlid);
-	strncpy(data->subsysnqn, ctrl->opts->subsysnqn, NVMF_NQN_SIZE);
-	strncpy(data->hostnqn, ctrl->opts->host->nqn, NVMF_NQN_SIZE);
-
-	ret = __nvme_submit_sync_cmd(ctrl->connect_q, &cmd, &res,
-			data, sizeof(*data), 0, qid, 1,
-			BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT);
-	if (ret) {
-		nvmf_log_connect_error(ctrl, ret, le32_to_cpu(res.u32),
-				       &cmd, data);
-	}
-	kfree(data);
-	return ret;
-}
-EXPORT_SYMBOL_GPL(nvmf_connect_io_queue);
-
-bool nvmf_should_reconnect(struct nvme_ctrl *ctrl)
-{
-	if (ctrl->opts->max_reconnects != -1 &&
-	    ctrl->nr_reconnects < ctrl->opts->max_reconnects)
-		return true;
-
-	return false;
-}
-EXPORT_SYMBOL_GPL(nvmf_should_reconnect);
-
-/**
- * nvmf_register_transport() - NVMe Fabrics Library registration function.
- * @ops:	Transport ops instance to be registered to the
- *		common fabrics library.
- *
- * API function that registers the type of specific transport fabric
- * being implemented to the common NVMe fabrics library. Part of
- * the overall init sequence of starting up a fabrics driver.
- */
-int nvmf_register_transport(struct nvmf_transport_ops *ops)
-{
-	if (!ops->create_ctrl)
-		return -EINVAL;
-
-	down_write(&nvmf_transports_rwsem);
-	list_add_tail(&ops->entry, &nvmf_transports);
-	up_write(&nvmf_transports_rwsem);
-
-	return 0;
-}
-EXPORT_SYMBOL_GPL(nvmf_register_transport);
-
-/**
- * nvmf_unregister_transport() - NVMe Fabrics Library unregistration function.
- * @ops:	Transport ops instance to be unregistered from the
- *		common fabrics library.
- *
- * Fabrics API function that unregisters the type of specific transport
- * fabric being implemented from the common NVMe fabrics library.
- * Part of the overall exit sequence of unloading the implemented driver.
- */
-void nvmf_unregister_transport(struct nvmf_transport_ops *ops)
-{
-	down_write(&nvmf_transports_rwsem);
-	list_del(&ops->entry);
-	up_write(&nvmf_transports_rwsem);
-}
-EXPORT_SYMBOL_GPL(nvmf_unregister_transport);
-
-static struct nvmf_transport_ops *nvmf_lookup_transport(
-		struct nvmf_ctrl_options *opts)
-{
-	struct nvmf_transport_ops *ops;
-
-	lockdep_assert_held(&nvmf_transports_rwsem);
-
-	list_for_each_entry(ops, &nvmf_transports, entry) {
-		if (strcmp(ops->name, opts->transport) == 0)
-			return ops;
-	}
-
-	return NULL;
-}
-
-static const match_table_t opt_tokens = {
-	{ NVMF_OPT_TRANSPORT,		"transport=%s"		},
-	{ NVMF_OPT_TRADDR,		"traddr=%s"		},
-	{ NVMF_OPT_TRSVCID,		"trsvcid=%s"		},
-	{ NVMF_OPT_NQN,			"nqn=%s"		},
-	{ NVMF_OPT_QUEUE_SIZE,		"queue_size=%d"		},
-	{ NVMF_OPT_NR_IO_QUEUES,	"nr_io_queues=%d"	},
-	{ NVMF_OPT_RECONNECT_DELAY,	"reconnect_delay=%d"	},
-	{ NVMF_OPT_CTRL_LOSS_TMO,	"ctrl_loss_tmo=%d"	},
-	{ NVMF_OPT_KATO,		"keep_alive_tmo=%d"	},
-	{ NVMF_OPT_HOSTNQN,		"hostnqn=%s"		},
-	{ NVMF_OPT_HOST_TRADDR,		"host_traddr=%s"	},
-	{ NVMF_OPT_HOST_ID,		"hostid=%s"		},
-	{ NVMF_OPT_ERR,			NULL			}
-};
-
-static int nvmf_parse_options(struct nvmf_ctrl_options *opts,
-		const char *buf)
-{
-	substring_t args[MAX_OPT_ARGS];
-	char *options, *o, *p;
-	int token, ret = 0;
-	size_t nqnlen  = 0;
-	int ctrl_loss_tmo = NVMF_DEF_CTRL_LOSS_TMO;
-	uuid_t hostid;
-
-	/* Set defaults */
-	opts->queue_size = NVMF_DEF_QUEUE_SIZE;
-	opts->nr_io_queues = num_online_cpus();
-	opts->reconnect_delay = NVMF_DEF_RECONNECT_DELAY;
-	opts->kato = NVME_DEFAULT_KATO;
-
-	options = o = kstrdup(buf, GFP_KERNEL);
-	if (!options)
-		return -ENOMEM;
-
-	uuid_gen(&hostid);
-
-	while ((p = strsep(&o, ",\n")) != NULL) {
-		if (!*p)
-			continue;
-
-		token = match_token(p, opt_tokens, args);
-		opts->mask |= token;
-		switch (token) {
-		case NVMF_OPT_TRANSPORT:
-			p = match_strdup(args);
-			if (!p) {
-				ret = -ENOMEM;
-				goto out;
-			}
-			kfree(opts->transport);
-			opts->transport = p;
-			break;
-		case NVMF_OPT_NQN:
-			p = match_strdup(args);
-			if (!p) {
-				ret = -ENOMEM;
-				goto out;
-			}
-			kfree(opts->subsysnqn);
-			opts->subsysnqn = p;
-			nqnlen = strlen(opts->subsysnqn);
-			if (nqnlen >= NVMF_NQN_SIZE) {
-				pr_err("%s needs to be < %d bytes\n",
-					opts->subsysnqn, NVMF_NQN_SIZE);
-				ret = -EINVAL;
-				goto out;
-			}
-			opts->discovery_nqn =
-				!(strcmp(opts->subsysnqn,
-					 NVME_DISC_SUBSYS_NAME));
-			if (opts->discovery_nqn) {
-				opts->kato = 0;
-				opts->nr_io_queues = 0;
-			}
-			break;
-		case NVMF_OPT_TRADDR:
-			p = match_strdup(args);
-			if (!p) {
-				ret = -ENOMEM;
-				goto out;
-			}
-			kfree(opts->traddr);
-			opts->traddr = p;
-			break;
-		case NVMF_OPT_TRSVCID:
-			p = match_strdup(args);
-			if (!p) {
-				ret = -ENOMEM;
-				goto out;
-			}
-			kfree(opts->trsvcid);
-			opts->trsvcid = p;
-			break;
-		case NVMF_OPT_QUEUE_SIZE:
-			if (match_int(args, &token)) {
-				ret = -EINVAL;
-				goto out;
-			}
-			if (token < NVMF_MIN_QUEUE_SIZE ||
-			    token > NVMF_MAX_QUEUE_SIZE) {
-				pr_err("Invalid queue_size %d\n", token);
-				ret = -EINVAL;
-				goto out;
-			}
-			opts->queue_size = token;
-			break;
-		case NVMF_OPT_NR_IO_QUEUES:
-			if (match_int(args, &token)) {
-				ret = -EINVAL;
-				goto out;
-			}
-			if (token <= 0) {
-				pr_err("Invalid number of IOQs %d\n", token);
-				ret = -EINVAL;
-				goto out;
-			}
-			opts->nr_io_queues = min_t(unsigned int,
-					num_online_cpus(), token);
-			break;
-		case NVMF_OPT_KATO:
-			if (match_int(args, &token)) {
-				ret = -EINVAL;
-				goto out;
-			}
-
-			if (token < 0) {
-				pr_err("Invalid keep_alive_tmo %d\n", token);
-				ret = -EINVAL;
-				goto out;
-			} else if (token == 0 && !opts->discovery_nqn) {
-				/* Allowed for debug */
-				pr_warn("keep_alive_tmo 0 won't execute keep alives!!!\n");
-			}
-			opts->kato = token;
-
-			if (opts->discovery_nqn && opts->kato) {
-				pr_err("Discovery controllers cannot accept KATO != 0\n");
-				ret = -EINVAL;
-				goto out;
-			}
-
-			break;
-		case NVMF_OPT_CTRL_LOSS_TMO:
-			if (match_int(args, &token)) {
-				ret = -EINVAL;
-				goto out;
-			}
-
-			if (token < 0)
-				pr_warn("ctrl_loss_tmo < 0 will reconnect forever\n");
-			ctrl_loss_tmo = token;
-			break;
-		case NVMF_OPT_HOSTNQN:
-			if (opts->host) {
-				pr_err("hostnqn already user-assigned: %s\n",
-				       opts->host->nqn);
-				ret = -EADDRINUSE;
-				goto out;
-			}
-			p = match_strdup(args);
-			if (!p) {
-				ret = -ENOMEM;
-				goto out;
-			}
-			nqnlen = strlen(p);
-			if (nqnlen >= NVMF_NQN_SIZE) {
-				pr_err("%s needs to be < %d bytes\n",
-					p, NVMF_NQN_SIZE);
-				kfree(p);
-				ret = -EINVAL;
-				goto out;
-			}
-			nvmf_host_put(opts->host);
-			opts->host = nvmf_host_add(p);
-			kfree(p);
-			if (!opts->host) {
-				ret = -ENOMEM;
-				goto out;
-			}
-			break;
-		case NVMF_OPT_RECONNECT_DELAY:
-			if (match_int(args, &token)) {
-				ret = -EINVAL;
-				goto out;
-			}
-			if (token <= 0) {
-				pr_err("Invalid reconnect_delay %d\n", token);
-				ret = -EINVAL;
-				goto out;
-			}
-			opts->reconnect_delay = token;
-			break;
-		case NVMF_OPT_HOST_TRADDR:
-			p = match_strdup(args);
-			if (!p) {
-				ret = -ENOMEM;
-				goto out;
-			}
-			kfree(opts->host_traddr);
-			opts->host_traddr = p;
-			break;
-		case NVMF_OPT_HOST_ID:
-			p = match_strdup(args);
-			if (!p) {
-				ret = -ENOMEM;
-				goto out;
-			}
-			if (uuid_parse(p, &hostid)) {
-				pr_err("Invalid hostid %s\n", p);
-				ret = -EINVAL;
-				goto out;
-			}
-			break;
-		default:
-			pr_warn("unknown parameter or missing value '%s' in ctrl creation request\n",
-				p);
-			ret = -EINVAL;
-			goto out;
-		}
-	}
-
-	if (ctrl_loss_tmo < 0)
-		opts->max_reconnects = -1;
-	else
-		opts->max_reconnects = DIV_ROUND_UP(ctrl_loss_tmo,
-						opts->reconnect_delay);
-
-	if (!opts->host) {
-		kref_get(&nvmf_default_host->ref);
-		opts->host = nvmf_default_host;
-	}
-
-	uuid_copy(&opts->host->id, &hostid);
-
-out:
-	kfree(options);
-	return ret;
-}
-
-static int nvmf_check_required_opts(struct nvmf_ctrl_options *opts,
-		unsigned int required_opts)
-{
-	if ((opts->mask & required_opts) != required_opts) {
-		int i;
-
-		for (i = 0; i < ARRAY_SIZE(opt_tokens); i++) {
-			if ((opt_tokens[i].token & required_opts) &&
-			    !(opt_tokens[i].token & opts->mask)) {
-				pr_warn("missing parameter '%s'\n",
-					opt_tokens[i].pattern);
-			}
-		}
-
-		return -EINVAL;
-	}
-
-	return 0;
-}
-
-static int nvmf_check_allowed_opts(struct nvmf_ctrl_options *opts,
-		unsigned int allowed_opts)
-{
-	if (opts->mask & ~allowed_opts) {
-		int i;
-
-		for (i = 0; i < ARRAY_SIZE(opt_tokens); i++) {
-			if ((opt_tokens[i].token & opts->mask) &&
-			    (opt_tokens[i].token & ~allowed_opts)) {
-				pr_warn("invalid parameter '%s'\n",
-					opt_tokens[i].pattern);
-			}
-		}
-
-		return -EINVAL;
-	}
-
-	return 0;
-}
-
-void nvmf_free_options(struct nvmf_ctrl_options *opts)
-{
-	nvmf_host_put(opts->host);
-	kfree(opts->transport);
-	kfree(opts->traddr);
-	kfree(opts->trsvcid);
-	kfree(opts->subsysnqn);
-	kfree(opts->host_traddr);
-	kfree(opts);
-}
-EXPORT_SYMBOL_GPL(nvmf_free_options);
-
-#define NVMF_REQUIRED_OPTS	(NVMF_OPT_TRANSPORT | NVMF_OPT_NQN)
-#define NVMF_ALLOWED_OPTS	(NVMF_OPT_QUEUE_SIZE | NVMF_OPT_NR_IO_QUEUES | \
-				 NVMF_OPT_KATO | NVMF_OPT_HOSTNQN | \
-				 NVMF_OPT_HOST_ID)
-
-static struct nvme_ctrl *
-nvmf_create_ctrl(struct device *dev, const char *buf, size_t count)
-{
-	struct nvmf_ctrl_options *opts;
-	struct nvmf_transport_ops *ops;
-	struct nvme_ctrl *ctrl;
-	int ret;
-
-	opts = kzalloc(sizeof(*opts), GFP_KERNEL);
-	if (!opts)
-		return ERR_PTR(-ENOMEM);
-
-	ret = nvmf_parse_options(opts, buf);
-	if (ret)
-		goto out_free_opts;
-
-	/*
-	 * Check the generic options first as we need a valid transport for
-	 * the lookup below.  Then clear the generic flags so that transport
-	 * drivers don't have to care about them.
-	 */
-	ret = nvmf_check_required_opts(opts, NVMF_REQUIRED_OPTS);
-	if (ret)
-		goto out_free_opts;
-	opts->mask &= ~NVMF_REQUIRED_OPTS;
-
-	down_read(&nvmf_transports_rwsem);
-	ops = nvmf_lookup_transport(opts);
-	if (!ops) {
-		pr_info("no handler found for transport %s.\n",
-			opts->transport);
-		ret = -EINVAL;
-		goto out_unlock;
-	}
-
-	ret = nvmf_check_required_opts(opts, ops->required_opts);
-	if (ret)
-		goto out_unlock;
-	ret = nvmf_check_allowed_opts(opts, NVMF_ALLOWED_OPTS |
-				ops->allowed_opts | ops->required_opts);
-	if (ret)
-		goto out_unlock;
-
-	ctrl = ops->create_ctrl(dev, opts);
-	if (IS_ERR(ctrl)) {
-		ret = PTR_ERR(ctrl);
-		goto out_unlock;
-	}
-
-	if (strcmp(ctrl->subnqn, opts->subsysnqn)) {
-		dev_warn(ctrl->device,
-			"controller returned incorrect NQN: \"%s\".\n",
-			ctrl->subnqn);
-		up_read(&nvmf_transports_rwsem);
-		ctrl->ops->delete_ctrl(ctrl);
-		return ERR_PTR(-EINVAL);
-	}
-
-	up_read(&nvmf_transports_rwsem);
-	return ctrl;
-
-out_unlock:
-	up_read(&nvmf_transports_rwsem);
-out_free_opts:
-	nvmf_free_options(opts);
-	return ERR_PTR(ret);
-}
-
-static struct class *nvmf_class;
-static struct device *nvmf_device;
-static DEFINE_MUTEX(nvmf_dev_mutex);
-
-static ssize_t nvmf_dev_write(struct file *file, const char __user *ubuf,
-		size_t count, loff_t *pos)
-{
-	struct seq_file *seq_file = file->private_data;
-	struct nvme_ctrl *ctrl;
-	const char *buf;
-	int ret = 0;
-
-	if (count > PAGE_SIZE)
-		return -ENOMEM;
-
-	buf = memdup_user_nul(ubuf, count);
-	if (IS_ERR(buf))
-		return PTR_ERR(buf);
-
-	mutex_lock(&nvmf_dev_mutex);
-	if (seq_file->private) {
-		ret = -EINVAL;
-		goto out_unlock;
-	}
-
-	ctrl = nvmf_create_ctrl(nvmf_device, buf, count);
-	if (IS_ERR(ctrl)) {
-		ret = PTR_ERR(ctrl);
-		goto out_unlock;
-	}
-
-	seq_file->private = ctrl;
-
-out_unlock:
-	mutex_unlock(&nvmf_dev_mutex);
-	kfree(buf);
-	return ret ? ret : count;
-}
-
-static int nvmf_dev_show(struct seq_file *seq_file, void *private)
-{
-	struct nvme_ctrl *ctrl;
-	int ret = 0;
-
-	mutex_lock(&nvmf_dev_mutex);
-	ctrl = seq_file->private;
-	if (!ctrl) {
-		ret = -EINVAL;
-		goto out_unlock;
-	}
-
-	seq_printf(seq_file, "instance=%d,cntlid=%d\n",
-			ctrl->instance, ctrl->cntlid);
-
-out_unlock:
-	mutex_unlock(&nvmf_dev_mutex);
-	return ret;
-}
-
-static int nvmf_dev_open(struct inode *inode, struct file *file)
-{
-	/*
-	 * The miscdevice code initializes file->private_data, but doesn't
-	 * make use of it later.
-	 */
-	file->private_data = NULL;
-	return single_open(file, nvmf_dev_show, NULL);
-}
-
-static int nvmf_dev_release(struct inode *inode, struct file *file)
-{
-	struct seq_file *seq_file = file->private_data;
-	struct nvme_ctrl *ctrl = seq_file->private;
-
-	if (ctrl)
-		nvme_put_ctrl(ctrl);
-	return single_release(inode, file);
-}
-
-static const struct file_operations nvmf_dev_fops = {
-	.owner		= THIS_MODULE,
-	.write		= nvmf_dev_write,
-	.read		= seq_read,
-	.open		= nvmf_dev_open,
-	.release	= nvmf_dev_release,
-};
-
-static struct miscdevice nvmf_misc = {
-	.minor		= MISC_DYNAMIC_MINOR,
-	.name           = "nvme-fabrics",
-	.fops		= &nvmf_dev_fops,
-};
-
-static int __init nvmf_init(void)
-{
-	int ret;
-
-	nvmf_default_host = nvmf_host_default();
-	if (!nvmf_default_host)
-		return -ENOMEM;
-
-	nvmf_class = class_create(THIS_MODULE, "nvme-fabrics");
-	if (IS_ERR(nvmf_class)) {
-		pr_err("couldn't register class nvme-fabrics\n");
-		ret = PTR_ERR(nvmf_class);
-		goto out_free_host;
-	}
-
-	nvmf_device =
-		device_create(nvmf_class, NULL, MKDEV(0, 0), NULL, "ctl");
-	if (IS_ERR(nvmf_device)) {
-		pr_err("couldn't create nvme-fabris device!\n");
-		ret = PTR_ERR(nvmf_device);
-		goto out_destroy_class;
-	}
-
-	ret = misc_register(&nvmf_misc);
-	if (ret) {
-		pr_err("couldn't register misc device: %d\n", ret);
-		goto out_destroy_device;
-	}
-
-	return 0;
-
-out_destroy_device:
-	device_destroy(nvmf_class, MKDEV(0, 0));
-out_destroy_class:
-	class_destroy(nvmf_class);
-out_free_host:
-	nvmf_host_put(nvmf_default_host);
-	return ret;
-}
-
-static void __exit nvmf_exit(void)
-{
-	misc_deregister(&nvmf_misc);
-	device_destroy(nvmf_class, MKDEV(0, 0));
-	class_destroy(nvmf_class);
-	nvmf_host_put(nvmf_default_host);
-
-	BUILD_BUG_ON(sizeof(struct nvmf_connect_command) != 64);
-	BUILD_BUG_ON(sizeof(struct nvmf_property_get_command) != 64);
-	BUILD_BUG_ON(sizeof(struct nvmf_property_set_command) != 64);
-	BUILD_BUG_ON(sizeof(struct nvmf_connect_data) != 1024);
-}
-
-MODULE_LICENSE("GPL v2");
-
-module_init(nvmf_init);
-module_exit(nvmf_exit);
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/fabrics.h b/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/fabrics.h
deleted file mode 100644
index 9ff8529..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/fabrics.h
+++ /dev/null
@@ -1,175 +0,0 @@
-/*
- * NVMe over Fabrics common host code.
- * Copyright (c) 2015-2016 HGST, a Western Digital Company.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- */
-#ifndef _NVME_FABRICS_H
-#define _NVME_FABRICS_H 1
-
-#include <linux/in.h>
-#include <linux/inet.h>
-
-#define NVMF_MIN_QUEUE_SIZE	16
-#define NVMF_MAX_QUEUE_SIZE	1024
-#define NVMF_DEF_QUEUE_SIZE	128
-#define NVMF_DEF_RECONNECT_DELAY	10
-/* default to 600 seconds of reconnect attempts before giving up */
-#define NVMF_DEF_CTRL_LOSS_TMO		600
-
-/*
- * Define a host as seen by the target.  We allocate one at boot, but also
- * allow the override it when creating controllers.  This is both to provide
- * persistence of the Host NQN over multiple boots, and to allow using
- * multiple ones, for example in a container scenario.  Because we must not
- * use different Host NQNs with the same Host ID we generate a Host ID and
- * use this structure to keep track of the relation between the two.
- */
-struct nvmf_host {
-	struct kref		ref;
-	struct list_head	list;
-	char			nqn[NVMF_NQN_SIZE];
-	uuid_t			id;
-};
-
-/**
- * enum nvmf_parsing_opts - used to define the sysfs parsing options used.
- */
-enum {
-	NVMF_OPT_ERR		= 0,
-	NVMF_OPT_TRANSPORT	= 1 << 0,
-	NVMF_OPT_NQN		= 1 << 1,
-	NVMF_OPT_TRADDR		= 1 << 2,
-	NVMF_OPT_TRSVCID	= 1 << 3,
-	NVMF_OPT_QUEUE_SIZE	= 1 << 4,
-	NVMF_OPT_NR_IO_QUEUES	= 1 << 5,
-	NVMF_OPT_TL_RETRY_COUNT	= 1 << 6,
-	NVMF_OPT_KATO		= 1 << 7,
-	NVMF_OPT_HOSTNQN	= 1 << 8,
-	NVMF_OPT_RECONNECT_DELAY = 1 << 9,
-	NVMF_OPT_HOST_TRADDR	= 1 << 10,
-	NVMF_OPT_CTRL_LOSS_TMO	= 1 << 11,
-	NVMF_OPT_HOST_ID	= 1 << 12,
-};
-
-/**
- * struct nvmf_ctrl_options - Used to hold the options specified
- *			      with the parsing opts enum.
- * @mask:	Used by the fabrics library to parse through sysfs options
- *		on adding a NVMe controller.
- * @transport:	Holds the fabric transport "technology name" (for a lack of
- *		better description) that will be used by an NVMe controller
- *		being added.
- * @subsysnqn:	Hold the fully qualified NQN subystem name (format defined
- *		in the NVMe specification, "NVMe Qualified Names").
- * @traddr:	The transport-specific TRADDR field for a port on the
- *              subsystem which is adding a controller.
- * @trsvcid:	The transport-specific TRSVCID field for a port on the
- *              subsystem which is adding a controller.
- * @host_traddr: A transport-specific field identifying the NVME host port
- *              to use for the connection to the controller.
- * @queue_size: Number of IO queue elements.
- * @nr_io_queues: Number of controller IO queues that will be established.
- * @reconnect_delay: Time between two consecutive reconnect attempts.
- * @discovery_nqn: indicates if the subsysnqn is the well-known discovery NQN.
- * @kato:	Keep-alive timeout.
- * @host:	Virtual NVMe host, contains the NQN and Host ID.
- * @max_reconnects: maximum number of allowed reconnect attempts before removing
- *              the controller, (-1) means reconnect forever, zero means remove
- *              immediately;
- */
-struct nvmf_ctrl_options {
-	unsigned		mask;
-	char			*transport;
-	char			*subsysnqn;
-	char			*traddr;
-	char			*trsvcid;
-	char			*host_traddr;
-	size_t			queue_size;
-	unsigned int		nr_io_queues;
-	unsigned int		reconnect_delay;
-	bool			discovery_nqn;
-	unsigned int		kato;
-	struct nvmf_host	*host;
-	int			max_reconnects;
-};
-
-/*
- * struct nvmf_transport_ops - used to register a specific
- *			       fabric implementation of NVMe fabrics.
- * @entry:		Used by the fabrics library to add the new
- *			registration entry to its linked-list internal tree.
- * @name:		Name of the NVMe fabric driver implementation.
- * @required_opts:	sysfs command-line options that must be specified
- *			when adding a new NVMe controller.
- * @allowed_opts:	sysfs command-line options that can be specified
- *			when adding a new NVMe controller.
- * @create_ctrl():	function pointer that points to a non-NVMe
- *			implementation-specific fabric technology
- *			that would go into starting up that fabric
- *			for the purpose of conneciton to an NVMe controller
- *			using that fabric technology.
- *
- * Notes:
- *	1. At minimum, 'required_opts' and 'allowed_opts' should
- *	   be set to the same enum parsing options defined earlier.
- *	2. create_ctrl() must be defined (even if it does nothing)
- */
-struct nvmf_transport_ops {
-	struct list_head	entry;
-	const char		*name;
-	int			required_opts;
-	int			allowed_opts;
-	struct nvme_ctrl	*(*create_ctrl)(struct device *dev,
-					struct nvmf_ctrl_options *opts);
-};
-
-int nvmf_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val);
-int nvmf_reg_read64(struct nvme_ctrl *ctrl, u32 off, u64 *val);
-int nvmf_reg_write32(struct nvme_ctrl *ctrl, u32 off, u32 val);
-int nvmf_connect_admin_queue(struct nvme_ctrl *ctrl);
-int nvmf_connect_io_queue(struct nvme_ctrl *ctrl, u16 qid);
-int nvmf_register_transport(struct nvmf_transport_ops *ops);
-void nvmf_unregister_transport(struct nvmf_transport_ops *ops);
-void nvmf_free_options(struct nvmf_ctrl_options *opts);
-int nvmf_get_address(struct nvme_ctrl *ctrl, char *buf, int size);
-bool nvmf_should_reconnect(struct nvme_ctrl *ctrl);
-
-static inline blk_status_t nvmf_check_init_req(struct nvme_ctrl *ctrl,
-		struct request *rq)
-{
-	struct nvme_command *cmd = nvme_req(rq)->cmd;
-
-	/*
-	 * We cannot accept any other command until the connect command has
-	 * completed, so only allow connect to pass.
-	 */
-	if (!blk_rq_is_passthrough(rq) ||
-	    cmd->common.opcode != nvme_fabrics_command ||
-	    cmd->fabrics.fctype != nvme_fabrics_type_connect) {
-		/*
-		 * Reconnecting state means transport disruption, which can take
-		 * a long time and even might fail permanently, fail fast to
-		 * give upper layers a chance to failover.
-		 * Deleting state means that the ctrl will never accept commands
-		 * again, fail it permanently.
-		 */
-		if (ctrl->state == NVME_CTRL_RECONNECTING ||
-		    ctrl->state == NVME_CTRL_DELETING) {
-			nvme_req(rq)->status = NVME_SC_ABORT_REQ;
-			return BLK_STS_IOERR;
-		}
-		return BLK_STS_RESOURCE; /* try again later */
-	}
-
-	return BLK_STS_OK;
-}
-
-#endif /* _NVME_FABRICS_H */
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/fc.c b/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/fc.c
deleted file mode 100644
index 058d542..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/fc.c
+++ /dev/null
@@ -1,3067 +0,0 @@
-/*
- * Copyright (c) 2016 Avago Technologies.  All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful.
- * ALL EXPRESS OR IMPLIED CONDITIONS, REPRESENTATIONS AND WARRANTIES,
- * INCLUDING ANY IMPLIED WARRANTY OF MERCHANTABILITY, FITNESS FOR A
- * PARTICULAR PURPOSE, OR NON-INFRINGEMENT, ARE DISCLAIMED, EXCEPT TO
- * THE EXTENT THAT SUCH DISCLAIMERS ARE HELD TO BE LEGALLY INVALID.
- * See the GNU General Public License for more details, a copy of which
- * can be found in the file COPYING included with this package
- *
- */
-#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
-#include <linux/module.h>
-#include <linux/parser.h>
-#include <uapi/scsi/fc/fc_fs.h>
-#include <uapi/scsi/fc/fc_els.h>
-#include <linux/delay.h>
-
-#include "nvme.h"
-#include "fabrics.h"
-#include <linux/nvme-fc-driver.h>
-#include <linux/nvme-fc.h>
-
-
-/* *************************** Data Structures/Defines ****************** */
-
-
-/*
- * We handle AEN commands ourselves and don't even let the
- * block layer know about them.
- */
-#define NVME_FC_NR_AEN_COMMANDS	1
-#define NVME_FC_AQ_BLKMQ_DEPTH	\
-	(NVME_AQ_DEPTH - NVME_FC_NR_AEN_COMMANDS)
-#define AEN_CMDID_BASE		(NVME_FC_AQ_BLKMQ_DEPTH + 1)
-
-enum nvme_fc_queue_flags {
-	NVME_FC_Q_CONNECTED = (1 << 0),
-	NVME_FC_Q_LIVE = (1 << 1),
-};
-
-#define NVMEFC_QUEUE_DELAY	3		/* ms units */
-
-struct nvme_fc_queue {
-	struct nvme_fc_ctrl	*ctrl;
-	struct device		*dev;
-	struct blk_mq_hw_ctx	*hctx;
-	void			*lldd_handle;
-	int			queue_size;
-	size_t			cmnd_capsule_len;
-	u32			qnum;
-	u32			rqcnt;
-	u32			seqno;
-
-	u64			connection_id;
-	atomic_t		csn;
-
-	unsigned long		flags;
-} __aligned(sizeof(u64));	/* alignment for other things alloc'd with */
-
-enum nvme_fcop_flags {
-	FCOP_FLAGS_TERMIO	= (1 << 0),
-	FCOP_FLAGS_RELEASED	= (1 << 1),
-	FCOP_FLAGS_COMPLETE	= (1 << 2),
-	FCOP_FLAGS_AEN		= (1 << 3),
-};
-
-struct nvmefc_ls_req_op {
-	struct nvmefc_ls_req	ls_req;
-
-	struct nvme_fc_rport	*rport;
-	struct nvme_fc_queue	*queue;
-	struct request		*rq;
-	u32			flags;
-
-	int			ls_error;
-	struct completion	ls_done;
-	struct list_head	lsreq_list;	/* rport->ls_req_list */
-	bool			req_queued;
-};
-
-enum nvme_fcpop_state {
-	FCPOP_STATE_UNINIT	= 0,
-	FCPOP_STATE_IDLE	= 1,
-	FCPOP_STATE_ACTIVE	= 2,
-	FCPOP_STATE_ABORTED	= 3,
-	FCPOP_STATE_COMPLETE	= 4,
-};
-
-struct nvme_fc_fcp_op {
-	struct nvme_request	nreq;		/*
-						 * nvme/host/core.c
-						 * requires this to be
-						 * the 1st element in the
-						 * private structure
-						 * associated with the
-						 * request.
-						 */
-	struct nvmefc_fcp_req	fcp_req;
-
-	struct nvme_fc_ctrl	*ctrl;
-	struct nvme_fc_queue	*queue;
-	struct request		*rq;
-
-	atomic_t		state;
-	u32			flags;
-	u32			rqno;
-	u32			nents;
-
-	struct nvme_fc_cmd_iu	cmd_iu;
-	struct nvme_fc_ersp_iu	rsp_iu;
-};
-
-struct nvme_fc_lport {
-	struct nvme_fc_local_port	localport;
-
-	struct ida			endp_cnt;
-	struct list_head		port_list;	/* nvme_fc_port_list */
-	struct list_head		endp_list;
-	struct device			*dev;	/* physical device for dma */
-	struct nvme_fc_port_template	*ops;
-	struct kref			ref;
-} __aligned(sizeof(u64));	/* alignment for other things alloc'd with */
-
-struct nvme_fc_rport {
-	struct nvme_fc_remote_port	remoteport;
-
-	struct list_head		endp_list; /* for lport->endp_list */
-	struct list_head		ctrl_list;
-	struct list_head		ls_req_list;
-	struct device			*dev;	/* physical device for dma */
-	struct nvme_fc_lport		*lport;
-	spinlock_t			lock;
-	struct kref			ref;
-} __aligned(sizeof(u64));	/* alignment for other things alloc'd with */
-
-enum nvme_fcctrl_flags {
-	FCCTRL_TERMIO		= (1 << 0),
-};
-
-struct nvme_fc_ctrl {
-	spinlock_t		lock;
-	struct nvme_fc_queue	*queues;
-	struct device		*dev;
-	struct nvme_fc_lport	*lport;
-	struct nvme_fc_rport	*rport;
-	u32			cnum;
-
-	u64			association_id;
-
-	struct list_head	ctrl_list;	/* rport->ctrl_list */
-
-	struct blk_mq_tag_set	admin_tag_set;
-	struct blk_mq_tag_set	tag_set;
-
-	struct work_struct	delete_work;
-	struct delayed_work	connect_work;
-
-	struct kref		ref;
-	u32			flags;
-	u32			iocnt;
-	wait_queue_head_t	ioabort_wait;
-
-	struct nvme_fc_fcp_op	aen_ops[NVME_FC_NR_AEN_COMMANDS];
-
-	struct nvme_ctrl	ctrl;
-};
-
-static inline struct nvme_fc_ctrl *
-to_fc_ctrl(struct nvme_ctrl *ctrl)
-{
-	return container_of(ctrl, struct nvme_fc_ctrl, ctrl);
-}
-
-static inline struct nvme_fc_lport *
-localport_to_lport(struct nvme_fc_local_port *portptr)
-{
-	return container_of(portptr, struct nvme_fc_lport, localport);
-}
-
-static inline struct nvme_fc_rport *
-remoteport_to_rport(struct nvme_fc_remote_port *portptr)
-{
-	return container_of(portptr, struct nvme_fc_rport, remoteport);
-}
-
-static inline struct nvmefc_ls_req_op *
-ls_req_to_lsop(struct nvmefc_ls_req *lsreq)
-{
-	return container_of(lsreq, struct nvmefc_ls_req_op, ls_req);
-}
-
-static inline struct nvme_fc_fcp_op *
-fcp_req_to_fcp_op(struct nvmefc_fcp_req *fcpreq)
-{
-	return container_of(fcpreq, struct nvme_fc_fcp_op, fcp_req);
-}
-
-
-
-/* *************************** Globals **************************** */
-
-
-static DEFINE_SPINLOCK(nvme_fc_lock);
-
-static LIST_HEAD(nvme_fc_lport_list);
-static DEFINE_IDA(nvme_fc_local_port_cnt);
-static DEFINE_IDA(nvme_fc_ctrl_cnt);
-
-
-
-
-/* *********************** FC-NVME Port Management ************************ */
-
-static int __nvme_fc_del_ctrl(struct nvme_fc_ctrl *);
-static void __nvme_fc_delete_hw_queue(struct nvme_fc_ctrl *,
-			struct nvme_fc_queue *, unsigned int);
-
-static void
-nvme_fc_free_lport(struct kref *ref)
-{
-	struct nvme_fc_lport *lport =
-		container_of(ref, struct nvme_fc_lport, ref);
-	unsigned long flags;
-
-	WARN_ON(lport->localport.port_state != FC_OBJSTATE_DELETED);
-	WARN_ON(!list_empty(&lport->endp_list));
-
-	/* remove from transport list */
-	spin_lock_irqsave(&nvme_fc_lock, flags);
-	list_del(&lport->port_list);
-	spin_unlock_irqrestore(&nvme_fc_lock, flags);
-
-	/* let the LLDD know we've finished tearing it down */
-	lport->ops->localport_delete(&lport->localport);
-
-	ida_simple_remove(&nvme_fc_local_port_cnt, lport->localport.port_num);
-	ida_destroy(&lport->endp_cnt);
-
-	put_device(lport->dev);
-
-	kfree(lport);
-}
-
-static void
-nvme_fc_lport_put(struct nvme_fc_lport *lport)
-{
-	kref_put(&lport->ref, nvme_fc_free_lport);
-}
-
-static int
-nvme_fc_lport_get(struct nvme_fc_lport *lport)
-{
-	return kref_get_unless_zero(&lport->ref);
-}
-
-
-static struct nvme_fc_lport *
-nvme_fc_attach_to_unreg_lport(struct nvme_fc_port_info *pinfo)
-{
-	struct nvme_fc_lport *lport;
-	unsigned long flags;
-
-	spin_lock_irqsave(&nvme_fc_lock, flags);
-
-	list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
-		if (lport->localport.node_name != pinfo->node_name ||
-		    lport->localport.port_name != pinfo->port_name)
-			continue;
-
-		if (lport->localport.port_state != FC_OBJSTATE_DELETED) {
-			lport = ERR_PTR(-EEXIST);
-			goto out_done;
-		}
-
-		if (!nvme_fc_lport_get(lport)) {
-			/*
-			 * fails if ref cnt already 0. If so,
-			 * act as if lport already deleted
-			 */
-			lport = NULL;
-			goto out_done;
-		}
-
-		/* resume the lport */
-
-		lport->localport.port_role = pinfo->port_role;
-		lport->localport.port_id = pinfo->port_id;
-		lport->localport.port_state = FC_OBJSTATE_ONLINE;
-
-		spin_unlock_irqrestore(&nvme_fc_lock, flags);
-
-		return lport;
-	}
-
-	lport = NULL;
-
-out_done:
-	spin_unlock_irqrestore(&nvme_fc_lock, flags);
-
-	return lport;
-}
-
-/**
- * nvme_fc_register_localport - transport entry point called by an
- *                              LLDD to register the existence of a NVME
- *                              host FC port.
- * @pinfo:     pointer to information about the port to be registered
- * @template:  LLDD entrypoints and operational parameters for the port
- * @dev:       physical hardware device node port corresponds to. Will be
- *             used for DMA mappings
- * @lport_p:   pointer to a local port pointer. Upon success, the routine
- *             will allocate a nvme_fc_local_port structure and place its
- *             address in the local port pointer. Upon failure, local port
- *             pointer will be set to 0.
- *
- * Returns:
- * a completion status. Must be 0 upon success; a negative errno
- * (ex: -ENXIO) upon failure.
- */
-int
-nvme_fc_register_localport(struct nvme_fc_port_info *pinfo,
-			struct nvme_fc_port_template *template,
-			struct device *dev,
-			struct nvme_fc_local_port **portptr)
-{
-	struct nvme_fc_lport *newrec;
-	unsigned long flags;
-	int ret, idx;
-
-	if (!template->localport_delete || !template->remoteport_delete ||
-	    !template->ls_req || !template->fcp_io ||
-	    !template->ls_abort || !template->fcp_abort ||
-	    !template->max_hw_queues || !template->max_sgl_segments ||
-	    !template->max_dif_sgl_segments || !template->dma_boundary) {
-		ret = -EINVAL;
-		goto out_reghost_failed;
-	}
-
-	/*
-	 * look to see if there is already a localport that had been
-	 * deregistered and in the process of waiting for all the
-	 * references to fully be removed.  If the references haven't
-	 * expired, we can simply re-enable the localport. Remoteports
-	 * and controller reconnections should resume naturally.
-	 */
-	newrec = nvme_fc_attach_to_unreg_lport(pinfo);
-
-	/* found an lport, but something about its state is bad */
-	if (IS_ERR(newrec)) {
-		ret = PTR_ERR(newrec);
-		goto out_reghost_failed;
-
-	/* found existing lport, which was resumed */
-	} else if (newrec) {
-		*portptr = &newrec->localport;
-		return 0;
-	}
-
-	/* nothing found - allocate a new localport struct */
-
-	newrec = kmalloc((sizeof(*newrec) + template->local_priv_sz),
-			 GFP_KERNEL);
-	if (!newrec) {
-		ret = -ENOMEM;
-		goto out_reghost_failed;
-	}
-
-	idx = ida_simple_get(&nvme_fc_local_port_cnt, 0, 0, GFP_KERNEL);
-	if (idx < 0) {
-		ret = -ENOSPC;
-		goto out_fail_kfree;
-	}
-
-	if (!get_device(dev) && dev) {
-		ret = -ENODEV;
-		goto out_ida_put;
-	}
-
-	INIT_LIST_HEAD(&newrec->port_list);
-	INIT_LIST_HEAD(&newrec->endp_list);
-	kref_init(&newrec->ref);
-	newrec->ops = template;
-	newrec->dev = dev;
-	ida_init(&newrec->endp_cnt);
-	newrec->localport.private = &newrec[1];
-	newrec->localport.node_name = pinfo->node_name;
-	newrec->localport.port_name = pinfo->port_name;
-	newrec->localport.port_role = pinfo->port_role;
-	newrec->localport.port_id = pinfo->port_id;
-	newrec->localport.port_state = FC_OBJSTATE_ONLINE;
-	newrec->localport.port_num = idx;
-
-	spin_lock_irqsave(&nvme_fc_lock, flags);
-	list_add_tail(&newrec->port_list, &nvme_fc_lport_list);
-	spin_unlock_irqrestore(&nvme_fc_lock, flags);
-
-	if (dev)
-		dma_set_seg_boundary(dev, template->dma_boundary);
-
-	*portptr = &newrec->localport;
-	return 0;
-
-out_ida_put:
-	ida_simple_remove(&nvme_fc_local_port_cnt, idx);
-out_fail_kfree:
-	kfree(newrec);
-out_reghost_failed:
-	*portptr = NULL;
-
-	return ret;
-}
-EXPORT_SYMBOL_GPL(nvme_fc_register_localport);
-
-/**
- * nvme_fc_unregister_localport - transport entry point called by an
- *                              LLDD to deregister/remove a previously
- *                              registered a NVME host FC port.
- * @localport: pointer to the (registered) local port that is to be
- *             deregistered.
- *
- * Returns:
- * a completion status. Must be 0 upon success; a negative errno
- * (ex: -ENXIO) upon failure.
- */
-int
-nvme_fc_unregister_localport(struct nvme_fc_local_port *portptr)
-{
-	struct nvme_fc_lport *lport = localport_to_lport(portptr);
-	unsigned long flags;
-
-	if (!portptr)
-		return -EINVAL;
-
-	spin_lock_irqsave(&nvme_fc_lock, flags);
-
-	if (portptr->port_state != FC_OBJSTATE_ONLINE) {
-		spin_unlock_irqrestore(&nvme_fc_lock, flags);
-		return -EINVAL;
-	}
-	portptr->port_state = FC_OBJSTATE_DELETED;
-
-	spin_unlock_irqrestore(&nvme_fc_lock, flags);
-
-	nvme_fc_lport_put(lport);
-
-	return 0;
-}
-EXPORT_SYMBOL_GPL(nvme_fc_unregister_localport);
-
-/**
- * nvme_fc_register_remoteport - transport entry point called by an
- *                              LLDD to register the existence of a NVME
- *                              subsystem FC port on its fabric.
- * @localport: pointer to the (registered) local port that the remote
- *             subsystem port is connected to.
- * @pinfo:     pointer to information about the port to be registered
- * @rport_p:   pointer to a remote port pointer. Upon success, the routine
- *             will allocate a nvme_fc_remote_port structure and place its
- *             address in the remote port pointer. Upon failure, remote port
- *             pointer will be set to 0.
- *
- * Returns:
- * a completion status. Must be 0 upon success; a negative errno
- * (ex: -ENXIO) upon failure.
- */
-int
-nvme_fc_register_remoteport(struct nvme_fc_local_port *localport,
-				struct nvme_fc_port_info *pinfo,
-				struct nvme_fc_remote_port **portptr)
-{
-	struct nvme_fc_lport *lport = localport_to_lport(localport);
-	struct nvme_fc_rport *newrec;
-	unsigned long flags;
-	int ret, idx;
-
-	newrec = kmalloc((sizeof(*newrec) + lport->ops->remote_priv_sz),
-			 GFP_KERNEL);
-	if (!newrec) {
-		ret = -ENOMEM;
-		goto out_reghost_failed;
-	}
-
-	if (!nvme_fc_lport_get(lport)) {
-		ret = -ESHUTDOWN;
-		goto out_kfree_rport;
-	}
-
-	idx = ida_simple_get(&lport->endp_cnt, 0, 0, GFP_KERNEL);
-	if (idx < 0) {
-		ret = -ENOSPC;
-		goto out_lport_put;
-	}
-
-	INIT_LIST_HEAD(&newrec->endp_list);
-	INIT_LIST_HEAD(&newrec->ctrl_list);
-	INIT_LIST_HEAD(&newrec->ls_req_list);
-	kref_init(&newrec->ref);
-	spin_lock_init(&newrec->lock);
-	newrec->remoteport.localport = &lport->localport;
-	newrec->dev = lport->dev;
-	newrec->lport = lport;
-	newrec->remoteport.private = &newrec[1];
-	newrec->remoteport.port_role = pinfo->port_role;
-	newrec->remoteport.node_name = pinfo->node_name;
-	newrec->remoteport.port_name = pinfo->port_name;
-	newrec->remoteport.port_id = pinfo->port_id;
-	newrec->remoteport.port_state = FC_OBJSTATE_ONLINE;
-	newrec->remoteport.port_num = idx;
-
-	spin_lock_irqsave(&nvme_fc_lock, flags);
-	list_add_tail(&newrec->endp_list, &lport->endp_list);
-	spin_unlock_irqrestore(&nvme_fc_lock, flags);
-
-	*portptr = &newrec->remoteport;
-	return 0;
-
-out_lport_put:
-	nvme_fc_lport_put(lport);
-out_kfree_rport:
-	kfree(newrec);
-out_reghost_failed:
-	*portptr = NULL;
-	return ret;
-}
-EXPORT_SYMBOL_GPL(nvme_fc_register_remoteport);
-
-static void
-nvme_fc_free_rport(struct kref *ref)
-{
-	struct nvme_fc_rport *rport =
-		container_of(ref, struct nvme_fc_rport, ref);
-	struct nvme_fc_lport *lport =
-			localport_to_lport(rport->remoteport.localport);
-	unsigned long flags;
-
-	WARN_ON(rport->remoteport.port_state != FC_OBJSTATE_DELETED);
-	WARN_ON(!list_empty(&rport->ctrl_list));
-
-	/* remove from lport list */
-	spin_lock_irqsave(&nvme_fc_lock, flags);
-	list_del(&rport->endp_list);
-	spin_unlock_irqrestore(&nvme_fc_lock, flags);
-
-	/* let the LLDD know we've finished tearing it down */
-	lport->ops->remoteport_delete(&rport->remoteport);
-
-	ida_simple_remove(&lport->endp_cnt, rport->remoteport.port_num);
-
-	kfree(rport);
-
-	nvme_fc_lport_put(lport);
-}
-
-static void
-nvme_fc_rport_put(struct nvme_fc_rport *rport)
-{
-	kref_put(&rport->ref, nvme_fc_free_rport);
-}
-
-static int
-nvme_fc_rport_get(struct nvme_fc_rport *rport)
-{
-	return kref_get_unless_zero(&rport->ref);
-}
-
-static int
-nvme_fc_abort_lsops(struct nvme_fc_rport *rport)
-{
-	struct nvmefc_ls_req_op *lsop;
-	unsigned long flags;
-
-restart:
-	spin_lock_irqsave(&rport->lock, flags);
-
-	list_for_each_entry(lsop, &rport->ls_req_list, lsreq_list) {
-		if (!(lsop->flags & FCOP_FLAGS_TERMIO)) {
-			lsop->flags |= FCOP_FLAGS_TERMIO;
-			spin_unlock_irqrestore(&rport->lock, flags);
-			rport->lport->ops->ls_abort(&rport->lport->localport,
-						&rport->remoteport,
-						&lsop->ls_req);
-			goto restart;
-		}
-	}
-	spin_unlock_irqrestore(&rport->lock, flags);
-
-	return 0;
-}
-
-/**
- * nvme_fc_unregister_remoteport - transport entry point called by an
- *                              LLDD to deregister/remove a previously
- *                              registered a NVME subsystem FC port.
- * @remoteport: pointer to the (registered) remote port that is to be
- *              deregistered.
- *
- * Returns:
- * a completion status. Must be 0 upon success; a negative errno
- * (ex: -ENXIO) upon failure.
- */
-int
-nvme_fc_unregister_remoteport(struct nvme_fc_remote_port *portptr)
-{
-	struct nvme_fc_rport *rport = remoteport_to_rport(portptr);
-	struct nvme_fc_ctrl *ctrl;
-	unsigned long flags;
-
-	if (!portptr)
-		return -EINVAL;
-
-	spin_lock_irqsave(&rport->lock, flags);
-
-	if (portptr->port_state != FC_OBJSTATE_ONLINE) {
-		spin_unlock_irqrestore(&rport->lock, flags);
-		return -EINVAL;
-	}
-	portptr->port_state = FC_OBJSTATE_DELETED;
-
-	/* tear down all associations to the remote port */
-	list_for_each_entry(ctrl, &rport->ctrl_list, ctrl_list)
-		__nvme_fc_del_ctrl(ctrl);
-
-	spin_unlock_irqrestore(&rport->lock, flags);
-
-	nvme_fc_abort_lsops(rport);
-
-	nvme_fc_rport_put(rport);
-	return 0;
-}
-EXPORT_SYMBOL_GPL(nvme_fc_unregister_remoteport);
-
-
-/* *********************** FC-NVME DMA Handling **************************** */
-
-/*
- * The fcloop device passes in a NULL device pointer. Real LLD's will
- * pass in a valid device pointer. If NULL is passed to the dma mapping
- * routines, depending on the platform, it may or may not succeed, and
- * may crash.
- *
- * As such:
- * Wrapper all the dma routines and check the dev pointer.
- *
- * If simple mappings (return just a dma address, we'll noop them,
- * returning a dma address of 0.
- *
- * On more complex mappings (dma_map_sg), a pseudo routine fills
- * in the scatter list, setting all dma addresses to 0.
- */
-
-static inline dma_addr_t
-fc_dma_map_single(struct device *dev, void *ptr, size_t size,
-		enum dma_data_direction dir)
-{
-	return dev ? dma_map_single(dev, ptr, size, dir) : (dma_addr_t)0L;
-}
-
-static inline int
-fc_dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
-{
-	return dev ? dma_mapping_error(dev, dma_addr) : 0;
-}
-
-static inline void
-fc_dma_unmap_single(struct device *dev, dma_addr_t addr, size_t size,
-	enum dma_data_direction dir)
-{
-	if (dev)
-		dma_unmap_single(dev, addr, size, dir);
-}
-
-static inline void
-fc_dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr, size_t size,
-		enum dma_data_direction dir)
-{
-	if (dev)
-		dma_sync_single_for_cpu(dev, addr, size, dir);
-}
-
-static inline void
-fc_dma_sync_single_for_device(struct device *dev, dma_addr_t addr, size_t size,
-		enum dma_data_direction dir)
-{
-	if (dev)
-		dma_sync_single_for_device(dev, addr, size, dir);
-}
-
-/* pseudo dma_map_sg call */
-static int
-fc_map_sg(struct scatterlist *sg, int nents)
-{
-	struct scatterlist *s;
-	int i;
-
-	WARN_ON(nents == 0 || sg[0].length == 0);
-
-	for_each_sg(sg, s, nents, i) {
-		s->dma_address = 0L;
-#ifdef CONFIG_NEED_SG_DMA_LENGTH
-		s->dma_length = s->length;
-#endif
-	}
-	return nents;
-}
-
-static inline int
-fc_dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
-		enum dma_data_direction dir)
-{
-	return dev ? dma_map_sg(dev, sg, nents, dir) : fc_map_sg(sg, nents);
-}
-
-static inline void
-fc_dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,
-		enum dma_data_direction dir)
-{
-	if (dev)
-		dma_unmap_sg(dev, sg, nents, dir);
-}
-
-
-/* *********************** FC-NVME LS Handling **************************** */
-
-static void nvme_fc_ctrl_put(struct nvme_fc_ctrl *);
-static int nvme_fc_ctrl_get(struct nvme_fc_ctrl *);
-
-
-static void
-__nvme_fc_finish_ls_req(struct nvmefc_ls_req_op *lsop)
-{
-	struct nvme_fc_rport *rport = lsop->rport;
-	struct nvmefc_ls_req *lsreq = &lsop->ls_req;
-	unsigned long flags;
-
-	spin_lock_irqsave(&rport->lock, flags);
-
-	if (!lsop->req_queued) {
-		spin_unlock_irqrestore(&rport->lock, flags);
-		return;
-	}
-
-	list_del(&lsop->lsreq_list);
-
-	lsop->req_queued = false;
-
-	spin_unlock_irqrestore(&rport->lock, flags);
-
-	fc_dma_unmap_single(rport->dev, lsreq->rqstdma,
-				  (lsreq->rqstlen + lsreq->rsplen),
-				  DMA_BIDIRECTIONAL);
-
-	nvme_fc_rport_put(rport);
-}
-
-static int
-__nvme_fc_send_ls_req(struct nvme_fc_rport *rport,
-		struct nvmefc_ls_req_op *lsop,
-		void (*done)(struct nvmefc_ls_req *req, int status))
-{
-	struct nvmefc_ls_req *lsreq = &lsop->ls_req;
-	unsigned long flags;
-	int ret = 0;
-
-	if (rport->remoteport.port_state != FC_OBJSTATE_ONLINE)
-		return -ECONNREFUSED;
-
-	if (!nvme_fc_rport_get(rport))
-		return -ESHUTDOWN;
-
-	lsreq->done = done;
-	lsop->rport = rport;
-	lsop->req_queued = false;
-	INIT_LIST_HEAD(&lsop->lsreq_list);
-	init_completion(&lsop->ls_done);
-
-	lsreq->rqstdma = fc_dma_map_single(rport->dev, lsreq->rqstaddr,
-				  lsreq->rqstlen + lsreq->rsplen,
-				  DMA_BIDIRECTIONAL);
-	if (fc_dma_mapping_error(rport->dev, lsreq->rqstdma)) {
-		ret = -EFAULT;
-		goto out_putrport;
-	}
-	lsreq->rspdma = lsreq->rqstdma + lsreq->rqstlen;
-
-	spin_lock_irqsave(&rport->lock, flags);
-
-	list_add_tail(&lsop->lsreq_list, &rport->ls_req_list);
-
-	lsop->req_queued = true;
-
-	spin_unlock_irqrestore(&rport->lock, flags);
-
-	ret = rport->lport->ops->ls_req(&rport->lport->localport,
-					&rport->remoteport, lsreq);
-	if (ret)
-		goto out_unlink;
-
-	return 0;
-
-out_unlink:
-	lsop->ls_error = ret;
-	spin_lock_irqsave(&rport->lock, flags);
-	lsop->req_queued = false;
-	list_del(&lsop->lsreq_list);
-	spin_unlock_irqrestore(&rport->lock, flags);
-	fc_dma_unmap_single(rport->dev, lsreq->rqstdma,
-				  (lsreq->rqstlen + lsreq->rsplen),
-				  DMA_BIDIRECTIONAL);
-out_putrport:
-	nvme_fc_rport_put(rport);
-
-	return ret;
-}
-
-static void
-nvme_fc_send_ls_req_done(struct nvmefc_ls_req *lsreq, int status)
-{
-	struct nvmefc_ls_req_op *lsop = ls_req_to_lsop(lsreq);
-
-	lsop->ls_error = status;
-	complete(&lsop->ls_done);
-}
-
-static int
-nvme_fc_send_ls_req(struct nvme_fc_rport *rport, struct nvmefc_ls_req_op *lsop)
-{
-	struct nvmefc_ls_req *lsreq = &lsop->ls_req;
-	struct fcnvme_ls_rjt *rjt = lsreq->rspaddr;
-	int ret;
-
-	ret = __nvme_fc_send_ls_req(rport, lsop, nvme_fc_send_ls_req_done);
-
-	if (!ret) {
-		/*
-		 * No timeout/not interruptible as we need the struct
-		 * to exist until the lldd calls us back. Thus mandate
-		 * wait until driver calls back. lldd responsible for
-		 * the timeout action
-		 */
-		wait_for_completion(&lsop->ls_done);
-
-		__nvme_fc_finish_ls_req(lsop);
-
-		ret = lsop->ls_error;
-	}
-
-	if (ret)
-		return ret;
-
-	/* ACC or RJT payload ? */
-	if (rjt->w0.ls_cmd == FCNVME_LS_RJT)
-		return -ENXIO;
-
-	return 0;
-}
-
-static int
-nvme_fc_send_ls_req_async(struct nvme_fc_rport *rport,
-		struct nvmefc_ls_req_op *lsop,
-		void (*done)(struct nvmefc_ls_req *req, int status))
-{
-	/* don't wait for completion */
-
-	return __nvme_fc_send_ls_req(rport, lsop, done);
-}
-
-/* Validation Error indexes into the string table below */
-enum {
-	VERR_NO_ERROR		= 0,
-	VERR_LSACC		= 1,
-	VERR_LSDESC_RQST	= 2,
-	VERR_LSDESC_RQST_LEN	= 3,
-	VERR_ASSOC_ID		= 4,
-	VERR_ASSOC_ID_LEN	= 5,
-	VERR_CONN_ID		= 6,
-	VERR_CONN_ID_LEN	= 7,
-	VERR_CR_ASSOC		= 8,
-	VERR_CR_ASSOC_ACC_LEN	= 9,
-	VERR_CR_CONN		= 10,
-	VERR_CR_CONN_ACC_LEN	= 11,
-	VERR_DISCONN		= 12,
-	VERR_DISCONN_ACC_LEN	= 13,
-};
-
-static char *validation_errors[] = {
-	"OK",
-	"Not LS_ACC",
-	"Not LSDESC_RQST",
-	"Bad LSDESC_RQST Length",
-	"Not Association ID",
-	"Bad Association ID Length",
-	"Not Connection ID",
-	"Bad Connection ID Length",
-	"Not CR_ASSOC Rqst",
-	"Bad CR_ASSOC ACC Length",
-	"Not CR_CONN Rqst",
-	"Bad CR_CONN ACC Length",
-	"Not Disconnect Rqst",
-	"Bad Disconnect ACC Length",
-};
-
-static int
-nvme_fc_connect_admin_queue(struct nvme_fc_ctrl *ctrl,
-	struct nvme_fc_queue *queue, u16 qsize, u16 ersp_ratio)
-{
-	struct nvmefc_ls_req_op *lsop;
-	struct nvmefc_ls_req *lsreq;
-	struct fcnvme_ls_cr_assoc_rqst *assoc_rqst;
-	struct fcnvme_ls_cr_assoc_acc *assoc_acc;
-	int ret, fcret = 0;
-
-	lsop = kzalloc((sizeof(*lsop) +
-			 ctrl->lport->ops->lsrqst_priv_sz +
-			 sizeof(*assoc_rqst) + sizeof(*assoc_acc)), GFP_KERNEL);
-	if (!lsop) {
-		ret = -ENOMEM;
-		goto out_no_memory;
-	}
-	lsreq = &lsop->ls_req;
-
-	lsreq->private = (void *)&lsop[1];
-	assoc_rqst = (struct fcnvme_ls_cr_assoc_rqst *)
-			(lsreq->private + ctrl->lport->ops->lsrqst_priv_sz);
-	assoc_acc = (struct fcnvme_ls_cr_assoc_acc *)&assoc_rqst[1];
-
-	assoc_rqst->w0.ls_cmd = FCNVME_LS_CREATE_ASSOCIATION;
-	assoc_rqst->desc_list_len =
-			cpu_to_be32(sizeof(struct fcnvme_lsdesc_cr_assoc_cmd));
-
-	assoc_rqst->assoc_cmd.desc_tag =
-			cpu_to_be32(FCNVME_LSDESC_CREATE_ASSOC_CMD);
-	assoc_rqst->assoc_cmd.desc_len =
-			fcnvme_lsdesc_len(
-				sizeof(struct fcnvme_lsdesc_cr_assoc_cmd));
-
-	assoc_rqst->assoc_cmd.ersp_ratio = cpu_to_be16(ersp_ratio);
-	assoc_rqst->assoc_cmd.sqsize = cpu_to_be16(qsize);
-	/* Linux supports only Dynamic controllers */
-	assoc_rqst->assoc_cmd.cntlid = cpu_to_be16(0xffff);
-	uuid_copy(&assoc_rqst->assoc_cmd.hostid, &ctrl->ctrl.opts->host->id);
-	strncpy(assoc_rqst->assoc_cmd.hostnqn, ctrl->ctrl.opts->host->nqn,
-		min(FCNVME_ASSOC_HOSTNQN_LEN, NVMF_NQN_SIZE));
-	strncpy(assoc_rqst->assoc_cmd.subnqn, ctrl->ctrl.opts->subsysnqn,
-		min(FCNVME_ASSOC_SUBNQN_LEN, NVMF_NQN_SIZE));
-
-	lsop->queue = queue;
-	lsreq->rqstaddr = assoc_rqst;
-	lsreq->rqstlen = sizeof(*assoc_rqst);
-	lsreq->rspaddr = assoc_acc;
-	lsreq->rsplen = sizeof(*assoc_acc);
-	lsreq->timeout = NVME_FC_CONNECT_TIMEOUT_SEC;
-
-	ret = nvme_fc_send_ls_req(ctrl->rport, lsop);
-	if (ret)
-		goto out_free_buffer;
-
-	/* process connect LS completion */
-
-	/* validate the ACC response */
-	if (assoc_acc->hdr.w0.ls_cmd != FCNVME_LS_ACC)
-		fcret = VERR_LSACC;
-	else if (assoc_acc->hdr.desc_list_len !=
-			fcnvme_lsdesc_len(
-				sizeof(struct fcnvme_ls_cr_assoc_acc)))
-		fcret = VERR_CR_ASSOC_ACC_LEN;
-	else if (assoc_acc->hdr.rqst.desc_tag !=
-			cpu_to_be32(FCNVME_LSDESC_RQST))
-		fcret = VERR_LSDESC_RQST;
-	else if (assoc_acc->hdr.rqst.desc_len !=
-			fcnvme_lsdesc_len(sizeof(struct fcnvme_lsdesc_rqst)))
-		fcret = VERR_LSDESC_RQST_LEN;
-	else if (assoc_acc->hdr.rqst.w0.ls_cmd != FCNVME_LS_CREATE_ASSOCIATION)
-		fcret = VERR_CR_ASSOC;
-	else if (assoc_acc->associd.desc_tag !=
-			cpu_to_be32(FCNVME_LSDESC_ASSOC_ID))
-		fcret = VERR_ASSOC_ID;
-	else if (assoc_acc->associd.desc_len !=
-			fcnvme_lsdesc_len(
-				sizeof(struct fcnvme_lsdesc_assoc_id)))
-		fcret = VERR_ASSOC_ID_LEN;
-	else if (assoc_acc->connectid.desc_tag !=
-			cpu_to_be32(FCNVME_LSDESC_CONN_ID))
-		fcret = VERR_CONN_ID;
-	else if (assoc_acc->connectid.desc_len !=
-			fcnvme_lsdesc_len(sizeof(struct fcnvme_lsdesc_conn_id)))
-		fcret = VERR_CONN_ID_LEN;
-
-	if (fcret) {
-		ret = -EBADF;
-		dev_err(ctrl->dev,
-			"q %d connect failed: %s\n",
-			queue->qnum, validation_errors[fcret]);
-	} else {
-		ctrl->association_id =
-			be64_to_cpu(assoc_acc->associd.association_id);
-		queue->connection_id =
-			be64_to_cpu(assoc_acc->connectid.connection_id);
-		set_bit(NVME_FC_Q_CONNECTED, &queue->flags);
-	}
-
-out_free_buffer:
-	kfree(lsop);
-out_no_memory:
-	if (ret)
-		dev_err(ctrl->dev,
-			"queue %d connect admin queue failed (%d).\n",
-			queue->qnum, ret);
-	return ret;
-}
-
-static int
-nvme_fc_connect_queue(struct nvme_fc_ctrl *ctrl, struct nvme_fc_queue *queue,
-			u16 qsize, u16 ersp_ratio)
-{
-	struct nvmefc_ls_req_op *lsop;
-	struct nvmefc_ls_req *lsreq;
-	struct fcnvme_ls_cr_conn_rqst *conn_rqst;
-	struct fcnvme_ls_cr_conn_acc *conn_acc;
-	int ret, fcret = 0;
-
-	lsop = kzalloc((sizeof(*lsop) +
-			 ctrl->lport->ops->lsrqst_priv_sz +
-			 sizeof(*conn_rqst) + sizeof(*conn_acc)), GFP_KERNEL);
-	if (!lsop) {
-		ret = -ENOMEM;
-		goto out_no_memory;
-	}
-	lsreq = &lsop->ls_req;
-
-	lsreq->private = (void *)&lsop[1];
-	conn_rqst = (struct fcnvme_ls_cr_conn_rqst *)
-			(lsreq->private + ctrl->lport->ops->lsrqst_priv_sz);
-	conn_acc = (struct fcnvme_ls_cr_conn_acc *)&conn_rqst[1];
-
-	conn_rqst->w0.ls_cmd = FCNVME_LS_CREATE_CONNECTION;
-	conn_rqst->desc_list_len = cpu_to_be32(
-				sizeof(struct fcnvme_lsdesc_assoc_id) +
-				sizeof(struct fcnvme_lsdesc_cr_conn_cmd));
-
-	conn_rqst->associd.desc_tag = cpu_to_be32(FCNVME_LSDESC_ASSOC_ID);
-	conn_rqst->associd.desc_len =
-			fcnvme_lsdesc_len(
-				sizeof(struct fcnvme_lsdesc_assoc_id));
-	conn_rqst->associd.association_id = cpu_to_be64(ctrl->association_id);
-	conn_rqst->connect_cmd.desc_tag =
-			cpu_to_be32(FCNVME_LSDESC_CREATE_CONN_CMD);
-	conn_rqst->connect_cmd.desc_len =
-			fcnvme_lsdesc_len(
-				sizeof(struct fcnvme_lsdesc_cr_conn_cmd));
-	conn_rqst->connect_cmd.ersp_ratio = cpu_to_be16(ersp_ratio);
-	conn_rqst->connect_cmd.qid  = cpu_to_be16(queue->qnum);
-	conn_rqst->connect_cmd.sqsize = cpu_to_be16(qsize);
-
-	lsop->queue = queue;
-	lsreq->rqstaddr = conn_rqst;
-	lsreq->rqstlen = sizeof(*conn_rqst);
-	lsreq->rspaddr = conn_acc;
-	lsreq->rsplen = sizeof(*conn_acc);
-	lsreq->timeout = NVME_FC_CONNECT_TIMEOUT_SEC;
-
-	ret = nvme_fc_send_ls_req(ctrl->rport, lsop);
-	if (ret)
-		goto out_free_buffer;
-
-	/* process connect LS completion */
-
-	/* validate the ACC response */
-	if (conn_acc->hdr.w0.ls_cmd != FCNVME_LS_ACC)
-		fcret = VERR_LSACC;
-	else if (conn_acc->hdr.desc_list_len !=
-			fcnvme_lsdesc_len(sizeof(struct fcnvme_ls_cr_conn_acc)))
-		fcret = VERR_CR_CONN_ACC_LEN;
-	else if (conn_acc->hdr.rqst.desc_tag != cpu_to_be32(FCNVME_LSDESC_RQST))
-		fcret = VERR_LSDESC_RQST;
-	else if (conn_acc->hdr.rqst.desc_len !=
-			fcnvme_lsdesc_len(sizeof(struct fcnvme_lsdesc_rqst)))
-		fcret = VERR_LSDESC_RQST_LEN;
-	else if (conn_acc->hdr.rqst.w0.ls_cmd != FCNVME_LS_CREATE_CONNECTION)
-		fcret = VERR_CR_CONN;
-	else if (conn_acc->connectid.desc_tag !=
-			cpu_to_be32(FCNVME_LSDESC_CONN_ID))
-		fcret = VERR_CONN_ID;
-	else if (conn_acc->connectid.desc_len !=
-			fcnvme_lsdesc_len(sizeof(struct fcnvme_lsdesc_conn_id)))
-		fcret = VERR_CONN_ID_LEN;
-
-	if (fcret) {
-		ret = -EBADF;
-		dev_err(ctrl->dev,
-			"q %d connect failed: %s\n",
-			queue->qnum, validation_errors[fcret]);
-	} else {
-		queue->connection_id =
-			be64_to_cpu(conn_acc->connectid.connection_id);
-		set_bit(NVME_FC_Q_CONNECTED, &queue->flags);
-	}
-
-out_free_buffer:
-	kfree(lsop);
-out_no_memory:
-	if (ret)
-		dev_err(ctrl->dev,
-			"queue %d connect command failed (%d).\n",
-			queue->qnum, ret);
-	return ret;
-}
-
-static void
-nvme_fc_disconnect_assoc_done(struct nvmefc_ls_req *lsreq, int status)
-{
-	struct nvmefc_ls_req_op *lsop = ls_req_to_lsop(lsreq);
-
-	__nvme_fc_finish_ls_req(lsop);
-
-	/* fc-nvme iniator doesn't care about success or failure of cmd */
-
-	kfree(lsop);
-}
-
-/*
- * This routine sends a FC-NVME LS to disconnect (aka terminate)
- * the FC-NVME Association.  Terminating the association also
- * terminates the FC-NVME connections (per queue, both admin and io
- * queues) that are part of the association. E.g. things are torn
- * down, and the related FC-NVME Association ID and Connection IDs
- * become invalid.
- *
- * The behavior of the fc-nvme initiator is such that it's
- * understanding of the association and connections will implicitly
- * be torn down. The action is implicit as it may be due to a loss of
- * connectivity with the fc-nvme target, so you may never get a
- * response even if you tried.  As such, the action of this routine
- * is to asynchronously send the LS, ignore any results of the LS, and
- * continue on with terminating the association. If the fc-nvme target
- * is present and receives the LS, it too can tear down.
- */
-static void
-nvme_fc_xmt_disconnect_assoc(struct nvme_fc_ctrl *ctrl)
-{
-	struct fcnvme_ls_disconnect_rqst *discon_rqst;
-	struct fcnvme_ls_disconnect_acc *discon_acc;
-	struct nvmefc_ls_req_op *lsop;
-	struct nvmefc_ls_req *lsreq;
-	int ret;
-
-	lsop = kzalloc((sizeof(*lsop) +
-			 ctrl->lport->ops->lsrqst_priv_sz +
-			 sizeof(*discon_rqst) + sizeof(*discon_acc)),
-			GFP_KERNEL);
-	if (!lsop)
-		/* couldn't sent it... too bad */
-		return;
-
-	lsreq = &lsop->ls_req;
-
-	lsreq->private = (void *)&lsop[1];
-	discon_rqst = (struct fcnvme_ls_disconnect_rqst *)
-			(lsreq->private + ctrl->lport->ops->lsrqst_priv_sz);
-	discon_acc = (struct fcnvme_ls_disconnect_acc *)&discon_rqst[1];
-
-	discon_rqst->w0.ls_cmd = FCNVME_LS_DISCONNECT;
-	discon_rqst->desc_list_len = cpu_to_be32(
-				sizeof(struct fcnvme_lsdesc_assoc_id) +
-				sizeof(struct fcnvme_lsdesc_disconn_cmd));
-
-	discon_rqst->associd.desc_tag = cpu_to_be32(FCNVME_LSDESC_ASSOC_ID);
-	discon_rqst->associd.desc_len =
-			fcnvme_lsdesc_len(
-				sizeof(struct fcnvme_lsdesc_assoc_id));
-
-	discon_rqst->associd.association_id = cpu_to_be64(ctrl->association_id);
-
-	discon_rqst->discon_cmd.desc_tag = cpu_to_be32(
-						FCNVME_LSDESC_DISCONN_CMD);
-	discon_rqst->discon_cmd.desc_len =
-			fcnvme_lsdesc_len(
-				sizeof(struct fcnvme_lsdesc_disconn_cmd));
-	discon_rqst->discon_cmd.scope = FCNVME_DISCONN_ASSOCIATION;
-	discon_rqst->discon_cmd.id = cpu_to_be64(ctrl->association_id);
-
-	lsreq->rqstaddr = discon_rqst;
-	lsreq->rqstlen = sizeof(*discon_rqst);
-	lsreq->rspaddr = discon_acc;
-	lsreq->rsplen = sizeof(*discon_acc);
-	lsreq->timeout = NVME_FC_CONNECT_TIMEOUT_SEC;
-
-	ret = nvme_fc_send_ls_req_async(ctrl->rport, lsop,
-				nvme_fc_disconnect_assoc_done);
-	if (ret)
-		kfree(lsop);
-
-	/* only meaningful part to terminating the association */
-	ctrl->association_id = 0;
-}
-
-
-/* *********************** NVME Ctrl Routines **************************** */
-
-static void __nvme_fc_final_op_cleanup(struct request *rq);
-static void nvme_fc_error_recovery(struct nvme_fc_ctrl *ctrl, char *errmsg);
-
-static int
-nvme_fc_reinit_request(void *data, struct request *rq)
-{
-	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
-	struct nvme_fc_cmd_iu *cmdiu = &op->cmd_iu;
-
-	memset(cmdiu, 0, sizeof(*cmdiu));
-	cmdiu->scsi_id = NVME_CMD_SCSI_ID;
-	cmdiu->fc_id = NVME_CMD_FC_ID;
-	cmdiu->iu_len = cpu_to_be16(sizeof(*cmdiu) / sizeof(u32));
-	memset(&op->rsp_iu, 0, sizeof(op->rsp_iu));
-
-	return 0;
-}
-
-static void
-__nvme_fc_exit_request(struct nvme_fc_ctrl *ctrl,
-		struct nvme_fc_fcp_op *op)
-{
-	fc_dma_unmap_single(ctrl->lport->dev, op->fcp_req.rspdma,
-				sizeof(op->rsp_iu), DMA_FROM_DEVICE);
-	fc_dma_unmap_single(ctrl->lport->dev, op->fcp_req.cmddma,
-				sizeof(op->cmd_iu), DMA_TO_DEVICE);
-
-	atomic_set(&op->state, FCPOP_STATE_UNINIT);
-}
-
-static void
-nvme_fc_exit_request(struct blk_mq_tag_set *set, struct request *rq,
-		unsigned int hctx_idx)
-{
-	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
-
-	return __nvme_fc_exit_request(set->driver_data, op);
-}
-
-static int
-__nvme_fc_abort_op(struct nvme_fc_ctrl *ctrl, struct nvme_fc_fcp_op *op)
-{
-	int state;
-
-	state = atomic_xchg(&op->state, FCPOP_STATE_ABORTED);
-	if (state != FCPOP_STATE_ACTIVE) {
-		atomic_set(&op->state, state);
-		return -ECANCELED;
-	}
-
-	ctrl->lport->ops->fcp_abort(&ctrl->lport->localport,
-					&ctrl->rport->remoteport,
-					op->queue->lldd_handle,
-					&op->fcp_req);
-
-	return 0;
-}
-
-static void
-nvme_fc_abort_aen_ops(struct nvme_fc_ctrl *ctrl)
-{
-	struct nvme_fc_fcp_op *aen_op = ctrl->aen_ops;
-	unsigned long flags;
-	int i, ret;
-
-	for (i = 0; i < NVME_FC_NR_AEN_COMMANDS; i++, aen_op++) {
-		if (atomic_read(&aen_op->state) != FCPOP_STATE_ACTIVE)
-			continue;
-
-		spin_lock_irqsave(&ctrl->lock, flags);
-		if (ctrl->flags & FCCTRL_TERMIO) {
-			ctrl->iocnt++;
-			aen_op->flags |= FCOP_FLAGS_TERMIO;
-		}
-		spin_unlock_irqrestore(&ctrl->lock, flags);
-
-		ret = __nvme_fc_abort_op(ctrl, aen_op);
-		if (ret) {
-			/*
-			 * if __nvme_fc_abort_op failed the io wasn't
-			 * active. Thus this call path is running in
-			 * parallel to the io complete. Treat as non-error.
-			 */
-
-			/* back out the flags/counters */
-			spin_lock_irqsave(&ctrl->lock, flags);
-			if (ctrl->flags & FCCTRL_TERMIO)
-				ctrl->iocnt--;
-			aen_op->flags &= ~FCOP_FLAGS_TERMIO;
-			spin_unlock_irqrestore(&ctrl->lock, flags);
-			return;
-		}
-	}
-}
-
-static inline int
-__nvme_fc_fcpop_chk_teardowns(struct nvme_fc_ctrl *ctrl,
-		struct nvme_fc_fcp_op *op)
-{
-	unsigned long flags;
-	bool complete_rq = false;
-
-	spin_lock_irqsave(&ctrl->lock, flags);
-	if (unlikely(op->flags & FCOP_FLAGS_TERMIO)) {
-		if (ctrl->flags & FCCTRL_TERMIO) {
-			if (!--ctrl->iocnt)
-				wake_up(&ctrl->ioabort_wait);
-		}
-	}
-	if (op->flags & FCOP_FLAGS_RELEASED)
-		complete_rq = true;
-	else
-		op->flags |= FCOP_FLAGS_COMPLETE;
-	spin_unlock_irqrestore(&ctrl->lock, flags);
-
-	return complete_rq;
-}
-
-static void
-nvme_fc_fcpio_done(struct nvmefc_fcp_req *req)
-{
-	struct nvme_fc_fcp_op *op = fcp_req_to_fcp_op(req);
-	struct request *rq = op->rq;
-	struct nvmefc_fcp_req *freq = &op->fcp_req;
-	struct nvme_fc_ctrl *ctrl = op->ctrl;
-	struct nvme_fc_queue *queue = op->queue;
-	struct nvme_completion *cqe = &op->rsp_iu.cqe;
-	struct nvme_command *sqe = &op->cmd_iu.sqe;
-	__le16 status = cpu_to_le16(NVME_SC_SUCCESS << 1);
-	union nvme_result result;
-	bool complete_rq, terminate_assoc = true;
-
-	/*
-	 * WARNING:
-	 * The current linux implementation of a nvme controller
-	 * allocates a single tag set for all io queues and sizes
-	 * the io queues to fully hold all possible tags. Thus, the
-	 * implementation does not reference or care about the sqhd
-	 * value as it never needs to use the sqhd/sqtail pointers
-	 * for submission pacing.
-	 *
-	 * This affects the FC-NVME implementation in two ways:
-	 * 1) As the value doesn't matter, we don't need to waste
-	 *    cycles extracting it from ERSPs and stamping it in the
-	 *    cases where the transport fabricates CQEs on successful
-	 *    completions.
-	 * 2) The FC-NVME implementation requires that delivery of
-	 *    ERSP completions are to go back to the nvme layer in order
-	 *    relative to the rsn, such that the sqhd value will always
-	 *    be "in order" for the nvme layer. As the nvme layer in
-	 *    linux doesn't care about sqhd, there's no need to return
-	 *    them in order.
-	 *
-	 * Additionally:
-	 * As the core nvme layer in linux currently does not look at
-	 * every field in the cqe - in cases where the FC transport must
-	 * fabricate a CQE, the following fields will not be set as they
-	 * are not referenced:
-	 *      cqe.sqid,  cqe.sqhd,  cqe.command_id
-	 *
-	 * Failure or error of an individual i/o, in a transport
-	 * detected fashion unrelated to the nvme completion status,
-	 * potentially cause the initiator and target sides to get out
-	 * of sync on SQ head/tail (aka outstanding io count allowed).
-	 * Per FC-NVME spec, failure of an individual command requires
-	 * the connection to be terminated, which in turn requires the
-	 * association to be terminated.
-	 */
-
-	fc_dma_sync_single_for_cpu(ctrl->lport->dev, op->fcp_req.rspdma,
-				sizeof(op->rsp_iu), DMA_FROM_DEVICE);
-
-	if (atomic_read(&op->state) == FCPOP_STATE_ABORTED)
-		status = cpu_to_le16((NVME_SC_ABORT_REQ | NVME_SC_DNR) << 1);
-	else if (freq->status)
-		status = cpu_to_le16(NVME_SC_INTERNAL << 1);
-
-	/*
-	 * For the linux implementation, if we have an unsuccesful
-	 * status, they blk-mq layer can typically be called with the
-	 * non-zero status and the content of the cqe isn't important.
-	 */
-	if (status)
-		goto done;
-
-	/*
-	 * command completed successfully relative to the wire
-	 * protocol. However, validate anything received and
-	 * extract the status and result from the cqe (create it
-	 * where necessary).
-	 */
-
-	switch (freq->rcv_rsplen) {
-
-	case 0:
-	case NVME_FC_SIZEOF_ZEROS_RSP:
-		/*
-		 * No response payload or 12 bytes of payload (which
-		 * should all be zeros) are considered successful and
-		 * no payload in the CQE by the transport.
-		 */
-		if (freq->transferred_length !=
-			be32_to_cpu(op->cmd_iu.data_len)) {
-			status = cpu_to_le16(NVME_SC_INTERNAL << 1);
-			goto done;
-		}
-		result.u64 = 0;
-		break;
-
-	case sizeof(struct nvme_fc_ersp_iu):
-		/*
-		 * The ERSP IU contains a full completion with CQE.
-		 * Validate ERSP IU and look at cqe.
-		 */
-		if (unlikely(be16_to_cpu(op->rsp_iu.iu_len) !=
-					(freq->rcv_rsplen / 4) ||
-			     be32_to_cpu(op->rsp_iu.xfrd_len) !=
-					freq->transferred_length ||
-			     op->rsp_iu.status_code ||
-			     sqe->common.command_id != cqe->command_id)) {
-			status = cpu_to_le16(NVME_SC_INTERNAL << 1);
-			goto done;
-		}
-		result = cqe->result;
-		status = cqe->status;
-		break;
-
-	default:
-		status = cpu_to_le16(NVME_SC_INTERNAL << 1);
-		goto done;
-	}
-
-	terminate_assoc = false;
-
-done:
-	if (op->flags & FCOP_FLAGS_AEN) {
-		nvme_complete_async_event(&queue->ctrl->ctrl, status, &result);
-		complete_rq = __nvme_fc_fcpop_chk_teardowns(ctrl, op);
-		atomic_set(&op->state, FCPOP_STATE_IDLE);
-		op->flags = FCOP_FLAGS_AEN;	/* clear other flags */
-		nvme_fc_ctrl_put(ctrl);
-		goto check_error;
-	}
-
-	complete_rq = __nvme_fc_fcpop_chk_teardowns(ctrl, op);
-	if (!complete_rq) {
-		if (unlikely(op->flags & FCOP_FLAGS_TERMIO)) {
-			status = cpu_to_le16(NVME_SC_ABORT_REQ << 1);
-			if (blk_queue_dying(rq->q))
-				status |= cpu_to_le16(NVME_SC_DNR << 1);
-		}
-		nvme_end_request(rq, status, result);
-	} else
-		__nvme_fc_final_op_cleanup(rq);
-
-check_error:
-	if (terminate_assoc)
-		nvme_fc_error_recovery(ctrl, "transport detected io error");
-}
-
-static int
-__nvme_fc_init_request(struct nvme_fc_ctrl *ctrl,
-		struct nvme_fc_queue *queue, struct nvme_fc_fcp_op *op,
-		struct request *rq, u32 rqno)
-{
-	struct nvme_fc_cmd_iu *cmdiu = &op->cmd_iu;
-	int ret = 0;
-
-	memset(op, 0, sizeof(*op));
-	op->fcp_req.cmdaddr = &op->cmd_iu;
-	op->fcp_req.cmdlen = sizeof(op->cmd_iu);
-	op->fcp_req.rspaddr = &op->rsp_iu;
-	op->fcp_req.rsplen = sizeof(op->rsp_iu);
-	op->fcp_req.done = nvme_fc_fcpio_done;
-	op->fcp_req.first_sgl = (struct scatterlist *)&op[1];
-	op->fcp_req.private = &op->fcp_req.first_sgl[SG_CHUNK_SIZE];
-	op->ctrl = ctrl;
-	op->queue = queue;
-	op->rq = rq;
-	op->rqno = rqno;
-
-	cmdiu->scsi_id = NVME_CMD_SCSI_ID;
-	cmdiu->fc_id = NVME_CMD_FC_ID;
-	cmdiu->iu_len = cpu_to_be16(sizeof(*cmdiu) / sizeof(u32));
-
-	op->fcp_req.cmddma = fc_dma_map_single(ctrl->lport->dev,
-				&op->cmd_iu, sizeof(op->cmd_iu), DMA_TO_DEVICE);
-	if (fc_dma_mapping_error(ctrl->lport->dev, op->fcp_req.cmddma)) {
-		dev_err(ctrl->dev,
-			"FCP Op failed - cmdiu dma mapping failed.\n");
-		ret = EFAULT;
-		goto out_on_error;
-	}
-
-	op->fcp_req.rspdma = fc_dma_map_single(ctrl->lport->dev,
-				&op->rsp_iu, sizeof(op->rsp_iu),
-				DMA_FROM_DEVICE);
-	if (fc_dma_mapping_error(ctrl->lport->dev, op->fcp_req.rspdma)) {
-		dev_err(ctrl->dev,
-			"FCP Op failed - rspiu dma mapping failed.\n");
-		ret = EFAULT;
-	}
-
-	atomic_set(&op->state, FCPOP_STATE_IDLE);
-out_on_error:
-	return ret;
-}
-
-static int
-nvme_fc_init_request(struct blk_mq_tag_set *set, struct request *rq,
-		unsigned int hctx_idx, unsigned int numa_node)
-{
-	struct nvme_fc_ctrl *ctrl = set->driver_data;
-	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
-	int queue_idx = (set == &ctrl->tag_set) ? hctx_idx + 1 : 0;
-	struct nvme_fc_queue *queue = &ctrl->queues[queue_idx];
-
-	return __nvme_fc_init_request(ctrl, queue, op, rq, queue->rqcnt++);
-}
-
-static int
-nvme_fc_init_aen_ops(struct nvme_fc_ctrl *ctrl)
-{
-	struct nvme_fc_fcp_op *aen_op;
-	struct nvme_fc_cmd_iu *cmdiu;
-	struct nvme_command *sqe;
-	void *private;
-	int i, ret;
-
-	aen_op = ctrl->aen_ops;
-	for (i = 0; i < NVME_FC_NR_AEN_COMMANDS; i++, aen_op++) {
-		private = kzalloc(ctrl->lport->ops->fcprqst_priv_sz,
-						GFP_KERNEL);
-		if (!private)
-			return -ENOMEM;
-
-		cmdiu = &aen_op->cmd_iu;
-		sqe = &cmdiu->sqe;
-		ret = __nvme_fc_init_request(ctrl, &ctrl->queues[0],
-				aen_op, (struct request *)NULL,
-				(AEN_CMDID_BASE + i));
-		if (ret) {
-			kfree(private);
-			return ret;
-		}
-
-		aen_op->flags = FCOP_FLAGS_AEN;
-		aen_op->fcp_req.first_sgl = NULL; /* no sg list */
-		aen_op->fcp_req.private = private;
-
-		memset(sqe, 0, sizeof(*sqe));
-		sqe->common.opcode = nvme_admin_async_event;
-		/* Note: core layer may overwrite the sqe.command_id value */
-		sqe->common.command_id = AEN_CMDID_BASE + i;
-	}
-	return 0;
-}
-
-static void
-nvme_fc_term_aen_ops(struct nvme_fc_ctrl *ctrl)
-{
-	struct nvme_fc_fcp_op *aen_op;
-	int i;
-
-	aen_op = ctrl->aen_ops;
-	for (i = 0; i < NVME_FC_NR_AEN_COMMANDS; i++, aen_op++) {
-		if (!aen_op->fcp_req.private)
-			continue;
-
-		__nvme_fc_exit_request(ctrl, aen_op);
-
-		kfree(aen_op->fcp_req.private);
-		aen_op->fcp_req.private = NULL;
-	}
-}
-
-static inline void
-__nvme_fc_init_hctx(struct blk_mq_hw_ctx *hctx, struct nvme_fc_ctrl *ctrl,
-		unsigned int qidx)
-{
-	struct nvme_fc_queue *queue = &ctrl->queues[qidx];
-
-	hctx->driver_data = queue;
-	queue->hctx = hctx;
-}
-
-static int
-nvme_fc_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
-		unsigned int hctx_idx)
-{
-	struct nvme_fc_ctrl *ctrl = data;
-
-	__nvme_fc_init_hctx(hctx, ctrl, hctx_idx + 1);
-
-	return 0;
-}
-
-static int
-nvme_fc_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
-		unsigned int hctx_idx)
-{
-	struct nvme_fc_ctrl *ctrl = data;
-
-	__nvme_fc_init_hctx(hctx, ctrl, hctx_idx);
-
-	return 0;
-}
-
-static void
-nvme_fc_init_queue(struct nvme_fc_ctrl *ctrl, int idx, size_t queue_size)
-{
-	struct nvme_fc_queue *queue;
-
-	queue = &ctrl->queues[idx];
-	memset(queue, 0, sizeof(*queue));
-	queue->ctrl = ctrl;
-	queue->qnum = idx;
-	atomic_set(&queue->csn, 1);
-	queue->dev = ctrl->dev;
-
-	if (idx > 0)
-		queue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;
-	else
-		queue->cmnd_capsule_len = sizeof(struct nvme_command);
-
-	queue->queue_size = queue_size;
-
-	/*
-	 * Considered whether we should allocate buffers for all SQEs
-	 * and CQEs and dma map them - mapping their respective entries
-	 * into the request structures (kernel vm addr and dma address)
-	 * thus the driver could use the buffers/mappings directly.
-	 * It only makes sense if the LLDD would use them for its
-	 * messaging api. It's very unlikely most adapter api's would use
-	 * a native NVME sqe/cqe. More reasonable if FC-NVME IU payload
-	 * structures were used instead.
-	 */
-}
-
-/*
- * This routine terminates a queue at the transport level.
- * The transport has already ensured that all outstanding ios on
- * the queue have been terminated.
- * The transport will send a Disconnect LS request to terminate
- * the queue's connection. Termination of the admin queue will also
- * terminate the association at the target.
- */
-static void
-nvme_fc_free_queue(struct nvme_fc_queue *queue)
-{
-	if (!test_and_clear_bit(NVME_FC_Q_CONNECTED, &queue->flags))
-		return;
-
-	clear_bit(NVME_FC_Q_LIVE, &queue->flags);
-	/*
-	 * Current implementation never disconnects a single queue.
-	 * It always terminates a whole association. So there is never
-	 * a disconnect(queue) LS sent to the target.
-	 */
-
-	queue->connection_id = 0;
-}
-
-static void
-__nvme_fc_delete_hw_queue(struct nvme_fc_ctrl *ctrl,
-	struct nvme_fc_queue *queue, unsigned int qidx)
-{
-	if (ctrl->lport->ops->delete_queue)
-		ctrl->lport->ops->delete_queue(&ctrl->lport->localport, qidx,
-				queue->lldd_handle);
-	queue->lldd_handle = NULL;
-}
-
-static void
-nvme_fc_free_io_queues(struct nvme_fc_ctrl *ctrl)
-{
-	int i;
-
-	for (i = 1; i < ctrl->ctrl.queue_count; i++)
-		nvme_fc_free_queue(&ctrl->queues[i]);
-}
-
-static int
-__nvme_fc_create_hw_queue(struct nvme_fc_ctrl *ctrl,
-	struct nvme_fc_queue *queue, unsigned int qidx, u16 qsize)
-{
-	int ret = 0;
-
-	queue->lldd_handle = NULL;
-	if (ctrl->lport->ops->create_queue)
-		ret = ctrl->lport->ops->create_queue(&ctrl->lport->localport,
-				qidx, qsize, &queue->lldd_handle);
-
-	return ret;
-}
-
-static void
-nvme_fc_delete_hw_io_queues(struct nvme_fc_ctrl *ctrl)
-{
-	struct nvme_fc_queue *queue = &ctrl->queues[ctrl->ctrl.queue_count - 1];
-	int i;
-
-	for (i = ctrl->ctrl.queue_count - 1; i >= 1; i--, queue--)
-		__nvme_fc_delete_hw_queue(ctrl, queue, i);
-}
-
-static int
-nvme_fc_create_hw_io_queues(struct nvme_fc_ctrl *ctrl, u16 qsize)
-{
-	struct nvme_fc_queue *queue = &ctrl->queues[1];
-	int i, ret;
-
-	for (i = 1; i < ctrl->ctrl.queue_count; i++, queue++) {
-		ret = __nvme_fc_create_hw_queue(ctrl, queue, i, qsize);
-		if (ret)
-			goto delete_queues;
-	}
-
-	return 0;
-
-delete_queues:
-	for (; i >= 0; i--)
-		__nvme_fc_delete_hw_queue(ctrl, &ctrl->queues[i], i);
-	return ret;
-}
-
-static int
-nvme_fc_connect_io_queues(struct nvme_fc_ctrl *ctrl, u16 qsize)
-{
-	int i, ret = 0;
-
-	for (i = 1; i < ctrl->ctrl.queue_count; i++) {
-		ret = nvme_fc_connect_queue(ctrl, &ctrl->queues[i], qsize,
-					(qsize / 5));
-		if (ret)
-			break;
-		ret = nvmf_connect_io_queue(&ctrl->ctrl, i);
-		if (ret)
-			break;
-
-		set_bit(NVME_FC_Q_LIVE, &ctrl->queues[i].flags);
-	}
-
-	return ret;
-}
-
-static void
-nvme_fc_init_io_queues(struct nvme_fc_ctrl *ctrl)
-{
-	int i;
-
-	for (i = 1; i < ctrl->ctrl.queue_count; i++)
-		nvme_fc_init_queue(ctrl, i, ctrl->ctrl.sqsize);
-}
-
-static void
-nvme_fc_ctrl_free(struct kref *ref)
-{
-	struct nvme_fc_ctrl *ctrl =
-		container_of(ref, struct nvme_fc_ctrl, ref);
-	unsigned long flags;
-
-	if (ctrl->ctrl.tagset) {
-		blk_cleanup_queue(ctrl->ctrl.connect_q);
-		blk_mq_free_tag_set(&ctrl->tag_set);
-	}
-
-	/* remove from rport list */
-	spin_lock_irqsave(&ctrl->rport->lock, flags);
-	list_del(&ctrl->ctrl_list);
-	spin_unlock_irqrestore(&ctrl->rport->lock, flags);
-
-	blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
-	blk_cleanup_queue(ctrl->ctrl.admin_q);
-	blk_mq_free_tag_set(&ctrl->admin_tag_set);
-
-	kfree(ctrl->queues);
-
-	put_device(ctrl->dev);
-	nvme_fc_rport_put(ctrl->rport);
-
-	ida_simple_remove(&nvme_fc_ctrl_cnt, ctrl->cnum);
-	if (ctrl->ctrl.opts)
-		nvmf_free_options(ctrl->ctrl.opts);
-	kfree(ctrl);
-}
-
-static void
-nvme_fc_ctrl_put(struct nvme_fc_ctrl *ctrl)
-{
-	kref_put(&ctrl->ref, nvme_fc_ctrl_free);
-}
-
-static int
-nvme_fc_ctrl_get(struct nvme_fc_ctrl *ctrl)
-{
-	return kref_get_unless_zero(&ctrl->ref);
-}
-
-/*
- * All accesses from nvme core layer done - can now free the
- * controller. Called after last nvme_put_ctrl() call
- */
-static void
-nvme_fc_nvme_ctrl_freed(struct nvme_ctrl *nctrl)
-{
-	struct nvme_fc_ctrl *ctrl = to_fc_ctrl(nctrl);
-
-	WARN_ON(nctrl != &ctrl->ctrl);
-
-	nvme_fc_ctrl_put(ctrl);
-}
-
-static void
-nvme_fc_error_recovery(struct nvme_fc_ctrl *ctrl, char *errmsg)
-{
-	/* only proceed if in LIVE state - e.g. on first error */
-	if (ctrl->ctrl.state != NVME_CTRL_LIVE)
-		return;
-
-	dev_warn(ctrl->ctrl.device,
-		"NVME-FC{%d}: transport association error detected: %s\n",
-		ctrl->cnum, errmsg);
-	dev_warn(ctrl->ctrl.device,
-		"NVME-FC{%d}: resetting controller\n", ctrl->cnum);
-
-	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RECONNECTING)) {
-		dev_err(ctrl->ctrl.device,
-			"NVME-FC{%d}: error_recovery: Couldn't change state "
-			"to RECONNECTING\n", ctrl->cnum);
-		return;
-	}
-
-	nvme_reset_ctrl(&ctrl->ctrl);
-}
-
-static enum blk_eh_timer_return
-nvme_fc_timeout(struct request *rq, bool reserved)
-{
-	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
-	struct nvme_fc_ctrl *ctrl = op->ctrl;
-	int ret;
-
-	if (reserved)
-		return BLK_EH_RESET_TIMER;
-
-	ret = __nvme_fc_abort_op(ctrl, op);
-	if (ret)
-		/* io wasn't active to abort consider it done */
-		return BLK_EH_HANDLED;
-
-	/*
-	 * we can't individually ABTS an io without affecting the queue,
-	 * thus killing the queue, adn thus the association.
-	 * So resolve by performing a controller reset, which will stop
-	 * the host/io stack, terminate the association on the link,
-	 * and recreate an association on the link.
-	 */
-	nvme_fc_error_recovery(ctrl, "io timeout error");
-
-	return BLK_EH_HANDLED;
-}
-
-static int
-nvme_fc_map_data(struct nvme_fc_ctrl *ctrl, struct request *rq,
-		struct nvme_fc_fcp_op *op)
-{
-	struct nvmefc_fcp_req *freq = &op->fcp_req;
-	enum dma_data_direction dir;
-	int ret;
-
-	freq->sg_cnt = 0;
-
-	if (!blk_rq_payload_bytes(rq))
-		return 0;
-
-	freq->sg_table.sgl = freq->first_sgl;
-	ret = sg_alloc_table_chained(&freq->sg_table,
-			blk_rq_nr_phys_segments(rq), freq->sg_table.sgl);
-	if (ret)
-		return -ENOMEM;
-
-	op->nents = blk_rq_map_sg(rq->q, rq, freq->sg_table.sgl);
-	WARN_ON(op->nents > blk_rq_nr_phys_segments(rq));
-	dir = (rq_data_dir(rq) == WRITE) ? DMA_TO_DEVICE : DMA_FROM_DEVICE;
-	freq->sg_cnt = fc_dma_map_sg(ctrl->lport->dev, freq->sg_table.sgl,
-				op->nents, dir);
-	if (unlikely(freq->sg_cnt <= 0)) {
-		sg_free_table_chained(&freq->sg_table, true);
-		freq->sg_cnt = 0;
-		return -EFAULT;
-	}
-
-	/*
-	 * TODO: blk_integrity_rq(rq)  for DIF
-	 */
-	return 0;
-}
-
-static void
-nvme_fc_unmap_data(struct nvme_fc_ctrl *ctrl, struct request *rq,
-		struct nvme_fc_fcp_op *op)
-{
-	struct nvmefc_fcp_req *freq = &op->fcp_req;
-
-	if (!freq->sg_cnt)
-		return;
-
-	fc_dma_unmap_sg(ctrl->lport->dev, freq->sg_table.sgl, op->nents,
-				((rq_data_dir(rq) == WRITE) ?
-					DMA_TO_DEVICE : DMA_FROM_DEVICE));
-
-	nvme_cleanup_cmd(rq);
-
-	sg_free_table_chained(&freq->sg_table, true);
-
-	freq->sg_cnt = 0;
-}
-
-/*
- * In FC, the queue is a logical thing. At transport connect, the target
- * creates its "queue" and returns a handle that is to be given to the
- * target whenever it posts something to the corresponding SQ.  When an
- * SQE is sent on a SQ, FC effectively considers the SQE, or rather the
- * command contained within the SQE, an io, and assigns a FC exchange
- * to it. The SQE and the associated SQ handle are sent in the initial
- * CMD IU sents on the exchange. All transfers relative to the io occur
- * as part of the exchange.  The CQE is the last thing for the io,
- * which is transferred (explicitly or implicitly) with the RSP IU
- * sent on the exchange. After the CQE is received, the FC exchange is
- * terminaed and the Exchange may be used on a different io.
- *
- * The transport to LLDD api has the transport making a request for a
- * new fcp io request to the LLDD. The LLDD then allocates a FC exchange
- * resource and transfers the command. The LLDD will then process all
- * steps to complete the io. Upon completion, the transport done routine
- * is called.
- *
- * So - while the operation is outstanding to the LLDD, there is a link
- * level FC exchange resource that is also outstanding. This must be
- * considered in all cleanup operations.
- */
-static blk_status_t
-nvme_fc_start_fcp_op(struct nvme_fc_ctrl *ctrl, struct nvme_fc_queue *queue,
-	struct nvme_fc_fcp_op *op, u32 data_len,
-	enum nvmefc_fcp_datadir	io_dir)
-{
-	struct nvme_fc_cmd_iu *cmdiu = &op->cmd_iu;
-	struct nvme_command *sqe = &cmdiu->sqe;
-	u32 csn;
-	int ret;
-
-	/*
-	 * before attempting to send the io, check to see if we believe
-	 * the target device is present
-	 */
-	if (ctrl->rport->remoteport.port_state != FC_OBJSTATE_ONLINE)
-		goto busy;
-
-	if (!nvme_fc_ctrl_get(ctrl))
-		return BLK_STS_IOERR;
-
-	/* format the FC-NVME CMD IU and fcp_req */
-	cmdiu->connection_id = cpu_to_be64(queue->connection_id);
-	csn = atomic_inc_return(&queue->csn);
-	cmdiu->csn = cpu_to_be32(csn);
-	cmdiu->data_len = cpu_to_be32(data_len);
-	switch (io_dir) {
-	case NVMEFC_FCP_WRITE:
-		cmdiu->flags = FCNVME_CMD_FLAGS_WRITE;
-		break;
-	case NVMEFC_FCP_READ:
-		cmdiu->flags = FCNVME_CMD_FLAGS_READ;
-		break;
-	case NVMEFC_FCP_NODATA:
-		cmdiu->flags = 0;
-		break;
-	}
-	op->fcp_req.payload_length = data_len;
-	op->fcp_req.io_dir = io_dir;
-	op->fcp_req.transferred_length = 0;
-	op->fcp_req.rcv_rsplen = 0;
-	op->fcp_req.status = NVME_SC_SUCCESS;
-	op->fcp_req.sqid = cpu_to_le16(queue->qnum);
-
-	/*
-	 * validate per fabric rules, set fields mandated by fabric spec
-	 * as well as those by FC-NVME spec.
-	 */
-	WARN_ON_ONCE(sqe->common.metadata);
-	sqe->common.flags |= NVME_CMD_SGL_METABUF;
-
-	/*
-	 * format SQE DPTR field per FC-NVME rules:
-	 *    type=0x5     Transport SGL Data Block Descriptor
-	 *    subtype=0xA  Transport-specific value
-	 *    address=0
-	 *    length=length of the data series
-	 */
-	sqe->rw.dptr.sgl.type = (NVME_TRANSPORT_SGL_DATA_DESC << 4) |
-					NVME_SGL_FMT_TRANSPORT_A;
-	sqe->rw.dptr.sgl.length = cpu_to_le32(data_len);
-	sqe->rw.dptr.sgl.addr = 0;
-
-	if (!(op->flags & FCOP_FLAGS_AEN)) {
-		ret = nvme_fc_map_data(ctrl, op->rq, op);
-		if (ret < 0) {
-			nvme_cleanup_cmd(op->rq);
-			nvme_fc_ctrl_put(ctrl);
-			if (ret == -ENOMEM || ret == -EAGAIN)
-				return BLK_STS_RESOURCE;
-			return BLK_STS_IOERR;
-		}
-	}
-
-	fc_dma_sync_single_for_device(ctrl->lport->dev, op->fcp_req.cmddma,
-				  sizeof(op->cmd_iu), DMA_TO_DEVICE);
-
-	atomic_set(&op->state, FCPOP_STATE_ACTIVE);
-
-	if (!(op->flags & FCOP_FLAGS_AEN))
-		blk_mq_start_request(op->rq);
-
-	ret = ctrl->lport->ops->fcp_io(&ctrl->lport->localport,
-					&ctrl->rport->remoteport,
-					queue->lldd_handle, &op->fcp_req);
-
-	if (ret) {
-		if (!(op->flags & FCOP_FLAGS_AEN))
-			nvme_fc_unmap_data(ctrl, op->rq, op);
-
-		nvme_fc_ctrl_put(ctrl);
-
-		if (ctrl->rport->remoteport.port_state == FC_OBJSTATE_ONLINE &&
-				ret != -EBUSY)
-			return BLK_STS_IOERR;
-
-		goto busy;
-	}
-
-	return BLK_STS_OK;
-
-busy:
-	if (!(op->flags & FCOP_FLAGS_AEN) && queue->hctx)
-		blk_mq_delay_run_hw_queue(queue->hctx, NVMEFC_QUEUE_DELAY);
-
-	return BLK_STS_RESOURCE;
-}
-
-static inline blk_status_t nvme_fc_is_ready(struct nvme_fc_queue *queue,
-		struct request *rq)
-{
-	if (unlikely(!test_bit(NVME_FC_Q_LIVE, &queue->flags)))
-		return nvmf_check_init_req(&queue->ctrl->ctrl, rq);
-	return BLK_STS_OK;
-}
-
-static blk_status_t
-nvme_fc_queue_rq(struct blk_mq_hw_ctx *hctx,
-			const struct blk_mq_queue_data *bd)
-{
-	struct nvme_ns *ns = hctx->queue->queuedata;
-	struct nvme_fc_queue *queue = hctx->driver_data;
-	struct nvme_fc_ctrl *ctrl = queue->ctrl;
-	struct request *rq = bd->rq;
-	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
-	struct nvme_fc_cmd_iu *cmdiu = &op->cmd_iu;
-	struct nvme_command *sqe = &cmdiu->sqe;
-	enum nvmefc_fcp_datadir	io_dir;
-	u32 data_len;
-	blk_status_t ret;
-
-	ret = nvme_fc_is_ready(queue, rq);
-	if (unlikely(ret))
-		return ret;
-
-	ret = nvme_setup_cmd(ns, rq, sqe);
-	if (ret)
-		return ret;
-
-	data_len = blk_rq_payload_bytes(rq);
-	if (data_len)
-		io_dir = ((rq_data_dir(rq) == WRITE) ?
-					NVMEFC_FCP_WRITE : NVMEFC_FCP_READ);
-	else
-		io_dir = NVMEFC_FCP_NODATA;
-
-	return nvme_fc_start_fcp_op(ctrl, queue, op, data_len, io_dir);
-}
-
-static struct blk_mq_tags *
-nvme_fc_tagset(struct nvme_fc_queue *queue)
-{
-	if (queue->qnum == 0)
-		return queue->ctrl->admin_tag_set.tags[queue->qnum];
-
-	return queue->ctrl->tag_set.tags[queue->qnum - 1];
-}
-
-static int
-nvme_fc_poll(struct blk_mq_hw_ctx *hctx, unsigned int tag)
-
-{
-	struct nvme_fc_queue *queue = hctx->driver_data;
-	struct nvme_fc_ctrl *ctrl = queue->ctrl;
-	struct request *req;
-	struct nvme_fc_fcp_op *op;
-
-	req = blk_mq_tag_to_rq(nvme_fc_tagset(queue), tag);
-	if (!req)
-		return 0;
-
-	op = blk_mq_rq_to_pdu(req);
-
-	if ((atomic_read(&op->state) == FCPOP_STATE_ACTIVE) &&
-		 (ctrl->lport->ops->poll_queue))
-		ctrl->lport->ops->poll_queue(&ctrl->lport->localport,
-						 queue->lldd_handle);
-
-	return ((atomic_read(&op->state) != FCPOP_STATE_ACTIVE));
-}
-
-static void
-nvme_fc_submit_async_event(struct nvme_ctrl *arg, int aer_idx)
-{
-	struct nvme_fc_ctrl *ctrl = to_fc_ctrl(arg);
-	struct nvme_fc_fcp_op *aen_op;
-	unsigned long flags;
-	bool terminating = false;
-	blk_status_t ret;
-
-	if (aer_idx > NVME_FC_NR_AEN_COMMANDS)
-		return;
-
-	spin_lock_irqsave(&ctrl->lock, flags);
-	if (ctrl->flags & FCCTRL_TERMIO)
-		terminating = true;
-	spin_unlock_irqrestore(&ctrl->lock, flags);
-
-	if (terminating)
-		return;
-
-	aen_op = &ctrl->aen_ops[aer_idx];
-
-	ret = nvme_fc_start_fcp_op(ctrl, aen_op->queue, aen_op, 0,
-					NVMEFC_FCP_NODATA);
-	if (ret)
-		dev_err(ctrl->ctrl.device,
-			"failed async event work [%d]\n", aer_idx);
-}
-
-static void
-__nvme_fc_final_op_cleanup(struct request *rq)
-{
-	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
-	struct nvme_fc_ctrl *ctrl = op->ctrl;
-
-	atomic_set(&op->state, FCPOP_STATE_IDLE);
-	op->flags &= ~(FCOP_FLAGS_TERMIO | FCOP_FLAGS_RELEASED |
-			FCOP_FLAGS_COMPLETE);
-
-	nvme_fc_unmap_data(ctrl, rq, op);
-	nvme_complete_rq(rq);
-	nvme_fc_ctrl_put(ctrl);
-
-}
-
-static void
-nvme_fc_complete_rq(struct request *rq)
-{
-	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
-	struct nvme_fc_ctrl *ctrl = op->ctrl;
-	unsigned long flags;
-	bool completed = false;
-
-	/*
-	 * the core layer, on controller resets after calling
-	 * nvme_shutdown_ctrl(), calls complete_rq without our
-	 * calling blk_mq_complete_request(), thus there may still
-	 * be live i/o outstanding with the LLDD. Means transport has
-	 * to track complete calls vs fcpio_done calls to know what
-	 * path to take on completes and dones.
-	 */
-	spin_lock_irqsave(&ctrl->lock, flags);
-	if (op->flags & FCOP_FLAGS_COMPLETE)
-		completed = true;
-	else
-		op->flags |= FCOP_FLAGS_RELEASED;
-	spin_unlock_irqrestore(&ctrl->lock, flags);
-
-	if (completed)
-		__nvme_fc_final_op_cleanup(rq);
-}
-
-/*
- * This routine is used by the transport when it needs to find active
- * io on a queue that is to be terminated. The transport uses
- * blk_mq_tagset_busy_itr() to find the busy requests, which then invoke
- * this routine to kill them on a 1 by 1 basis.
- *
- * As FC allocates FC exchange for each io, the transport must contact
- * the LLDD to terminate the exchange, thus releasing the FC exchange.
- * After terminating the exchange the LLDD will call the transport's
- * normal io done path for the request, but it will have an aborted
- * status. The done path will return the io request back to the block
- * layer with an error status.
- */
-static void
-nvme_fc_terminate_exchange(struct request *req, void *data, bool reserved)
-{
-	struct nvme_ctrl *nctrl = data;
-	struct nvme_fc_ctrl *ctrl = to_fc_ctrl(nctrl);
-	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(req);
-	unsigned long flags;
-	int status;
-
-	if (!blk_mq_request_started(req))
-		return;
-
-	spin_lock_irqsave(&ctrl->lock, flags);
-	if (ctrl->flags & FCCTRL_TERMIO) {
-		ctrl->iocnt++;
-		op->flags |= FCOP_FLAGS_TERMIO;
-	}
-	spin_unlock_irqrestore(&ctrl->lock, flags);
-
-	status = __nvme_fc_abort_op(ctrl, op);
-	if (status) {
-		/*
-		 * if __nvme_fc_abort_op failed the io wasn't
-		 * active. Thus this call path is running in
-		 * parallel to the io complete. Treat as non-error.
-		 */
-
-		/* back out the flags/counters */
-		spin_lock_irqsave(&ctrl->lock, flags);
-		if (ctrl->flags & FCCTRL_TERMIO)
-			ctrl->iocnt--;
-		op->flags &= ~FCOP_FLAGS_TERMIO;
-		spin_unlock_irqrestore(&ctrl->lock, flags);
-		return;
-	}
-}
-
-
-static const struct blk_mq_ops nvme_fc_mq_ops = {
-	.queue_rq	= nvme_fc_queue_rq,
-	.complete	= nvme_fc_complete_rq,
-	.init_request	= nvme_fc_init_request,
-	.exit_request	= nvme_fc_exit_request,
-	.init_hctx	= nvme_fc_init_hctx,
-	.poll		= nvme_fc_poll,
-	.timeout	= nvme_fc_timeout,
-};
-
-static int
-nvme_fc_create_io_queues(struct nvme_fc_ctrl *ctrl)
-{
-	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
-	unsigned int nr_io_queues;
-	int ret;
-
-	nr_io_queues = min(min(opts->nr_io_queues, num_online_cpus()),
-				ctrl->lport->ops->max_hw_queues);
-	ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
-	if (ret) {
-		dev_info(ctrl->ctrl.device,
-			"set_queue_count failed: %d\n", ret);
-		return ret;
-	}
-
-	ctrl->ctrl.queue_count = nr_io_queues + 1;
-	if (!nr_io_queues)
-		return 0;
-
-	nvme_fc_init_io_queues(ctrl);
-
-	memset(&ctrl->tag_set, 0, sizeof(ctrl->tag_set));
-	ctrl->tag_set.ops = &nvme_fc_mq_ops;
-	ctrl->tag_set.queue_depth = ctrl->ctrl.opts->queue_size;
-	ctrl->tag_set.reserved_tags = 1; /* fabric connect */
-	ctrl->tag_set.numa_node = NUMA_NO_NODE;
-	ctrl->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
-	ctrl->tag_set.cmd_size = sizeof(struct nvme_fc_fcp_op) +
-					(SG_CHUNK_SIZE *
-						sizeof(struct scatterlist)) +
-					ctrl->lport->ops->fcprqst_priv_sz;
-	ctrl->tag_set.driver_data = ctrl;
-	ctrl->tag_set.nr_hw_queues = ctrl->ctrl.queue_count - 1;
-	ctrl->tag_set.timeout = NVME_IO_TIMEOUT;
-
-	ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
-	if (ret)
-		return ret;
-
-	ctrl->ctrl.tagset = &ctrl->tag_set;
-
-	ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
-	if (IS_ERR(ctrl->ctrl.connect_q)) {
-		ret = PTR_ERR(ctrl->ctrl.connect_q);
-		goto out_free_tag_set;
-	}
-
-	ret = nvme_fc_create_hw_io_queues(ctrl, ctrl->ctrl.opts->queue_size);
-	if (ret)
-		goto out_cleanup_blk_queue;
-
-	ret = nvme_fc_connect_io_queues(ctrl, ctrl->ctrl.opts->queue_size);
-	if (ret)
-		goto out_delete_hw_queues;
-
-	return 0;
-
-out_delete_hw_queues:
-	nvme_fc_delete_hw_io_queues(ctrl);
-out_cleanup_blk_queue:
-	blk_cleanup_queue(ctrl->ctrl.connect_q);
-out_free_tag_set:
-	blk_mq_free_tag_set(&ctrl->tag_set);
-	nvme_fc_free_io_queues(ctrl);
-
-	/* force put free routine to ignore io queues */
-	ctrl->ctrl.tagset = NULL;
-
-	return ret;
-}
-
-static int
-nvme_fc_reinit_io_queues(struct nvme_fc_ctrl *ctrl)
-{
-	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
-	unsigned int nr_io_queues;
-	int ret;
-
-	nr_io_queues = min(min(opts->nr_io_queues, num_online_cpus()),
-				ctrl->lport->ops->max_hw_queues);
-	ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
-	if (ret) {
-		dev_info(ctrl->ctrl.device,
-			"set_queue_count failed: %d\n", ret);
-		return ret;
-	}
-
-	ctrl->ctrl.queue_count = nr_io_queues + 1;
-	/* check for io queues existing */
-	if (ctrl->ctrl.queue_count == 1)
-		return 0;
-
-	nvme_fc_init_io_queues(ctrl);
-
-	ret = blk_mq_reinit_tagset(&ctrl->tag_set, nvme_fc_reinit_request);
-	if (ret)
-		goto out_free_io_queues;
-
-	ret = nvme_fc_create_hw_io_queues(ctrl, ctrl->ctrl.opts->queue_size);
-	if (ret)
-		goto out_free_io_queues;
-
-	ret = nvme_fc_connect_io_queues(ctrl, ctrl->ctrl.opts->queue_size);
-	if (ret)
-		goto out_delete_hw_queues;
-
-	blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
-
-	return 0;
-
-out_delete_hw_queues:
-	nvme_fc_delete_hw_io_queues(ctrl);
-out_free_io_queues:
-	nvme_fc_free_io_queues(ctrl);
-	return ret;
-}
-
-/*
- * This routine restarts the controller on the host side, and
- * on the link side, recreates the controller association.
- */
-static int
-nvme_fc_create_association(struct nvme_fc_ctrl *ctrl)
-{
-	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
-	u32 segs;
-	int ret;
-	bool changed;
-
-	++ctrl->ctrl.nr_reconnects;
-
-	/*
-	 * Create the admin queue
-	 */
-
-	nvme_fc_init_queue(ctrl, 0, NVME_FC_AQ_BLKMQ_DEPTH);
-
-	ret = __nvme_fc_create_hw_queue(ctrl, &ctrl->queues[0], 0,
-				NVME_FC_AQ_BLKMQ_DEPTH);
-	if (ret)
-		goto out_free_queue;
-
-	ret = nvme_fc_connect_admin_queue(ctrl, &ctrl->queues[0],
-				NVME_FC_AQ_BLKMQ_DEPTH,
-				(NVME_FC_AQ_BLKMQ_DEPTH / 4));
-	if (ret)
-		goto out_delete_hw_queue;
-
-	if (ctrl->ctrl.state != NVME_CTRL_NEW)
-		blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
-
-	ret = nvmf_connect_admin_queue(&ctrl->ctrl);
-	if (ret)
-		goto out_disconnect_admin_queue;
-
-	set_bit(NVME_FC_Q_LIVE, &ctrl->queues[0].flags);
-
-	/*
-	 * Check controller capabilities
-	 *
-	 * todo:- add code to check if ctrl attributes changed from
-	 * prior connection values
-	 */
-
-	ret = nvmf_reg_read64(&ctrl->ctrl, NVME_REG_CAP, &ctrl->ctrl.cap);
-	if (ret) {
-		dev_err(ctrl->ctrl.device,
-			"prop_get NVME_REG_CAP failed\n");
-		goto out_disconnect_admin_queue;
-	}
-
-	ctrl->ctrl.sqsize =
-		min_t(int, NVME_CAP_MQES(ctrl->ctrl.cap) + 1, ctrl->ctrl.sqsize);
-
-	ret = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
-	if (ret)
-		goto out_disconnect_admin_queue;
-
-	segs = min_t(u32, NVME_FC_MAX_SEGMENTS,
-			ctrl->lport->ops->max_sgl_segments);
-	ctrl->ctrl.max_hw_sectors = (segs - 1) << (PAGE_SHIFT - 9);
-
-	ret = nvme_init_identify(&ctrl->ctrl);
-	if (ret)
-		goto out_disconnect_admin_queue;
-
-	/* sanity checks */
-
-	/* FC-NVME does not have other data in the capsule */
-	if (ctrl->ctrl.icdoff) {
-		dev_err(ctrl->ctrl.device, "icdoff %d is not supported!\n",
-				ctrl->ctrl.icdoff);
-		goto out_disconnect_admin_queue;
-	}
-
-	/* FC-NVME supports normal SGL Data Block Descriptors */
-
-	if (opts->queue_size > ctrl->ctrl.maxcmd) {
-		/* warn if maxcmd is lower than queue_size */
-		dev_warn(ctrl->ctrl.device,
-			"queue_size %zu > ctrl maxcmd %u, reducing "
-			"to queue_size\n",
-			opts->queue_size, ctrl->ctrl.maxcmd);
-		opts->queue_size = ctrl->ctrl.maxcmd;
-	}
-
-	ret = nvme_fc_init_aen_ops(ctrl);
-	if (ret)
-		goto out_term_aen_ops;
-
-	/*
-	 * Create the io queues
-	 */
-
-	if (ctrl->ctrl.queue_count > 1) {
-		if (ctrl->ctrl.state == NVME_CTRL_NEW)
-			ret = nvme_fc_create_io_queues(ctrl);
-		else
-			ret = nvme_fc_reinit_io_queues(ctrl);
-		if (ret)
-			goto out_term_aen_ops;
-	}
-
-	changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
-	WARN_ON_ONCE(!changed);
-
-	ctrl->ctrl.nr_reconnects = 0;
-
-	nvme_start_ctrl(&ctrl->ctrl);
-
-	return 0;	/* Success */
-
-out_term_aen_ops:
-	nvme_fc_term_aen_ops(ctrl);
-out_disconnect_admin_queue:
-	/* send a Disconnect(association) LS to fc-nvme target */
-	nvme_fc_xmt_disconnect_assoc(ctrl);
-out_delete_hw_queue:
-	__nvme_fc_delete_hw_queue(ctrl, &ctrl->queues[0], 0);
-out_free_queue:
-	nvme_fc_free_queue(&ctrl->queues[0]);
-
-	return ret;
-}
-
-/*
- * This routine stops operation of the controller on the host side.
- * On the host os stack side: Admin and IO queues are stopped,
- *   outstanding ios on them terminated via FC ABTS.
- * On the link side: the association is terminated.
- */
-static void
-nvme_fc_delete_association(struct nvme_fc_ctrl *ctrl)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&ctrl->lock, flags);
-	ctrl->flags |= FCCTRL_TERMIO;
-	ctrl->iocnt = 0;
-	spin_unlock_irqrestore(&ctrl->lock, flags);
-
-	/*
-	 * If io queues are present, stop them and terminate all outstanding
-	 * ios on them. As FC allocates FC exchange for each io, the
-	 * transport must contact the LLDD to terminate the exchange,
-	 * thus releasing the FC exchange. We use blk_mq_tagset_busy_itr()
-	 * to tell us what io's are busy and invoke a transport routine
-	 * to kill them with the LLDD.  After terminating the exchange
-	 * the LLDD will call the transport's normal io done path, but it
-	 * will have an aborted status. The done path will return the
-	 * io requests back to the block layer as part of normal completions
-	 * (but with error status).
-	 */
-	if (ctrl->ctrl.queue_count > 1) {
-		nvme_stop_queues(&ctrl->ctrl);
-		blk_mq_tagset_busy_iter(&ctrl->tag_set,
-				nvme_fc_terminate_exchange, &ctrl->ctrl);
-	}
-
-	/*
-	 * Other transports, which don't have link-level contexts bound
-	 * to sqe's, would try to gracefully shutdown the controller by
-	 * writing the registers for shutdown and polling (call
-	 * nvme_shutdown_ctrl()). Given a bunch of i/o was potentially
-	 * just aborted and we will wait on those contexts, and given
-	 * there was no indication of how live the controlelr is on the
-	 * link, don't send more io to create more contexts for the
-	 * shutdown. Let the controller fail via keepalive failure if
-	 * its still present.
-	 */
-
-	/*
-	 * clean up the admin queue. Same thing as above.
-	 * use blk_mq_tagset_busy_itr() and the transport routine to
-	 * terminate the exchanges.
-	 */
-	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
-	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
-				nvme_fc_terminate_exchange, &ctrl->ctrl);
-
-	/* kill the aens as they are a separate path */
-	nvme_fc_abort_aen_ops(ctrl);
-
-	/* wait for all io that had to be aborted */
-	spin_lock_irq(&ctrl->lock);
-	wait_event_lock_irq(ctrl->ioabort_wait, ctrl->iocnt == 0, ctrl->lock);
-	ctrl->flags &= ~FCCTRL_TERMIO;
-	spin_unlock_irq(&ctrl->lock);
-
-	nvme_fc_term_aen_ops(ctrl);
-
-	/*
-	 * send a Disconnect(association) LS to fc-nvme target
-	 * Note: could have been sent at top of process, but
-	 * cleaner on link traffic if after the aborts complete.
-	 * Note: if association doesn't exist, association_id will be 0
-	 */
-	if (ctrl->association_id)
-		nvme_fc_xmt_disconnect_assoc(ctrl);
-
-	if (ctrl->ctrl.tagset) {
-		nvme_fc_delete_hw_io_queues(ctrl);
-		nvme_fc_free_io_queues(ctrl);
-	}
-
-	__nvme_fc_delete_hw_queue(ctrl, &ctrl->queues[0], 0);
-	nvme_fc_free_queue(&ctrl->queues[0]);
-}
-
-static void
-nvme_fc_delete_ctrl_work(struct work_struct *work)
-{
-	struct nvme_fc_ctrl *ctrl =
-		container_of(work, struct nvme_fc_ctrl, delete_work);
-
-	cancel_work_sync(&ctrl->ctrl.reset_work);
-	cancel_delayed_work_sync(&ctrl->connect_work);
-	nvme_stop_ctrl(&ctrl->ctrl);
-	nvme_remove_namespaces(&ctrl->ctrl);
-	/*
-	 * kill the association on the link side.  this will block
-	 * waiting for io to terminate
-	 */
-	nvme_fc_delete_association(ctrl);
-
-	/*
-	 * tear down the controller
-	 * After the last reference on the nvme ctrl is removed,
-	 * the transport nvme_fc_nvme_ctrl_freed() callback will be
-	 * invoked. From there, the transport will tear down it's
-	 * logical queues and association.
-	 */
-	nvme_uninit_ctrl(&ctrl->ctrl);
-
-	nvme_put_ctrl(&ctrl->ctrl);
-}
-
-static bool
-__nvme_fc_schedule_delete_work(struct nvme_fc_ctrl *ctrl)
-{
-	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_DELETING))
-		return true;
-
-	if (!queue_work(nvme_wq, &ctrl->delete_work))
-		return true;
-
-	return false;
-}
-
-static int
-__nvme_fc_del_ctrl(struct nvme_fc_ctrl *ctrl)
-{
-	return __nvme_fc_schedule_delete_work(ctrl) ? -EBUSY : 0;
-}
-
-/*
- * Request from nvme core layer to delete the controller
- */
-static int
-nvme_fc_del_nvme_ctrl(struct nvme_ctrl *nctrl)
-{
-	struct nvme_fc_ctrl *ctrl = to_fc_ctrl(nctrl);
-	int ret;
-
-	if (!kref_get_unless_zero(&ctrl->ctrl.kref))
-		return -EBUSY;
-
-	ret = __nvme_fc_del_ctrl(ctrl);
-
-	if (!ret)
-		flush_workqueue(nvme_wq);
-
-	nvme_put_ctrl(&ctrl->ctrl);
-
-	return ret;
-}
-
-static void
-nvme_fc_reconnect_or_delete(struct nvme_fc_ctrl *ctrl, int status)
-{
-	/* If we are resetting/deleting then do nothing */
-	if (ctrl->ctrl.state != NVME_CTRL_RECONNECTING) {
-		WARN_ON_ONCE(ctrl->ctrl.state == NVME_CTRL_NEW ||
-			ctrl->ctrl.state == NVME_CTRL_LIVE);
-		return;
-	}
-
-	dev_info(ctrl->ctrl.device,
-		"NVME-FC{%d}: reset: Reconnect attempt failed (%d)\n",
-		ctrl->cnum, status);
-
-	if (nvmf_should_reconnect(&ctrl->ctrl)) {
-		dev_info(ctrl->ctrl.device,
-			"NVME-FC{%d}: Reconnect attempt in %d seconds.\n",
-			ctrl->cnum, ctrl->ctrl.opts->reconnect_delay);
-		queue_delayed_work(nvme_wq, &ctrl->connect_work,
-				ctrl->ctrl.opts->reconnect_delay * HZ);
-	} else {
-		dev_warn(ctrl->ctrl.device,
-				"NVME-FC{%d}: Max reconnect attempts (%d) "
-				"reached. Removing controller\n",
-				ctrl->cnum, ctrl->ctrl.nr_reconnects);
-		WARN_ON(__nvme_fc_schedule_delete_work(ctrl));
-	}
-}
-
-static void
-nvme_fc_reset_ctrl_work(struct work_struct *work)
-{
-	struct nvme_fc_ctrl *ctrl =
-		container_of(work, struct nvme_fc_ctrl, ctrl.reset_work);
-	int ret;
-
-	nvme_stop_ctrl(&ctrl->ctrl);
-	/* will block will waiting for io to terminate */
-	nvme_fc_delete_association(ctrl);
-
-	ret = nvme_fc_create_association(ctrl);
-	if (ret)
-		nvme_fc_reconnect_or_delete(ctrl, ret);
-	else
-		dev_info(ctrl->ctrl.device,
-			"NVME-FC{%d}: controller reset complete\n", ctrl->cnum);
-}
-
-static const struct nvme_ctrl_ops nvme_fc_ctrl_ops = {
-	.name			= "fc",
-	.module			= THIS_MODULE,
-	.flags			= NVME_F_FABRICS,
-	.reg_read32		= nvmf_reg_read32,
-	.reg_read64		= nvmf_reg_read64,
-	.reg_write32		= nvmf_reg_write32,
-	.free_ctrl		= nvme_fc_nvme_ctrl_freed,
-	.submit_async_event	= nvme_fc_submit_async_event,
-	.delete_ctrl		= nvme_fc_del_nvme_ctrl,
-	.get_address		= nvmf_get_address,
-};
-
-static void
-nvme_fc_connect_ctrl_work(struct work_struct *work)
-{
-	int ret;
-
-	struct nvme_fc_ctrl *ctrl =
-			container_of(to_delayed_work(work),
-				struct nvme_fc_ctrl, connect_work);
-
-	ret = nvme_fc_create_association(ctrl);
-	if (ret)
-		nvme_fc_reconnect_or_delete(ctrl, ret);
-	else
-		dev_info(ctrl->ctrl.device,
-			"NVME-FC{%d}: controller reconnect complete\n",
-			ctrl->cnum);
-}
-
-
-static const struct blk_mq_ops nvme_fc_admin_mq_ops = {
-	.queue_rq	= nvme_fc_queue_rq,
-	.complete	= nvme_fc_complete_rq,
-	.init_request	= nvme_fc_init_request,
-	.exit_request	= nvme_fc_exit_request,
-	.init_hctx	= nvme_fc_init_admin_hctx,
-	.timeout	= nvme_fc_timeout,
-};
-
-
-static struct nvme_ctrl *
-nvme_fc_init_ctrl(struct device *dev, struct nvmf_ctrl_options *opts,
-	struct nvme_fc_lport *lport, struct nvme_fc_rport *rport)
-{
-	struct nvme_fc_ctrl *ctrl;
-	unsigned long flags;
-	int ret, idx, retry;
-
-	if (!(rport->remoteport.port_role &
-	    (FC_PORT_ROLE_NVME_DISCOVERY | FC_PORT_ROLE_NVME_TARGET))) {
-		ret = -EBADR;
-		goto out_fail;
-	}
-
-	ctrl = kzalloc(sizeof(*ctrl), GFP_KERNEL);
-	if (!ctrl) {
-		ret = -ENOMEM;
-		goto out_fail;
-	}
-
-	idx = ida_simple_get(&nvme_fc_ctrl_cnt, 0, 0, GFP_KERNEL);
-	if (idx < 0) {
-		ret = -ENOSPC;
-		goto out_free_ctrl;
-	}
-
-	ctrl->ctrl.opts = opts;
-	INIT_LIST_HEAD(&ctrl->ctrl_list);
-	ctrl->lport = lport;
-	ctrl->rport = rport;
-	ctrl->dev = lport->dev;
-	ctrl->cnum = idx;
-	init_waitqueue_head(&ctrl->ioabort_wait);
-
-	get_device(ctrl->dev);
-	kref_init(&ctrl->ref);
-
-	INIT_WORK(&ctrl->delete_work, nvme_fc_delete_ctrl_work);
-	INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
-	INIT_DELAYED_WORK(&ctrl->connect_work, nvme_fc_connect_ctrl_work);
-	spin_lock_init(&ctrl->lock);
-
-	/* io queue count */
-	ctrl->ctrl.queue_count = min_t(unsigned int,
-				opts->nr_io_queues,
-				lport->ops->max_hw_queues);
-	ctrl->ctrl.queue_count++;	/* +1 for admin queue */
-
-	ctrl->ctrl.sqsize = opts->queue_size - 1;
-	ctrl->ctrl.kato = opts->kato;
-
-	ret = -ENOMEM;
-	ctrl->queues = kcalloc(ctrl->ctrl.queue_count,
-				sizeof(struct nvme_fc_queue), GFP_KERNEL);
-	if (!ctrl->queues)
-		goto out_free_ida;
-
-	memset(&ctrl->admin_tag_set, 0, sizeof(ctrl->admin_tag_set));
-	ctrl->admin_tag_set.ops = &nvme_fc_admin_mq_ops;
-	ctrl->admin_tag_set.queue_depth = NVME_FC_AQ_BLKMQ_DEPTH;
-	ctrl->admin_tag_set.reserved_tags = 2; /* fabric connect + Keep-Alive */
-	ctrl->admin_tag_set.numa_node = NUMA_NO_NODE;
-	ctrl->admin_tag_set.cmd_size = sizeof(struct nvme_fc_fcp_op) +
-					(SG_CHUNK_SIZE *
-						sizeof(struct scatterlist)) +
-					ctrl->lport->ops->fcprqst_priv_sz;
-	ctrl->admin_tag_set.driver_data = ctrl;
-	ctrl->admin_tag_set.nr_hw_queues = 1;
-	ctrl->admin_tag_set.timeout = ADMIN_TIMEOUT;
-
-	ret = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
-	if (ret)
-		goto out_free_queues;
-	ctrl->ctrl.admin_tagset = &ctrl->admin_tag_set;
-
-	ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
-	if (IS_ERR(ctrl->ctrl.admin_q)) {
-		ret = PTR_ERR(ctrl->ctrl.admin_q);
-		goto out_free_admin_tag_set;
-	}
-
-	/*
-	 * Would have been nice to init io queues tag set as well.
-	 * However, we require interaction from the controller
-	 * for max io queue count before we can do so.
-	 * Defer this to the connect path.
-	 */
-
-	ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_fc_ctrl_ops, 0);
-	if (ret)
-		goto out_cleanup_admin_q;
-
-	/* at this point, teardown path changes to ref counting on nvme ctrl */
-
-	spin_lock_irqsave(&rport->lock, flags);
-	list_add_tail(&ctrl->ctrl_list, &rport->ctrl_list);
-	spin_unlock_irqrestore(&rport->lock, flags);
-
-	/*
-	 * It's possible that transactions used to create the association
-	 * may fail. Examples: CreateAssociation LS or CreateIOConnection
-	 * LS gets dropped/corrupted/fails; or a frame gets dropped or a
-	 * command times out for one of the actions to init the controller
-	 * (Connect, Get/Set_Property, Set_Features, etc). Many of these
-	 * transport errors (frame drop, LS failure) inherently must kill
-	 * the association. The transport is coded so that any command used
-	 * to create the association (prior to a LIVE state transition
-	 * while NEW or RECONNECTING) will fail if it completes in error or
-	 * times out.
-	 *
-	 * As such: as the connect request was mostly likely due to a
-	 * udev event that discovered the remote port, meaning there is
-	 * not an admin or script there to restart if the connect
-	 * request fails, retry the initial connection creation up to
-	 * three times before giving up and declaring failure.
-	 */
-	for (retry = 0; retry < 3; retry++) {
-		ret = nvme_fc_create_association(ctrl);
-		if (!ret)
-			break;
-	}
-
-	if (ret) {
-		nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_DELETING);
-		cancel_work_sync(&ctrl->ctrl.reset_work);
-		cancel_delayed_work_sync(&ctrl->connect_work);
-
-		/* couldn't schedule retry - fail out */
-		dev_err(ctrl->ctrl.device,
-			"NVME-FC{%d}: Connect retry failed\n", ctrl->cnum);
-
-		ctrl->ctrl.opts = NULL;
-
-		/* initiate nvme ctrl ref counting teardown */
-		nvme_uninit_ctrl(&ctrl->ctrl);
-
-		/* Remove core ctrl ref. */
-		nvme_put_ctrl(&ctrl->ctrl);
-
-		/* as we're past the point where we transition to the ref
-		 * counting teardown path, if we return a bad pointer here,
-		 * the calling routine, thinking it's prior to the
-		 * transition, will do an rport put. Since the teardown
-		 * path also does a rport put, we do an extra get here to
-		 * so proper order/teardown happens.
-		 */
-		nvme_fc_rport_get(rport);
-
-		if (ret > 0)
-			ret = -EIO;
-		return ERR_PTR(ret);
-	}
-
-	kref_get(&ctrl->ctrl.kref);
-
-	dev_info(ctrl->ctrl.device,
-		"NVME-FC{%d}: new ctrl: NQN \"%s\"\n",
-		ctrl->cnum, ctrl->ctrl.opts->subsysnqn);
-
-	return &ctrl->ctrl;
-
-out_cleanup_admin_q:
-	blk_cleanup_queue(ctrl->ctrl.admin_q);
-out_free_admin_tag_set:
-	blk_mq_free_tag_set(&ctrl->admin_tag_set);
-out_free_queues:
-	kfree(ctrl->queues);
-out_free_ida:
-	put_device(ctrl->dev);
-	ida_simple_remove(&nvme_fc_ctrl_cnt, ctrl->cnum);
-out_free_ctrl:
-	kfree(ctrl);
-out_fail:
-	/* exit via here doesn't follow ctlr ref points */
-	return ERR_PTR(ret);
-}
-
-
-struct nvmet_fc_traddr {
-	u64	nn;
-	u64	pn;
-};
-
-static int
-__nvme_fc_parse_u64(substring_t *sstr, u64 *val)
-{
-	u64 token64;
-
-	if (match_u64(sstr, &token64))
-		return -EINVAL;
-	*val = token64;
-
-	return 0;
-}
-
-/*
- * This routine validates and extracts the WWN's from the TRADDR string.
- * As kernel parsers need the 0x to determine number base, universally
- * build string to parse with 0x prefix before parsing name strings.
- */
-static int
-nvme_fc_parse_traddr(struct nvmet_fc_traddr *traddr, char *buf, size_t blen)
-{
-	char name[2 + NVME_FC_TRADDR_HEXNAMELEN + 1];
-	substring_t wwn = { name, &name[sizeof(name)-1] };
-	int nnoffset, pnoffset;
-
-	/* validate it string one of the 2 allowed formats */
-	if (strnlen(buf, blen) == NVME_FC_TRADDR_MAXLENGTH &&
-			!strncmp(buf, "nn-0x", NVME_FC_TRADDR_OXNNLEN) &&
-			!strncmp(&buf[NVME_FC_TRADDR_MAX_PN_OFFSET],
-				"pn-0x", NVME_FC_TRADDR_OXNNLEN)) {
-		nnoffset = NVME_FC_TRADDR_OXNNLEN;
-		pnoffset = NVME_FC_TRADDR_MAX_PN_OFFSET +
-						NVME_FC_TRADDR_OXNNLEN;
-	} else if ((strnlen(buf, blen) == NVME_FC_TRADDR_MINLENGTH &&
-			!strncmp(buf, "nn-", NVME_FC_TRADDR_NNLEN) &&
-			!strncmp(&buf[NVME_FC_TRADDR_MIN_PN_OFFSET],
-				"pn-", NVME_FC_TRADDR_NNLEN))) {
-		nnoffset = NVME_FC_TRADDR_NNLEN;
-		pnoffset = NVME_FC_TRADDR_MIN_PN_OFFSET + NVME_FC_TRADDR_NNLEN;
-	} else
-		goto out_einval;
-
-	name[0] = '0';
-	name[1] = 'x';
-	name[2 + NVME_FC_TRADDR_HEXNAMELEN] = 0;
-
-	memcpy(&name[2], &buf[nnoffset], NVME_FC_TRADDR_HEXNAMELEN);
-	if (__nvme_fc_parse_u64(&wwn, &traddr->nn))
-		goto out_einval;
-
-	memcpy(&name[2], &buf[pnoffset], NVME_FC_TRADDR_HEXNAMELEN);
-	if (__nvme_fc_parse_u64(&wwn, &traddr->pn))
-		goto out_einval;
-
-	return 0;
-
-out_einval:
-	pr_warn("%s: bad traddr string\n", __func__);
-	return -EINVAL;
-}
-
-static struct nvme_ctrl *
-nvme_fc_create_ctrl(struct device *dev, struct nvmf_ctrl_options *opts)
-{
-	struct nvme_fc_lport *lport;
-	struct nvme_fc_rport *rport;
-	struct nvme_ctrl *ctrl;
-	struct nvmet_fc_traddr laddr = { 0L, 0L };
-	struct nvmet_fc_traddr raddr = { 0L, 0L };
-	unsigned long flags;
-	int ret;
-
-	ret = nvme_fc_parse_traddr(&raddr, opts->traddr, NVMF_TRADDR_SIZE);
-	if (ret || !raddr.nn || !raddr.pn)
-		return ERR_PTR(-EINVAL);
-
-	ret = nvme_fc_parse_traddr(&laddr, opts->host_traddr, NVMF_TRADDR_SIZE);
-	if (ret || !laddr.nn || !laddr.pn)
-		return ERR_PTR(-EINVAL);
-
-	/* find the host and remote ports to connect together */
-	spin_lock_irqsave(&nvme_fc_lock, flags);
-	list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
-		if (lport->localport.node_name != laddr.nn ||
-		    lport->localport.port_name != laddr.pn)
-			continue;
-
-		list_for_each_entry(rport, &lport->endp_list, endp_list) {
-			if (rport->remoteport.node_name != raddr.nn ||
-			    rport->remoteport.port_name != raddr.pn)
-				continue;
-
-			/* if fail to get reference fall through. Will error */
-			if (!nvme_fc_rport_get(rport))
-				break;
-
-			spin_unlock_irqrestore(&nvme_fc_lock, flags);
-
-			ctrl = nvme_fc_init_ctrl(dev, opts, lport, rport);
-			if (IS_ERR(ctrl))
-				nvme_fc_rport_put(rport);
-			return ctrl;
-		}
-	}
-	spin_unlock_irqrestore(&nvme_fc_lock, flags);
-
-	return ERR_PTR(-ENOENT);
-}
-
-
-static struct nvmf_transport_ops nvme_fc_transport = {
-	.name		= "fc",
-	.required_opts	= NVMF_OPT_TRADDR | NVMF_OPT_HOST_TRADDR,
-	.allowed_opts	= NVMF_OPT_RECONNECT_DELAY | NVMF_OPT_CTRL_LOSS_TMO,
-	.create_ctrl	= nvme_fc_create_ctrl,
-};
-
-static int __init nvme_fc_init_module(void)
-{
-	return nvmf_register_transport(&nvme_fc_transport);
-}
-
-static void __exit nvme_fc_exit_module(void)
-{
-	/* sanity check - all lports should be removed */
-	if (!list_empty(&nvme_fc_lport_list))
-		pr_warn("%s: localport list not empty\n", __func__);
-
-	nvmf_unregister_transport(&nvme_fc_transport);
-
-	ida_destroy(&nvme_fc_local_port_cnt);
-	ida_destroy(&nvme_fc_ctrl_cnt);
-}
-
-module_init(nvme_fc_init_module);
-module_exit(nvme_fc_exit_module);
-
-MODULE_LICENSE("GPL v2");
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/lightnvm.c b/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/lightnvm.c
deleted file mode 100644
index 1f79e3f..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/lightnvm.c
+++ /dev/null
@@ -1,957 +0,0 @@
-/*
- * nvme-lightnvm.c - LightNVM NVMe device
- *
- * Copyright (C) 2014-2015 IT University of Copenhagen
- * Initial release: Matias Bjorling <mb@lightnvm.io>
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License version
- * 2 as published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; see the file COPYING.  If not, write to
- * the Free Software Foundation, 675 Mass Ave, Cambridge, MA 02139,
- * USA.
- *
- */
-
-#include "nvme.h"
-
-#include <linux/nvme.h>
-#include <linux/bitops.h>
-#include <linux/lightnvm.h>
-#include <linux/vmalloc.h>
-#include <linux/sched/sysctl.h>
-#include <uapi/linux/lightnvm.h>
-
-enum nvme_nvm_admin_opcode {
-	nvme_nvm_admin_identity		= 0xe2,
-	nvme_nvm_admin_get_l2p_tbl	= 0xea,
-	nvme_nvm_admin_get_bb_tbl	= 0xf2,
-	nvme_nvm_admin_set_bb_tbl	= 0xf1,
-};
-
-struct nvme_nvm_hb_rw {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2;
-	__le64			metadata;
-	__le64			prp1;
-	__le64			prp2;
-	__le64			spba;
-	__le16			length;
-	__le16			control;
-	__le32			dsmgmt;
-	__le64			slba;
-};
-
-struct nvme_nvm_ph_rw {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2;
-	__le64			metadata;
-	__le64			prp1;
-	__le64			prp2;
-	__le64			spba;
-	__le16			length;
-	__le16			control;
-	__le32			dsmgmt;
-	__le64			resv;
-};
-
-struct nvme_nvm_identity {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd[2];
-	__le64			prp1;
-	__le64			prp2;
-	__le32			chnl_off;
-	__u32			rsvd11[5];
-};
-
-struct nvme_nvm_l2ptbl {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__le32			cdw2[4];
-	__le64			prp1;
-	__le64			prp2;
-	__le64			slba;
-	__le32			nlb;
-	__le16			cdw14[6];
-};
-
-struct nvme_nvm_getbbtbl {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd[2];
-	__le64			prp1;
-	__le64			prp2;
-	__le64			spba;
-	__u32			rsvd4[4];
-};
-
-struct nvme_nvm_setbbtbl {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__le64			rsvd[2];
-	__le64			prp1;
-	__le64			prp2;
-	__le64			spba;
-	__le16			nlb;
-	__u8			value;
-	__u8			rsvd3;
-	__u32			rsvd4[3];
-};
-
-struct nvme_nvm_erase_blk {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd[2];
-	__le64			prp1;
-	__le64			prp2;
-	__le64			spba;
-	__le16			length;
-	__le16			control;
-	__le32			dsmgmt;
-	__le64			resv;
-};
-
-struct nvme_nvm_command {
-	union {
-		struct nvme_common_command common;
-		struct nvme_nvm_identity identity;
-		struct nvme_nvm_hb_rw hb_rw;
-		struct nvme_nvm_ph_rw ph_rw;
-		struct nvme_nvm_l2ptbl l2p;
-		struct nvme_nvm_getbbtbl get_bb;
-		struct nvme_nvm_setbbtbl set_bb;
-		struct nvme_nvm_erase_blk erase;
-	};
-};
-
-#define NVME_NVM_LP_MLC_PAIRS 886
-struct nvme_nvm_lp_mlc {
-	__le16			num_pairs;
-	__u8			pairs[NVME_NVM_LP_MLC_PAIRS];
-};
-
-struct nvme_nvm_lp_tbl {
-	__u8			id[8];
-	struct nvme_nvm_lp_mlc	mlc;
-};
-
-struct nvme_nvm_id_group {
-	__u8			mtype;
-	__u8			fmtype;
-	__le16			res16;
-	__u8			num_ch;
-	__u8			num_lun;
-	__u8			num_pln;
-	__u8			rsvd1;
-	__le16			num_blk;
-	__le16			num_pg;
-	__le16			fpg_sz;
-	__le16			csecs;
-	__le16			sos;
-	__le16			rsvd2;
-	__le32			trdt;
-	__le32			trdm;
-	__le32			tprt;
-	__le32			tprm;
-	__le32			tbet;
-	__le32			tbem;
-	__le32			mpos;
-	__le32			mccap;
-	__le16			cpar;
-	__u8			reserved[10];
-	struct nvme_nvm_lp_tbl lptbl;
-} __packed;
-
-struct nvme_nvm_addr_format {
-	__u8			ch_offset;
-	__u8			ch_len;
-	__u8			lun_offset;
-	__u8			lun_len;
-	__u8			pln_offset;
-	__u8			pln_len;
-	__u8			blk_offset;
-	__u8			blk_len;
-	__u8			pg_offset;
-	__u8			pg_len;
-	__u8			sect_offset;
-	__u8			sect_len;
-	__u8			res[4];
-} __packed;
-
-struct nvme_nvm_id {
-	__u8			ver_id;
-	__u8			vmnt;
-	__u8			cgrps;
-	__u8			res;
-	__le32			cap;
-	__le32			dom;
-	struct nvme_nvm_addr_format ppaf;
-	__u8			resv[228];
-	struct nvme_nvm_id_group groups[4];
-} __packed;
-
-struct nvme_nvm_bb_tbl {
-	__u8	tblid[4];
-	__le16	verid;
-	__le16	revid;
-	__le32	rvsd1;
-	__le32	tblks;
-	__le32	tfact;
-	__le32	tgrown;
-	__le32	tdresv;
-	__le32	thresv;
-	__le32	rsvd2[8];
-	__u8	blk[0];
-};
-
-/*
- * Check we didn't inadvertently grow the command struct
- */
-static inline void _nvme_nvm_check_size(void)
-{
-	BUILD_BUG_ON(sizeof(struct nvme_nvm_identity) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_nvm_hb_rw) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_nvm_ph_rw) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_nvm_getbbtbl) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_nvm_setbbtbl) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_nvm_l2ptbl) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_nvm_erase_blk) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_nvm_id_group) != 960);
-	BUILD_BUG_ON(sizeof(struct nvme_nvm_addr_format) != 16);
-	BUILD_BUG_ON(sizeof(struct nvme_nvm_id) != NVME_IDENTIFY_DATA_SIZE);
-	BUILD_BUG_ON(sizeof(struct nvme_nvm_bb_tbl) != 64);
-}
-
-static int init_grps(struct nvm_id *nvm_id, struct nvme_nvm_id *nvme_nvm_id)
-{
-	struct nvme_nvm_id_group *src;
-	struct nvm_id_group *dst;
-
-	if (nvme_nvm_id->cgrps != 1)
-		return -EINVAL;
-
-	src = &nvme_nvm_id->groups[0];
-	dst = &nvm_id->grp;
-
-	dst->mtype = src->mtype;
-	dst->fmtype = src->fmtype;
-	dst->num_ch = src->num_ch;
-	dst->num_lun = src->num_lun;
-	dst->num_pln = src->num_pln;
-
-	dst->num_pg = le16_to_cpu(src->num_pg);
-	dst->num_blk = le16_to_cpu(src->num_blk);
-	dst->fpg_sz = le16_to_cpu(src->fpg_sz);
-	dst->csecs = le16_to_cpu(src->csecs);
-	dst->sos = le16_to_cpu(src->sos);
-
-	dst->trdt = le32_to_cpu(src->trdt);
-	dst->trdm = le32_to_cpu(src->trdm);
-	dst->tprt = le32_to_cpu(src->tprt);
-	dst->tprm = le32_to_cpu(src->tprm);
-	dst->tbet = le32_to_cpu(src->tbet);
-	dst->tbem = le32_to_cpu(src->tbem);
-	dst->mpos = le32_to_cpu(src->mpos);
-	dst->mccap = le32_to_cpu(src->mccap);
-
-	dst->cpar = le16_to_cpu(src->cpar);
-
-	if (dst->fmtype == NVM_ID_FMTYPE_MLC) {
-		memcpy(dst->lptbl.id, src->lptbl.id, 8);
-		dst->lptbl.mlc.num_pairs =
-				le16_to_cpu(src->lptbl.mlc.num_pairs);
-
-		if (dst->lptbl.mlc.num_pairs > NVME_NVM_LP_MLC_PAIRS) {
-			pr_err("nvm: number of MLC pairs not supported\n");
-			return -EINVAL;
-		}
-
-		memcpy(dst->lptbl.mlc.pairs, src->lptbl.mlc.pairs,
-					dst->lptbl.mlc.num_pairs);
-	}
-
-	return 0;
-}
-
-static int nvme_nvm_identity(struct nvm_dev *nvmdev, struct nvm_id *nvm_id)
-{
-	struct nvme_ns *ns = nvmdev->q->queuedata;
-	struct nvme_nvm_id *nvme_nvm_id;
-	struct nvme_nvm_command c = {};
-	int ret;
-
-	c.identity.opcode = nvme_nvm_admin_identity;
-	c.identity.nsid = cpu_to_le32(ns->ns_id);
-	c.identity.chnl_off = 0;
-
-	nvme_nvm_id = kmalloc(sizeof(struct nvme_nvm_id), GFP_KERNEL);
-	if (!nvme_nvm_id)
-		return -ENOMEM;
-
-	ret = nvme_submit_sync_cmd(ns->ctrl->admin_q, (struct nvme_command *)&c,
-				nvme_nvm_id, sizeof(struct nvme_nvm_id));
-	if (ret) {
-		ret = -EIO;
-		goto out;
-	}
-
-	nvm_id->ver_id = nvme_nvm_id->ver_id;
-	nvm_id->vmnt = nvme_nvm_id->vmnt;
-	nvm_id->cap = le32_to_cpu(nvme_nvm_id->cap);
-	nvm_id->dom = le32_to_cpu(nvme_nvm_id->dom);
-	memcpy(&nvm_id->ppaf, &nvme_nvm_id->ppaf,
-					sizeof(struct nvm_addr_format));
-
-	ret = init_grps(nvm_id, nvme_nvm_id);
-out:
-	kfree(nvme_nvm_id);
-	return ret;
-}
-
-static int nvme_nvm_get_l2p_tbl(struct nvm_dev *nvmdev, u64 slba, u32 nlb,
-				nvm_l2p_update_fn *update_l2p, void *priv)
-{
-	struct nvme_ns *ns = nvmdev->q->queuedata;
-	struct nvme_nvm_command c = {};
-	u32 len = queue_max_hw_sectors(ns->ctrl->admin_q) << 9;
-	u32 nlb_pr_rq = len / sizeof(u64);
-	u64 cmd_slba = slba;
-	void *entries;
-	int ret = 0;
-
-	c.l2p.opcode = nvme_nvm_admin_get_l2p_tbl;
-	c.l2p.nsid = cpu_to_le32(ns->ns_id);
-	entries = kmalloc(len, GFP_KERNEL);
-	if (!entries)
-		return -ENOMEM;
-
-	while (nlb) {
-		u32 cmd_nlb = min(nlb_pr_rq, nlb);
-		u64 elba = slba + cmd_nlb;
-
-		c.l2p.slba = cpu_to_le64(cmd_slba);
-		c.l2p.nlb = cpu_to_le32(cmd_nlb);
-
-		ret = nvme_submit_sync_cmd(ns->ctrl->admin_q,
-				(struct nvme_command *)&c, entries, len);
-		if (ret) {
-			dev_err(ns->ctrl->device,
-				"L2P table transfer failed (%d)\n", ret);
-			ret = -EIO;
-			goto out;
-		}
-
-		if (unlikely(elba > nvmdev->total_secs)) {
-			pr_err("nvm: L2P data from device is out of bounds!\n");
-			ret = -EINVAL;
-			goto out;
-		}
-
-		/* Transform physical address to target address space */
-		nvm_part_to_tgt(nvmdev, entries, cmd_nlb);
-
-		if (update_l2p(cmd_slba, cmd_nlb, entries, priv)) {
-			ret = -EINTR;
-			goto out;
-		}
-
-		cmd_slba += cmd_nlb;
-		nlb -= cmd_nlb;
-	}
-
-out:
-	kfree(entries);
-	return ret;
-}
-
-static int nvme_nvm_get_bb_tbl(struct nvm_dev *nvmdev, struct ppa_addr ppa,
-								u8 *blks)
-{
-	struct request_queue *q = nvmdev->q;
-	struct nvm_geo *geo = &nvmdev->geo;
-	struct nvme_ns *ns = q->queuedata;
-	struct nvme_ctrl *ctrl = ns->ctrl;
-	struct nvme_nvm_command c = {};
-	struct nvme_nvm_bb_tbl *bb_tbl;
-	int nr_blks = geo->blks_per_lun * geo->plane_mode;
-	int tblsz = sizeof(struct nvme_nvm_bb_tbl) + nr_blks;
-	int ret = 0;
-
-	c.get_bb.opcode = nvme_nvm_admin_get_bb_tbl;
-	c.get_bb.nsid = cpu_to_le32(ns->ns_id);
-	c.get_bb.spba = cpu_to_le64(ppa.ppa);
-
-	bb_tbl = kzalloc(tblsz, GFP_KERNEL);
-	if (!bb_tbl)
-		return -ENOMEM;
-
-	ret = nvme_submit_sync_cmd(ctrl->admin_q, (struct nvme_command *)&c,
-								bb_tbl, tblsz);
-	if (ret) {
-		dev_err(ctrl->device, "get bad block table failed (%d)\n", ret);
-		ret = -EIO;
-		goto out;
-	}
-
-	if (bb_tbl->tblid[0] != 'B' || bb_tbl->tblid[1] != 'B' ||
-		bb_tbl->tblid[2] != 'L' || bb_tbl->tblid[3] != 'T') {
-		dev_err(ctrl->device, "bbt format mismatch\n");
-		ret = -EINVAL;
-		goto out;
-	}
-
-	if (le16_to_cpu(bb_tbl->verid) != 1) {
-		ret = -EINVAL;
-		dev_err(ctrl->device, "bbt version not supported\n");
-		goto out;
-	}
-
-	if (le32_to_cpu(bb_tbl->tblks) != nr_blks) {
-		ret = -EINVAL;
-		dev_err(ctrl->device,
-				"bbt unsuspected blocks returned (%u!=%u)",
-				le32_to_cpu(bb_tbl->tblks), nr_blks);
-		goto out;
-	}
-
-	memcpy(blks, bb_tbl->blk, geo->blks_per_lun * geo->plane_mode);
-out:
-	kfree(bb_tbl);
-	return ret;
-}
-
-static int nvme_nvm_set_bb_tbl(struct nvm_dev *nvmdev, struct ppa_addr *ppas,
-							int nr_ppas, int type)
-{
-	struct nvme_ns *ns = nvmdev->q->queuedata;
-	struct nvme_nvm_command c = {};
-	int ret = 0;
-
-	c.set_bb.opcode = nvme_nvm_admin_set_bb_tbl;
-	c.set_bb.nsid = cpu_to_le32(ns->ns_id);
-	c.set_bb.spba = cpu_to_le64(ppas->ppa);
-	c.set_bb.nlb = cpu_to_le16(nr_ppas - 1);
-	c.set_bb.value = type;
-
-	ret = nvme_submit_sync_cmd(ns->ctrl->admin_q, (struct nvme_command *)&c,
-								NULL, 0);
-	if (ret)
-		dev_err(ns->ctrl->device, "set bad block table failed (%d)\n",
-									ret);
-	return ret;
-}
-
-static inline void nvme_nvm_rqtocmd(struct nvm_rq *rqd, struct nvme_ns *ns,
-				    struct nvme_nvm_command *c)
-{
-	c->ph_rw.opcode = rqd->opcode;
-	c->ph_rw.nsid = cpu_to_le32(ns->ns_id);
-	c->ph_rw.spba = cpu_to_le64(rqd->ppa_addr.ppa);
-	c->ph_rw.metadata = cpu_to_le64(rqd->dma_meta_list);
-	c->ph_rw.control = cpu_to_le16(rqd->flags);
-	c->ph_rw.length = cpu_to_le16(rqd->nr_ppas - 1);
-
-	if (rqd->opcode == NVM_OP_HBWRITE || rqd->opcode == NVM_OP_HBREAD)
-		c->hb_rw.slba = cpu_to_le64(nvme_block_nr(ns,
-					rqd->bio->bi_iter.bi_sector));
-}
-
-static void nvme_nvm_end_io(struct request *rq, blk_status_t status)
-{
-	struct nvm_rq *rqd = rq->end_io_data;
-
-	rqd->ppa_status = le64_to_cpu(nvme_req(rq)->result.u64);
-	rqd->error = nvme_req(rq)->status;
-	nvm_end_io(rqd);
-
-	kfree(nvme_req(rq)->cmd);
-	blk_mq_free_request(rq);
-}
-
-static int nvme_nvm_submit_io(struct nvm_dev *dev, struct nvm_rq *rqd)
-{
-	struct request_queue *q = dev->q;
-	struct nvme_ns *ns = q->queuedata;
-	struct request *rq;
-	struct bio *bio = rqd->bio;
-	struct nvme_nvm_command *cmd;
-
-	cmd = kzalloc(sizeof(struct nvme_nvm_command), GFP_KERNEL);
-	if (!cmd)
-		return -ENOMEM;
-
-	nvme_nvm_rqtocmd(rqd, ns, cmd);
-
-	rq = nvme_alloc_request(q, (struct nvme_command *)cmd, 0, NVME_QID_ANY);
-	if (IS_ERR(rq)) {
-		kfree(cmd);
-		return PTR_ERR(rq);
-	}
-	rq->cmd_flags &= ~REQ_FAILFAST_DRIVER;
-
-	if (bio) {
-		blk_init_request_from_bio(rq, bio);
-	} else {
-		rq->ioprio = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_BE, IOPRIO_NORM);
-		rq->__data_len = 0;
-	}
-
-	rq->end_io_data = rqd;
-
-	blk_execute_rq_nowait(q, NULL, rq, 0, nvme_nvm_end_io);
-
-	return 0;
-}
-
-static void *nvme_nvm_create_dma_pool(struct nvm_dev *nvmdev, char *name)
-{
-	struct nvme_ns *ns = nvmdev->q->queuedata;
-
-	return dma_pool_create(name, ns->ctrl->dev, PAGE_SIZE, PAGE_SIZE, 0);
-}
-
-static void nvme_nvm_destroy_dma_pool(void *pool)
-{
-	struct dma_pool *dma_pool = pool;
-
-	dma_pool_destroy(dma_pool);
-}
-
-static void *nvme_nvm_dev_dma_alloc(struct nvm_dev *dev, void *pool,
-				    gfp_t mem_flags, dma_addr_t *dma_handler)
-{
-	return dma_pool_alloc(pool, mem_flags, dma_handler);
-}
-
-static void nvme_nvm_dev_dma_free(void *pool, void *addr,
-							dma_addr_t dma_handler)
-{
-	dma_pool_free(pool, addr, dma_handler);
-}
-
-static struct nvm_dev_ops nvme_nvm_dev_ops = {
-	.identity		= nvme_nvm_identity,
-
-	.get_l2p_tbl		= nvme_nvm_get_l2p_tbl,
-
-	.get_bb_tbl		= nvme_nvm_get_bb_tbl,
-	.set_bb_tbl		= nvme_nvm_set_bb_tbl,
-
-	.submit_io		= nvme_nvm_submit_io,
-
-	.create_dma_pool	= nvme_nvm_create_dma_pool,
-	.destroy_dma_pool	= nvme_nvm_destroy_dma_pool,
-	.dev_dma_alloc		= nvme_nvm_dev_dma_alloc,
-	.dev_dma_free		= nvme_nvm_dev_dma_free,
-
-	.max_phys_sect		= 64,
-};
-
-static int nvme_nvm_submit_user_cmd(struct request_queue *q,
-				struct nvme_ns *ns,
-				struct nvme_nvm_command *vcmd,
-				void __user *ubuf, unsigned int bufflen,
-				void __user *meta_buf, unsigned int meta_len,
-				void __user *ppa_buf, unsigned int ppa_len,
-				u32 *result, u64 *status, unsigned int timeout)
-{
-	bool write = nvme_is_write((struct nvme_command *)vcmd);
-	struct nvm_dev *dev = ns->ndev;
-	struct gendisk *disk = ns->disk;
-	struct request *rq;
-	struct bio *bio = NULL;
-	__le64 *ppa_list = NULL;
-	dma_addr_t ppa_dma;
-	__le64 *metadata = NULL;
-	dma_addr_t metadata_dma;
-	DECLARE_COMPLETION_ONSTACK(wait);
-	int ret = 0;
-
-	rq = nvme_alloc_request(q, (struct nvme_command *)vcmd, 0,
-			NVME_QID_ANY);
-	if (IS_ERR(rq)) {
-		ret = -ENOMEM;
-		goto err_cmd;
-	}
-
-	rq->timeout = timeout ? timeout : ADMIN_TIMEOUT;
-
-	rq->cmd_flags &= ~REQ_FAILFAST_DRIVER;
-
-	if (ppa_buf && ppa_len) {
-		ppa_list = dma_pool_alloc(dev->dma_pool, GFP_KERNEL, &ppa_dma);
-		if (!ppa_list) {
-			ret = -ENOMEM;
-			goto err_rq;
-		}
-		if (copy_from_user(ppa_list, (void __user *)ppa_buf,
-						sizeof(u64) * (ppa_len + 1))) {
-			ret = -EFAULT;
-			goto err_ppa;
-		}
-		vcmd->ph_rw.spba = cpu_to_le64(ppa_dma);
-	} else {
-		vcmd->ph_rw.spba = cpu_to_le64((uintptr_t)ppa_buf);
-	}
-
-	if (ubuf && bufflen) {
-		ret = blk_rq_map_user(q, rq, NULL, ubuf, bufflen, GFP_KERNEL);
-		if (ret)
-			goto err_ppa;
-		bio = rq->bio;
-
-		if (meta_buf && meta_len) {
-			metadata = dma_pool_alloc(dev->dma_pool, GFP_KERNEL,
-								&metadata_dma);
-			if (!metadata) {
-				ret = -ENOMEM;
-				goto err_map;
-			}
-
-			if (write) {
-				if (copy_from_user(metadata,
-						(void __user *)meta_buf,
-						meta_len)) {
-					ret = -EFAULT;
-					goto err_meta;
-				}
-			}
-			vcmd->ph_rw.metadata = cpu_to_le64(metadata_dma);
-		}
-
-		bio->bi_disk = disk;
-	}
-
-	blk_execute_rq(q, NULL, rq, 0);
-
-	if (nvme_req(rq)->flags & NVME_REQ_CANCELLED)
-		ret = -EINTR;
-	else if (nvme_req(rq)->status & 0x7ff)
-		ret = -EIO;
-	if (result)
-		*result = nvme_req(rq)->status & 0x7ff;
-	if (status)
-		*status = le64_to_cpu(nvme_req(rq)->result.u64);
-
-	if (metadata && !ret && !write) {
-		if (copy_to_user(meta_buf, (void *)metadata, meta_len))
-			ret = -EFAULT;
-	}
-err_meta:
-	if (meta_buf && meta_len)
-		dma_pool_free(dev->dma_pool, metadata, metadata_dma);
-err_map:
-	if (bio)
-		blk_rq_unmap_user(bio);
-err_ppa:
-	if (ppa_buf && ppa_len)
-		dma_pool_free(dev->dma_pool, ppa_list, ppa_dma);
-err_rq:
-	blk_mq_free_request(rq);
-err_cmd:
-	return ret;
-}
-
-static int nvme_nvm_submit_vio(struct nvme_ns *ns,
-					struct nvm_user_vio __user *uvio)
-{
-	struct nvm_user_vio vio;
-	struct nvme_nvm_command c;
-	unsigned int length;
-	int ret;
-
-	if (copy_from_user(&vio, uvio, sizeof(vio)))
-		return -EFAULT;
-	if (vio.flags)
-		return -EINVAL;
-
-	memset(&c, 0, sizeof(c));
-	c.ph_rw.opcode = vio.opcode;
-	c.ph_rw.nsid = cpu_to_le32(ns->ns_id);
-	c.ph_rw.control = cpu_to_le16(vio.control);
-	c.ph_rw.length = cpu_to_le16(vio.nppas);
-
-	length = (vio.nppas + 1) << ns->lba_shift;
-
-	ret = nvme_nvm_submit_user_cmd(ns->queue, ns, &c,
-			(void __user *)(uintptr_t)vio.addr, length,
-			(void __user *)(uintptr_t)vio.metadata,
-							vio.metadata_len,
-			(void __user *)(uintptr_t)vio.ppa_list, vio.nppas,
-			&vio.result, &vio.status, 0);
-
-	if (ret && copy_to_user(uvio, &vio, sizeof(vio)))
-		return -EFAULT;
-
-	return ret;
-}
-
-static int nvme_nvm_user_vcmd(struct nvme_ns *ns, int admin,
-					struct nvm_passthru_vio __user *uvcmd)
-{
-	struct nvm_passthru_vio vcmd;
-	struct nvme_nvm_command c;
-	struct request_queue *q;
-	unsigned int timeout = 0;
-	int ret;
-
-	if (copy_from_user(&vcmd, uvcmd, sizeof(vcmd)))
-		return -EFAULT;
-	if ((vcmd.opcode != 0xF2) && (!capable(CAP_SYS_ADMIN)))
-		return -EACCES;
-	if (vcmd.flags)
-		return -EINVAL;
-
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = vcmd.opcode;
-	c.common.nsid = cpu_to_le32(ns->ns_id);
-	c.common.cdw2[0] = cpu_to_le32(vcmd.cdw2);
-	c.common.cdw2[1] = cpu_to_le32(vcmd.cdw3);
-	/* cdw11-12 */
-	c.ph_rw.length = cpu_to_le16(vcmd.nppas);
-	c.ph_rw.control  = cpu_to_le16(vcmd.control);
-	c.common.cdw10[3] = cpu_to_le32(vcmd.cdw13);
-	c.common.cdw10[4] = cpu_to_le32(vcmd.cdw14);
-	c.common.cdw10[5] = cpu_to_le32(vcmd.cdw15);
-
-	if (vcmd.timeout_ms)
-		timeout = msecs_to_jiffies(vcmd.timeout_ms);
-
-	q = admin ? ns->ctrl->admin_q : ns->queue;
-
-	ret = nvme_nvm_submit_user_cmd(q, ns,
-			(struct nvme_nvm_command *)&c,
-			(void __user *)(uintptr_t)vcmd.addr, vcmd.data_len,
-			(void __user *)(uintptr_t)vcmd.metadata,
-							vcmd.metadata_len,
-			(void __user *)(uintptr_t)vcmd.ppa_list, vcmd.nppas,
-			&vcmd.result, &vcmd.status, timeout);
-
-	if (ret && copy_to_user(uvcmd, &vcmd, sizeof(vcmd)))
-		return -EFAULT;
-
-	return ret;
-}
-
-int nvme_nvm_ioctl(struct nvme_ns *ns, unsigned int cmd, unsigned long arg)
-{
-	switch (cmd) {
-	case NVME_NVM_IOCTL_ADMIN_VIO:
-		return nvme_nvm_user_vcmd(ns, 1, (void __user *)arg);
-	case NVME_NVM_IOCTL_IO_VIO:
-		return nvme_nvm_user_vcmd(ns, 0, (void __user *)arg);
-	case NVME_NVM_IOCTL_SUBMIT_VIO:
-		return nvme_nvm_submit_vio(ns, (void __user *)arg);
-	default:
-		return -ENOTTY;
-	}
-}
-
-int nvme_nvm_register(struct nvme_ns *ns, char *disk_name, int node)
-{
-	struct request_queue *q = ns->queue;
-	struct nvm_dev *dev;
-
-	_nvme_nvm_check_size();
-
-	dev = nvm_alloc_dev(node);
-	if (!dev)
-		return -ENOMEM;
-
-	dev->q = q;
-	memcpy(dev->name, disk_name, DISK_NAME_LEN);
-	dev->ops = &nvme_nvm_dev_ops;
-	dev->private_data = ns;
-	ns->ndev = dev;
-
-	return nvm_register(dev);
-}
-
-void nvme_nvm_unregister(struct nvme_ns *ns)
-{
-	nvm_unregister(ns->ndev);
-}
-
-static ssize_t nvm_dev_attr_show(struct device *dev,
-				 struct device_attribute *dattr, char *page)
-{
-	struct nvme_ns *ns = nvme_get_ns_from_dev(dev);
-	struct nvm_dev *ndev = ns->ndev;
-	struct nvm_id *id;
-	struct nvm_id_group *grp;
-	struct attribute *attr;
-
-	if (!ndev)
-		return 0;
-
-	id = &ndev->identity;
-	grp = &id->grp;
-	attr = &dattr->attr;
-
-	if (strcmp(attr->name, "version") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", id->ver_id);
-	} else if (strcmp(attr->name, "vendor_opcode") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", id->vmnt);
-	} else if (strcmp(attr->name, "capabilities") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", id->cap);
-	} else if (strcmp(attr->name, "device_mode") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", id->dom);
-	/* kept for compatibility */
-	} else if (strcmp(attr->name, "media_manager") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%s\n", "gennvm");
-	} else if (strcmp(attr->name, "ppa_format") == 0) {
-		return scnprintf(page, PAGE_SIZE,
-			"0x%02x%02x%02x%02x%02x%02x%02x%02x%02x%02x%02x%02x\n",
-			id->ppaf.ch_offset, id->ppaf.ch_len,
-			id->ppaf.lun_offset, id->ppaf.lun_len,
-			id->ppaf.pln_offset, id->ppaf.pln_len,
-			id->ppaf.blk_offset, id->ppaf.blk_len,
-			id->ppaf.pg_offset, id->ppaf.pg_len,
-			id->ppaf.sect_offset, id->ppaf.sect_len);
-	} else if (strcmp(attr->name, "media_type") == 0) {	/* u8 */
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->mtype);
-	} else if (strcmp(attr->name, "flash_media_type") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->fmtype);
-	} else if (strcmp(attr->name, "num_channels") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->num_ch);
-	} else if (strcmp(attr->name, "num_luns") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->num_lun);
-	} else if (strcmp(attr->name, "num_planes") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->num_pln);
-	} else if (strcmp(attr->name, "num_blocks") == 0) {	/* u16 */
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->num_blk);
-	} else if (strcmp(attr->name, "num_pages") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->num_pg);
-	} else if (strcmp(attr->name, "page_size") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->fpg_sz);
-	} else if (strcmp(attr->name, "hw_sector_size") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->csecs);
-	} else if (strcmp(attr->name, "oob_sector_size") == 0) {/* u32 */
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->sos);
-	} else if (strcmp(attr->name, "read_typ") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->trdt);
-	} else if (strcmp(attr->name, "read_max") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->trdm);
-	} else if (strcmp(attr->name, "prog_typ") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->tprt);
-	} else if (strcmp(attr->name, "prog_max") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->tprm);
-	} else if (strcmp(attr->name, "erase_typ") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->tbet);
-	} else if (strcmp(attr->name, "erase_max") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n", grp->tbem);
-	} else if (strcmp(attr->name, "multiplane_modes") == 0) {
-		return scnprintf(page, PAGE_SIZE, "0x%08x\n", grp->mpos);
-	} else if (strcmp(attr->name, "media_capabilities") == 0) {
-		return scnprintf(page, PAGE_SIZE, "0x%08x\n", grp->mccap);
-	} else if (strcmp(attr->name, "max_phys_secs") == 0) {
-		return scnprintf(page, PAGE_SIZE, "%u\n",
-				ndev->ops->max_phys_sect);
-	} else {
-		return scnprintf(page,
-				 PAGE_SIZE,
-				 "Unhandled attr(%s) in `nvm_dev_attr_show`\n",
-				 attr->name);
-	}
-}
-
-#define NVM_DEV_ATTR_RO(_name)						\
-	DEVICE_ATTR(_name, S_IRUGO, nvm_dev_attr_show, NULL)
-
-static NVM_DEV_ATTR_RO(version);
-static NVM_DEV_ATTR_RO(vendor_opcode);
-static NVM_DEV_ATTR_RO(capabilities);
-static NVM_DEV_ATTR_RO(device_mode);
-static NVM_DEV_ATTR_RO(ppa_format);
-static NVM_DEV_ATTR_RO(media_manager);
-
-static NVM_DEV_ATTR_RO(media_type);
-static NVM_DEV_ATTR_RO(flash_media_type);
-static NVM_DEV_ATTR_RO(num_channels);
-static NVM_DEV_ATTR_RO(num_luns);
-static NVM_DEV_ATTR_RO(num_planes);
-static NVM_DEV_ATTR_RO(num_blocks);
-static NVM_DEV_ATTR_RO(num_pages);
-static NVM_DEV_ATTR_RO(page_size);
-static NVM_DEV_ATTR_RO(hw_sector_size);
-static NVM_DEV_ATTR_RO(oob_sector_size);
-static NVM_DEV_ATTR_RO(read_typ);
-static NVM_DEV_ATTR_RO(read_max);
-static NVM_DEV_ATTR_RO(prog_typ);
-static NVM_DEV_ATTR_RO(prog_max);
-static NVM_DEV_ATTR_RO(erase_typ);
-static NVM_DEV_ATTR_RO(erase_max);
-static NVM_DEV_ATTR_RO(multiplane_modes);
-static NVM_DEV_ATTR_RO(media_capabilities);
-static NVM_DEV_ATTR_RO(max_phys_secs);
-
-static struct attribute *nvm_dev_attrs[] = {
-	&dev_attr_version.attr,
-	&dev_attr_vendor_opcode.attr,
-	&dev_attr_capabilities.attr,
-	&dev_attr_device_mode.attr,
-	&dev_attr_media_manager.attr,
-
-	&dev_attr_ppa_format.attr,
-	&dev_attr_media_type.attr,
-	&dev_attr_flash_media_type.attr,
-	&dev_attr_num_channels.attr,
-	&dev_attr_num_luns.attr,
-	&dev_attr_num_planes.attr,
-	&dev_attr_num_blocks.attr,
-	&dev_attr_num_pages.attr,
-	&dev_attr_page_size.attr,
-	&dev_attr_hw_sector_size.attr,
-	&dev_attr_oob_sector_size.attr,
-	&dev_attr_read_typ.attr,
-	&dev_attr_read_max.attr,
-	&dev_attr_prog_typ.attr,
-	&dev_attr_prog_max.attr,
-	&dev_attr_erase_typ.attr,
-	&dev_attr_erase_max.attr,
-	&dev_attr_multiplane_modes.attr,
-	&dev_attr_media_capabilities.attr,
-	&dev_attr_max_phys_secs.attr,
-	NULL,
-};
-
-static const struct attribute_group nvm_dev_attr_group = {
-	.name		= "lightnvm",
-	.attrs		= nvm_dev_attrs,
-};
-
-int nvme_nvm_register_sysfs(struct nvme_ns *ns)
-{
-	return sysfs_create_group(&disk_to_dev(ns->disk)->kobj,
-					&nvm_dev_attr_group);
-}
-
-void nvme_nvm_unregister_sysfs(struct nvme_ns *ns)
-{
-	sysfs_remove_group(&disk_to_dev(ns->disk)->kobj,
-					&nvm_dev_attr_group);
-}
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/linux_nvme.h b/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/linux_nvme.h
deleted file mode 100644
index b3d7934..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/linux_nvme.h
+++ /dev/null
@@ -1,1344 +0,0 @@
-/*
- * Definitions for the NVM Express interface
- * Copyright (c) 2011-2014, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- */
-
-#ifndef _LINUX_NVME_H
-#define _LINUX_NVME_H
-
-#include <linux/types.h>
-#include <linux/uuid.h>
-
-/* NQN names in commands fields specified one size */
-#define NVMF_NQN_FIELD_LEN	256
-
-/* However the max length of a qualified name is another size */
-#define NVMF_NQN_SIZE		223
-
-#define NVMF_TRSVCID_SIZE	32
-#define NVMF_TRADDR_SIZE	256
-#define NVMF_TSAS_SIZE		256
-
-#define NVME_DISC_SUBSYS_NAME	"nqn.2014-08.org.nvmexpress.discovery"
-
-#define NVME_RDMA_IP_PORT	4420
-
-#define NVME_NSID_ALL		0xffffffff
-
-enum nvme_subsys_type {
-	NVME_NQN_DISC	= 1,		/* Discovery type target subsystem */
-	NVME_NQN_NVME	= 2,		/* NVME type target subsystem */
-};
-
-/* Address Family codes for Discovery Log Page entry ADRFAM field */
-enum {
-	NVMF_ADDR_FAMILY_PCI	= 0,	/* PCIe */
-	NVMF_ADDR_FAMILY_IP4	= 1,	/* IP4 */
-	NVMF_ADDR_FAMILY_IP6	= 2,	/* IP6 */
-	NVMF_ADDR_FAMILY_IB	= 3,	/* InfiniBand */
-	NVMF_ADDR_FAMILY_FC	= 4,	/* Fibre Channel */
-};
-
-/* Transport Type codes for Discovery Log Page entry TRTYPE field */
-enum {
-	NVMF_TRTYPE_RDMA	= 1,	/* RDMA */
-	NVMF_TRTYPE_FC		= 2,	/* Fibre Channel */
-	NVMF_TRTYPE_LOOP	= 254,	/* Reserved for host usage */
-	NVMF_TRTYPE_MAX,
-};
-
-/* Transport Requirements codes for Discovery Log Page entry TREQ field */
-enum {
-	NVMF_TREQ_NOT_SPECIFIED	= 0,	/* Not specified */
-	NVMF_TREQ_REQUIRED	= 1,	/* Required */
-	NVMF_TREQ_NOT_REQUIRED	= 2,	/* Not Required */
-};
-
-/* RDMA QP Service Type codes for Discovery Log Page entry TSAS
- * RDMA_QPTYPE field
- */
-enum {
-	NVMF_RDMA_QPTYPE_CONNECTED	= 1, /* Reliable Connected */
-	NVMF_RDMA_QPTYPE_DATAGRAM	= 2, /* Reliable Datagram */
-};
-
-/* RDMA QP Service Type codes for Discovery Log Page entry TSAS
- * RDMA_QPTYPE field
- */
-enum {
-	NVMF_RDMA_PRTYPE_NOT_SPECIFIED	= 1, /* No Provider Specified */
-	NVMF_RDMA_PRTYPE_IB		= 2, /* InfiniBand */
-	NVMF_RDMA_PRTYPE_ROCE		= 3, /* InfiniBand RoCE */
-	NVMF_RDMA_PRTYPE_ROCEV2		= 4, /* InfiniBand RoCEV2 */
-	NVMF_RDMA_PRTYPE_IWARP		= 5, /* IWARP */
-};
-
-/* RDMA Connection Management Service Type codes for Discovery Log Page
- * entry TSAS RDMA_CMS field
- */
-enum {
-	NVMF_RDMA_CMS_RDMA_CM	= 1, /* Sockets based endpoint addressing */
-};
-
-#define NVME_AQ_DEPTH		32
-
-enum {
-	NVME_REG_CAP	= 0x0000,	/* Controller Capabilities */
-	NVME_REG_VS	= 0x0008,	/* Version */
-	NVME_REG_INTMS	= 0x000c,	/* Interrupt Mask Set */
-	NVME_REG_INTMC	= 0x0010,	/* Interrupt Mask Clear */
-	NVME_REG_CC	= 0x0014,	/* Controller Configuration */
-	NVME_REG_CSTS	= 0x001c,	/* Controller Status */
-	NVME_REG_NSSR	= 0x0020,	/* NVM Subsystem Reset */
-	NVME_REG_AQA	= 0x0024,	/* Admin Queue Attributes */
-	NVME_REG_ASQ	= 0x0028,	/* Admin SQ Base Address */
-	NVME_REG_ACQ	= 0x0030,	/* Admin CQ Base Address */
-	NVME_REG_CMBLOC = 0x0038,	/* Controller Memory Buffer Location */
-	NVME_REG_CMBSZ	= 0x003c,	/* Controller Memory Buffer Size */
-	NVME_REG_DBS	= 0x1000,	/* SQ 0 Tail Doorbell */
-};
-
-#define NVME_CAP_MQES(cap)	((cap) & 0xffff)
-#define NVME_CAP_TIMEOUT(cap)	(((cap) >> 24) & 0xff)
-#define NVME_CAP_STRIDE(cap)	(((cap) >> 32) & 0xf)
-#define NVME_CAP_NSSRC(cap)	(((cap) >> 36) & 0x1)
-#define NVME_CAP_MPSMIN(cap)	(((cap) >> 48) & 0xf)
-#define NVME_CAP_MPSMAX(cap)	(((cap) >> 52) & 0xf)
-
-#define NVME_CMB_BIR(cmbloc)	((cmbloc) & 0x7)
-#define NVME_CMB_OFST(cmbloc)	(((cmbloc) >> 12) & 0xfffff)
-#define NVME_CMB_SZ(cmbsz)	(((cmbsz) >> 12) & 0xfffff)
-#define NVME_CMB_SZU(cmbsz)	(((cmbsz) >> 8) & 0xf)
-
-#define NVME_CMB_WDS(cmbsz)	((cmbsz) & 0x10)
-#define NVME_CMB_RDS(cmbsz)	((cmbsz) & 0x8)
-#define NVME_CMB_LISTS(cmbsz)	((cmbsz) & 0x4)
-#define NVME_CMB_CQS(cmbsz)	((cmbsz) & 0x2)
-#define NVME_CMB_SQS(cmbsz)	((cmbsz) & 0x1)
-
-/*
- * Submission and Completion Queue Entry Sizes for the NVM command set.
- * (In bytes and specified as a power of two (2^n)).
- */
-#define NVME_NVM_IOSQES		6
-#define NVME_NVM_IOCQES		4
-
-enum {
-	NVME_CC_ENABLE		= 1 << 0,
-	NVME_CC_CSS_NVM		= 0 << 4,
-	NVME_CC_EN_SHIFT	= 0,
-	NVME_CC_CSS_SHIFT	= 4,
-	NVME_CC_MPS_SHIFT	= 7,
-	NVME_CC_AMS_SHIFT	= 11,
-	NVME_CC_SHN_SHIFT	= 14,
-	NVME_CC_IOSQES_SHIFT	= 16,
-	NVME_CC_IOCQES_SHIFT	= 20,
-	NVME_CC_AMS_RR		= 0 << NVME_CC_AMS_SHIFT,
-	NVME_CC_AMS_WRRU	= 1 << NVME_CC_AMS_SHIFT,
-	NVME_CC_AMS_VS		= 7 << NVME_CC_AMS_SHIFT,
-	NVME_CC_SHN_NONE	= 0 << NVME_CC_SHN_SHIFT,
-	NVME_CC_SHN_NORMAL	= 1 << NVME_CC_SHN_SHIFT,
-	NVME_CC_SHN_ABRUPT	= 2 << NVME_CC_SHN_SHIFT,
-	NVME_CC_SHN_MASK	= 3 << NVME_CC_SHN_SHIFT,
-	NVME_CC_IOSQES		= NVME_NVM_IOSQES << NVME_CC_IOSQES_SHIFT,
-	NVME_CC_IOCQES		= NVME_NVM_IOCQES << NVME_CC_IOCQES_SHIFT,
-	NVME_CSTS_RDY		= 1 << 0,
-	NVME_CSTS_CFS		= 1 << 1,
-	NVME_CSTS_NSSRO		= 1 << 4,
-	NVME_CSTS_PP		= 1 << 5,
-	NVME_CSTS_SHST_NORMAL	= 0 << 2,
-	NVME_CSTS_SHST_OCCUR	= 1 << 2,
-	NVME_CSTS_SHST_CMPLT	= 2 << 2,
-	NVME_CSTS_SHST_MASK	= 3 << 2,
-};
-
-struct nvme_id_power_state {
-	__le16			max_power;	/* centiwatts */
-	__u8			rsvd2;
-	__u8			flags;
-	__le32			entry_lat;	/* microseconds */
-	__le32			exit_lat;	/* microseconds */
-	__u8			read_tput;
-	__u8			read_lat;
-	__u8			write_tput;
-	__u8			write_lat;
-	__le16			idle_power;
-	__u8			idle_scale;
-	__u8			rsvd19;
-	__le16			active_power;
-	__u8			active_work_scale;
-	__u8			rsvd23[9];
-};
-
-enum {
-	NVME_PS_FLAGS_MAX_POWER_SCALE	= 1 << 0,
-	NVME_PS_FLAGS_NON_OP_STATE	= 1 << 1,
-};
-
-struct nvme_id_ctrl {
-	__le16			vid;
-	__le16			ssvid;
-	char			sn[20];
-	char			mn[40];
-	char			fr[8];
-	__u8			rab;
-	__u8			ieee[3];
-	__u8			cmic;
-	__u8			mdts;
-	__le16			cntlid;
-	__le32			ver;
-	__le32			rtd3r;
-	__le32			rtd3e;
-	__le32			oaes;
-	__le32			ctratt;
-	__u8			rsvd100[156];
-	__le16			oacs;
-	__u8			acl;
-	__u8			aerl;
-	__u8			frmw;
-	__u8			lpa;
-	__u8			elpe;
-	__u8			npss;
-	__u8			avscc;
-	__u8			apsta;
-	__le16			wctemp;
-	__le16			cctemp;
-	__le16			mtfa;
-	__le32			hmpre;
-	__le32			hmmin;
-	__u8			tnvmcap[16];
-	__u8			unvmcap[16];
-	__le32			rpmbs;
-	__le16			edstt;
-	__u8			dsto;
-	__u8			fwug;
-	__le16			kas;
-	__le16			hctma;
-	__le16			mntmt;
-	__le16			mxtmt;
-	__le32			sanicap;
-	__le32			hmminds;
-	__le16			hmmaxd;
-	__u8			rsvd338[174];
-	__u8			sqes;
-	__u8			cqes;
-	__le16			maxcmd;
-	__le32			nn;
-	__le16			oncs;
-	__le16			fuses;
-	__u8			fna;
-	__u8			vwc;
-	__le16			awun;
-	__le16			awupf;
-	__u8			nvscc;
-	__u8			rsvd531;
-	__le16			acwu;
-	__u8			rsvd534[2];
-	__le32			sgls;
-	__u8			rsvd540[228];
-	char			subnqn[256];
-	__u8			rsvd1024[768];
-	__le32			ioccsz;
-	__le32			iorcsz;
-	__le16			icdoff;
-	__u8			ctrattr;
-	__u8			msdbd;
-	__u8			rsvd1804[244];
-	struct nvme_id_power_state	psd[32];
-	__u8			vs[1024];
-};
-
-enum {
-	NVME_CTRL_ONCS_COMPARE			= 1 << 0,
-	NVME_CTRL_ONCS_WRITE_UNCORRECTABLE	= 1 << 1,
-	NVME_CTRL_ONCS_DSM			= 1 << 2,
-	NVME_CTRL_ONCS_WRITE_ZEROES		= 1 << 3,
-	NVME_CTRL_ONCS_TIMESTAMP		= 1 << 6,
-	NVME_CTRL_VWC_PRESENT			= 1 << 0,
-	NVME_CTRL_OACS_SEC_SUPP                 = 1 << 0,
-	NVME_CTRL_OACS_DIRECTIVES		= 1 << 5,
-	NVME_CTRL_OACS_DBBUF_SUPP		= 1 << 8,
-};
-
-struct nvme_lbaf {
-	__le16			ms;
-	__u8			ds;
-	__u8			rp;
-};
-
-struct nvme_id_ns {
-	__le64			nsze;
-	__le64			ncap;
-	__le64			nuse;
-	__u8			nsfeat;
-	__u8			nlbaf;
-	__u8			flbas;
-	__u8			mc;
-	__u8			dpc;
-	__u8			dps;
-	__u8			nmic;
-	__u8			rescap;
-	__u8			fpi;
-	__u8			rsvd33;
-	__le16			nawun;
-	__le16			nawupf;
-	__le16			nacwu;
-	__le16			nabsn;
-	__le16			nabo;
-	__le16			nabspf;
-	__le16			noiob;
-	__u8			nvmcap[16];
-	__u8			rsvd64[40];
-	__u8			nguid[16];
-	__u8			eui64[8];
-	struct nvme_lbaf	lbaf[16];
-	__u8			rsvd192[192];
-	__u8			vs[3712];
-};
-
-enum {
-	NVME_ID_CNS_NS			= 0x00,
-	NVME_ID_CNS_CTRL		= 0x01,
-	NVME_ID_CNS_NS_ACTIVE_LIST	= 0x02,
-	NVME_ID_CNS_NS_DESC_LIST	= 0x03,
-	NVME_ID_CNS_NS_PRESENT_LIST	= 0x10,
-	NVME_ID_CNS_NS_PRESENT		= 0x11,
-	NVME_ID_CNS_CTRL_NS_LIST	= 0x12,
-	NVME_ID_CNS_CTRL_LIST		= 0x13,
-};
-
-enum {
-	NVME_DIR_IDENTIFY		= 0x00,
-	NVME_DIR_STREAMS		= 0x01,
-	NVME_DIR_SND_ID_OP_ENABLE	= 0x01,
-	NVME_DIR_SND_ST_OP_REL_ID	= 0x01,
-	NVME_DIR_SND_ST_OP_REL_RSC	= 0x02,
-	NVME_DIR_RCV_ID_OP_PARAM	= 0x01,
-	NVME_DIR_RCV_ST_OP_PARAM	= 0x01,
-	NVME_DIR_RCV_ST_OP_STATUS	= 0x02,
-	NVME_DIR_RCV_ST_OP_RESOURCE	= 0x03,
-	NVME_DIR_ENDIR			= 0x01,
-};
-
-enum {
-	NVME_NS_FEAT_THIN	= 1 << 0,
-	NVME_NS_FLBAS_LBA_MASK	= 0xf,
-	NVME_NS_FLBAS_META_EXT	= 0x10,
-	NVME_LBAF_RP_BEST	= 0,
-	NVME_LBAF_RP_BETTER	= 1,
-	NVME_LBAF_RP_GOOD	= 2,
-	NVME_LBAF_RP_DEGRADED	= 3,
-	NVME_NS_DPC_PI_LAST	= 1 << 4,
-	NVME_NS_DPC_PI_FIRST	= 1 << 3,
-	NVME_NS_DPC_PI_TYPE3	= 1 << 2,
-	NVME_NS_DPC_PI_TYPE2	= 1 << 1,
-	NVME_NS_DPC_PI_TYPE1	= 1 << 0,
-	NVME_NS_DPS_PI_FIRST	= 1 << 3,
-	NVME_NS_DPS_PI_MASK	= 0x7,
-	NVME_NS_DPS_PI_TYPE1	= 1,
-	NVME_NS_DPS_PI_TYPE2	= 2,
-	NVME_NS_DPS_PI_TYPE3	= 3,
-};
-
-struct nvme_ns_id_desc {
-	__u8 nidt;
-	__u8 nidl;
-	__le16 reserved;
-};
-
-#define NVME_NIDT_EUI64_LEN	8
-#define NVME_NIDT_NGUID_LEN	16
-#define NVME_NIDT_UUID_LEN	16
-
-enum {
-	NVME_NIDT_EUI64		= 0x01,
-	NVME_NIDT_NGUID		= 0x02,
-	NVME_NIDT_UUID		= 0x03,
-};
-
-struct nvme_smart_log {
-	__u8			critical_warning;
-	__u8			temperature[2];
-	__u8			avail_spare;
-	__u8			spare_thresh;
-	__u8			percent_used;
-	__u8			rsvd6[26];
-	__u8			data_units_read[16];
-	__u8			data_units_written[16];
-	__u8			host_reads[16];
-	__u8			host_writes[16];
-	__u8			ctrl_busy_time[16];
-	__u8			power_cycles[16];
-	__u8			power_on_hours[16];
-	__u8			unsafe_shutdowns[16];
-	__u8			media_errors[16];
-	__u8			num_err_log_entries[16];
-	__le32			warning_temp_time;
-	__le32			critical_comp_time;
-	__le16			temp_sensor[8];
-	__u8			rsvd216[296];
-};
-
-struct nvme_fw_slot_info_log {
-	__u8			afi;
-	__u8			rsvd1[7];
-	__le64			frs[7];
-	__u8			rsvd64[448];
-};
-
-enum {
-	NVME_SMART_CRIT_SPARE		= 1 << 0,
-	NVME_SMART_CRIT_TEMPERATURE	= 1 << 1,
-	NVME_SMART_CRIT_RELIABILITY	= 1 << 2,
-	NVME_SMART_CRIT_MEDIA		= 1 << 3,
-	NVME_SMART_CRIT_VOLATILE_MEMORY	= 1 << 4,
-};
-
-enum {
-	NVME_AER_NOTICE_NS_CHANGED	= 0x0002,
-	NVME_AER_NOTICE_FW_ACT_STARTING = 0x0102,
-};
-
-struct nvme_lba_range_type {
-	__u8			type;
-	__u8			attributes;
-	__u8			rsvd2[14];
-	__u64			slba;
-	__u64			nlb;
-	__u8			guid[16];
-	__u8			rsvd48[16];
-};
-
-enum {
-	NVME_LBART_TYPE_FS	= 0x01,
-	NVME_LBART_TYPE_RAID	= 0x02,
-	NVME_LBART_TYPE_CACHE	= 0x03,
-	NVME_LBART_TYPE_SWAP	= 0x04,
-
-	NVME_LBART_ATTRIB_TEMP	= 1 << 0,
-	NVME_LBART_ATTRIB_HIDE	= 1 << 1,
-};
-
-struct nvme_reservation_status {
-	__le32	gen;
-	__u8	rtype;
-	__u8	regctl[2];
-	__u8	resv5[2];
-	__u8	ptpls;
-	__u8	resv10[13];
-	struct {
-		__le16	cntlid;
-		__u8	rcsts;
-		__u8	resv3[5];
-		__le64	hostid;
-		__le64	rkey;
-	} regctl_ds[];
-};
-
-enum nvme_async_event_type {
-	NVME_AER_TYPE_ERROR	= 0,
-	NVME_AER_TYPE_SMART	= 1,
-	NVME_AER_TYPE_NOTICE	= 2,
-};
-
-/* I/O commands */
-
-enum nvme_opcode {
-	nvme_cmd_flush		= 0x00,
-	nvme_cmd_write		= 0x01,
-	nvme_cmd_read		= 0x02,
-	nvme_cmd_write_uncor	= 0x04,
-	nvme_cmd_compare	= 0x05,
-	nvme_cmd_write_zeroes	= 0x08,
-	nvme_cmd_dsm		= 0x09,
-	nvme_cmd_resv_register	= 0x0d,
-	nvme_cmd_resv_report	= 0x0e,
-	nvme_cmd_resv_acquire	= 0x11,
-	nvme_cmd_resv_release	= 0x15,
-
-  nvme_cmd_kv_store = 0x81,
-  nvme_cmd_kv_append = 0x83,
-  nvme_cmd_kv_retrieve = 0x90,
-  nvme_cmd_kv_delete = 0xA1,
-  nvme_cmd_kv_iter_req = 0xB1,
-  nvme_cmd_kv_iter_read = 0xB2,
-  nvme_cmd_kv_exist = 0xB3,
-};
-
-#define KVCMD_INLINE_KEY_MAX    (16)
-#define KVCMD_MAX_KEY_SIZE      (255)
-#define KVCMD_MIN_KEY_SIZE      (4)
-
-/*
- * Descriptor subtype - lower 4 bits of nvme_(keyed_)sgl_desc identifier
- *
- * @NVME_SGL_FMT_ADDRESS:     absolute address of the data block
- * @NVME_SGL_FMT_OFFSET:      relative offset of the in-capsule data block
- * @NVME_SGL_FMT_TRANSPORT_A: transport defined format, value 0xA
- * @NVME_SGL_FMT_INVALIDATE:  RDMA transport specific remote invalidation
- *                            request subtype
- */
-enum {
-	NVME_SGL_FMT_ADDRESS		= 0x00,
-	NVME_SGL_FMT_OFFSET		= 0x01,
-	NVME_SGL_FMT_TRANSPORT_A	= 0x0A,
-	NVME_SGL_FMT_INVALIDATE		= 0x0f,
-};
-
-/*
- * Descriptor type - upper 4 bits of nvme_(keyed_)sgl_desc identifier
- *
- * For struct nvme_sgl_desc:
- *   @NVME_SGL_FMT_DATA_DESC:		data block descriptor
- *   @NVME_SGL_FMT_SEG_DESC:		sgl segment descriptor
- *   @NVME_SGL_FMT_LAST_SEG_DESC:	last sgl segment descriptor
- *
- * For struct nvme_keyed_sgl_desc:
- *   @NVME_KEY_SGL_FMT_DATA_DESC:	keyed data block descriptor
- *
- * Transport-specific SGL types:
- *   @NVME_TRANSPORT_SGL_DATA_DESC:	Transport SGL data dlock descriptor
- */
-enum {
-	NVME_SGL_FMT_DATA_DESC		= 0x00,
-	NVME_SGL_FMT_SEG_DESC		= 0x02,
-	NVME_SGL_FMT_LAST_SEG_DESC	= 0x03,
-	NVME_KEY_SGL_FMT_DATA_DESC	= 0x04,
-	NVME_TRANSPORT_SGL_DATA_DESC	= 0x05,
-};
-
-struct nvme_sgl_desc {
-	__le64	addr;
-	__le32	length;
-	__u8	rsvd[3];
-	__u8	type;
-};
-
-struct nvme_keyed_sgl_desc {
-	__le64	addr;
-	__u8	length[3];
-	__u8	key[4];
-	__u8	type;
-};
-
-union nvme_data_ptr {
-	struct {
-		__le64	prp1;
-		__le64	prp2;
-	};
-	struct nvme_sgl_desc	sgl;
-	struct nvme_keyed_sgl_desc ksgl;
-};
-
-/*
- * Lowest two bits of our flags field (FUSE field in the spec):
- *
- * @NVME_CMD_FUSE_FIRST:   Fused Operation, first command
- * @NVME_CMD_FUSE_SECOND:  Fused Operation, second command
- *
- * Highest two bits in our flags field (PSDT field in the spec):
- *
- * @NVME_CMD_PSDT_SGL_METABUF:	Use SGLS for this transfer,
- *	If used, MPTR contains addr of single physical buffer (byte aligned).
- * @NVME_CMD_PSDT_SGL_METASEG:	Use SGLS for this transfer,
- *	If used, MPTR contains an address of an SGL segment containing
- *	exactly 1 SGL descriptor (qword aligned).
- */
-enum {
-	NVME_CMD_FUSE_FIRST	= (1 << 0),
-	NVME_CMD_FUSE_SECOND	= (1 << 1),
-
-	NVME_CMD_SGL_METABUF	= (1 << 6),
-	NVME_CMD_SGL_METASEG	= (1 << 7),
-	NVME_CMD_SGL_ALL	= NVME_CMD_SGL_METABUF | NVME_CMD_SGL_METASEG,
-};
-
-struct nvme_common_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__le32			cdw2[2];
-	__le64			metadata;
-	union nvme_data_ptr	dptr;
-	__le32			cdw10[6];
-};
-
-struct nvme_rw_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2;
-	__le64			metadata;
-	union nvme_data_ptr	dptr;
-	__le64			slba;
-	__le16			length;
-	__le16			control;
-	__le32			dsmgmt;
-	__le32			reftag;
-	__le16			apptag;
-	__le16			appmask;
-};
-
-enum {
-	NVME_RW_LR			= 1 << 15,
-	NVME_RW_FUA			= 1 << 14,
-	NVME_RW_DSM_FREQ_UNSPEC		= 0,
-	NVME_RW_DSM_FREQ_TYPICAL	= 1,
-	NVME_RW_DSM_FREQ_RARE		= 2,
-	NVME_RW_DSM_FREQ_READS		= 3,
-	NVME_RW_DSM_FREQ_WRITES		= 4,
-	NVME_RW_DSM_FREQ_RW		= 5,
-	NVME_RW_DSM_FREQ_ONCE		= 6,
-	NVME_RW_DSM_FREQ_PREFETCH	= 7,
-	NVME_RW_DSM_FREQ_TEMP		= 8,
-	NVME_RW_DSM_LATENCY_NONE	= 0 << 4,
-	NVME_RW_DSM_LATENCY_IDLE	= 1 << 4,
-	NVME_RW_DSM_LATENCY_NORM	= 2 << 4,
-	NVME_RW_DSM_LATENCY_LOW		= 3 << 4,
-	NVME_RW_DSM_SEQ_REQ		= 1 << 6,
-	NVME_RW_DSM_COMPRESSED		= 1 << 7,
-	NVME_RW_PRINFO_PRCHK_REF	= 1 << 10,
-	NVME_RW_PRINFO_PRCHK_APP	= 1 << 11,
-	NVME_RW_PRINFO_PRCHK_GUARD	= 1 << 12,
-	NVME_RW_PRINFO_PRACT		= 1 << 13,
-	NVME_RW_DTYPE_STREAMS		= 1 << 4,
-};
-
-struct nvme_dsm_cmd {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[2];
-	union nvme_data_ptr	dptr;
-	__le32			nr;
-	__le32			attributes;
-	__u32			rsvd12[4];
-};
-
-enum {
-	NVME_DSMGMT_IDR		= 1 << 0,
-	NVME_DSMGMT_IDW		= 1 << 1,
-	NVME_DSMGMT_AD		= 1 << 2,
-};
-
-#define NVME_DSM_MAX_RANGES	256
-
-struct nvme_dsm_range {
-	__le32			cattr;
-	__le32			nlb;
-	__le64			slba;
-};
-
-struct nvme_write_zeroes_cmd {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2;
-	__le64			metadata;
-	union nvme_data_ptr	dptr;
-	__le64			slba;
-	__le16			length;
-	__le16			control;
-	__le32			dsmgmt;
-	__le32			reftag;
-	__le16			apptag;
-	__le16			appmask;
-};
-
-/* Features */
-
-struct nvme_feat_auto_pst {
-	__le64 entries[32];
-};
-
-enum {
-	NVME_HOST_MEM_ENABLE	= (1 << 0),
-	NVME_HOST_MEM_RETURN	= (1 << 1),
-};
-
-/*KV SSD Command*/
-struct nvme_kv_store_command {
-  __u8    opcde;
-  __u8    flags;
-  __u16   command_id;
-  __le32  nsid;
-  __u64   rsvd;
-  __le32  offset;
-  __u32   rsvd2;
-  union nvme_data_ptr dptr; /* value dptr prp1,2 */
-  __le32  value_len;        /* size in word */
-  __u8    key_len;          /* 0 ~ 255 (keylen - 1) */
-  __u8    option;
-  __u8    invalid_byte:2;
-  __u8    rsvd3:6;
-  __u8    rsvd4;
-  union {
-    struct {
-      char    key[16];
-    };
-    struct {
-      __le64  key_prp;
-      __le64  key_prp2;
-    };
-  };
-};
-
-struct nvme_kv_append_command {
-  __u8    opcde;
-  __u8    flags;
-  __u16   command_id;
-  __le32  nsid;
-  __u64   rsvd;
-  __le32  offset;
-  __u32   rsvd2;
-  union nvme_data_ptr dptr; /* value dptr prp1,2 */
-  __le32  value_len;        /* size in word */
-  __u8    key_len;          /* 0 ~ 255 (keylen - 1) */
-  __u8    option;
-  __u8    invalid_byte:2;
-  __u8    rsvd3:6;
-  __u8    rsvd4;
-  union {
-    struct {
-      char    key[16];
-    };
-    struct {
-      __le64  key_prp;
-      __le64  key_prp2;
-    };
-  };
-};
-
-struct nvme_kv_retrieve_command {
-  __u8    opcde;
-  __u8    flags;
-  __u16   command_id;
-  __le32  nsid;
-  __u64   rsvd;
-  __le32  offset;
-  __u32   rsvd2;
-  union nvme_data_ptr dptr; /* value dptr prp1,2 */
-  __le32  value_len;        /* size in word */
-  __u8    key_len;          /* 0 ~ 255 (keylen - 1) */
-  __u8    option;
-  __u16   rsvd3;
-  union {
-    struct {
-      char    key[16];
-    };
-    struct {
-      __le64  key_prp;
-      __le64  key_prp2;
-    };
-  };
-};
-
-struct nvme_kv_delete_command {
-  __u8    opcde;
-  __u8    flags;
-  __u16   command_id;
-  __le32  nsid;
-  __u64   rsvd;
-  __le32  offset;
-  __u32   rsvd2;
-  __u64   rsvd3[2];
-  __le32  value_len;        /* size in word */
-  __u8    key_len;          /* 0 ~ 255 (keylen - 1) */
-  __u8    option;
-  __u16   rsvd4;
-  union {
-    struct {
-      char    key[16];
-    };
-    struct {
-      __le64  key_prp;
-      __le64  key_prp2;
-    };
-  };
-};
-
-struct nvme_kv_iter_req_command {
-  __u8    opcde;
-  __u8    flags;
-  __u16   command_id;
-  __le32  nsid;
-  __u64   rsvd[4];
-  __le32  zero;             /* should be zero */
-  __u8    iter_handle;
-  __u8    option;
-  __u16   rsvd2;
-  __u32   iter_val;
-  __u32   iter_bitmask;
-  __u64   rsvd3;
-};
-
-struct nvme_kv_iter_read_command {
-  __u8    opcde;
-  __u8    flags;
-  __u16   command_id;
-  __le32  nsid;
-  __u64   rsvd[2];
-  union nvme_data_ptr dptr; /* value dptr prp1,2 */
-  __le32  value_len;        /* size in word */
-  __u8    iter_handle;
-  __u8    option;
-  __u16   rsvd2;
-  __u64   rsvd3[2];
-};
-
-struct nvme_kv_exist_command {
-  __u8    opcde;
-  __u8    flags;
-  __u16   command_id;
-  __le32  nsid;
-  __u64   rsvd;
-  __le32  offset;
-  __u32   rsvd2;
-  __u64   rsvd3[2];
-  __le32  value_len;        /* size in word */
-  __u8    key_len;          /* 0 ~ 255 (keylen - 1) */
-  __u8    option;
-  __u16   rsvd4;
-  union {
-    struct {
-      char    key[16];
-    };
-    struct {
-      __le64  key_prp;
-      __le64  key_prp2;
-    };
-  };
-};
-
-/* Admin commands */
-
-enum nvme_admin_opcode {
-	nvme_admin_delete_sq		= 0x00,
-	nvme_admin_create_sq		= 0x01,
-	nvme_admin_get_log_page		= 0x02,
-	nvme_admin_delete_cq		= 0x04,
-	nvme_admin_create_cq		= 0x05,
-	nvme_admin_identify		= 0x06,
-	nvme_admin_abort_cmd		= 0x08,
-	nvme_admin_set_features		= 0x09,
-	nvme_admin_get_features		= 0x0a,
-	nvme_admin_async_event		= 0x0c,
-	nvme_admin_ns_mgmt		= 0x0d,
-	nvme_admin_activate_fw		= 0x10,
-	nvme_admin_download_fw		= 0x11,
-	nvme_admin_ns_attach		= 0x15,
-	nvme_admin_keep_alive		= 0x18,
-	nvme_admin_directive_send	= 0x19,
-	nvme_admin_directive_recv	= 0x1a,
-	nvme_admin_dbbuf		= 0x7C,
-	nvme_admin_format_nvm		= 0x80,
-	nvme_admin_security_send	= 0x81,
-	nvme_admin_security_recv	= 0x82,
-};
-
-enum {
-	NVME_QUEUE_PHYS_CONTIG	= (1 << 0),
-	NVME_CQ_IRQ_ENABLED	= (1 << 1),
-	NVME_SQ_PRIO_URGENT	= (0 << 1),
-	NVME_SQ_PRIO_HIGH	= (1 << 1),
-	NVME_SQ_PRIO_MEDIUM	= (2 << 1),
-	NVME_SQ_PRIO_LOW	= (3 << 1),
-	NVME_FEAT_ARBITRATION	= 0x01,
-	NVME_FEAT_POWER_MGMT	= 0x02,
-	NVME_FEAT_LBA_RANGE	= 0x03,
-	NVME_FEAT_TEMP_THRESH	= 0x04,
-	NVME_FEAT_ERR_RECOVERY	= 0x05,
-	NVME_FEAT_VOLATILE_WC	= 0x06,
-	NVME_FEAT_NUM_QUEUES	= 0x07,
-	NVME_FEAT_IRQ_COALESCE	= 0x08,
-	NVME_FEAT_IRQ_CONFIG	= 0x09,
-	NVME_FEAT_WRITE_ATOMIC	= 0x0a,
-	NVME_FEAT_ASYNC_EVENT	= 0x0b,
-	NVME_FEAT_AUTO_PST	= 0x0c,
-	NVME_FEAT_HOST_MEM_BUF	= 0x0d,
-	NVME_FEAT_TIMESTAMP	= 0x0e,
-	NVME_FEAT_KATO		= 0x0f,
-	NVME_FEAT_SW_PROGRESS	= 0x80,
-	NVME_FEAT_HOST_ID	= 0x81,
-	NVME_FEAT_RESV_MASK	= 0x82,
-	NVME_FEAT_RESV_PERSIST	= 0x83,
-	NVME_LOG_ERROR		= 0x01,
-	NVME_LOG_SMART		= 0x02,
-	NVME_LOG_FW_SLOT	= 0x03,
-	NVME_LOG_DISC		= 0x70,
-	NVME_LOG_RESERVATION	= 0x80,
-	NVME_FWACT_REPL		= (0 << 3),
-	NVME_FWACT_REPL_ACTV	= (1 << 3),
-	NVME_FWACT_ACTV		= (2 << 3),
-};
-
-struct nvme_identify {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[2];
-	union nvme_data_ptr	dptr;
-	__u8			cns;
-	__u8			rsvd3;
-	__le16			ctrlid;
-	__u32			rsvd11[5];
-};
-
-#define NVME_IDENTIFY_DATA_SIZE 4096
-
-struct nvme_features {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[2];
-	union nvme_data_ptr	dptr;
-	__le32			fid;
-	__le32			dword11;
-	__le32                  dword12;
-	__le32                  dword13;
-	__le32                  dword14;
-	__le32                  dword15;
-};
-
-struct nvme_host_mem_buf_desc {
-	__le64			addr;
-	__le32			size;
-	__u32			rsvd;
-};
-
-struct nvme_create_cq {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[5];
-	__le64			prp1;
-	__u64			rsvd8;
-	__le16			cqid;
-	__le16			qsize;
-	__le16			cq_flags;
-	__le16			irq_vector;
-	__u32			rsvd12[4];
-};
-
-struct nvme_create_sq {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[5];
-	__le64			prp1;
-	__u64			rsvd8;
-	__le16			sqid;
-	__le16			qsize;
-	__le16			sq_flags;
-	__le16			cqid;
-	__u32			rsvd12[4];
-};
-
-struct nvme_delete_queue {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[9];
-	__le16			qid;
-	__u16			rsvd10;
-	__u32			rsvd11[5];
-};
-
-struct nvme_abort_cmd {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[9];
-	__le16			sqid;
-	__u16			cid;
-	__u32			rsvd11[5];
-};
-
-struct nvme_download_firmware {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[5];
-	union nvme_data_ptr	dptr;
-	__le32			numd;
-	__le32			offset;
-	__u32			rsvd12[4];
-};
-
-struct nvme_format_cmd {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[4];
-	__le32			cdw10;
-	__u32			rsvd11[5];
-};
-
-struct nvme_get_log_page_command {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[2];
-	union nvme_data_ptr	dptr;
-	__u8			lid;
-	__u8			rsvd10;
-	__le16			numdl;
-	__le16			numdu;
-	__u16			rsvd11;
-	__le32			lpol;
-	__le32			lpou;
-	__u32			rsvd14[2];
-};
-
-struct nvme_directive_cmd {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__le32			nsid;
-	__u64			rsvd2[2];
-	union nvme_data_ptr	dptr;
-	__le32			numd;
-	__u8			doper;
-	__u8			dtype;
-	__le16			dspec;
-	__u8			endir;
-	__u8			tdtype;
-	__u16			rsvd15;
-
-	__u32			rsvd16[3];
-};
-
-/*
- * Fabrics subcommands.
- */
-enum nvmf_fabrics_opcode {
-	nvme_fabrics_command		= 0x7f,
-};
-
-enum nvmf_capsule_command {
-	nvme_fabrics_type_property_set	= 0x00,
-	nvme_fabrics_type_connect	= 0x01,
-	nvme_fabrics_type_property_get	= 0x04,
-};
-
-struct nvmf_common_command {
-	__u8	opcode;
-	__u8	resv1;
-	__u16	command_id;
-	__u8	fctype;
-	__u8	resv2[35];
-	__u8	ts[24];
-};
-
-/*
- * The legal cntlid range a NVMe Target will provide.
- * Note that cntlid of value 0 is considered illegal in the fabrics world.
- * Devices based on earlier specs did not have the subsystem concept;
- * therefore, those devices had their cntlid value set to 0 as a result.
- */
-#define NVME_CNTLID_MIN		1
-#define NVME_CNTLID_MAX		0xffef
-#define NVME_CNTLID_DYNAMIC	0xffff
-
-#define MAX_DISC_LOGS	255
-
-/* Discovery log page entry */
-struct nvmf_disc_rsp_page_entry {
-	__u8		trtype;
-	__u8		adrfam;
-	__u8		subtype;
-	__u8		treq;
-	__le16		portid;
-	__le16		cntlid;
-	__le16		asqsz;
-	__u8		resv8[22];
-	char		trsvcid[NVMF_TRSVCID_SIZE];
-	__u8		resv64[192];
-	char		subnqn[NVMF_NQN_FIELD_LEN];
-	char		traddr[NVMF_TRADDR_SIZE];
-	union tsas {
-		char		common[NVMF_TSAS_SIZE];
-		struct rdma {
-			__u8	qptype;
-			__u8	prtype;
-			__u8	cms;
-			__u8	resv3[5];
-			__u16	pkey;
-			__u8	resv10[246];
-		} rdma;
-	} tsas;
-};
-
-/* Discovery log page header */
-struct nvmf_disc_rsp_page_hdr {
-	__le64		genctr;
-	__le64		numrec;
-	__le16		recfmt;
-	__u8		resv14[1006];
-	struct nvmf_disc_rsp_page_entry entries[0];
-};
-
-struct nvmf_connect_command {
-	__u8		opcode;
-	__u8		resv1;
-	__u16		command_id;
-	__u8		fctype;
-	__u8		resv2[19];
-	union nvme_data_ptr dptr;
-	__le16		recfmt;
-	__le16		qid;
-	__le16		sqsize;
-	__u8		cattr;
-	__u8		resv3;
-	__le32		kato;
-	__u8		resv4[12];
-};
-
-struct nvmf_connect_data {
-	uuid_t		hostid;
-	__le16		cntlid;
-	char		resv4[238];
-	char		subsysnqn[NVMF_NQN_FIELD_LEN];
-	char		hostnqn[NVMF_NQN_FIELD_LEN];
-	char		resv5[256];
-};
-
-struct nvmf_property_set_command {
-	__u8		opcode;
-	__u8		resv1;
-	__u16		command_id;
-	__u8		fctype;
-	__u8		resv2[35];
-	__u8		attrib;
-	__u8		resv3[3];
-	__le32		offset;
-	__le64		value;
-	__u8		resv4[8];
-};
-
-struct nvmf_property_get_command {
-	__u8		opcode;
-	__u8		resv1;
-	__u16		command_id;
-	__u8		fctype;
-	__u8		resv2[35];
-	__u8		attrib;
-	__u8		resv3[3];
-	__le32		offset;
-	__u8		resv4[16];
-};
-
-struct nvme_dbbuf {
-	__u8			opcode;
-	__u8			flags;
-	__u16			command_id;
-	__u32			rsvd1[5];
-	__le64			prp1;
-	__le64			prp2;
-	__u32			rsvd12[6];
-};
-
-struct streams_directive_params {
-	__le16	msl;
-	__le16	nssa;
-	__le16	nsso;
-	__u8	rsvd[10];
-	__le32	sws;
-	__le16	sgs;
-	__le16	nsa;
-	__le16	nso;
-	__u8	rsvd2[6];
-};
-
-struct nvme_command {
-	union {
-		struct nvme_common_command common;
-		struct nvme_rw_command rw;
-		struct nvme_identify identify;
-		struct nvme_features features;
-		struct nvme_create_cq create_cq;
-		struct nvme_create_sq create_sq;
-		struct nvme_delete_queue delete_queue;
-		struct nvme_download_firmware dlfw;
-		struct nvme_format_cmd format;
-		struct nvme_dsm_cmd dsm;
-		struct nvme_write_zeroes_cmd write_zeroes;
-		struct nvme_abort_cmd abort;
-		struct nvme_get_log_page_command get_log_page;
-		struct nvmf_common_command fabrics;
-		struct nvmf_connect_command connect;
-		struct nvmf_property_set_command prop_set;
-		struct nvmf_property_get_command prop_get;
-		struct nvme_dbbuf dbbuf;
-		struct nvme_directive_cmd directive;
-
-    struct nvme_kv_store_command kv_store;
-    struct nvme_kv_retrieve_command kv_retrieve;
-    struct nvme_kv_delete_command kv_delete;
-    struct nvme_kv_append_command kv_append;
-    struct nvme_kv_iter_req_command kv_iter_req;
-    struct nvme_kv_iter_read_command kv_iter_read;
-    struct nvme_kv_exist_command kv_exist;
-	};
-};
-
-static inline bool nvme_is_write(struct nvme_command *cmd)
-{
-	/*
-	 * What a mess...
-	 *
-	 * Why can't we simply have a Fabrics In and Fabrics out command?
-	 */
-  /* check nvme kv command */
-  if (cmd->common.opcode == nvme_cmd_kv_store || cmd->common.opcode == nvme_cmd_kv_append)
-    return 1;
-  if (cmd->common.opcode == nvme_cmd_kv_retrieve ||
-      cmd->common.opcode == nvme_cmd_kv_delete ||
-      cmd->common.opcode == nvme_cmd_kv_iter_req ||
-      cmd->common.opcode == nvme_cmd_kv_iter_read ||
-      cmd->common.opcode == nvme_cmd_kv_exist)
-    return 0;
-
-	if (unlikely(cmd->common.opcode == nvme_fabrics_command))
-		return cmd->fabrics.fctype & 1;
-	return cmd->common.opcode & 1;
-}
-
-enum {
-	/*
-	 * Generic Command Status:
-	 */
-	NVME_SC_SUCCESS			= 0x0,
-	NVME_SC_INVALID_OPCODE		= 0x1,
-	NVME_SC_INVALID_FIELD		= 0x2,
-	NVME_SC_CMDID_CONFLICT		= 0x3,
-	NVME_SC_DATA_XFER_ERROR		= 0x4,
-	NVME_SC_POWER_LOSS		= 0x5,
-	NVME_SC_INTERNAL		= 0x6,
-	NVME_SC_ABORT_REQ		= 0x7,
-	NVME_SC_ABORT_QUEUE		= 0x8,
-	NVME_SC_FUSED_FAIL		= 0x9,
-	NVME_SC_FUSED_MISSING		= 0xa,
-	NVME_SC_INVALID_NS		= 0xb,
-	NVME_SC_CMD_SEQ_ERROR		= 0xc,
-	NVME_SC_SGL_INVALID_LAST	= 0xd,
-	NVME_SC_SGL_INVALID_COUNT	= 0xe,
-	NVME_SC_SGL_INVALID_DATA	= 0xf,
-	NVME_SC_SGL_INVALID_METADATA	= 0x10,
-	NVME_SC_SGL_INVALID_TYPE	= 0x11,
-
-	NVME_SC_SGL_INVALID_OFFSET	= 0x16,
-	NVME_SC_SGL_INVALID_SUBTYPE	= 0x17,
-
-	NVME_SC_LBA_RANGE		= 0x80,
-	NVME_SC_CAP_EXCEEDED		= 0x81,
-	NVME_SC_NS_NOT_READY		= 0x82,
-	NVME_SC_RESERVATION_CONFLICT	= 0x83,
-
-	/*
-	 * Command Specific Status:
-	 */
-	NVME_SC_CQ_INVALID		= 0x100,
-	NVME_SC_QID_INVALID		= 0x101,
-	NVME_SC_QUEUE_SIZE		= 0x102,
-	NVME_SC_ABORT_LIMIT		= 0x103,
-	NVME_SC_ABORT_MISSING		= 0x104,
-	NVME_SC_ASYNC_LIMIT		= 0x105,
-	NVME_SC_FIRMWARE_SLOT		= 0x106,
-	NVME_SC_FIRMWARE_IMAGE		= 0x107,
-	NVME_SC_INVALID_VECTOR		= 0x108,
-	NVME_SC_INVALID_LOG_PAGE	= 0x109,
-	NVME_SC_INVALID_FORMAT		= 0x10a,
-	NVME_SC_FW_NEEDS_CONV_RESET	= 0x10b,
-	NVME_SC_INVALID_QUEUE		= 0x10c,
-	NVME_SC_FEATURE_NOT_SAVEABLE	= 0x10d,
-	NVME_SC_FEATURE_NOT_CHANGEABLE	= 0x10e,
-	NVME_SC_FEATURE_NOT_PER_NS	= 0x10f,
-	NVME_SC_FW_NEEDS_SUBSYS_RESET	= 0x110,
-	NVME_SC_FW_NEEDS_RESET		= 0x111,
-	NVME_SC_FW_NEEDS_MAX_TIME	= 0x112,
-	NVME_SC_FW_ACIVATE_PROHIBITED	= 0x113,
-	NVME_SC_OVERLAPPING_RANGE	= 0x114,
-	NVME_SC_NS_INSUFFICENT_CAP	= 0x115,
-	NVME_SC_NS_ID_UNAVAILABLE	= 0x116,
-	NVME_SC_NS_ALREADY_ATTACHED	= 0x118,
-	NVME_SC_NS_IS_PRIVATE		= 0x119,
-	NVME_SC_NS_NOT_ATTACHED		= 0x11a,
-	NVME_SC_THIN_PROV_NOT_SUPP	= 0x11b,
-	NVME_SC_CTRL_LIST_INVALID	= 0x11c,
-
-	/*
-	 * I/O Command Set Specific - NVM commands:
-	 */
-	NVME_SC_BAD_ATTRIBUTES		= 0x180,
-	NVME_SC_INVALID_PI		= 0x181,
-	NVME_SC_READ_ONLY		= 0x182,
-	NVME_SC_ONCS_NOT_SUPPORTED	= 0x183,
-
-	/*
-	 * I/O Command Set Specific - Fabrics commands:
-	 */
-	NVME_SC_CONNECT_FORMAT		= 0x180,
-	NVME_SC_CONNECT_CTRL_BUSY	= 0x181,
-	NVME_SC_CONNECT_INVALID_PARAM	= 0x182,
-	NVME_SC_CONNECT_RESTART_DISC	= 0x183,
-	NVME_SC_CONNECT_INVALID_HOST	= 0x184,
-
-	NVME_SC_DISCOVERY_RESTART	= 0x190,
-	NVME_SC_AUTH_REQUIRED		= 0x191,
-
-	/*
-	 * Media and Data Integrity Errors:
-	 */
-	NVME_SC_WRITE_FAULT		= 0x280,
-	NVME_SC_READ_ERROR		= 0x281,
-	NVME_SC_GUARD_CHECK		= 0x282,
-	NVME_SC_APPTAG_CHECK		= 0x283,
-	NVME_SC_REFTAG_CHECK		= 0x284,
-	NVME_SC_COMPARE_FAILED		= 0x285,
-	NVME_SC_ACCESS_DENIED		= 0x286,
-	NVME_SC_UNWRITTEN_BLOCK		= 0x287,
-
-	NVME_SC_DNR			= 0x4000,
-};
-
-struct nvme_completion {
-	/*
-	 * Used by Admin and Fabrics commands to return data:
-	 */
-	union nvme_result {
-		__le16	u16;
-		__le32	u32;
-		__le64	u64;
-	} result;
-	__le16	sq_head;	/* how much of this queue may be reclaimed */
-	__le16	sq_id;		/* submission queue that generated this entry */
-	__u16	command_id;	/* of the command which completed */
-	__le16	status;		/* did the command fail, and if so, why? */
-};
-
-#define NVME_VS(major, minor, tertiary) \
-	(((major) << 16) | ((minor) << 8) | (tertiary))
-
-#define NVME_MAJOR(ver)		((ver) >> 16)
-#define NVME_MINOR(ver)		(((ver) >> 8) & 0xff)
-#define NVME_TERTIARY(ver)	((ver) & 0xff)
-
-#endif /* _LINUX_NVME_H */
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/linux_nvme_ioctl.h b/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/linux_nvme_ioctl.h
deleted file mode 100644
index 5a57514..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/linux_nvme_ioctl.h
+++ /dev/null
@@ -1,152 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
-/*
- * Definitions for the NVM Express ioctl interface
- * Copyright (c) 2011-2014, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- */
-
-#ifndef _UAPI_LINUX_NVME_IOCTL_H
-#define _UAPI_LINUX_NVME_IOCTL_H
-
-#include <linux/types.h>
-
-struct nvme_user_io {
-	__u8	opcode;
-	__u8	flags;
-	__u16	control;
-	__u16	nblocks;
-	__u16	rsvd;
-	__u64	metadata;
-	__u64	addr;
-	__u64	slba;
-	__u32	dsmgmt;
-	__u32	reftag;
-	__u16	apptag;
-	__u16	appmask;
-};
-
-struct nvme_passthru_cmd {
-	__u8	opcode;
-	__u8	flags;
-	__u16	rsvd1;
-	__u32	nsid;
-	__u32	cdw2;
-	__u32	cdw3;
-	__u64	metadata;
-	__u64	addr;
-	__u32	metadata_len;
-	__u32	data_len;
-	__u32	cdw10;
-	__u32	cdw11;
-	__u32	cdw12;
-	__u32	cdw13;
-	__u32	cdw14;
-	__u32	cdw15;
-	__u32	timeout_ms;
-	__u32	result;
-};
-
-#define KVS_SUCCESS                   0
-#define KVS_ERR_ALIGNMENT             (-1)
-#define KVS_ERR_CAPACITY              (-2)
-#define KVS_ERR_CLOSE                 (-3)
-#define KVS_ERR_CONT_EXIST            (-4)
-#define KVS_ERR_CONT_NAME             (-5)
-#define KVS_ERR_CONT_NOT_EXIST        (-6)
-#define KVS_ERR_DEVICE_NOT_EXIST      (-7)
-#define KVS_ERR_GROUP                 (-8)
-#define KVS_ERR_INDEX                 (-9)
-#define KVS_ERR_IO                    (-10)
-#define KVS_ERR_KEY                   (-11)
-#define KVS_ERR_KEY_TYPE              (-12)
-#define KVS_ERR_MEMORY                (-13)
-#define KVS_ERR_NULL_INPUT            (-14)
-#define KVS_ERR_OFFSET                (-15)
-#define KVS_ERR_OPEN                  (-16)
-#define KVS_ERR_OPTION_NOT_SUPPORTED  (-17)
-#define KVS_ERR_PERMISSION            (-18)
-#define KVS_ERR_SPACE                 (-19)
-#define KVS_ERR_TIMEOUT               (-20)
-#define KVS_ERR_TUPLE_EXIST           (-21)
-#define KVS_ERR_TUPLE_NOT_EXIST       (-22)
-#define KVS_ERR_VALUE                 (-23)
-
-struct nvme_passthru_kv_cmd {
-	__u8  opcode;
-	__u8  flags;
-	__u16 rsvd1;
-	__u32 nsid;
-	__u32 cdw2;
-	__u32 cdw3;
-	__u32 cdw4;
-	__u32 cdw5;
-	__u64 data_addr;
-	__u32 data_len;
-	__u32 key_len;
-	__u32 cdw10;
-	__u32 cdw11;
-	union {
-		struct {
-			__u64 key_addr;
-			__u32 rsvd5;
-			__u32 rsvd6;
-		};
-		__u8  key[16];
-		struct {
-			__u32 cdw12;
-			__u32 cdw13;
-			__u32 cdw14;
-			__u32 cdw15;
-		};
-	};
-	__u32 timeout_ms;
-	__u32 result;
-	__u32 status;
-	__u32 ctxid;
-	__u32 reqid;
-};
-
-struct nvme_aioctx {
-	__u32 ctxid;
-	__u32 eventfd;
-};
-
-struct nvme_aioevent {
-	__u64 reqid;
-	__u32 ctxid;
-	__u32 result;
-	__u32 status;
-};
-
-#define MAX_AIO_EVENTS  128
-struct nvme_aioevents {
-	__u16 nr;
-	__u32 ctxid;
-	struct nvme_aioevent events[MAX_AIO_EVENTS];
-};
-
-#define nvme_admin_cmd nvme_passthru_cmd
-
-#define NVME_IOCTL_ID		_IO('N', 0x40)
-#define NVME_IOCTL_ADMIN_CMD	_IOWR('N', 0x41, struct nvme_admin_cmd)
-#define NVME_IOCTL_SUBMIT_IO	_IOW('N', 0x42, struct nvme_user_io)
-#define NVME_IOCTL_IO_CMD	_IOWR('N', 0x43, struct nvme_passthru_cmd)
-#define NVME_IOCTL_RESET	_IO('N', 0x44)
-#define NVME_IOCTL_SUBSYS_RESET	_IO('N', 0x45)
-#define NVME_IOCTL_RESCAN	_IO('N', 0x46)
-
-#define NVME_IOCTL_AIO_CMD      _IOWR('N', 0X47, struct nvme_passthru_kv_cmd)
-#define NVME_IOCTL_SET_AIOCTX   _IOWR('N', 0x48, struct nvme_aioctx)
-#define NVME_IOCTL_DEL_AIOCTX   _IOWR('N', 0x49, struct nvme_aioctx)
-#define NVME_IOCTL_GET_AIOEVENT _IOWR('N', 0x50, struct nvme_aioevents)
-#define NVME_IOCTL_IO_KV_CMD    _IOWR('N', 0x51, struct nvme_passthru_kv_cmd)
-
-#endif /* _UAPI_LINUX_NVME_IOCTL_H */
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/nvme.h b/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/nvme.h
deleted file mode 100644
index 65e37f7..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/nvme.h
+++ /dev/null
@@ -1,399 +0,0 @@
-/*
- * Copyright (c) 2011-2014, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- */
-
-#ifndef _NVME_H
-#define _NVME_H
-
-#include "linux_nvme.h"
-#include <linux/pci.h>
-#include <linux/kref.h>
-#include <linux/blk-mq.h>
-#include <linux/lightnvm.h>
-#include <linux/sed-opal.h>
-
-#define KSID_SUPPORT
-
-#define is_kv_append_cmd(opcode)      ((opcode) == nvme_cmd_kv_append)
-#define is_kv_store_cmd(opcode)       ((opcode) == nvme_cmd_kv_store)
-#define is_kv_retrieve_cmd(opcode)    ((opcode) == nvme_cmd_kv_retrieve)
-#define is_kv_delete_cmd(opcode)      ((opcode) == nvme_cmd_kv_delete)
-#define is_kv_iter_req_cmd(opcode)    ((opcode) == nvme_cmd_kv_iter_req)
-#define is_kv_iter_read_cmd(opcode)   ((opcode) == nvme_cmd_kv_iter_read)
-#define is_kv_exist_cmd(opcode)       ((opcode) == nvme_cmd_kv_exist)
-#define is_kv_cmd(opcode)             (is_kv_append_cmd(opcode) ||\
-                                       is_kv_store_cmd(opcode) ||\
-                                       is_kv_retrieve_cmd(opcode) ||\
-                                       is_kv_delete_cmd(opcode) ||\
-                                       is_kv_iter_req_cmd(opcode) ||\
-                                       is_kv_iter_read_cmd(opcode) ||\
-                                       is_kv_exist_cmd(opcode))
-
-extern unsigned char nvme_io_timeout;
-#define NVME_IO_TIMEOUT	(nvme_io_timeout * HZ)
-
-extern unsigned char admin_timeout;
-#define ADMIN_TIMEOUT	(admin_timeout * HZ)
-
-#define NVME_DEFAULT_KATO	5
-#define NVME_KATO_GRACE		10
-
-extern struct workqueue_struct *nvme_wq;
-
-enum {
-	NVME_NS_LBA		= 0,
-	NVME_NS_LIGHTNVM	= 1,
-};
-
-/*
- * List of workarounds for devices that required behavior not specified in
- * the standard.
- */
-enum nvme_quirks {
-	/*
-	 * Prefers I/O aligned to a stripe size specified in a vendor
-	 * specific Identify field.
-	 */
-	NVME_QUIRK_STRIPE_SIZE			= (1 << 0),
-
-	/*
-	 * The controller doesn't handle Identify value others than 0 or 1
-	 * correctly.
-	 */
-	NVME_QUIRK_IDENTIFY_CNS			= (1 << 1),
-
-	/*
-	 * The controller deterministically returns O's on reads to
-	 * logical blocks that deallocate was called on.
-	 */
-	NVME_QUIRK_DEALLOCATE_ZEROES		= (1 << 2),
-
-	/*
-	 * The controller needs a delay before starts checking the device
-	 * readiness, which is done by reading the NVME_CSTS_RDY bit.
-	 */
-	NVME_QUIRK_DELAY_BEFORE_CHK_RDY		= (1 << 3),
-
-	/*
-	 * APST should not be used.
-	 */
-	NVME_QUIRK_NO_APST			= (1 << 4),
-
-	/*
-	 * The deepest sleep state should not be used.
-	 */
-	NVME_QUIRK_NO_DEEPEST_PS		= (1 << 5),
-
-	/*
-	 * Supports the LighNVM command set if indicated in vs[1].
-	 */
-	NVME_QUIRK_LIGHTNVM			= (1 << 6),
-
-	/*
-	 * Set MEDIUM priority on SQ creation
-	 */
-	NVME_QUIRK_MEDIUM_PRIO_SQ		= (1 << 7),
-};
-
-/*
- * Common request structure for NVMe passthrough.  All drivers must have
- * this structure as the first member of their request-private data.
- */
-struct nvme_request {
-	struct nvme_command	*cmd;
-	union nvme_result	result;
-	u8			retries;
-	u8			flags;
-	u16			status;
-};
-
-struct nvme_io_param {
-  struct nvme_request req;
-  struct scatterlist* kv_data_sg_ptr;
-  struct scatterlist* kv_meta_sg_ptr;
-  int kv_data_nents;
-  int kv_data_len;
-};
-
-static inline struct nvme_io_param* nvme_io_param(struct request* req)
-{
-  return blk_mq_rq_to_pdu(req);
-}
-
-enum {
-	NVME_REQ_CANCELLED		= (1 << 0),
-};
-
-static inline struct nvme_request *nvme_req(struct request *req)
-{
-	return blk_mq_rq_to_pdu(req);
-}
-
-/* The below value is the specific amount of delay needed before checking
- * readiness in case of the PCI_DEVICE(0x1c58, 0x0003), which needs the
- * NVME_QUIRK_DELAY_BEFORE_CHK_RDY quirk enabled. The value (in ms) was
- * found empirically.
- */
-#define NVME_QUIRK_DELAY_AMOUNT		2300
-
-enum nvme_ctrl_state {
-	NVME_CTRL_NEW,
-	NVME_CTRL_LIVE,
-	NVME_CTRL_RESETTING,
-	NVME_CTRL_RECONNECTING,
-	NVME_CTRL_DELETING,
-	NVME_CTRL_DEAD,
-};
-
-struct nvme_ctrl {
-	enum nvme_ctrl_state state;
-	bool identified;
-	spinlock_t lock;
-	const struct nvme_ctrl_ops *ops;
-	struct request_queue *admin_q;
-	struct request_queue *connect_q;
-	struct device *dev;
-	struct kref kref;
-	int instance;
-	struct blk_mq_tag_set *tagset;
-	struct blk_mq_tag_set *admin_tagset;
-	struct list_head namespaces;
-	struct mutex namespaces_mutex;
-	struct device *device;	/* char device */
-	struct list_head node;
-	struct ida ns_ida;
-	struct work_struct reset_work;
-
-	struct opal_dev *opal_dev;
-
-	char name[12];
-	char serial[20];
-	char model[40];
-	char firmware_rev[8];
-	char subnqn[NVMF_NQN_SIZE];
-	u16 cntlid;
-
-	u32 ctrl_config;
-	u16 mtfa;
-	u32 queue_count;
-
-	u64 cap;
-	u32 page_size;
-	u32 max_hw_sectors;
-	u16 oncs;
-	u16 vid;
-	u16 oacs;
-	u16 nssa;
-	u16 nr_streams;
-	atomic_t abort_limit;
-	u8 event_limit;
-	u8 vwc;
-	u32 vs;
-	u32 sgls;
-	u16 kas;
-	u8 npss;
-	u8 apsta;
-	unsigned int shutdown_timeout;
-	unsigned int kato;
-	bool subsystem;
-	unsigned long quirks;
-	struct nvme_id_power_state psd[32];
-	struct work_struct scan_work;
-	struct work_struct async_event_work;
-	struct delayed_work ka_work;
-	struct work_struct fw_act_work;
-
-	/* Power saving configuration */
-	u64 ps_max_latency_us;
-	bool apst_enabled;
-
-	/* PCIe only: */
-	u32 hmpre;
-	u32 hmmin;
-	u32 hmminds;
-	u16 hmmaxd;
-
-	/* Fabrics only */
-	u16 sqsize;
-	u32 ioccsz;
-	u32 iorcsz;
-	u16 icdoff;
-	u16 maxcmd;
-	int nr_reconnects;
-	struct nvmf_ctrl_options *opts;
-};
-
-struct nvme_ns {
-	struct list_head list;
-
-	struct nvme_ctrl *ctrl;
-	struct request_queue *queue;
-	struct gendisk *disk;
-	struct nvm_dev *ndev;
-	struct kref kref;
-	int instance;
-
-	u8 eui[8];
-	u8 nguid[16];
-	uuid_t uuid;
-
-	unsigned ns_id;
-	int lba_shift;
-	u16 ms;
-	u16 sgs;
-	u32 sws;
-	bool ext;
-	u8 pi_type;
-	unsigned long flags;
-#define NVME_NS_REMOVING 0
-#define NVME_NS_DEAD     1
-	u16 noiob;
-};
-
-struct nvme_ctrl_ops {
-	const char *name;
-	struct module *module;
-	unsigned int flags;
-#define NVME_F_FABRICS			(1 << 0)
-#define NVME_F_METADATA_SUPPORTED	(1 << 1)
-	int (*reg_read32)(struct nvme_ctrl *ctrl, u32 off, u32 *val);
-	int (*reg_write32)(struct nvme_ctrl *ctrl, u32 off, u32 val);
-	int (*reg_read64)(struct nvme_ctrl *ctrl, u32 off, u64 *val);
-	void (*free_ctrl)(struct nvme_ctrl *ctrl);
-	void (*submit_async_event)(struct nvme_ctrl *ctrl, int aer_idx);
-	int (*delete_ctrl)(struct nvme_ctrl *ctrl);
-	int (*get_address)(struct nvme_ctrl *ctrl, char *buf, int size);
-};
-
-static inline bool nvme_ctrl_ready(struct nvme_ctrl *ctrl)
-{
-	u32 val = 0;
-
-	if (ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &val))
-		return false;
-	return val & NVME_CSTS_RDY;
-}
-
-static inline int nvme_reset_subsystem(struct nvme_ctrl *ctrl)
-{
-	if (!ctrl->subsystem)
-		return -ENOTTY;
-	return ctrl->ops->reg_write32(ctrl, NVME_REG_NSSR, 0x4E564D65);
-}
-
-static inline u64 nvme_block_nr(struct nvme_ns *ns, sector_t sector)
-{
-	return (sector >> (ns->lba_shift - 9));
-}
-
-static inline void nvme_cleanup_cmd(struct request *req)
-{
-	if (req->rq_flags & RQF_SPECIAL_PAYLOAD) {
-		kfree(page_address(req->special_vec.bv_page) +
-		      req->special_vec.bv_offset);
-	}
-}
-
-static inline void nvme_end_request(struct request *req, __le16 status,
-		union nvme_result result)
-{
-	struct nvme_request *rq = nvme_req(req);
-
-	rq->status = le16_to_cpu(status) >> 1;
-	rq->result = result;
-	blk_mq_complete_request(req);
-}
-
-void nvme_complete_rq(struct request *req);
-void nvme_cancel_request(struct request *req, void *data, bool reserved);
-bool nvme_change_ctrl_state(struct nvme_ctrl *ctrl,
-		enum nvme_ctrl_state new_state);
-int nvme_disable_ctrl(struct nvme_ctrl *ctrl, u64 cap);
-int nvme_enable_ctrl(struct nvme_ctrl *ctrl, u64 cap);
-int nvme_shutdown_ctrl(struct nvme_ctrl *ctrl);
-int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
-		const struct nvme_ctrl_ops *ops, unsigned long quirks);
-void nvme_uninit_ctrl(struct nvme_ctrl *ctrl);
-void nvme_start_ctrl(struct nvme_ctrl *ctrl);
-void nvme_stop_ctrl(struct nvme_ctrl *ctrl);
-void nvme_put_ctrl(struct nvme_ctrl *ctrl);
-int nvme_init_identify(struct nvme_ctrl *ctrl);
-
-void nvme_queue_scan(struct nvme_ctrl *ctrl);
-void nvme_remove_namespaces(struct nvme_ctrl *ctrl);
-
-int nvme_sec_submit(void *data, u16 spsp, u8 secp, void *buffer, size_t len,
-		bool send);
-
-#define NVME_NR_AERS	1
-void nvme_complete_async_event(struct nvme_ctrl *ctrl, __le16 status,
-		union nvme_result *res);
-void nvme_queue_async_events(struct nvme_ctrl *ctrl);
-
-void nvme_stop_queues(struct nvme_ctrl *ctrl);
-void nvme_start_queues(struct nvme_ctrl *ctrl);
-void nvme_kill_queues(struct nvme_ctrl *ctrl);
-void nvme_unfreeze(struct nvme_ctrl *ctrl);
-void nvme_wait_freeze(struct nvme_ctrl *ctrl);
-void nvme_wait_freeze_timeout(struct nvme_ctrl *ctrl, long timeout);
-void nvme_start_freeze(struct nvme_ctrl *ctrl);
-
-#define NVME_QID_ANY -1
-struct request *nvme_alloc_request(struct request_queue *q,
-		struct nvme_command *cmd, unsigned int flags, int qid);
-blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
-		struct nvme_command *cmd);
-int nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
-		void *buf, unsigned bufflen);
-int __nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
-		union nvme_result *result, void *buffer, unsigned bufflen,
-		unsigned timeout, int qid, int at_head, int flags);
-int nvme_set_queue_count(struct nvme_ctrl *ctrl, int *count);
-void nvme_start_keep_alive(struct nvme_ctrl *ctrl);
-void nvme_stop_keep_alive(struct nvme_ctrl *ctrl);
-int nvme_reset_ctrl(struct nvme_ctrl *ctrl);
-
-#ifdef CONFIG_NVM
-int nvme_nvm_register(struct nvme_ns *ns, char *disk_name, int node);
-void nvme_nvm_unregister(struct nvme_ns *ns);
-int nvme_nvm_register_sysfs(struct nvme_ns *ns);
-void nvme_nvm_unregister_sysfs(struct nvme_ns *ns);
-int nvme_nvm_ioctl(struct nvme_ns *ns, unsigned int cmd, unsigned long arg);
-#else
-static inline int nvme_nvm_register(struct nvme_ns *ns, char *disk_name,
-				    int node)
-{
-	return 0;
-}
-
-static inline void nvme_nvm_unregister(struct nvme_ns *ns) {};
-static inline int nvme_nvm_register_sysfs(struct nvme_ns *ns)
-{
-	return 0;
-}
-static inline void nvme_nvm_unregister_sysfs(struct nvme_ns *ns) {};
-static inline int nvme_nvm_ioctl(struct nvme_ns *ns, unsigned int cmd,
-							unsigned long arg)
-{
-	return -ENOTTY;
-}
-#endif /* CONFIG_NVM */
-
-static inline struct nvme_ns *nvme_get_ns_from_dev(struct device *dev)
-{
-	return dev_to_disk(dev)->private_data;
-}
-
-int __init nvme_core_init(void);
-void nvme_core_exit(void);
-
-#endif /* _NVME_H */
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/pci.c b/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/pci.c
deleted file mode 100644
index 35e5dac..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/pci.c
+++ /dev/null
@@ -1,2745 +0,0 @@
-/*
- * NVM Express device driver
- * Copyright (c) 2011-2014, Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- */
-
-#include <linux/aer.h>
-#include <linux/bitops.h>
-#include <linux/blkdev.h>
-#include <linux/blk-mq.h>
-#include <linux/blk-mq-pci.h>
-#include <linux/dmi.h>
-#include <linux/init.h>
-#include <linux/interrupt.h>
-#include <linux/io.h>
-#include <linux/mm.h>
-#include <linux/module.h>
-#include <linux/mutex.h>
-#include <linux/once.h>
-#include <linux/pci.h>
-#include <linux/poison.h>
-#include <linux/t10-pi.h>
-#include <linux/timer.h>
-#include <linux/types.h>
-#include <linux/io-64-nonatomic-lo-hi.h>
-#include <asm/unaligned.h>
-#include <linux/sed-opal.h>
-
-#include "nvme.h"
-
-#define SQ_SIZE(depth)		(depth * sizeof(struct nvme_command))
-#define CQ_SIZE(depth)		(depth * sizeof(struct nvme_completion))
-
-/*
- * We handle AEN commands ourselves and don't even let the
- * block layer know about them.
- */
-#define NVME_AQ_BLKMQ_DEPTH	(NVME_AQ_DEPTH - NVME_NR_AERS)
-
-static int use_threaded_interrupts;
-module_param(use_threaded_interrupts, int, 0);
-
-static bool use_cmb_sqes = true;
-module_param(use_cmb_sqes, bool, 0644);
-MODULE_PARM_DESC(use_cmb_sqes, "use controller's memory buffer for I/O SQes");
-
-static unsigned int max_host_mem_size_mb = 128;
-module_param(max_host_mem_size_mb, uint, 0444);
-MODULE_PARM_DESC(max_host_mem_size_mb,
-	"Maximum Host Memory Buffer (HMB) size per controller (in MiB)");
-
-static int io_queue_depth_set(const char *val, const struct kernel_param *kp);
-static const struct kernel_param_ops io_queue_depth_ops = {
-	.set = io_queue_depth_set,
-	.get = param_get_int,
-};
-
-static int io_queue_depth = 1024;
-module_param_cb(io_queue_depth, &io_queue_depth_ops, &io_queue_depth, 0644);
-MODULE_PARM_DESC(io_queue_depth, "set io queue depth, should >= 2");
-
-struct nvme_dev;
-struct nvme_queue;
-
-static void nvme_process_cq(struct nvme_queue *nvmeq);
-static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown);
-
-/*
- * Represents an NVM Express device.  Each nvme_dev is a PCI function.
- */
-struct nvme_dev {
-	struct nvme_queue *queues;
-	struct blk_mq_tag_set tagset;
-	struct blk_mq_tag_set admin_tagset;
-	u32 __iomem *dbs;
-	struct device *dev;
-	struct dma_pool *prp_page_pool;
-	struct dma_pool *prp_small_pool;
-	unsigned online_queues;
-	unsigned max_qid;
-	int q_depth;
-	u32 db_stride;
-	void __iomem *bar;
-	unsigned long bar_mapped_size;
-	struct work_struct remove_work;
-	struct mutex shutdown_lock;
-	bool subsystem;
-	void __iomem *cmb;
-	pci_bus_addr_t cmb_bus_addr;
-	u64 cmb_size;
-	u32 cmbsz;
-	u32 cmbloc;
-	struct nvme_ctrl ctrl;
-	struct completion ioq_wait;
-
-	/* shadow doorbell buffer support: */
-	u32 *dbbuf_dbs;
-	dma_addr_t dbbuf_dbs_dma_addr;
-	u32 *dbbuf_eis;
-	dma_addr_t dbbuf_eis_dma_addr;
-
-	/* host memory buffer support: */
-	u64 host_mem_size;
-	u32 nr_host_mem_descs;
-	dma_addr_t host_mem_descs_dma;
-	struct nvme_host_mem_buf_desc *host_mem_descs;
-	void **host_mem_desc_bufs;
-};
-
-static int io_queue_depth_set(const char *val, const struct kernel_param *kp)
-{
-	int n = 0, ret;
-
-	ret = kstrtoint(val, 10, &n);
-	if (ret != 0 || n < 2)
-		return -EINVAL;
-
-	return param_set_int(val, kp);
-}
-
-static inline unsigned int sq_idx(unsigned int qid, u32 stride)
-{
-	return qid * 2 * stride;
-}
-
-static inline unsigned int cq_idx(unsigned int qid, u32 stride)
-{
-	return (qid * 2 + 1) * stride;
-}
-
-static inline struct nvme_dev *to_nvme_dev(struct nvme_ctrl *ctrl)
-{
-	return container_of(ctrl, struct nvme_dev, ctrl);
-}
-
-/*
- * An NVM Express queue.  Each device has at least two (one for admin
- * commands and one for I/O commands).
- */
-struct nvme_queue {
-	struct device *q_dmadev;
-	struct nvme_dev *dev;
-	spinlock_t q_lock;
-	struct nvme_command *sq_cmds;
-	struct nvme_command __iomem *sq_cmds_io;
-	volatile struct nvme_completion *cqes;
-	struct blk_mq_tags **tags;
-	dma_addr_t sq_dma_addr;
-	dma_addr_t cq_dma_addr;
-	u32 __iomem *q_db;
-	u16 q_depth;
-	s16 cq_vector;
-	u16 sq_tail;
-	u16 cq_head;
-	u16 qid;
-	u8 cq_phase;
-	u8 cqe_seen;
-	u32 *dbbuf_sq_db;
-	u32 *dbbuf_cq_db;
-	u32 *dbbuf_sq_ei;
-	u32 *dbbuf_cq_ei;
-};
-
-/*
- * The nvme_iod describes the data in an I/O, including the list of PRP
- * entries.  You can't see it in this data structure because C doesn't let
- * me express that.  Use nvme_init_iod to ensure there's enough space
- * allocated to store the PRP list.
- */
-struct nvme_iod {
-	struct nvme_io_param param;
-	struct nvme_request req;
-	struct nvme_queue *nvmeq;
-	int kv_cmd;
-	int rsvd;
-	int aborted;
-	int npages;		/* In the PRP list. 0 means small pool in use */
-	int nents;		/* Used in scatterlist */
-	int length;		/* Of data, in bytes */
-	dma_addr_t first_dma;
-	struct scatterlist meta_sg; /* metadata requires single contiguous buffer */
-	struct scatterlist *sg;
-	struct scatterlist inline_sg[0];
-};
-
-/*
- * Check we didin't inadvertently grow the command struct
- */
-static inline void _nvme_check_size(void)
-{
-	BUILD_BUG_ON(sizeof(struct nvme_rw_command) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_create_cq) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_create_sq) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_delete_queue) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_features) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_format_cmd) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_abort_cmd) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_command) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_id_ctrl) != NVME_IDENTIFY_DATA_SIZE);
-	BUILD_BUG_ON(sizeof(struct nvme_id_ns) != NVME_IDENTIFY_DATA_SIZE);
-	BUILD_BUG_ON(sizeof(struct nvme_lba_range_type) != 64);
-	BUILD_BUG_ON(sizeof(struct nvme_smart_log) != 512);
-	BUILD_BUG_ON(sizeof(struct nvme_dbbuf) != 64);
-}
-
-static inline unsigned int nvme_dbbuf_size(u32 stride)
-{
-	return ((num_possible_cpus() + 1) * 8 * stride);
-}
-
-static int nvme_dbbuf_dma_alloc(struct nvme_dev *dev)
-{
-	unsigned int mem_size = nvme_dbbuf_size(dev->db_stride);
-
-	if (dev->dbbuf_dbs)
-		return 0;
-
-	dev->dbbuf_dbs = dma_alloc_coherent(dev->dev, mem_size,
-					    &dev->dbbuf_dbs_dma_addr,
-					    GFP_KERNEL);
-	if (!dev->dbbuf_dbs)
-		return -ENOMEM;
-	dev->dbbuf_eis = dma_alloc_coherent(dev->dev, mem_size,
-					    &dev->dbbuf_eis_dma_addr,
-					    GFP_KERNEL);
-	if (!dev->dbbuf_eis) {
-		dma_free_coherent(dev->dev, mem_size,
-				  dev->dbbuf_dbs, dev->dbbuf_dbs_dma_addr);
-		dev->dbbuf_dbs = NULL;
-		return -ENOMEM;
-	}
-
-	return 0;
-}
-
-static void nvme_dbbuf_dma_free(struct nvme_dev *dev)
-{
-	unsigned int mem_size = nvme_dbbuf_size(dev->db_stride);
-
-	if (dev->dbbuf_dbs) {
-		dma_free_coherent(dev->dev, mem_size,
-				  dev->dbbuf_dbs, dev->dbbuf_dbs_dma_addr);
-		dev->dbbuf_dbs = NULL;
-	}
-	if (dev->dbbuf_eis) {
-		dma_free_coherent(dev->dev, mem_size,
-				  dev->dbbuf_eis, dev->dbbuf_eis_dma_addr);
-		dev->dbbuf_eis = NULL;
-	}
-}
-
-static void nvme_dbbuf_init(struct nvme_dev *dev,
-			    struct nvme_queue *nvmeq, int qid)
-{
-	if (!dev->dbbuf_dbs || !qid)
-		return;
-
-	nvmeq->dbbuf_sq_db = &dev->dbbuf_dbs[sq_idx(qid, dev->db_stride)];
-	nvmeq->dbbuf_cq_db = &dev->dbbuf_dbs[cq_idx(qid, dev->db_stride)];
-	nvmeq->dbbuf_sq_ei = &dev->dbbuf_eis[sq_idx(qid, dev->db_stride)];
-	nvmeq->dbbuf_cq_ei = &dev->dbbuf_eis[cq_idx(qid, dev->db_stride)];
-}
-
-static void nvme_dbbuf_set(struct nvme_dev *dev)
-{
-	struct nvme_command c;
-
-	if (!dev->dbbuf_dbs)
-		return;
-
-	memset(&c, 0, sizeof(c));
-	c.dbbuf.opcode = nvme_admin_dbbuf;
-	c.dbbuf.prp1 = cpu_to_le64(dev->dbbuf_dbs_dma_addr);
-	c.dbbuf.prp2 = cpu_to_le64(dev->dbbuf_eis_dma_addr);
-
-	if (nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0)) {
-		dev_warn(dev->ctrl.device, "unable to set dbbuf\n");
-		/* Free memory and continue on */
-		nvme_dbbuf_dma_free(dev);
-	}
-}
-
-static inline int nvme_dbbuf_need_event(u16 event_idx, u16 new_idx, u16 old)
-{
-	return (u16)(new_idx - event_idx - 1) < (u16)(new_idx - old);
-}
-
-/* Update dbbuf and return true if an MMIO is required */
-static bool nvme_dbbuf_update_and_check_event(u16 value, u32 *dbbuf_db,
-					      volatile u32 *dbbuf_ei)
-{
-	if (dbbuf_db) {
-		u16 old_value;
-
-		/*
-		 * Ensure that the queue is written before updating
-		 * the doorbell in memory
-		 */
-		wmb();
-
-		old_value = *dbbuf_db;
-		*dbbuf_db = value;
-
-		/*
-		 * Ensure that the doorbell is updated before reading the event
-		 * index from memory.  The controller needs to provide similar
-		 * ordering to ensure the envent index is updated before reading
-		 * the doorbell.
-		 */
-		mb();
-
-		if (!nvme_dbbuf_need_event(*dbbuf_ei, value, old_value))
-			return false;
-	}
-
-	return true;
-}
-
-/*
- * Max size of iod being embedded in the request payload
- */
-#define NVME_INT_PAGES		2
-#define NVME_INT_BYTES(dev)	(NVME_INT_PAGES * (dev)->ctrl.page_size)
-
-/*
- * Will slightly overestimate the number of pages needed.  This is OK
- * as it only leads to a small amount of wasted memory for the lifetime of
- * the I/O.
- */
-static int nvme_npages(unsigned size, struct nvme_dev *dev)
-{
-	unsigned nprps = DIV_ROUND_UP(size + dev->ctrl.page_size,
-				      dev->ctrl.page_size);
-	return DIV_ROUND_UP(8 * nprps, PAGE_SIZE - 8);
-}
-
-static unsigned int nvme_iod_alloc_size(struct nvme_dev *dev,
-		unsigned int size, unsigned int nseg)
-{
-	return sizeof(__le64 *) * nvme_npages(size, dev) +
-			sizeof(struct scatterlist) * nseg;
-}
-
-static unsigned int nvme_cmd_size(struct nvme_dev *dev)
-{
-	return sizeof(struct nvme_iod) +
-		nvme_iod_alloc_size(dev, NVME_INT_BYTES(dev), NVME_INT_PAGES);
-}
-
-static int nvme_admin_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
-				unsigned int hctx_idx)
-{
-	struct nvme_dev *dev = data;
-	struct nvme_queue *nvmeq = &dev->queues[0];
-
-	WARN_ON(hctx_idx != 0);
-	WARN_ON(dev->admin_tagset.tags[0] != hctx->tags);
-	WARN_ON(nvmeq->tags);
-
-	hctx->driver_data = nvmeq;
-	nvmeq->tags = &dev->admin_tagset.tags[0];
-	return 0;
-}
-
-static void nvme_admin_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
-{
-	struct nvme_queue *nvmeq = hctx->driver_data;
-
-	nvmeq->tags = NULL;
-}
-
-static int nvme_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
-			  unsigned int hctx_idx)
-{
-	struct nvme_dev *dev = data;
-	struct nvme_queue *nvmeq = &dev->queues[hctx_idx + 1];
-
-	if (!nvmeq->tags)
-		nvmeq->tags = &dev->tagset.tags[hctx_idx];
-
-	WARN_ON(dev->tagset.tags[hctx_idx] != hctx->tags);
-	hctx->driver_data = nvmeq;
-	return 0;
-}
-
-static int nvme_init_request(struct blk_mq_tag_set *set, struct request *req,
-		unsigned int hctx_idx, unsigned int numa_node)
-{
-	struct nvme_dev *dev = set->driver_data;
-	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
-	int queue_idx = (set == &dev->tagset) ? hctx_idx + 1 : 0;
-	struct nvme_queue *nvmeq = &dev->queues[queue_idx];
-
-	BUG_ON(!nvmeq);
-	iod->nvmeq = nvmeq;
-	return 0;
-}
-
-static int nvme_pci_map_queues(struct blk_mq_tag_set *set)
-{
-	struct nvme_dev *dev = set->driver_data;
-
-	return blk_mq_pci_map_queues(set, to_pci_dev(dev->dev));
-}
-
-/**
- * __nvme_submit_cmd() - Copy a command into a queue and ring the doorbell
- * @nvmeq: The queue to use
- * @cmd: The command to send
- *
- * Safe to use from interrupt context
- */
-static void __nvme_submit_cmd(struct nvme_queue *nvmeq,
-						struct nvme_command *cmd)
-{
-	u16 tail = nvmeq->sq_tail;
-
-	if (nvmeq->sq_cmds_io)
-		memcpy_toio(&nvmeq->sq_cmds_io[tail], cmd, sizeof(*cmd));
-	else
-		memcpy(&nvmeq->sq_cmds[tail], cmd, sizeof(*cmd));
-
-	if (++tail == nvmeq->q_depth)
-		tail = 0;
-	if (nvme_dbbuf_update_and_check_event(tail, nvmeq->dbbuf_sq_db,
-					      nvmeq->dbbuf_sq_ei))
-		writel(tail, nvmeq->q_db);
-	nvmeq->sq_tail = tail;
-}
-
-static __le64 **iod_list(struct request *req)
-{
-	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
-	return (__le64 **)(iod->sg + blk_rq_nr_phys_segments(req));
-}
-
-static blk_status_t __nvme_init_iod(struct request *rq, struct nvme_dev *dev, bool kv_cmd)
-{
-	struct nvme_iod *iod = blk_mq_rq_to_pdu(rq);
-	int nseg;
-	unsigned int size;
-
-	if(kv_cmd) {
-		struct nvme_io_param* param = &iod->param;
-		nseg = param->kv_data_nents;
-		size = param->kv_data_len;
-	} else {
-		nseg = blk_rq_nr_phys_segments(rq);
-		size = blk_rq_payload_bytes(rq);
-	}
-
-	if (nseg > NVME_INT_PAGES || size > NVME_INT_BYTES(dev)) {
-		iod->sg = kmalloc(nvme_iod_alloc_size(dev, size, nseg), GFP_ATOMIC);
-		if (!iod->sg)
-			return BLK_STS_RESOURCE;
-	} else {
-		iod->sg = iod->inline_sg;
-	}
-
-	iod->kv_cmd = 0;
-	iod->aborted = 0;
-	iod->npages = -1;
-	iod->nents = 0;
-	iod->length = size;
-
-	return BLK_STS_OK;
-}
-
-static blk_status_t nvme_init_iod(struct request* rq, struct nvme_dev* dev)
-{
-	return __nvme_init_iod(rq, dev, false);
-}
-
-static blk_status_t nvme_init_iod_for_kv(struct request* rq, struct nvme_dev* dev)
-{
-	return __nvme_init_iod(rq, dev, true);
-}
-
-static void nvme_free_iod(struct nvme_dev *dev, struct request *req)
-{
-	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
-	const int last_prp = dev->ctrl.page_size / 8 - 1;
-	int i;
-	__le64 **list = iod_list(req);
-	dma_addr_t prp_dma = iod->first_dma;
-
-	if (iod->npages == 0)
-		dma_pool_free(dev->prp_small_pool, list[0], prp_dma);
-	for (i = 0; i < iod->npages; i++) {
-		__le64 *prp_list = list[i];
-		dma_addr_t next_prp_dma = le64_to_cpu(prp_list[last_prp]);
-		dma_pool_free(dev->prp_page_pool, prp_list, prp_dma);
-		prp_dma = next_prp_dma;
-	}
-
-	if (iod->sg != iod->inline_sg)
-		kfree(iod->sg);
-}
-
-#ifdef CONFIG_BLK_DEV_INTEGRITY
-static void nvme_dif_prep(u32 p, u32 v, struct t10_pi_tuple *pi)
-{
-	if (be32_to_cpu(pi->ref_tag) == v)
-		pi->ref_tag = cpu_to_be32(p);
-}
-
-static void nvme_dif_complete(u32 p, u32 v, struct t10_pi_tuple *pi)
-{
-	if (be32_to_cpu(pi->ref_tag) == p)
-		pi->ref_tag = cpu_to_be32(v);
-}
-
-/**
- * nvme_dif_remap - remaps ref tags to bip seed and physical lba
- *
- * The virtual start sector is the one that was originally submitted by the
- * block layer.	Due to partitioning, MD/DM cloning, etc. the actual physical
- * start sector may be different. Remap protection information to match the
- * physical LBA on writes, and back to the original seed on reads.
- *
- * Type 0 and 3 do not have a ref tag, so no remapping required.
- */
-static void nvme_dif_remap(struct request *req,
-			void (*dif_swap)(u32 p, u32 v, struct t10_pi_tuple *pi))
-{
-	struct nvme_ns *ns = req->rq_disk->private_data;
-	struct bio_integrity_payload *bip;
-	struct t10_pi_tuple *pi;
-	void *p, *pmap;
-	u32 i, nlb, ts, phys, virt;
-
-	if (!ns->pi_type || ns->pi_type == NVME_NS_DPS_PI_TYPE3)
-		return;
-
-	bip = bio_integrity(req->bio);
-	if (!bip)
-		return;
-
-	pmap = kmap_atomic(bip->bip_vec->bv_page) + bip->bip_vec->bv_offset;
-
-	p = pmap;
-	virt = bip_get_seed(bip);
-	phys = nvme_block_nr(ns, blk_rq_pos(req));
-	nlb = (blk_rq_bytes(req) >> ns->lba_shift);
-	ts = ns->disk->queue->integrity.tuple_size;
-
-	for (i = 0; i < nlb; i++, virt++, phys++) {
-		pi = (struct t10_pi_tuple *)p;
-		dif_swap(phys, virt, pi);
-		p += ts;
-	}
-	kunmap_atomic(pmap);
-}
-#else /* CONFIG_BLK_DEV_INTEGRITY */
-static void nvme_dif_remap(struct request *req,
-			void (*dif_swap)(u32 p, u32 v, struct t10_pi_tuple *pi))
-{
-}
-static void nvme_dif_prep(u32 p, u32 v, struct t10_pi_tuple *pi)
-{
-}
-static void nvme_dif_complete(u32 p, u32 v, struct t10_pi_tuple *pi)
-{
-}
-#endif
-
-static void nvme_print_sgl(struct scatterlist *sgl, int nents)
-{
-	int i;
-	struct scatterlist *sg;
-
-	for_each_sg(sgl, sg, nents, i) {
-		dma_addr_t phys = sg_phys(sg);
-		pr_warn("sg[%d] phys_addr:%pad offset:%d length:%d "
-			"dma_address:%pad dma_length:%d\n",
-			i, &phys, sg->offset, sg->length, &sg_dma_address(sg),
-			sg_dma_len(sg));
-	}
-}
-
-static blk_status_t nvme_setup_prps(struct nvme_dev *dev, struct request *req)
-{
-	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
-	struct dma_pool *pool;
-	int length;
-	struct scatterlist *sg = iod->sg;
-	int dma_len = sg_dma_len(sg);
-	u64 dma_addr = sg_dma_address(sg);
-	u32 page_size = dev->ctrl.page_size;
-	int offset = dma_addr & (page_size - 1);
-	__le64 *prp_list;
-	__le64 **list = iod_list(req);
-	dma_addr_t prp_dma;
-	int nprps, i;
-
-	if(iod->kv_cmd) {
-		length = iod->param.kv_data_len;
-	} else {
-		length = blk_rq_payload_bytes(req);
-	}
-
-	length -= (page_size - offset);
-	if (length <= 0) {
-		iod->first_dma = 0;
-		return BLK_STS_OK;
-	}
-
-	dma_len -= (page_size - offset);
-	if (dma_len) {
-		dma_addr += (page_size - offset);
-	} else {
-		sg = sg_next(sg);
-		dma_addr = sg_dma_address(sg);
-		dma_len = sg_dma_len(sg);
-	}
-
-	if (length <= page_size) {
-		iod->first_dma = dma_addr;
-		return BLK_STS_OK;
-	}
-
-	nprps = DIV_ROUND_UP(length, page_size);
-	if (nprps <= (256 / 8)) {
-		pool = dev->prp_small_pool;
-		iod->npages = 0;
-	} else {
-		pool = dev->prp_page_pool;
-		iod->npages = 1;
-	}
-
-	prp_list = dma_pool_alloc(pool, GFP_ATOMIC, &prp_dma);
-	if (!prp_list) {
-		iod->first_dma = dma_addr;
-		iod->npages = -1;
-		return BLK_STS_RESOURCE;
-	}
-	list[0] = prp_list;
-	iod->first_dma = prp_dma;
-	i = 0;
-	for (;;) {
-		if (i == page_size >> 3) {
-			__le64 *old_prp_list = prp_list;
-			prp_list = dma_pool_alloc(pool, GFP_ATOMIC, &prp_dma);
-			if (!prp_list)
-				return BLK_STS_RESOURCE;
-			list[iod->npages++] = prp_list;
-			prp_list[0] = old_prp_list[i - 1];
-			old_prp_list[i - 1] = cpu_to_le64(prp_dma);
-			i = 1;
-		}
-		prp_list[i++] = cpu_to_le64(dma_addr);
-		dma_len -= page_size;
-		dma_addr += page_size;
-		length -= page_size;
-		if (length <= 0)
-			break;
-		if (dma_len > 0)
-			continue;
-		if (unlikely(dma_len < 0))
-			goto bad_sgl;
-		sg = sg_next(sg);
-		dma_addr = sg_dma_address(sg);
-		dma_len = sg_dma_len(sg);
-	}
-
-	return BLK_STS_OK;
-
- bad_sgl:
-	WARN(DO_ONCE(nvme_print_sgl, iod->sg, iod->nents),
-			"Invalid SGL for payload:%d nents:%d\n",
-			blk_rq_payload_bytes(req), iod->nents);
-	return BLK_STS_IOERR;
-}
-
-static blk_status_t nvme_kv_map_data(struct nvme_dev *dev, struct request *req,
-		struct nvme_command *cmnd)
-{
-	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
-	struct nvme_io_param *param = &iod->param;
-	enum dma_data_direction dma_dir = DMA_BIDIRECTIONAL;
-	blk_status_t ret = BLK_STS_IOERR;
-
-	if (param->kv_data_sg_ptr) {
-		struct scatterlist *sg;
-		int i;
-		/* copy user sg to iod->sg */
-		for_each_sg(param->kv_data_sg_ptr, sg, param->kv_data_nents, i) {
-			iod->sg[i] = *sg;
-		}
-		iod->nents = param->kv_data_nents;
-		ret = BLK_STS_RESOURCE;
-		if (!dma_map_sg_attrs(dev->dev, iod->sg, iod->nents, dma_dir, DMA_ATTR_NO_WARN))
-			goto out;
-
-		ret = nvme_setup_prps(dev, req);
-		if (ret != BLK_STS_OK)
-			goto out_unmap;
-
-		cmnd->kv_store.dptr.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
-		cmnd->kv_store.dptr.prp2 = cpu_to_le64(iod->first_dma);
-	}
-	ret = BLK_STS_IOERR;
-
-	if (param->kv_meta_sg_ptr) {
-		if (!dma_map_sg(dev->dev, param->kv_meta_sg_ptr, 1, DMA_BIDIRECTIONAL))
-			goto out_unmap;
-		cmnd->kv_store.key_prp = cpu_to_le64(sg_dma_address(param->kv_meta_sg_ptr));
-	}
-	return BLK_STS_OK;
-
-out_unmap:
-	if (param->kv_data_sg_ptr)
-		dma_unmap_sg(dev->dev, iod->sg, iod->nents, dma_dir);
-out:
-	return ret;
-}
-
-static void nvme_kv_unmap_data(struct nvme_dev *dev, struct request *req)
-{
-	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
-	struct nvme_io_param *param = &iod->param;
-	enum dma_data_direction dma_dir = DMA_BIDIRECTIONAL;
-
-	if (param->kv_data_sg_ptr) {
-		dma_unmap_sg(dev->dev, iod->sg, iod->nents, dma_dir);
-	}
-	if (param->kv_meta_sg_ptr) {
-		dma_unmap_sg(dev->dev, param->kv_meta_sg_ptr, 1, DMA_BIDIRECTIONAL);
-	}
-	nvme_cleanup_cmd(req);
-	nvme_free_iod(dev, req);
-}
-
-static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
-		struct nvme_command *cmnd)
-{
-	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
-	struct request_queue *q = req->q;
-	enum dma_data_direction dma_dir = rq_data_dir(req) ?
-			DMA_TO_DEVICE : DMA_FROM_DEVICE;
-	blk_status_t ret = BLK_STS_IOERR;
-
-	if (is_kv_cmd(cmnd->common.opcode))
-		return nvme_kv_map_data(dev, req, cmnd);
-
-	sg_init_table(iod->sg, blk_rq_nr_phys_segments(req));
-	iod->nents = blk_rq_map_sg(q, req, iod->sg);
-	if (!iod->nents)
-		goto out;
-
-	ret = BLK_STS_RESOURCE;
-	if (!dma_map_sg_attrs(dev->dev, iod->sg, iod->nents, dma_dir,
-				DMA_ATTR_NO_WARN))
-		goto out;
-
-	ret = nvme_setup_prps(dev, req);
-	if (ret != BLK_STS_OK)
-		goto out_unmap;
-
-	ret = BLK_STS_IOERR;
-	if (blk_integrity_rq(req)) {
-		if (blk_rq_count_integrity_sg(q, req->bio) != 1)
-			goto out_unmap;
-
-		sg_init_table(&iod->meta_sg, 1);
-		if (blk_rq_map_integrity_sg(q, req->bio, &iod->meta_sg) != 1)
-			goto out_unmap;
-
-		if (req_op(req) == REQ_OP_WRITE)
-			nvme_dif_remap(req, nvme_dif_prep);
-
-		if (!dma_map_sg(dev->dev, &iod->meta_sg, 1, dma_dir))
-			goto out_unmap;
-	}
-
-	cmnd->rw.dptr.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
-	cmnd->rw.dptr.prp2 = cpu_to_le64(iod->first_dma);
-	if (blk_integrity_rq(req))
-		cmnd->rw.metadata = cpu_to_le64(sg_dma_address(&iod->meta_sg));
-	return BLK_STS_OK;
-
-out_unmap:
-	dma_unmap_sg(dev->dev, iod->sg, iod->nents, dma_dir);
-out:
-	return ret;
-}
-
-static void nvme_unmap_data(struct nvme_dev *dev, struct request *req)
-{
-	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
-	enum dma_data_direction dma_dir = rq_data_dir(req) ?
-			DMA_TO_DEVICE : DMA_FROM_DEVICE;
-
-	if (iod->kv_cmd)
-		return nvme_kv_unmap_data(dev, req);
-
-	if (iod->nents) {
-		dma_unmap_sg(dev->dev, iod->sg, iod->nents, dma_dir);
-		if (blk_integrity_rq(req)) {
-			if (req_op(req) == REQ_OP_READ)
-				nvme_dif_remap(req, nvme_dif_complete);
-			dma_unmap_sg(dev->dev, &iod->meta_sg, 1, dma_dir);
-		}
-	}
-
-	nvme_cleanup_cmd(req);
-	nvme_free_iod(dev, req);
-}
-
-/*
- * NOTE: ns is NULL when called on the admin queue.
- */
-static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
-			 const struct blk_mq_queue_data *bd)
-{
-	struct nvme_ns *ns = hctx->queue->queuedata;
-	struct nvme_queue *nvmeq = hctx->driver_data;
-	struct nvme_dev *dev = nvmeq->dev;
-	struct request *req = bd->rq;
-	struct nvme_command cmnd;
-	blk_status_t ret;
-	bool b_kv_cmd = false;
-
-	ret = nvme_setup_cmd(ns, req, &cmnd);
-	if (ret)
-		return ret;
-
-	b_kv_cmd = is_kv_cmd(cmnd.common.opcode);
-	if (b_kv_cmd) {
-		ret = nvme_init_iod_for_kv(req, dev);
-		if (ret)
-			goto out_free_cmd;
-	} else {
-		ret = nvme_init_iod(req, dev);
-		if (ret)
-			goto out_free_cmd;
-	}
-
-	if (b_kv_cmd) {
-		struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
-		iod->kv_cmd = 1;
-	}
-
-	if (blk_rq_nr_phys_segments(req) || is_kv_cmd(cmnd.common.opcode)) {
-		ret = nvme_map_data(dev, req, &cmnd);
-		if (ret)
-			goto out_cleanup_iod;
-	}
-
-	blk_mq_start_request(req);
-
-#ifdef KV_NVME_DUMMY_OP
-	if (is_kv_cmd(cmnd.common.opcode)) {
-#ifdef KV_NVME_DUMMY_OP_REPORT
-		__u32 *data = (__u32*) cmnd.common.cdw2;
-		pr_err("[dump nvme kv comand]\n");
-		pr_err("\topcode:(%02x)\n", cmnd.common.opcode);
-		pr_err("\tflags:(%02x)\n", cmnd.common.flags);
-		pr_err("\tcommand id:(%04x)\n", cmnd.common.command_id);
-		pr_err("\tns id:(%08x)\n", cmnd.common.nsid);
-		pr_err("\tcdw2:(%08x)\n", data[0]);
-		pr_err("\tcdw3:(%08x)\n", data[1]);
-		pr_err("\tcdw4:(%08x)\n", data[2]);
-		pr_err("\tcdw5:(%08x)\n", data[3]);
-		pr_err("\tcdw6:(%08x)\n", data[4]);
-		pr_err("\tcdw7:(%08x)\n", data[5]);
-		pr_err("\tcdw8:(%08x)\n", data[6]);
-		pr_err("\tcdw9:(%08x)\n", data[7]);
-		pr_err("\tcdw10:(%08x)\n", data[8]);
-		pr_err("\tcdw11:(%08x)\n", data[9]);
-		pr_err("\tcdw12:(%08x)\n", data[10]);
-		pr_err("\tcdw13:(%08x)\n", data[11]);
-		pr_err("\tcdw14:(%08x)\n", data[12]);
-		pr_err("\tcdw15:(%08x)\n", data[13]);
-#endif
-		nvme_req(req)->status = NVME_SC_SUCCESS;
-		nvme_req(req)->result.u32 = NVME_SC_SUCCESS;
-		if (is_kv_retrieve_cmd(cmnd.common.opcode))
-			nvme_req(req)->result.u32 = cmnd.common.cdw10[5];
-		blk_mq_complete_request(req);
-		return BLK_STS_OK;
-	}
-#endif
-
-	spin_lock_irq(&nvmeq->q_lock);
-	if (unlikely(nvmeq->cq_vector < 0)) {
-		ret = BLK_STS_IOERR;
-		spin_unlock_irq(&nvmeq->q_lock);
-		goto out_cleanup_iod;
-	}
-	__nvme_submit_cmd(nvmeq, &cmnd);
-	nvme_process_cq(nvmeq);
-	spin_unlock_irq(&nvmeq->q_lock);
-	return BLK_STS_OK;
-out_cleanup_iod:
-	nvme_free_iod(dev, req);
-out_free_cmd:
-	nvme_cleanup_cmd(req);
-	return ret;
-}
-
-static void nvme_pci_complete_rq(struct request *req)
-{
-	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
-
-	nvme_unmap_data(iod->nvmeq->dev, req);
-	nvme_complete_rq(req);
-}
-
-/* We read the CQE phase first to check if the rest of the entry is valid */
-static inline bool nvme_cqe_valid(struct nvme_queue *nvmeq, u16 head,
-		u16 phase)
-{
-	return (le16_to_cpu(nvmeq->cqes[head].status) & 1) == phase;
-}
-
-static inline void nvme_ring_cq_doorbell(struct nvme_queue *nvmeq)
-{
-	u16 head = nvmeq->cq_head;
-
-	if (likely(nvmeq->cq_vector >= 0)) {
-		if (nvme_dbbuf_update_and_check_event(head, nvmeq->dbbuf_cq_db,
-						      nvmeq->dbbuf_cq_ei))
-			writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
-	}
-}
-
-static inline void nvme_handle_cqe(struct nvme_queue *nvmeq,
-		struct nvme_completion *cqe)
-{
-	struct request *req;
-
-	if (unlikely(cqe->command_id >= nvmeq->q_depth)) {
-		dev_warn(nvmeq->dev->ctrl.device,
-			"invalid id %d completed on queue %d\n",
-			cqe->command_id, le16_to_cpu(cqe->sq_id));
-		return;
-	}
-
-	/*
-	 * AEN requests are special as they don't time out and can
-	 * survive any kind of queue freeze and often don't respond to
-	 * aborts.  We don't even bother to allocate a struct request
-	 * for them but rather special case them here.
-	 */
-	if (unlikely(nvmeq->qid == 0 &&
-			cqe->command_id >= NVME_AQ_BLKMQ_DEPTH)) {
-		nvme_complete_async_event(&nvmeq->dev->ctrl,
-				cqe->status, &cqe->result);
-		return;
-	}
-
-	nvmeq->cqe_seen = 1;
-	req = blk_mq_tag_to_rq(*nvmeq->tags, cqe->command_id);
-	nvme_end_request(req, cqe->status, cqe->result);
-}
-
-static inline bool nvme_read_cqe(struct nvme_queue *nvmeq,
-		struct nvme_completion *cqe)
-{
-	if (nvme_cqe_valid(nvmeq, nvmeq->cq_head, nvmeq->cq_phase)) {
-		*cqe = nvmeq->cqes[nvmeq->cq_head];
-
-		if (++nvmeq->cq_head == nvmeq->q_depth) {
-			nvmeq->cq_head = 0;
-			nvmeq->cq_phase = !nvmeq->cq_phase;
-		}
-		return true;
-	}
-	return false;
-}
-
-static void nvme_process_cq(struct nvme_queue *nvmeq)
-{
-	struct nvme_completion cqe;
-	int consumed = 0;
-
-	while (nvme_read_cqe(nvmeq, &cqe)) {
-		nvme_handle_cqe(nvmeq, &cqe);
-		consumed++;
-	}
-
-	if (consumed)
-		nvme_ring_cq_doorbell(nvmeq);
-}
-
-static irqreturn_t nvme_irq(int irq, void *data)
-{
-	irqreturn_t result;
-	struct nvme_queue *nvmeq = data;
-	spin_lock(&nvmeq->q_lock);
-	nvme_process_cq(nvmeq);
-	result = nvmeq->cqe_seen ? IRQ_HANDLED : IRQ_NONE;
-	nvmeq->cqe_seen = 0;
-	spin_unlock(&nvmeq->q_lock);
-	return result;
-}
-
-static irqreturn_t nvme_irq_check(int irq, void *data)
-{
-	struct nvme_queue *nvmeq = data;
-	if (nvme_cqe_valid(nvmeq, nvmeq->cq_head, nvmeq->cq_phase))
-		return IRQ_WAKE_THREAD;
-	return IRQ_NONE;
-}
-
-static int __nvme_poll(struct nvme_queue *nvmeq, unsigned int tag)
-{
-	struct nvme_completion cqe;
-	int found = 0, consumed = 0;
-
-	if (!nvme_cqe_valid(nvmeq, nvmeq->cq_head, nvmeq->cq_phase))
-		return 0;
-
-	spin_lock_irq(&nvmeq->q_lock);
-	while (nvme_read_cqe(nvmeq, &cqe)) {
-		nvme_handle_cqe(nvmeq, &cqe);
-		consumed++;
-
-		if (tag == cqe.command_id) {
-			found = 1;
-			break;
-		}
-       }
-
-	if (consumed)
-		nvme_ring_cq_doorbell(nvmeq);
-	spin_unlock_irq(&nvmeq->q_lock);
-
-	return found;
-}
-
-static int nvme_poll(struct blk_mq_hw_ctx *hctx, unsigned int tag)
-{
-	struct nvme_queue *nvmeq = hctx->driver_data;
-
-	return __nvme_poll(nvmeq, tag);
-}
-
-static void nvme_pci_submit_async_event(struct nvme_ctrl *ctrl, int aer_idx)
-{
-	struct nvme_dev *dev = to_nvme_dev(ctrl);
-	struct nvme_queue *nvmeq = &dev->queues[0];
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.common.opcode = nvme_admin_async_event;
-	c.common.command_id = NVME_AQ_BLKMQ_DEPTH + aer_idx;
-
-	spin_lock_irq(&nvmeq->q_lock);
-	__nvme_submit_cmd(nvmeq, &c);
-	spin_unlock_irq(&nvmeq->q_lock);
-}
-
-static int adapter_delete_queue(struct nvme_dev *dev, u8 opcode, u16 id)
-{
-	struct nvme_command c;
-
-	memset(&c, 0, sizeof(c));
-	c.delete_queue.opcode = opcode;
-	c.delete_queue.qid = cpu_to_le16(id);
-
-	return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
-}
-
-static int adapter_alloc_cq(struct nvme_dev *dev, u16 qid,
-						struct nvme_queue *nvmeq)
-{
-	struct nvme_command c;
-	int flags = NVME_QUEUE_PHYS_CONTIG | NVME_CQ_IRQ_ENABLED;
-
-	/*
-	 * Note: we (ab)use the fact the the prp fields survive if no data
-	 * is attached to the request.
-	 */
-	memset(&c, 0, sizeof(c));
-	c.create_cq.opcode = nvme_admin_create_cq;
-	c.create_cq.prp1 = cpu_to_le64(nvmeq->cq_dma_addr);
-	c.create_cq.cqid = cpu_to_le16(qid);
-	c.create_cq.qsize = cpu_to_le16(nvmeq->q_depth - 1);
-	c.create_cq.cq_flags = cpu_to_le16(flags);
-	c.create_cq.irq_vector = cpu_to_le16(nvmeq->cq_vector);
-
-	return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
-}
-
-static int adapter_alloc_sq(struct nvme_dev *dev, u16 qid,
-						struct nvme_queue *nvmeq)
-{
-	struct nvme_ctrl *ctrl = &dev->ctrl;
-	struct nvme_command c;
-	int flags = NVME_QUEUE_PHYS_CONTIG;
-
-	/*
-	 * Some drives have a bug that auto-enables WRRU if MEDIUM isn't
-	 * set. Since URGENT priority is zeroes, it makes all queues
-	 * URGENT.
-	 */
-	if (ctrl->quirks & NVME_QUIRK_MEDIUM_PRIO_SQ)
-		flags |= NVME_SQ_PRIO_MEDIUM;
-
-	/*
-	 * Note: we (ab)use the fact the the prp fields survive if no data
-	 * is attached to the request.
-	 */
-	memset(&c, 0, sizeof(c));
-	c.create_sq.opcode = nvme_admin_create_sq;
-	c.create_sq.prp1 = cpu_to_le64(nvmeq->sq_dma_addr);
-	c.create_sq.sqid = cpu_to_le16(qid);
-	c.create_sq.qsize = cpu_to_le16(nvmeq->q_depth - 1);
-	c.create_sq.sq_flags = cpu_to_le16(flags);
-	c.create_sq.cqid = cpu_to_le16(qid);
-
-	return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
-}
-
-static int adapter_delete_cq(struct nvme_dev *dev, u16 cqid)
-{
-	return adapter_delete_queue(dev, nvme_admin_delete_cq, cqid);
-}
-
-static int adapter_delete_sq(struct nvme_dev *dev, u16 sqid)
-{
-	return adapter_delete_queue(dev, nvme_admin_delete_sq, sqid);
-}
-
-static void abort_endio(struct request *req, blk_status_t error)
-{
-	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
-	struct nvme_queue *nvmeq = iod->nvmeq;
-
-	dev_warn(nvmeq->dev->ctrl.device,
-		 "Abort status: 0x%x", nvme_req(req)->status);
-	atomic_inc(&nvmeq->dev->ctrl.abort_limit);
-	blk_mq_free_request(req);
-}
-
-static bool nvme_should_reset(struct nvme_dev *dev, u32 csts)
-{
-
-	/* If true, indicates loss of adapter communication, possibly by a
-	 * NVMe Subsystem reset.
-	 */
-	bool nssro = dev->subsystem && (csts & NVME_CSTS_NSSRO);
-
-	/* If there is a reset ongoing, we shouldn't reset again. */
-	if (dev->ctrl.state == NVME_CTRL_RESETTING)
-		return false;
-
-	/* We shouldn't reset unless the controller is on fatal error state
-	 * _or_ if we lost the communication with it.
-	 */
-	if (!(csts & NVME_CSTS_CFS) && !nssro)
-		return false;
-
-	return true;
-}
-
-static void nvme_warn_reset(struct nvme_dev *dev, u32 csts)
-{
-	/* Read a config register to help see what died. */
-	u16 pci_status;
-	int result;
-
-	result = pci_read_config_word(to_pci_dev(dev->dev), PCI_STATUS,
-				      &pci_status);
-	if (result == PCIBIOS_SUCCESSFUL)
-		dev_warn(dev->ctrl.device,
-			 "controller is down; will reset: CSTS=0x%x, PCI_STATUS=0x%hx\n",
-			 csts, pci_status);
-	else
-		dev_warn(dev->ctrl.device,
-			 "controller is down; will reset: CSTS=0x%x, PCI_STATUS read failed (%d)\n",
-			 csts, result);
-}
-
-static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
-{
-	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
-	struct nvme_queue *nvmeq = iod->nvmeq;
-	struct nvme_dev *dev = nvmeq->dev;
-	struct request *abort_req;
-	struct nvme_command cmd;
-	u32 csts = readl(dev->bar + NVME_REG_CSTS);
-
-	/* If PCI error recovery process is happening, we cannot reset or
-	 * the recovery mechanism will surely fail.
-	 */
-	mb();
-	if (pci_channel_offline(to_pci_dev(dev->dev)))
-		return BLK_EH_RESET_TIMER;
-
-	/*
-	 * Reset immediately if the controller is failed
-	 */
-	if (nvme_should_reset(dev, csts)) {
-		nvme_warn_reset(dev, csts);
-		nvme_dev_disable(dev, false);
-		nvme_reset_ctrl(&dev->ctrl);
-		return BLK_EH_HANDLED;
-	}
-
-	/*
-	 * Did we miss an interrupt?
-	 */
-	if (__nvme_poll(nvmeq, req->tag)) {
-		dev_warn(dev->ctrl.device,
-			 "I/O %d QID %d timeout, completion polled\n",
-			 req->tag, nvmeq->qid);
-		return BLK_EH_HANDLED;
-	}
-
-	/*
-	 * Shutdown immediately if controller times out while starting. The
-	 * reset work will see the pci device disabled when it gets the forced
-	 * cancellation error. All outstanding requests are completed on
-	 * shutdown, so we return BLK_EH_HANDLED.
-	 */
-	if (dev->ctrl.state == NVME_CTRL_RESETTING) {
-		dev_warn(dev->ctrl.device,
-			 "I/O %d QID %d timeout, disable controller\n",
-			 req->tag, nvmeq->qid);
-		nvme_dev_disable(dev, false);
-		nvme_req(req)->flags |= NVME_REQ_CANCELLED;
-		return BLK_EH_HANDLED;
-	}
-
-	/*
- 	 * Shutdown the controller immediately and schedule a reset if the
- 	 * command was already aborted once before and still hasn't been
- 	 * returned to the driver, or if this is the admin queue.
-	 */
-	if (!nvmeq->qid || iod->aborted) {
-		dev_warn(dev->ctrl.device,
-			 "I/O %d QID %d timeout, reset controller\n",
-			 req->tag, nvmeq->qid);
-		nvme_dev_disable(dev, false);
-		nvme_reset_ctrl(&dev->ctrl);
-
-		/*
-		 * Mark the request as handled, since the inline shutdown
-		 * forces all outstanding requests to complete.
-		 */
-		nvme_req(req)->flags |= NVME_REQ_CANCELLED;
-		return BLK_EH_HANDLED;
-	}
-
-	if (atomic_dec_return(&dev->ctrl.abort_limit) < 0) {
-		atomic_inc(&dev->ctrl.abort_limit);
-		return BLK_EH_RESET_TIMER;
-	}
-	iod->aborted = 1;
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.abort.opcode = nvme_admin_abort_cmd;
-	cmd.abort.cid = req->tag;
-	cmd.abort.sqid = cpu_to_le16(nvmeq->qid);
-
-	dev_warn(nvmeq->dev->ctrl.device,
-		"I/O %d QID %d timeout, aborting\n",
-		 req->tag, nvmeq->qid);
-
-	abort_req = nvme_alloc_request(dev->ctrl.admin_q, &cmd,
-			BLK_MQ_REQ_NOWAIT, NVME_QID_ANY);
-	if (IS_ERR(abort_req)) {
-		atomic_inc(&dev->ctrl.abort_limit);
-		return BLK_EH_RESET_TIMER;
-	}
-
-	abort_req->timeout = ADMIN_TIMEOUT;
-	abort_req->end_io_data = NULL;
-	blk_execute_rq_nowait(abort_req->q, NULL, abort_req, 0, abort_endio);
-
-	/*
-	 * The aborted req will be completed on receiving the abort req.
-	 * We enable the timer again. If hit twice, it'll cause a device reset,
-	 * as the device then is in a faulty state.
-	 */
-	return BLK_EH_RESET_TIMER;
-}
-
-static void nvme_free_queue(struct nvme_queue *nvmeq)
-{
-	dma_free_coherent(nvmeq->q_dmadev, CQ_SIZE(nvmeq->q_depth),
-				(void *)nvmeq->cqes, nvmeq->cq_dma_addr);
-	if (nvmeq->sq_cmds)
-		dma_free_coherent(nvmeq->q_dmadev, SQ_SIZE(nvmeq->q_depth),
-					nvmeq->sq_cmds, nvmeq->sq_dma_addr);
-}
-
-static void nvme_free_queues(struct nvme_dev *dev, int lowest)
-{
-	int i;
-
-	for (i = dev->ctrl.queue_count - 1; i >= lowest; i--) {
-		dev->ctrl.queue_count--;
-		nvme_free_queue(&dev->queues[i]);
-	}
-}
-
-/**
- * nvme_suspend_queue - put queue into suspended state
- * @nvmeq - queue to suspend
- */
-static int nvme_suspend_queue(struct nvme_queue *nvmeq)
-{
-	int vector;
-
-	spin_lock_irq(&nvmeq->q_lock);
-	if (nvmeq->cq_vector == -1) {
-		spin_unlock_irq(&nvmeq->q_lock);
-		return 1;
-	}
-	vector = nvmeq->cq_vector;
-	nvmeq->dev->online_queues--;
-	nvmeq->cq_vector = -1;
-	spin_unlock_irq(&nvmeq->q_lock);
-
-	if (!nvmeq->qid && nvmeq->dev->ctrl.admin_q)
-		blk_mq_quiesce_queue(nvmeq->dev->ctrl.admin_q);
-
-	pci_free_irq(to_pci_dev(nvmeq->dev->dev), vector, nvmeq);
-
-	return 0;
-}
-
-static void nvme_disable_admin_queue(struct nvme_dev *dev, bool shutdown)
-{
-	struct nvme_queue *nvmeq = &dev->queues[0];
-
-	if (nvme_suspend_queue(nvmeq))
-		return;
-
-	if (shutdown)
-		nvme_shutdown_ctrl(&dev->ctrl);
-	else
-		nvme_disable_ctrl(&dev->ctrl, dev->ctrl.cap);
-
-	spin_lock_irq(&nvmeq->q_lock);
-	nvme_process_cq(nvmeq);
-	spin_unlock_irq(&nvmeq->q_lock);
-}
-
-static int nvme_cmb_qdepth(struct nvme_dev *dev, int nr_io_queues,
-				int entry_size)
-{
-	int q_depth = dev->q_depth;
-	unsigned q_size_aligned = roundup(q_depth * entry_size,
-					  dev->ctrl.page_size);
-
-	if (q_size_aligned * nr_io_queues > dev->cmb_size) {
-		u64 mem_per_q = div_u64(dev->cmb_size, nr_io_queues);
-		mem_per_q = round_down(mem_per_q, dev->ctrl.page_size);
-		q_depth = div_u64(mem_per_q, entry_size);
-
-		/*
-		 * Ensure the reduced q_depth is above some threshold where it
-		 * would be better to map queues in system memory with the
-		 * original depth
-		 */
-		if (q_depth < 64)
-			return -ENOMEM;
-	}
-
-	return q_depth;
-}
-
-static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
-				int qid, int depth)
-{
-
-	/* CMB SQEs will be mapped before creation */
-	if (qid && dev->cmb && use_cmb_sqes && NVME_CMB_SQS(dev->cmbsz))
-		return 0;
-
-	nvmeq->sq_cmds = dma_alloc_coherent(dev->dev, SQ_SIZE(depth),
-					    &nvmeq->sq_dma_addr, GFP_KERNEL);
-	if (!nvmeq->sq_cmds)
-		return -ENOMEM;
-
-	return 0;
-}
-
-static int nvme_alloc_queue(struct nvme_dev *dev, int qid,
-		int depth, int node)
-{
-	struct nvme_queue *nvmeq = &dev->queues[qid];
-
-	if (dev->ctrl.queue_count > qid)
-		return 0;
-
-	nvmeq->cqes = dma_zalloc_coherent(dev->dev, CQ_SIZE(depth),
-					  &nvmeq->cq_dma_addr, GFP_KERNEL);
-	if (!nvmeq->cqes)
-		goto free_nvmeq;
-
-	if (nvme_alloc_sq_cmds(dev, nvmeq, qid, depth))
-		goto free_cqdma;
-
-	nvmeq->q_dmadev = dev->dev;
-	nvmeq->dev = dev;
-	spin_lock_init(&nvmeq->q_lock);
-	nvmeq->cq_head = 0;
-	nvmeq->cq_phase = 1;
-	nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
-	nvmeq->q_depth = depth;
-	nvmeq->qid = qid;
-	nvmeq->cq_vector = -1;
-	dev->ctrl.queue_count++;
-
-	return 0;
-
- free_cqdma:
-	dma_free_coherent(dev->dev, CQ_SIZE(depth), (void *)nvmeq->cqes,
-							nvmeq->cq_dma_addr);
- free_nvmeq:
-	return -ENOMEM;
-}
-
-static int queue_request_irq(struct nvme_queue *nvmeq)
-{
-	struct pci_dev *pdev = to_pci_dev(nvmeq->dev->dev);
-	int nr = nvmeq->dev->ctrl.instance;
-
-	if (use_threaded_interrupts) {
-		return pci_request_irq(pdev, nvmeq->cq_vector, nvme_irq_check,
-				nvme_irq, nvmeq, "nvme%dq%d", nr, nvmeq->qid);
-	} else {
-		return pci_request_irq(pdev, nvmeq->cq_vector, nvme_irq,
-				NULL, nvmeq, "nvme%dq%d", nr, nvmeq->qid);
-	}
-}
-
-static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
-{
-	struct nvme_dev *dev = nvmeq->dev;
-
-	spin_lock_irq(&nvmeq->q_lock);
-	nvmeq->sq_tail = 0;
-	nvmeq->cq_head = 0;
-	nvmeq->cq_phase = 1;
-	nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
-	memset((void *)nvmeq->cqes, 0, CQ_SIZE(nvmeq->q_depth));
-	nvme_dbbuf_init(dev, nvmeq, qid);
-	dev->online_queues++;
-	spin_unlock_irq(&nvmeq->q_lock);
-}
-
-static int nvme_create_queue(struct nvme_queue *nvmeq, int qid)
-{
-	struct nvme_dev *dev = nvmeq->dev;
-	int result;
-
-	if (qid && dev->cmb && use_cmb_sqes && NVME_CMB_SQS(dev->cmbsz)) {
-		unsigned offset = (qid - 1) * roundup(SQ_SIZE(nvmeq->q_depth),
-						      dev->ctrl.page_size);
-		nvmeq->sq_dma_addr = dev->cmb_bus_addr + offset;
-		nvmeq->sq_cmds_io = dev->cmb + offset;
-	}
-
-	nvmeq->cq_vector = qid - 1;
-	result = adapter_alloc_cq(dev, qid, nvmeq);
-	if (result < 0)
-		goto release_vector;
-
-	result = adapter_alloc_sq(dev, qid, nvmeq);
-	if (result < 0)
-		goto release_cq;
-
-	nvme_init_queue(nvmeq, qid);
-	result = queue_request_irq(nvmeq);
-	if (result < 0)
-		goto release_sq;
-
-	return result;
-
- release_sq:
-	dev->online_queues--;
-	adapter_delete_sq(dev, qid);
- release_cq:
-	adapter_delete_cq(dev, qid);
- release_vector:
-	nvmeq->cq_vector = -1;
-	return result;
-}
-
-static const struct blk_mq_ops nvme_mq_admin_ops = {
-	.queue_rq	= nvme_queue_rq,
-	.complete	= nvme_pci_complete_rq,
-	.init_hctx	= nvme_admin_init_hctx,
-	.exit_hctx      = nvme_admin_exit_hctx,
-	.init_request	= nvme_init_request,
-	.timeout	= nvme_timeout,
-};
-
-static const struct blk_mq_ops nvme_mq_ops = {
-	.queue_rq	= nvme_queue_rq,
-	.complete	= nvme_pci_complete_rq,
-	.init_hctx	= nvme_init_hctx,
-	.init_request	= nvme_init_request,
-	.map_queues	= nvme_pci_map_queues,
-	.timeout	= nvme_timeout,
-	.poll		= nvme_poll,
-};
-
-static void nvme_dev_remove_admin(struct nvme_dev *dev)
-{
-	if (dev->ctrl.admin_q && !blk_queue_dying(dev->ctrl.admin_q)) {
-		/*
-		 * If the controller was reset during removal, it's possible
-		 * user requests may be waiting on a stopped queue. Start the
-		 * queue to flush these to completion.
-		 */
-		blk_mq_unquiesce_queue(dev->ctrl.admin_q);
-		blk_cleanup_queue(dev->ctrl.admin_q);
-		blk_mq_free_tag_set(&dev->admin_tagset);
-	}
-}
-
-static int nvme_alloc_admin_tags(struct nvme_dev *dev)
-{
-	if (!dev->ctrl.admin_q) {
-		dev->admin_tagset.ops = &nvme_mq_admin_ops;
-		dev->admin_tagset.nr_hw_queues = 1;
-
-		/*
-		 * Subtract one to leave an empty queue entry for 'Full Queue'
-		 * condition. See NVM-Express 1.2 specification, section 4.1.2.
-		 */
-		dev->admin_tagset.queue_depth = NVME_AQ_BLKMQ_DEPTH - 1;
-		dev->admin_tagset.timeout = ADMIN_TIMEOUT;
-		dev->admin_tagset.numa_node = dev_to_node(dev->dev);
-		dev->admin_tagset.cmd_size = nvme_cmd_size(dev);
-		dev->admin_tagset.flags = BLK_MQ_F_NO_SCHED;
-		dev->admin_tagset.driver_data = dev;
-
-		if (blk_mq_alloc_tag_set(&dev->admin_tagset))
-			return -ENOMEM;
-		dev->ctrl.admin_tagset = &dev->admin_tagset;
-
-		dev->ctrl.admin_q = blk_mq_init_queue(&dev->admin_tagset);
-		if (IS_ERR(dev->ctrl.admin_q)) {
-			blk_mq_free_tag_set(&dev->admin_tagset);
-			return -ENOMEM;
-		}
-		if (!blk_get_queue(dev->ctrl.admin_q)) {
-			nvme_dev_remove_admin(dev);
-			dev->ctrl.admin_q = NULL;
-			return -ENODEV;
-		}
-	} else
-		blk_mq_unquiesce_queue(dev->ctrl.admin_q);
-
-	return 0;
-}
-
-static unsigned long db_bar_size(struct nvme_dev *dev, unsigned nr_io_queues)
-{
-	return NVME_REG_DBS + ((nr_io_queues + 1) * 8 * dev->db_stride);
-}
-
-static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
-{
-	struct pci_dev *pdev = to_pci_dev(dev->dev);
-
-	if (size <= dev->bar_mapped_size)
-		return 0;
-	if (size > pci_resource_len(pdev, 0))
-		return -ENOMEM;
-	if (dev->bar)
-		iounmap(dev->bar);
-	dev->bar = ioremap(pci_resource_start(pdev, 0), size);
-	if (!dev->bar) {
-		dev->bar_mapped_size = 0;
-		return -ENOMEM;
-	}
-	dev->bar_mapped_size = size;
-	dev->dbs = dev->bar + NVME_REG_DBS;
-
-	return 0;
-}
-
-static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
-{
-	int result;
-	u32 aqa;
-	struct nvme_queue *nvmeq;
-
-	result = nvme_remap_bar(dev, db_bar_size(dev, 0));
-	if (result < 0)
-		return result;
-
-	dev->subsystem = readl(dev->bar + NVME_REG_VS) >= NVME_VS(1, 1, 0) ?
-				NVME_CAP_NSSRC(dev->ctrl.cap) : 0;
-
-	if (dev->subsystem &&
-	    (readl(dev->bar + NVME_REG_CSTS) & NVME_CSTS_NSSRO))
-		writel(NVME_CSTS_NSSRO, dev->bar + NVME_REG_CSTS);
-
-	result = nvme_disable_ctrl(&dev->ctrl, dev->ctrl.cap);
-	if (result < 0)
-		return result;
-
-	result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH,
-			dev_to_node(dev->dev));
-	if (result)
-		return result;
-
-	nvmeq = &dev->queues[0];
-	aqa = nvmeq->q_depth - 1;
-	aqa |= aqa << 16;
-
-	writel(aqa, dev->bar + NVME_REG_AQA);
-	lo_hi_writeq(nvmeq->sq_dma_addr, dev->bar + NVME_REG_ASQ);
-	lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);
-
-	result = nvme_enable_ctrl(&dev->ctrl, dev->ctrl.cap);
-	if (result)
-		return result;
-
-	nvmeq->cq_vector = 0;
-	nvme_init_queue(nvmeq, 0);
-	result = queue_request_irq(nvmeq);
-	if (result) {
-		nvmeq->cq_vector = -1;
-		return result;
-	}
-
-	return result;
-}
-
-static int nvme_create_io_queues(struct nvme_dev *dev)
-{
-	unsigned i, max;
-	int ret = 0;
-
-	for (i = dev->ctrl.queue_count; i <= dev->max_qid; i++) {
-		/* vector == qid - 1, match nvme_create_queue */
-		if (nvme_alloc_queue(dev, i, dev->q_depth,
-		     pci_irq_get_node(to_pci_dev(dev->dev), i - 1))) {
-			ret = -ENOMEM;
-			break;
-		}
-	}
-
-	max = min(dev->max_qid, dev->ctrl.queue_count - 1);
-	for (i = dev->online_queues; i <= max; i++) {
-		ret = nvme_create_queue(&dev->queues[i], i);
-		if (ret)
-			break;
-	}
-
-	/*
-	 * Ignore failing Create SQ/CQ commands, we can continue with less
-	 * than the desired aount of queues, and even a controller without
-	 * I/O queues an still be used to issue admin commands.  This might
-	 * be useful to upgrade a buggy firmware for example.
-	 */
-	return ret >= 0 ? 0 : ret;
-}
-
-static ssize_t nvme_cmb_show(struct device *dev,
-			     struct device_attribute *attr,
-			     char *buf)
-{
-	struct nvme_dev *ndev = to_nvme_dev(dev_get_drvdata(dev));
-
-	return scnprintf(buf, PAGE_SIZE, "cmbloc : x%08x\ncmbsz  : x%08x\n",
-		       ndev->cmbloc, ndev->cmbsz);
-}
-static DEVICE_ATTR(cmb, S_IRUGO, nvme_cmb_show, NULL);
-
-static void __iomem *nvme_map_cmb(struct nvme_dev *dev)
-{
-	u64 szu, size, offset;
-	resource_size_t bar_size;
-	struct pci_dev *pdev = to_pci_dev(dev->dev);
-	void __iomem *cmb;
-	int bar;
-
-	dev->cmbsz = readl(dev->bar + NVME_REG_CMBSZ);
-	if (!(NVME_CMB_SZ(dev->cmbsz)))
-		return NULL;
-	dev->cmbloc = readl(dev->bar + NVME_REG_CMBLOC);
-
-	if (!use_cmb_sqes)
-		return NULL;
-
-	szu = (u64)1 << (12 + 4 * NVME_CMB_SZU(dev->cmbsz));
-	size = szu * NVME_CMB_SZ(dev->cmbsz);
-	offset = szu * NVME_CMB_OFST(dev->cmbloc);
-	bar = NVME_CMB_BIR(dev->cmbloc);
-	bar_size = pci_resource_len(pdev, bar);
-
-	if (offset > bar_size)
-		return NULL;
-
-	/*
-	 * Controllers may support a CMB size larger than their BAR,
-	 * for example, due to being behind a bridge. Reduce the CMB to
-	 * the reported size of the BAR
-	 */
-	if (size > bar_size - offset)
-		size = bar_size - offset;
-
-	cmb = ioremap_wc(pci_resource_start(pdev, bar) + offset, size);
-	if (!cmb)
-		return NULL;
-
-	dev->cmb_bus_addr = pci_bus_address(pdev, bar) + offset;
-	dev->cmb_size = size;
-	return cmb;
-}
-
-static inline void nvme_release_cmb(struct nvme_dev *dev)
-{
-	if (dev->cmb) {
-		iounmap(dev->cmb);
-		dev->cmb = NULL;
-		sysfs_remove_file_from_group(&dev->ctrl.device->kobj,
-					     &dev_attr_cmb.attr, NULL);
-		dev->cmbsz = 0;
-	}
-}
-
-static int nvme_set_host_mem(struct nvme_dev *dev, u32 bits)
-{
-	u64 dma_addr = dev->host_mem_descs_dma;
-	struct nvme_command c;
-	int ret;
-
-	memset(&c, 0, sizeof(c));
-	c.features.opcode	= nvme_admin_set_features;
-	c.features.fid		= cpu_to_le32(NVME_FEAT_HOST_MEM_BUF);
-	c.features.dword11	= cpu_to_le32(bits);
-	c.features.dword12	= cpu_to_le32(dev->host_mem_size >>
-					      ilog2(dev->ctrl.page_size));
-	c.features.dword13	= cpu_to_le32(lower_32_bits(dma_addr));
-	c.features.dword14	= cpu_to_le32(upper_32_bits(dma_addr));
-	c.features.dword15	= cpu_to_le32(dev->nr_host_mem_descs);
-
-	ret = nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
-	if (ret) {
-		dev_warn(dev->ctrl.device,
-			 "failed to set host mem (err %d, flags %#x).\n",
-			 ret, bits);
-	}
-	return ret;
-}
-
-static void nvme_free_host_mem(struct nvme_dev *dev)
-{
-	int i;
-
-	for (i = 0; i < dev->nr_host_mem_descs; i++) {
-		struct nvme_host_mem_buf_desc *desc = &dev->host_mem_descs[i];
-		size_t size = le32_to_cpu(desc->size) * dev->ctrl.page_size;
-
-		dma_free_coherent(dev->dev, size, dev->host_mem_desc_bufs[i],
-				le64_to_cpu(desc->addr));
-	}
-
-	kfree(dev->host_mem_desc_bufs);
-	dev->host_mem_desc_bufs = NULL;
-	dma_free_coherent(dev->dev,
-			dev->nr_host_mem_descs * sizeof(*dev->host_mem_descs),
-			dev->host_mem_descs, dev->host_mem_descs_dma);
-	dev->host_mem_descs = NULL;
-	dev->nr_host_mem_descs = 0;
-}
-
-static int __nvme_alloc_host_mem(struct nvme_dev *dev, u64 preferred,
-		u32 chunk_size)
-{
-	struct nvme_host_mem_buf_desc *descs;
-	u32 max_entries, len;
-	dma_addr_t descs_dma;
-	int i = 0;
-	void **bufs;
-	u64 size = 0, tmp;
-
-	tmp = (preferred + chunk_size - 1);
-	do_div(tmp, chunk_size);
-	max_entries = tmp;
-
-	if (dev->ctrl.hmmaxd && dev->ctrl.hmmaxd < max_entries)
-		max_entries = dev->ctrl.hmmaxd;
-
-	descs = dma_zalloc_coherent(dev->dev, max_entries * sizeof(*descs),
-			&descs_dma, GFP_KERNEL);
-	if (!descs)
-		goto out;
-
-	bufs = kcalloc(max_entries, sizeof(*bufs), GFP_KERNEL);
-	if (!bufs)
-		goto out_free_descs;
-
-	for (size = 0; size < preferred && i < max_entries; size += len) {
-		dma_addr_t dma_addr;
-
-		len = min_t(u64, chunk_size, preferred - size);
-		bufs[i] = dma_alloc_attrs(dev->dev, len, &dma_addr, GFP_KERNEL,
-				DMA_ATTR_NO_KERNEL_MAPPING | DMA_ATTR_NO_WARN);
-		if (!bufs[i])
-			break;
-
-		descs[i].addr = cpu_to_le64(dma_addr);
-		descs[i].size = cpu_to_le32(len / dev->ctrl.page_size);
-		i++;
-	}
-
-	if (!size)
-		goto out_free_bufs;
-
-	dev->nr_host_mem_descs = i;
-	dev->host_mem_size = size;
-	dev->host_mem_descs = descs;
-	dev->host_mem_descs_dma = descs_dma;
-	dev->host_mem_desc_bufs = bufs;
-	return 0;
-
-out_free_bufs:
-	while (--i >= 0) {
-		size_t size = le32_to_cpu(descs[i].size) * dev->ctrl.page_size;
-
-		dma_free_coherent(dev->dev, size, bufs[i],
-				le64_to_cpu(descs[i].addr));
-	}
-
-	kfree(bufs);
-out_free_descs:
-	dma_free_coherent(dev->dev, max_entries * sizeof(*descs), descs,
-			descs_dma);
-out:
-	dev->host_mem_descs = NULL;
-	return -ENOMEM;
-}
-
-static int nvme_alloc_host_mem(struct nvme_dev *dev, u64 min, u64 preferred)
-{
-	u32 chunk_size;
-
-	/* start big and work our way down */
-	for (chunk_size = min_t(u64, preferred, PAGE_SIZE * MAX_ORDER_NR_PAGES);
-	     chunk_size >= max_t(u32, dev->ctrl.hmminds * 4096, PAGE_SIZE * 2);
-	     chunk_size /= 2) {
-		if (!__nvme_alloc_host_mem(dev, preferred, chunk_size)) {
-			if (!min || dev->host_mem_size >= min)
-				return 0;
-			nvme_free_host_mem(dev);
-		}
-	}
-
-	return -ENOMEM;
-}
-
-static int nvme_setup_host_mem(struct nvme_dev *dev)
-{
-	u64 max = (u64)max_host_mem_size_mb * SZ_1M;
-	u64 preferred = (u64)dev->ctrl.hmpre * 4096;
-	u64 min = (u64)dev->ctrl.hmmin * 4096;
-	u32 enable_bits = NVME_HOST_MEM_ENABLE;
-	int ret = 0;
-
-	preferred = min(preferred, max);
-	if (min > max) {
-		dev_warn(dev->ctrl.device,
-			"min host memory (%lld MiB) above limit (%d MiB).\n",
-			min >> ilog2(SZ_1M), max_host_mem_size_mb);
-		nvme_free_host_mem(dev);
-		return 0;
-	}
-
-	/*
-	 * If we already have a buffer allocated check if we can reuse it.
-	 */
-	if (dev->host_mem_descs) {
-		if (dev->host_mem_size >= min)
-			enable_bits |= NVME_HOST_MEM_RETURN;
-		else
-			nvme_free_host_mem(dev);
-	}
-
-	if (!dev->host_mem_descs) {
-		if (nvme_alloc_host_mem(dev, min, preferred)) {
-			dev_warn(dev->ctrl.device,
-				"failed to allocate host memory buffer.\n");
-			return 0; /* controller must work without HMB */
-		}
-
-		dev_info(dev->ctrl.device,
-			"allocated %lld MiB host memory buffer.\n",
-			dev->host_mem_size >> ilog2(SZ_1M));
-	}
-
-	ret = nvme_set_host_mem(dev, enable_bits);
-	if (ret)
-		nvme_free_host_mem(dev);
-	return ret;
-}
-
-static int nvme_setup_io_queues(struct nvme_dev *dev)
-{
-	struct nvme_queue *adminq = &dev->queues[0];
-	struct pci_dev *pdev = to_pci_dev(dev->dev);
-	int result, nr_io_queues;
-	unsigned long size;
-
-	nr_io_queues = num_possible_cpus();
-	result = nvme_set_queue_count(&dev->ctrl, &nr_io_queues);
-	if (result < 0)
-		return result;
-
-	if (nr_io_queues == 0)
-		return 0;
-
-	if (dev->cmb && NVME_CMB_SQS(dev->cmbsz)) {
-		result = nvme_cmb_qdepth(dev, nr_io_queues,
-				sizeof(struct nvme_command));
-		if (result > 0)
-			dev->q_depth = result;
-		else
-			nvme_release_cmb(dev);
-	}
-
-	do {
-		size = db_bar_size(dev, nr_io_queues);
-		result = nvme_remap_bar(dev, size);
-		if (!result)
-			break;
-		if (!--nr_io_queues)
-			return -ENOMEM;
-	} while (1);
-	adminq->q_db = dev->dbs;
-
-	/* Deregister the admin queue's interrupt */
-	pci_free_irq(pdev, 0, adminq);
-
-	/*
-	 * If we enable msix early due to not intx, disable it again before
-	 * setting up the full range we need.
-	 */
-	pci_free_irq_vectors(pdev);
-	nr_io_queues = pci_alloc_irq_vectors(pdev, 1, nr_io_queues,
-			PCI_IRQ_ALL_TYPES | PCI_IRQ_AFFINITY);
-	if (nr_io_queues <= 0)
-		return -EIO;
-	dev->max_qid = nr_io_queues;
-
-	/*
-	 * Should investigate if there's a performance win from allocating
-	 * more queues than interrupt vectors; it might allow the submission
-	 * path to scale better, even if the receive path is limited by the
-	 * number of interrupts.
-	 */
-
-	result = queue_request_irq(adminq);
-	if (result) {
-		adminq->cq_vector = -1;
-		return result;
-	}
-	return nvme_create_io_queues(dev);
-}
-
-static void nvme_del_queue_end(struct request *req, blk_status_t error)
-{
-	struct nvme_queue *nvmeq = req->end_io_data;
-
-	blk_mq_free_request(req);
-	complete(&nvmeq->dev->ioq_wait);
-}
-
-static void nvme_del_cq_end(struct request *req, blk_status_t error)
-{
-	struct nvme_queue *nvmeq = req->end_io_data;
-
-	if (!error) {
-		unsigned long flags;
-
-		/*
-		 * We might be called with the AQ q_lock held
-		 * and the I/O queue q_lock should always
-		 * nest inside the AQ one.
-		 */
-		spin_lock_irqsave_nested(&nvmeq->q_lock, flags,
-					SINGLE_DEPTH_NESTING);
-		nvme_process_cq(nvmeq);
-		spin_unlock_irqrestore(&nvmeq->q_lock, flags);
-	}
-
-	nvme_del_queue_end(req, error);
-}
-
-static int nvme_delete_queue(struct nvme_queue *nvmeq, u8 opcode)
-{
-	struct request_queue *q = nvmeq->dev->ctrl.admin_q;
-	struct request *req;
-	struct nvme_command cmd;
-
-	memset(&cmd, 0, sizeof(cmd));
-	cmd.delete_queue.opcode = opcode;
-	cmd.delete_queue.qid = cpu_to_le16(nvmeq->qid);
-
-	req = nvme_alloc_request(q, &cmd, BLK_MQ_REQ_NOWAIT, NVME_QID_ANY);
-	if (IS_ERR(req))
-		return PTR_ERR(req);
-
-	req->timeout = ADMIN_TIMEOUT;
-	req->end_io_data = nvmeq;
-
-	blk_execute_rq_nowait(q, NULL, req, false,
-			opcode == nvme_admin_delete_cq ?
-				nvme_del_cq_end : nvme_del_queue_end);
-	return 0;
-}
-
-static void nvme_disable_io_queues(struct nvme_dev *dev, int queues)
-{
-	int pass;
-	unsigned long timeout;
-	u8 opcode = nvme_admin_delete_sq;
-
-	for (pass = 0; pass < 2; pass++) {
-		int sent = 0, i = queues;
-
-		reinit_completion(&dev->ioq_wait);
- retry:
-		timeout = ADMIN_TIMEOUT;
-		for (; i > 0; i--, sent++)
-			if (nvme_delete_queue(&dev->queues[i], opcode))
-				break;
-
-		while (sent--) {
-			timeout = wait_for_completion_io_timeout(&dev->ioq_wait, timeout);
-			if (timeout == 0)
-				return;
-			if (i)
-				goto retry;
-		}
-		opcode = nvme_admin_delete_cq;
-	}
-}
-
-/*
- * Return: error value if an error occurred setting up the queues or calling
- * Identify Device.  0 if these succeeded, even if adding some of the
- * namespaces failed.  At the moment, these failures are silent.  TBD which
- * failures should be reported.
- */
-static int nvme_dev_add(struct nvme_dev *dev)
-{
-	if (!dev->ctrl.tagset) {
-		dev->tagset.ops = &nvme_mq_ops;
-		dev->tagset.nr_hw_queues = dev->online_queues - 1;
-		dev->tagset.timeout = NVME_IO_TIMEOUT;
-		dev->tagset.numa_node = dev_to_node(dev->dev);
-		dev->tagset.queue_depth =
-				min_t(int, dev->q_depth, BLK_MQ_MAX_DEPTH) - 1;
-		dev->tagset.cmd_size = nvme_cmd_size(dev);
-		dev->tagset.flags = BLK_MQ_F_SHOULD_MERGE;
-		dev->tagset.driver_data = dev;
-
-		if (blk_mq_alloc_tag_set(&dev->tagset))
-			return 0;
-		dev->ctrl.tagset = &dev->tagset;
-
-		nvme_dbbuf_set(dev);
-	} else {
-		blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
-
-		/* Free previously allocated queues that are no longer usable */
-		nvme_free_queues(dev, dev->online_queues);
-	}
-
-	return 0;
-}
-
-static int nvme_pci_enable(struct nvme_dev *dev)
-{
-	int result = -ENOMEM;
-	struct pci_dev *pdev = to_pci_dev(dev->dev);
-
-	if (pci_enable_device_mem(pdev))
-		return result;
-
-	pci_set_master(pdev);
-
-	if (dma_set_mask_and_coherent(dev->dev, DMA_BIT_MASK(64)) &&
-	    dma_set_mask_and_coherent(dev->dev, DMA_BIT_MASK(32)))
-		goto disable;
-
-	if (readl(dev->bar + NVME_REG_CSTS) == -1) {
-		result = -ENODEV;
-		goto disable;
-	}
-
-	/*
-	 * Some devices and/or platforms don't advertise or work with INTx
-	 * interrupts. Pre-enable a single MSIX or MSI vec for setup. We'll
-	 * adjust this later.
-	 */
-	result = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_ALL_TYPES);
-	if (result < 0)
-		return result;
-
-	dev->ctrl.cap = lo_hi_readq(dev->bar + NVME_REG_CAP);
-
-	dev->q_depth = min_t(int, NVME_CAP_MQES(dev->ctrl.cap) + 1,
-				io_queue_depth);
-	dev->db_stride = 1 << NVME_CAP_STRIDE(dev->ctrl.cap);
-	dev->dbs = dev->bar + 4096;
-
-	/*
-	 * Temporary fix for the Apple controller found in the MacBook8,1 and
-	 * some MacBook7,1 to avoid controller resets and data loss.
-	 */
-	if (pdev->vendor == PCI_VENDOR_ID_APPLE && pdev->device == 0x2001) {
-		dev->q_depth = 2;
-		dev_warn(dev->ctrl.device, "detected Apple NVMe controller, "
-			"set queue depth=%u to work around controller resets\n",
-			dev->q_depth);
-	} else if (pdev->vendor == PCI_VENDOR_ID_SAMSUNG &&
-		   (pdev->device == 0xa821 || pdev->device == 0xa822) &&
-		   NVME_CAP_MQES(dev->ctrl.cap) == 0) {
-		dev->q_depth = 64;
-		dev_err(dev->ctrl.device, "detected PM1725 NVMe controller, "
-                        "set queue depth=%u\n", dev->q_depth);
-	}
-
-	/*
-	 * CMBs can currently only exist on >=1.2 PCIe devices. We only
-	 * populate sysfs if a CMB is implemented. Since nvme_dev_attrs_group
-	 * has no name we can pass NULL as final argument to
-	 * sysfs_add_file_to_group.
-	 */
-
-	if (readl(dev->bar + NVME_REG_VS) >= NVME_VS(1, 2, 0)) {
-		dev->cmb = nvme_map_cmb(dev);
-		if (dev->cmb) {
-			if (sysfs_add_file_to_group(&dev->ctrl.device->kobj,
-						    &dev_attr_cmb.attr, NULL))
-				dev_warn(dev->ctrl.device,
-					 "failed to add sysfs attribute for CMB\n");
-		}
-	}
-
-	pci_enable_pcie_error_reporting(pdev);
-	pci_save_state(pdev);
-	return 0;
-
- disable:
-	pci_disable_device(pdev);
-	return result;
-}
-
-static void nvme_dev_unmap(struct nvme_dev *dev)
-{
-	if (dev->bar)
-		iounmap(dev->bar);
-	pci_release_mem_regions(to_pci_dev(dev->dev));
-}
-
-static void nvme_pci_disable(struct nvme_dev *dev)
-{
-	struct pci_dev *pdev = to_pci_dev(dev->dev);
-
-	nvme_release_cmb(dev);
-	pci_free_irq_vectors(pdev);
-
-	if (pci_is_enabled(pdev)) {
-		pci_disable_pcie_error_reporting(pdev);
-		pci_disable_device(pdev);
-	}
-}
-
-static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
-{
-	int i, queues;
-	bool dead = true;
-	struct pci_dev *pdev = to_pci_dev(dev->dev);
-
-	mutex_lock(&dev->shutdown_lock);
-	if (pci_is_enabled(pdev)) {
-		u32 csts = readl(dev->bar + NVME_REG_CSTS);
-
-		if (dev->ctrl.state == NVME_CTRL_LIVE ||
-		    dev->ctrl.state == NVME_CTRL_RESETTING)
-			nvme_start_freeze(&dev->ctrl);
-		dead = !!((csts & NVME_CSTS_CFS) || !(csts & NVME_CSTS_RDY) ||
-			pdev->error_state  != pci_channel_io_normal);
-	}
-
-	/*
-	 * Give the controller a chance to complete all entered requests if
-	 * doing a safe shutdown.
-	 */
-	if (!dead) {
-		if (shutdown)
-			nvme_wait_freeze_timeout(&dev->ctrl, NVME_IO_TIMEOUT);
-
-		/*
-		 * If the controller is still alive tell it to stop using the
-		 * host memory buffer.  In theory the shutdown / reset should
-		 * make sure that it doesn't access the host memoery anymore,
-		 * but I'd rather be safe than sorry..
-		 */
-		if (dev->host_mem_descs)
-			nvme_set_host_mem(dev, 0);
-
-	}
-	nvme_stop_queues(&dev->ctrl);
-
-	queues = dev->online_queues - 1;
-	for (i = dev->ctrl.queue_count - 1; i > 0; i--)
-		nvme_suspend_queue(&dev->queues[i]);
-
-	if (dead) {
-		/* A device might become IO incapable very soon during
-		 * probe, before the admin queue is configured. Thus,
-		 * queue_count can be 0 here.
-		 */
-		if (dev->ctrl.queue_count)
-			nvme_suspend_queue(&dev->queues[0]);
-	} else {
-		nvme_disable_io_queues(dev, queues);
-		nvme_disable_admin_queue(dev, shutdown);
-	}
-	nvme_pci_disable(dev);
-
-	blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
-	blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
-
-	/*
-	 * The driver will not be starting up queues again if shutting down so
-	 * must flush all entered requests to their failed completion to avoid
-	 * deadlocking blk-mq hot-cpu notifier.
-	 */
-	if (shutdown)
-		nvme_start_queues(&dev->ctrl);
-	mutex_unlock(&dev->shutdown_lock);
-}
-
-static int nvme_setup_prp_pools(struct nvme_dev *dev)
-{
-	dev->prp_page_pool = dma_pool_create("prp list page", dev->dev,
-						PAGE_SIZE, PAGE_SIZE, 0);
-	if (!dev->prp_page_pool)
-		return -ENOMEM;
-
-	/* Optimisation for I/Os between 4k and 128k */
-	dev->prp_small_pool = dma_pool_create("prp list 256", dev->dev,
-						256, 256, 0);
-	if (!dev->prp_small_pool) {
-		dma_pool_destroy(dev->prp_page_pool);
-		return -ENOMEM;
-	}
-	return 0;
-}
-
-static void nvme_release_prp_pools(struct nvme_dev *dev)
-{
-	dma_pool_destroy(dev->prp_page_pool);
-	dma_pool_destroy(dev->prp_small_pool);
-}
-
-static void nvme_pci_free_ctrl(struct nvme_ctrl *ctrl)
-{
-	struct nvme_dev *dev = to_nvme_dev(ctrl);
-
-	nvme_dbbuf_dma_free(dev);
-	put_device(dev->dev);
-	if (dev->tagset.tags)
-		blk_mq_free_tag_set(&dev->tagset);
-	if (dev->ctrl.admin_q)
-		blk_put_queue(dev->ctrl.admin_q);
-	kfree(dev->queues);
-	free_opal_dev(dev->ctrl.opal_dev);
-	kfree(dev);
-}
-
-static void nvme_remove_dead_ctrl(struct nvme_dev *dev, int status)
-{
-	dev_warn(dev->ctrl.device, "Removing after probe failure status: %d\n", status);
-
-	kref_get(&dev->ctrl.kref);
-	nvme_dev_disable(dev, false);
-	if (!schedule_work(&dev->remove_work))
-		nvme_put_ctrl(&dev->ctrl);
-}
-
-static void nvme_reset_work(struct work_struct *work)
-{
-	struct nvme_dev *dev =
-		container_of(work, struct nvme_dev, ctrl.reset_work);
-	bool was_suspend = !!(dev->ctrl.ctrl_config & NVME_CC_SHN_NORMAL);
-	int result = -ENODEV;
-
-	if (WARN_ON(dev->ctrl.state != NVME_CTRL_RESETTING))
-		goto out;
-
-	/*
-	 * If we're called to reset a live controller first shut it down before
-	 * moving on.
-	 */
-	if (dev->ctrl.ctrl_config & NVME_CC_ENABLE)
-		nvme_dev_disable(dev, false);
-
-	result = nvme_pci_enable(dev);
-	if (result)
-		goto out;
-
-	result = nvme_pci_configure_admin_queue(dev);
-	if (result)
-		goto out;
-
-	result = nvme_alloc_admin_tags(dev);
-	if (result)
-		goto out;
-
-	result = nvme_init_identify(&dev->ctrl);
-	if (result)
-		goto out;
-
-	if (dev->ctrl.oacs & NVME_CTRL_OACS_SEC_SUPP) {
-		if (!dev->ctrl.opal_dev)
-			dev->ctrl.opal_dev =
-				init_opal_dev(&dev->ctrl, &nvme_sec_submit);
-		else if (was_suspend)
-			opal_unlock_from_suspend(dev->ctrl.opal_dev);
-	} else {
-		free_opal_dev(dev->ctrl.opal_dev);
-		dev->ctrl.opal_dev = NULL;
-	}
-
-	if (dev->ctrl.oacs & NVME_CTRL_OACS_DBBUF_SUPP) {
-		result = nvme_dbbuf_dma_alloc(dev);
-		if (result)
-			dev_warn(dev->dev,
-				 "unable to allocate dma for dbbuf\n");
-	}
-
-	if (dev->ctrl.hmpre) {
-		result = nvme_setup_host_mem(dev);
-		if (result < 0)
-			goto out;
-	}
-
-	result = nvme_setup_io_queues(dev);
-	if (result)
-		goto out;
-
-	/*
-	 * Keep the controller around but remove all namespaces if we don't have
-	 * any working I/O queue.
-	 */
-	if (dev->online_queues < 2) {
-		dev_warn(dev->ctrl.device, "IO queues not created\n");
-		nvme_kill_queues(&dev->ctrl);
-		nvme_remove_namespaces(&dev->ctrl);
-	} else {
-		nvme_start_queues(&dev->ctrl);
-		nvme_wait_freeze(&dev->ctrl);
-		nvme_dev_add(dev);
-		nvme_unfreeze(&dev->ctrl);
-	}
-
-	if (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_LIVE)) {
-		dev_warn(dev->ctrl.device, "failed to mark controller live\n");
-		goto out;
-	}
-
-	nvme_start_ctrl(&dev->ctrl);
-	return;
-
- out:
-	nvme_remove_dead_ctrl(dev, result);
-}
-
-static void nvme_remove_dead_ctrl_work(struct work_struct *work)
-{
-	struct nvme_dev *dev = container_of(work, struct nvme_dev, remove_work);
-	struct pci_dev *pdev = to_pci_dev(dev->dev);
-
-	nvme_kill_queues(&dev->ctrl);
-	if (pci_get_drvdata(pdev))
-		device_release_driver(&pdev->dev);
-	nvme_put_ctrl(&dev->ctrl);
-}
-
-static int nvme_pci_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val)
-{
-	*val = readl(to_nvme_dev(ctrl)->bar + off);
-	return 0;
-}
-
-static int nvme_pci_reg_write32(struct nvme_ctrl *ctrl, u32 off, u32 val)
-{
-	writel(val, to_nvme_dev(ctrl)->bar + off);
-	return 0;
-}
-
-static int nvme_pci_reg_read64(struct nvme_ctrl *ctrl, u32 off, u64 *val)
-{
-	*val = readq(to_nvme_dev(ctrl)->bar + off);
-	return 0;
-}
-
-static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
-	.name			= "pcie",
-	.module			= THIS_MODULE,
-	.flags			= NVME_F_METADATA_SUPPORTED,
-	.reg_read32		= nvme_pci_reg_read32,
-	.reg_write32		= nvme_pci_reg_write32,
-	.reg_read64		= nvme_pci_reg_read64,
-	.free_ctrl		= nvme_pci_free_ctrl,
-	.submit_async_event	= nvme_pci_submit_async_event,
-};
-
-static int nvme_dev_map(struct nvme_dev *dev)
-{
-	struct pci_dev *pdev = to_pci_dev(dev->dev);
-
-	if (pci_request_mem_regions(pdev, "nvme"))
-		return -ENODEV;
-
-	if (nvme_remap_bar(dev, NVME_REG_DBS + 4096))
-		goto release;
-
-	return 0;
-  release:
-	pci_release_mem_regions(pdev);
-	return -ENODEV;
-}
-
-static unsigned long check_vendor_combination_bug(struct pci_dev *pdev)
-{
-	if (pdev->vendor == 0x144d && pdev->device == 0xa802) {
-		/*
-		 * Several Samsung devices seem to drop off the PCIe bus
-		 * randomly when APST is on and uses the deepest sleep state.
-		 * This has been observed on a Samsung "SM951 NVMe SAMSUNG
-		 * 256GB", a "PM951 NVMe SAMSUNG 512GB", and a "Samsung SSD
-		 * 950 PRO 256GB", but it seems to be restricted to two Dell
-		 * laptops.
-		 */
-		if (dmi_match(DMI_SYS_VENDOR, "Dell Inc.") &&
-		    (dmi_match(DMI_PRODUCT_NAME, "XPS 15 9550") ||
-		     dmi_match(DMI_PRODUCT_NAME, "Precision 5510")))
-			return NVME_QUIRK_NO_DEEPEST_PS;
-	} else if (pdev->vendor == 0x144d && pdev->device == 0xa804) {
-		/*
-		 * Samsung SSD 960 EVO drops off the PCIe bus after system
-		 * suspend on a Ryzen board, ASUS PRIME B350M-A, as well as
-		 * within few minutes after bootup on a Coffee Lake board -
-		 * ASUS PRIME Z370-A
-		 */
-		if (dmi_match(DMI_BOARD_VENDOR, "ASUSTeK COMPUTER INC.") &&
-		    (dmi_match(DMI_BOARD_NAME, "PRIME B350M-A") ||
-		     dmi_match(DMI_BOARD_NAME, "PRIME Z370-A")))
-			return NVME_QUIRK_NO_APST;
-	}
-
-	return 0;
-}
-
-static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
-{
-	int node, result = -ENOMEM;
-	struct nvme_dev *dev;
-	unsigned long quirks = id->driver_data;
-
-	node = dev_to_node(&pdev->dev);
-	if (node == NUMA_NO_NODE)
-		set_dev_node(&pdev->dev, first_memory_node);
-
-	dev = kzalloc_node(sizeof(*dev), GFP_KERNEL, node);
-	if (!dev)
-		return -ENOMEM;
-
-	dev->queues = kzalloc_node((num_possible_cpus() + 1) * sizeof(struct nvme_queue),
-							GFP_KERNEL, node);
-	if (!dev->queues)
-		goto free;
-
-	dev->dev = get_device(&pdev->dev);
-	pci_set_drvdata(pdev, dev);
-
-	result = nvme_dev_map(dev);
-	if (result)
-		goto put_pci;
-
-	INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
-	INIT_WORK(&dev->remove_work, nvme_remove_dead_ctrl_work);
-	mutex_init(&dev->shutdown_lock);
-	init_completion(&dev->ioq_wait);
-
-	result = nvme_setup_prp_pools(dev);
-	if (result)
-		goto unmap;
-
-	quirks |= check_vendor_combination_bug(pdev);
-
-	result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
-			quirks);
-	if (result)
-		goto release_pools;
-
-	nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_RESETTING);
-	dev_info(dev->ctrl.device, "pci function %s\n", dev_name(&pdev->dev));
-
-	queue_work(nvme_wq, &dev->ctrl.reset_work);
-	return 0;
-
- release_pools:
-	nvme_release_prp_pools(dev);
- unmap:
-	nvme_dev_unmap(dev);
- put_pci:
-	put_device(dev->dev);
- free:
-	kfree(dev->queues);
-	kfree(dev);
-	return result;
-}
-
-static void nvme_reset_prepare(struct pci_dev *pdev)
-{
-	struct nvme_dev *dev = pci_get_drvdata(pdev);
-	nvme_dev_disable(dev, false);
-}
-
-static void nvme_reset_done(struct pci_dev *pdev)
-{
-	struct nvme_dev *dev = pci_get_drvdata(pdev);
-	nvme_reset_ctrl(&dev->ctrl);
-}
-
-static void nvme_shutdown(struct pci_dev *pdev)
-{
-	struct nvme_dev *dev = pci_get_drvdata(pdev);
-	nvme_dev_disable(dev, true);
-}
-
-/*
- * The driver's remove may be called on a device in a partially initialized
- * state. This function must not have any dependencies on the device state in
- * order to proceed.
- */
-static void nvme_remove(struct pci_dev *pdev)
-{
-	struct nvme_dev *dev = pci_get_drvdata(pdev);
-
-	nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DELETING);
-
-	cancel_work_sync(&dev->ctrl.reset_work);
-	pci_set_drvdata(pdev, NULL);
-
-	if (!pci_device_is_present(pdev)) {
-		nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DEAD);
-		nvme_dev_disable(dev, false);
-	}
-
-	flush_work(&dev->ctrl.reset_work);
-	nvme_stop_ctrl(&dev->ctrl);
-	nvme_remove_namespaces(&dev->ctrl);
-	nvme_dev_disable(dev, true);
-	nvme_free_host_mem(dev);
-	nvme_dev_remove_admin(dev);
-	nvme_free_queues(dev, 0);
-	nvme_uninit_ctrl(&dev->ctrl);
-	nvme_release_prp_pools(dev);
-	nvme_dev_unmap(dev);
-	nvme_put_ctrl(&dev->ctrl);
-}
-
-static int nvme_pci_sriov_configure(struct pci_dev *pdev, int numvfs)
-{
-	int ret = 0;
-
-	if (numvfs == 0) {
-		if (pci_vfs_assigned(pdev)) {
-			dev_warn(&pdev->dev,
-				"Cannot disable SR-IOV VFs while assigned\n");
-			return -EPERM;
-		}
-		pci_disable_sriov(pdev);
-		return 0;
-	}
-
-	ret = pci_enable_sriov(pdev, numvfs);
-	return ret ? ret : numvfs;
-}
-
-#ifdef CONFIG_PM_SLEEP
-static int nvme_suspend(struct device *dev)
-{
-	struct pci_dev *pdev = to_pci_dev(dev);
-	struct nvme_dev *ndev = pci_get_drvdata(pdev);
-
-	nvme_dev_disable(ndev, true);
-	return 0;
-}
-
-static int nvme_resume(struct device *dev)
-{
-	struct pci_dev *pdev = to_pci_dev(dev);
-	struct nvme_dev *ndev = pci_get_drvdata(pdev);
-
-	nvme_reset_ctrl(&ndev->ctrl);
-	return 0;
-}
-#endif
-
-static SIMPLE_DEV_PM_OPS(nvme_dev_pm_ops, nvme_suspend, nvme_resume);
-
-static pci_ers_result_t nvme_error_detected(struct pci_dev *pdev,
-						pci_channel_state_t state)
-{
-	struct nvme_dev *dev = pci_get_drvdata(pdev);
-
-	/*
-	 * A frozen channel requires a reset. When detected, this method will
-	 * shutdown the controller to quiesce. The controller will be restarted
-	 * after the slot reset through driver's slot_reset callback.
-	 */
-	switch (state) {
-	case pci_channel_io_normal:
-		return PCI_ERS_RESULT_CAN_RECOVER;
-	case pci_channel_io_frozen:
-		dev_warn(dev->ctrl.device,
-			"frozen state error detected, reset controller\n");
-		nvme_dev_disable(dev, false);
-		return PCI_ERS_RESULT_NEED_RESET;
-	case pci_channel_io_perm_failure:
-		dev_warn(dev->ctrl.device,
-			"failure state error detected, request disconnect\n");
-		return PCI_ERS_RESULT_DISCONNECT;
-	}
-	return PCI_ERS_RESULT_NEED_RESET;
-}
-
-static pci_ers_result_t nvme_slot_reset(struct pci_dev *pdev)
-{
-	struct nvme_dev *dev = pci_get_drvdata(pdev);
-
-	dev_info(dev->ctrl.device, "restart after slot reset\n");
-	pci_restore_state(pdev);
-	nvme_reset_ctrl(&dev->ctrl);
-	return PCI_ERS_RESULT_RECOVERED;
-}
-
-static void nvme_error_resume(struct pci_dev *pdev)
-{
-	struct nvme_dev *dev = pci_get_drvdata(pdev);
-
-	flush_work(&dev->ctrl.reset_work);
-	pci_cleanup_aer_uncorrect_error_status(pdev);
-}
-
-static const struct pci_error_handlers nvme_err_handler = {
-	.error_detected	= nvme_error_detected,
-	.slot_reset	= nvme_slot_reset,
-	.resume		= nvme_error_resume,
-	.reset_prepare	= nvme_reset_prepare,
-	.reset_done	= nvme_reset_done,
-};
-
-static const struct pci_device_id nvme_id_table[] = {
-	{ PCI_VDEVICE(INTEL, 0x0953),
-		.driver_data = NVME_QUIRK_STRIPE_SIZE |
-				NVME_QUIRK_DEALLOCATE_ZEROES, },
-	{ PCI_VDEVICE(INTEL, 0x0a53),
-		.driver_data = NVME_QUIRK_STRIPE_SIZE |
-				NVME_QUIRK_DEALLOCATE_ZEROES, },
-	{ PCI_VDEVICE(INTEL, 0x0a54),
-		.driver_data = NVME_QUIRK_STRIPE_SIZE |
-				NVME_QUIRK_DEALLOCATE_ZEROES, },
-	{ PCI_VDEVICE(INTEL, 0x0a55),
-		.driver_data = NVME_QUIRK_STRIPE_SIZE |
-				NVME_QUIRK_DEALLOCATE_ZEROES, },
-	{ PCI_VDEVICE(INTEL, 0xf1a5),	/* Intel 600P/P3100 */
-		.driver_data = NVME_QUIRK_NO_DEEPEST_PS |
-				NVME_QUIRK_MEDIUM_PRIO_SQ },
-	{ PCI_VDEVICE(INTEL, 0x5845),	/* Qemu emulated controller */
-		.driver_data = NVME_QUIRK_IDENTIFY_CNS, },
-	{ PCI_DEVICE(0x1c58, 0x0003),	/* HGST adapter */
-		.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },
-	{ PCI_DEVICE(0x1c58, 0x0023),	/* WDC SN200 adapter */
-		.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },
-	{ PCI_DEVICE(0x1c5f, 0x0540),	/* Memblaze Pblaze4 adapter */
-		.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },
-	{ PCI_DEVICE(0x144d, 0xa821),   /* Samsung PM1725 */
-		.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },
-	{ PCI_DEVICE(0x144d, 0xa822),   /* Samsung PM1725a */
-		.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },
-	{ PCI_DEVICE(0x1d1d, 0x1f1f),	/* LighNVM qemu device */
-		.driver_data = NVME_QUIRK_LIGHTNVM, },
-	{ PCI_DEVICE(0x1d1d, 0x2807),	/* CNEX WL */
-		.driver_data = NVME_QUIRK_LIGHTNVM, },
-	{ PCI_DEVICE(0x1d1d, 0x2601),	/* CNEX Granby */
-		.driver_data = NVME_QUIRK_LIGHTNVM, },
-	{ PCI_DEVICE_CLASS(PCI_CLASS_STORAGE_EXPRESS, 0xffffff) },
-	{ PCI_DEVICE(PCI_VENDOR_ID_APPLE, 0x2001) },
-	{ PCI_DEVICE(PCI_VENDOR_ID_APPLE, 0x2003) },
-	{ 0, }
-};
-MODULE_DEVICE_TABLE(pci, nvme_id_table);
-
-static struct pci_driver nvme_driver = {
-	.name		= "nvme",
-	.id_table	= nvme_id_table,
-	.probe		= nvme_probe,
-	.remove		= nvme_remove,
-	.shutdown	= nvme_shutdown,
-	.driver		= {
-		.pm	= &nvme_dev_pm_ops,
-	},
-	.sriov_configure = nvme_pci_sriov_configure,
-	.err_handler	= &nvme_err_handler,
-};
-
-static int __init nvme_init(void)
-{
-	return pci_register_driver(&nvme_driver);
-}
-
-static void __exit nvme_exit(void)
-{
-	pci_unregister_driver(&nvme_driver);
-	_nvme_check_size();
-}
-
-MODULE_AUTHOR("Matthew Wilcox <willy@linux.intel.com>");
-MODULE_LICENSE("GPL");
-MODULE_VERSION("1.0");
-module_init(nvme_init);
-module_exit(nvme_exit);
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/rdma.c b/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/rdma.c
deleted file mode 100644
index 9fffe41..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/rdma.c
+++ /dev/null
@@ -1,2027 +0,0 @@
-/*
- * NVMe over Fabrics RDMA host code.
- * Copyright (c) 2015-2016 HGST, a Western Digital Company.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- */
-#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
-#include <linux/module.h>
-#include <linux/init.h>
-#include <linux/slab.h>
-#include <linux/err.h>
-#include <linux/string.h>
-#include <linux/atomic.h>
-#include <linux/blk-mq.h>
-#include <linux/blk-mq-rdma.h>
-#include <linux/types.h>
-#include <linux/list.h>
-#include <linux/mutex.h>
-#include <linux/scatterlist.h>
-#include <linux/nvme.h>
-#include <asm/unaligned.h>
-
-#include <rdma/ib_verbs.h>
-#include <rdma/rdma_cm.h>
-#include <linux/nvme-rdma.h>
-
-#include "nvme.h"
-#include "fabrics.h"
-
-
-#define NVME_RDMA_CONNECT_TIMEOUT_MS	3000		/* 3 second */
-
-#define NVME_RDMA_MAX_SEGMENTS		256
-
-#define NVME_RDMA_MAX_INLINE_SEGMENTS	1
-
-/*
- * We handle AEN commands ourselves and don't even let the
- * block layer know about them.
- */
-#define NVME_RDMA_NR_AEN_COMMANDS      1
-#define NVME_RDMA_AQ_BLKMQ_DEPTH       \
-	(NVME_AQ_DEPTH - NVME_RDMA_NR_AEN_COMMANDS)
-
-struct nvme_rdma_device {
-	struct ib_device       *dev;
-	struct ib_pd	       *pd;
-	struct kref		ref;
-	struct list_head	entry;
-};
-
-struct nvme_rdma_qe {
-	struct ib_cqe		cqe;
-	void			*data;
-	u64			dma;
-};
-
-struct nvme_rdma_queue;
-struct nvme_rdma_request {
-	struct nvme_request	req;
-	struct ib_mr		*mr;
-	struct nvme_rdma_qe	sqe;
-	union nvme_result	result;
-	__le16			status;
-	refcount_t		ref;
-	struct ib_sge		sge[1 + NVME_RDMA_MAX_INLINE_SEGMENTS];
-	u32			num_sge;
-	int			nents;
-	bool			inline_data;
-	struct ib_reg_wr	reg_wr;
-	struct ib_cqe		reg_cqe;
-	struct nvme_rdma_queue  *queue;
-	struct sg_table		sg_table;
-	struct scatterlist	first_sgl[];
-};
-
-enum nvme_rdma_queue_flags {
-	NVME_RDMA_Q_LIVE		= 0,
-	NVME_RDMA_Q_DELETING		= 1,
-};
-
-struct nvme_rdma_queue {
-	struct nvme_rdma_qe	*rsp_ring;
-	int			queue_size;
-	size_t			cmnd_capsule_len;
-	struct nvme_rdma_ctrl	*ctrl;
-	struct nvme_rdma_device	*device;
-	struct ib_cq		*ib_cq;
-	struct ib_qp		*qp;
-
-	unsigned long		flags;
-	struct rdma_cm_id	*cm_id;
-	int			cm_error;
-	struct completion	cm_done;
-};
-
-struct nvme_rdma_ctrl {
-	/* read only in the hot path */
-	struct nvme_rdma_queue	*queues;
-
-	/* other member variables */
-	struct blk_mq_tag_set	tag_set;
-	struct work_struct	delete_work;
-	struct work_struct	err_work;
-
-	struct nvme_rdma_qe	async_event_sqe;
-
-	struct delayed_work	reconnect_work;
-
-	struct list_head	list;
-
-	struct blk_mq_tag_set	admin_tag_set;
-	struct nvme_rdma_device	*device;
-
-	u32			max_fr_pages;
-
-	struct sockaddr_storage addr;
-	struct sockaddr_storage src_addr;
-
-	struct nvme_ctrl	ctrl;
-};
-
-static inline struct nvme_rdma_ctrl *to_rdma_ctrl(struct nvme_ctrl *ctrl)
-{
-	return container_of(ctrl, struct nvme_rdma_ctrl, ctrl);
-}
-
-static LIST_HEAD(device_list);
-static DEFINE_MUTEX(device_list_mutex);
-
-static LIST_HEAD(nvme_rdma_ctrl_list);
-static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
-
-/*
- * Disabling this option makes small I/O goes faster, but is fundamentally
- * unsafe.  With it turned off we will have to register a global rkey that
- * allows read and write access to all physical memory.
- */
-static bool register_always = true;
-module_param(register_always, bool, 0444);
-MODULE_PARM_DESC(register_always,
-	 "Use memory registration even for contiguous memory regions");
-
-static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
-		struct rdma_cm_event *event);
-static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc);
-
-static const struct blk_mq_ops nvme_rdma_mq_ops;
-static const struct blk_mq_ops nvme_rdma_admin_mq_ops;
-
-/* XXX: really should move to a generic header sooner or later.. */
-static inline void put_unaligned_le24(u32 val, u8 *p)
-{
-	*p++ = val;
-	*p++ = val >> 8;
-	*p++ = val >> 16;
-}
-
-static inline int nvme_rdma_queue_idx(struct nvme_rdma_queue *queue)
-{
-	return queue - queue->ctrl->queues;
-}
-
-static inline size_t nvme_rdma_inline_data_size(struct nvme_rdma_queue *queue)
-{
-	return queue->cmnd_capsule_len - sizeof(struct nvme_command);
-}
-
-static void nvme_rdma_free_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
-		size_t capsule_size, enum dma_data_direction dir)
-{
-	ib_dma_unmap_single(ibdev, qe->dma, capsule_size, dir);
-	kfree(qe->data);
-}
-
-static int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
-		size_t capsule_size, enum dma_data_direction dir)
-{
-	qe->data = kzalloc(capsule_size, GFP_KERNEL);
-	if (!qe->data)
-		return -ENOMEM;
-
-	qe->dma = ib_dma_map_single(ibdev, qe->data, capsule_size, dir);
-	if (ib_dma_mapping_error(ibdev, qe->dma)) {
-		kfree(qe->data);
-		return -ENOMEM;
-	}
-
-	return 0;
-}
-
-static void nvme_rdma_free_ring(struct ib_device *ibdev,
-		struct nvme_rdma_qe *ring, size_t ib_queue_size,
-		size_t capsule_size, enum dma_data_direction dir)
-{
-	int i;
-
-	for (i = 0; i < ib_queue_size; i++)
-		nvme_rdma_free_qe(ibdev, &ring[i], capsule_size, dir);
-	kfree(ring);
-}
-
-static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
-		size_t ib_queue_size, size_t capsule_size,
-		enum dma_data_direction dir)
-{
-	struct nvme_rdma_qe *ring;
-	int i;
-
-	ring = kcalloc(ib_queue_size, sizeof(struct nvme_rdma_qe), GFP_KERNEL);
-	if (!ring)
-		return NULL;
-
-	for (i = 0; i < ib_queue_size; i++) {
-		if (nvme_rdma_alloc_qe(ibdev, &ring[i], capsule_size, dir))
-			goto out_free_ring;
-	}
-
-	return ring;
-
-out_free_ring:
-	nvme_rdma_free_ring(ibdev, ring, i, capsule_size, dir);
-	return NULL;
-}
-
-static void nvme_rdma_qp_event(struct ib_event *event, void *context)
-{
-	pr_debug("QP event %s (%d)\n",
-		 ib_event_msg(event->event), event->event);
-
-}
-
-static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
-{
-	wait_for_completion_interruptible_timeout(&queue->cm_done,
-			msecs_to_jiffies(NVME_RDMA_CONNECT_TIMEOUT_MS) + 1);
-	return queue->cm_error;
-}
-
-static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
-{
-	struct nvme_rdma_device *dev = queue->device;
-	struct ib_qp_init_attr init_attr;
-	int ret;
-
-	memset(&init_attr, 0, sizeof(init_attr));
-	init_attr.event_handler = nvme_rdma_qp_event;
-	/* +1 for drain */
-	init_attr.cap.max_send_wr = factor * queue->queue_size + 1;
-	/* +1 for drain */
-	init_attr.cap.max_recv_wr = queue->queue_size + 1;
-	init_attr.cap.max_recv_sge = 1;
-	init_attr.cap.max_send_sge = 1 + NVME_RDMA_MAX_INLINE_SEGMENTS;
-	init_attr.sq_sig_type = IB_SIGNAL_REQ_WR;
-	init_attr.qp_type = IB_QPT_RC;
-	init_attr.send_cq = queue->ib_cq;
-	init_attr.recv_cq = queue->ib_cq;
-
-	ret = rdma_create_qp(queue->cm_id, dev->pd, &init_attr);
-
-	queue->qp = queue->cm_id->qp;
-	return ret;
-}
-
-static int nvme_rdma_reinit_request(void *data, struct request *rq)
-{
-	struct nvme_rdma_ctrl *ctrl = data;
-	struct nvme_rdma_device *dev = ctrl->device;
-	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
-	int ret = 0;
-
-	ib_dereg_mr(req->mr);
-
-	req->mr = ib_alloc_mr(dev->pd, IB_MR_TYPE_MEM_REG,
-			ctrl->max_fr_pages);
-	if (IS_ERR(req->mr)) {
-		ret = PTR_ERR(req->mr);
-		req->mr = NULL;
-		goto out;
-	}
-
-	req->mr->need_inval = false;
-
-out:
-	return ret;
-}
-
-static void nvme_rdma_exit_request(struct blk_mq_tag_set *set,
-		struct request *rq, unsigned int hctx_idx)
-{
-	struct nvme_rdma_ctrl *ctrl = set->driver_data;
-	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
-	int queue_idx = (set == &ctrl->tag_set) ? hctx_idx + 1 : 0;
-	struct nvme_rdma_queue *queue = &ctrl->queues[queue_idx];
-	struct nvme_rdma_device *dev = queue->device;
-
-	if (req->mr)
-		ib_dereg_mr(req->mr);
-
-	nvme_rdma_free_qe(dev->dev, &req->sqe, sizeof(struct nvme_command),
-			DMA_TO_DEVICE);
-}
-
-static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
-		struct request *rq, unsigned int hctx_idx,
-		unsigned int numa_node)
-{
-	struct nvme_rdma_ctrl *ctrl = set->driver_data;
-	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
-	int queue_idx = (set == &ctrl->tag_set) ? hctx_idx + 1 : 0;
-	struct nvme_rdma_queue *queue = &ctrl->queues[queue_idx];
-	struct nvme_rdma_device *dev = queue->device;
-	struct ib_device *ibdev = dev->dev;
-	int ret;
-
-	ret = nvme_rdma_alloc_qe(ibdev, &req->sqe, sizeof(struct nvme_command),
-			DMA_TO_DEVICE);
-	if (ret)
-		return ret;
-
-	req->mr = ib_alloc_mr(dev->pd, IB_MR_TYPE_MEM_REG,
-			ctrl->max_fr_pages);
-	if (IS_ERR(req->mr)) {
-		ret = PTR_ERR(req->mr);
-		goto out_free_qe;
-	}
-
-	req->queue = queue;
-
-	return 0;
-
-out_free_qe:
-	nvme_rdma_free_qe(dev->dev, &req->sqe, sizeof(struct nvme_command),
-			DMA_TO_DEVICE);
-	return -ENOMEM;
-}
-
-static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
-		unsigned int hctx_idx)
-{
-	struct nvme_rdma_ctrl *ctrl = data;
-	struct nvme_rdma_queue *queue = &ctrl->queues[hctx_idx + 1];
-
-	BUG_ON(hctx_idx >= ctrl->ctrl.queue_count);
-
-	hctx->driver_data = queue;
-	return 0;
-}
-
-static int nvme_rdma_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
-		unsigned int hctx_idx)
-{
-	struct nvme_rdma_ctrl *ctrl = data;
-	struct nvme_rdma_queue *queue = &ctrl->queues[0];
-
-	BUG_ON(hctx_idx != 0);
-
-	hctx->driver_data = queue;
-	return 0;
-}
-
-static void nvme_rdma_free_dev(struct kref *ref)
-{
-	struct nvme_rdma_device *ndev =
-		container_of(ref, struct nvme_rdma_device, ref);
-
-	mutex_lock(&device_list_mutex);
-	list_del(&ndev->entry);
-	mutex_unlock(&device_list_mutex);
-
-	ib_dealloc_pd(ndev->pd);
-	kfree(ndev);
-}
-
-static void nvme_rdma_dev_put(struct nvme_rdma_device *dev)
-{
-	kref_put(&dev->ref, nvme_rdma_free_dev);
-}
-
-static int nvme_rdma_dev_get(struct nvme_rdma_device *dev)
-{
-	return kref_get_unless_zero(&dev->ref);
-}
-
-static struct nvme_rdma_device *
-nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
-{
-	struct nvme_rdma_device *ndev;
-
-	mutex_lock(&device_list_mutex);
-	list_for_each_entry(ndev, &device_list, entry) {
-		if (ndev->dev->node_guid == cm_id->device->node_guid &&
-		    nvme_rdma_dev_get(ndev))
-			goto out_unlock;
-	}
-
-	ndev = kzalloc(sizeof(*ndev), GFP_KERNEL);
-	if (!ndev)
-		goto out_err;
-
-	ndev->dev = cm_id->device;
-	kref_init(&ndev->ref);
-
-	ndev->pd = ib_alloc_pd(ndev->dev,
-		register_always ? 0 : IB_PD_UNSAFE_GLOBAL_RKEY);
-	if (IS_ERR(ndev->pd))
-		goto out_free_dev;
-
-	if (!(ndev->dev->attrs.device_cap_flags &
-	      IB_DEVICE_MEM_MGT_EXTENSIONS)) {
-		dev_err(&ndev->dev->dev,
-			"Memory registrations not supported.\n");
-		goto out_free_pd;
-	}
-
-	list_add(&ndev->entry, &device_list);
-out_unlock:
-	mutex_unlock(&device_list_mutex);
-	return ndev;
-
-out_free_pd:
-	ib_dealloc_pd(ndev->pd);
-out_free_dev:
-	kfree(ndev);
-out_err:
-	mutex_unlock(&device_list_mutex);
-	return NULL;
-}
-
-static void nvme_rdma_destroy_queue_ib(struct nvme_rdma_queue *queue)
-{
-	struct nvme_rdma_device *dev;
-	struct ib_device *ibdev;
-
-	dev = queue->device;
-	ibdev = dev->dev;
-	rdma_destroy_qp(queue->cm_id);
-	ib_free_cq(queue->ib_cq);
-
-	nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
-			sizeof(struct nvme_completion), DMA_FROM_DEVICE);
-
-	nvme_rdma_dev_put(dev);
-}
-
-static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
-{
-	struct ib_device *ibdev;
-	const int send_wr_factor = 3;			/* MR, SEND, INV */
-	const int cq_factor = send_wr_factor + 1;	/* + RECV */
-	int comp_vector, idx = nvme_rdma_queue_idx(queue);
-	int ret;
-
-	queue->device = nvme_rdma_find_get_device(queue->cm_id);
-	if (!queue->device) {
-		dev_err(queue->cm_id->device->dev.parent,
-			"no client data found!\n");
-		return -ECONNREFUSED;
-	}
-	ibdev = queue->device->dev;
-
-	/*
-	 * Spread I/O queues completion vectors according their queue index.
-	 * Admin queues can always go on completion vector 0.
-	 */
-	comp_vector = idx == 0 ? idx : idx - 1;
-
-	/* +1 for ib_stop_cq */
-	queue->ib_cq = ib_alloc_cq(ibdev, queue,
-				cq_factor * queue->queue_size + 1,
-				comp_vector, IB_POLL_SOFTIRQ);
-	if (IS_ERR(queue->ib_cq)) {
-		ret = PTR_ERR(queue->ib_cq);
-		goto out_put_dev;
-	}
-
-	ret = nvme_rdma_create_qp(queue, send_wr_factor);
-	if (ret)
-		goto out_destroy_ib_cq;
-
-	queue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,
-			sizeof(struct nvme_completion), DMA_FROM_DEVICE);
-	if (!queue->rsp_ring) {
-		ret = -ENOMEM;
-		goto out_destroy_qp;
-	}
-
-	return 0;
-
-out_destroy_qp:
-	ib_destroy_qp(queue->qp);
-out_destroy_ib_cq:
-	ib_free_cq(queue->ib_cq);
-out_put_dev:
-	nvme_rdma_dev_put(queue->device);
-	return ret;
-}
-
-static int nvme_rdma_alloc_queue(struct nvme_rdma_ctrl *ctrl,
-		int idx, size_t queue_size)
-{
-	struct nvme_rdma_queue *queue;
-	struct sockaddr *src_addr = NULL;
-	int ret;
-
-	queue = &ctrl->queues[idx];
-	queue->ctrl = ctrl;
-	init_completion(&queue->cm_done);
-
-	if (idx > 0)
-		queue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;
-	else
-		queue->cmnd_capsule_len = sizeof(struct nvme_command);
-
-	queue->queue_size = queue_size;
-
-	queue->cm_id = rdma_create_id(&init_net, nvme_rdma_cm_handler, queue,
-			RDMA_PS_TCP, IB_QPT_RC);
-	if (IS_ERR(queue->cm_id)) {
-		dev_info(ctrl->ctrl.device,
-			"failed to create CM ID: %ld\n", PTR_ERR(queue->cm_id));
-		return PTR_ERR(queue->cm_id);
-	}
-
-	if (ctrl->ctrl.opts->mask & NVMF_OPT_HOST_TRADDR)
-		src_addr = (struct sockaddr *)&ctrl->src_addr;
-
-	queue->cm_error = -ETIMEDOUT;
-	ret = rdma_resolve_addr(queue->cm_id, src_addr,
-			(struct sockaddr *)&ctrl->addr,
-			NVME_RDMA_CONNECT_TIMEOUT_MS);
-	if (ret) {
-		dev_info(ctrl->ctrl.device,
-			"rdma_resolve_addr failed (%d).\n", ret);
-		goto out_destroy_cm_id;
-	}
-
-	ret = nvme_rdma_wait_for_cm(queue);
-	if (ret) {
-		dev_info(ctrl->ctrl.device,
-			"rdma_resolve_addr wait failed (%d).\n", ret);
-		goto out_destroy_cm_id;
-	}
-
-	clear_bit(NVME_RDMA_Q_DELETING, &queue->flags);
-
-	return 0;
-
-out_destroy_cm_id:
-	rdma_destroy_id(queue->cm_id);
-	return ret;
-}
-
-static void nvme_rdma_stop_queue(struct nvme_rdma_queue *queue)
-{
-	if (!test_and_clear_bit(NVME_RDMA_Q_LIVE, &queue->flags))
-		return;
-
-	rdma_disconnect(queue->cm_id);
-	ib_drain_qp(queue->qp);
-}
-
-static void nvme_rdma_free_queue(struct nvme_rdma_queue *queue)
-{
-	if (test_and_set_bit(NVME_RDMA_Q_DELETING, &queue->flags))
-		return;
-
-	if (nvme_rdma_queue_idx(queue) == 0) {
-		nvme_rdma_free_qe(queue->device->dev,
-			&queue->ctrl->async_event_sqe,
-			sizeof(struct nvme_command), DMA_TO_DEVICE);
-	}
-
-	nvme_rdma_destroy_queue_ib(queue);
-	rdma_destroy_id(queue->cm_id);
-}
-
-static void nvme_rdma_free_io_queues(struct nvme_rdma_ctrl *ctrl)
-{
-	int i;
-
-	for (i = 1; i < ctrl->ctrl.queue_count; i++)
-		nvme_rdma_free_queue(&ctrl->queues[i]);
-}
-
-static void nvme_rdma_stop_io_queues(struct nvme_rdma_ctrl *ctrl)
-{
-	int i;
-
-	for (i = 1; i < ctrl->ctrl.queue_count; i++)
-		nvme_rdma_stop_queue(&ctrl->queues[i]);
-}
-
-static int nvme_rdma_start_queue(struct nvme_rdma_ctrl *ctrl, int idx)
-{
-	int ret;
-
-	if (idx)
-		ret = nvmf_connect_io_queue(&ctrl->ctrl, idx);
-	else
-		ret = nvmf_connect_admin_queue(&ctrl->ctrl);
-
-	if (!ret)
-		set_bit(NVME_RDMA_Q_LIVE, &ctrl->queues[idx].flags);
-	else
-		dev_info(ctrl->ctrl.device,
-			"failed to connect queue: %d ret=%d\n", idx, ret);
-	return ret;
-}
-
-static int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl)
-{
-	int i, ret = 0;
-
-	for (i = 1; i < ctrl->ctrl.queue_count; i++) {
-		ret = nvme_rdma_start_queue(ctrl, i);
-		if (ret)
-			goto out_stop_queues;
-	}
-
-	return 0;
-
-out_stop_queues:
-	for (i--; i >= 1; i--)
-		nvme_rdma_stop_queue(&ctrl->queues[i]);
-	return ret;
-}
-
-static int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)
-{
-	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
-	struct ib_device *ibdev = ctrl->device->dev;
-	unsigned int nr_io_queues;
-	int i, ret;
-
-	nr_io_queues = min(opts->nr_io_queues, num_online_cpus());
-
-	/*
-	 * we map queues according to the device irq vectors for
-	 * optimal locality so we don't need more queues than
-	 * completion vectors.
-	 */
-	nr_io_queues = min_t(unsigned int, nr_io_queues,
-				ibdev->num_comp_vectors);
-
-	ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
-	if (ret)
-		return ret;
-
-	ctrl->ctrl.queue_count = nr_io_queues + 1;
-	if (ctrl->ctrl.queue_count < 2)
-		return 0;
-
-	dev_info(ctrl->ctrl.device,
-		"creating %d I/O queues.\n", nr_io_queues);
-
-	for (i = 1; i < ctrl->ctrl.queue_count; i++) {
-		ret = nvme_rdma_alloc_queue(ctrl, i,
-				ctrl->ctrl.sqsize + 1);
-		if (ret)
-			goto out_free_queues;
-	}
-
-	return 0;
-
-out_free_queues:
-	for (i--; i >= 1; i--)
-		nvme_rdma_free_queue(&ctrl->queues[i]);
-
-	return ret;
-}
-
-static void nvme_rdma_free_tagset(struct nvme_ctrl *nctrl, bool admin)
-{
-	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
-	struct blk_mq_tag_set *set = admin ?
-			&ctrl->admin_tag_set : &ctrl->tag_set;
-
-	blk_mq_free_tag_set(set);
-	nvme_rdma_dev_put(ctrl->device);
-}
-
-static struct blk_mq_tag_set *nvme_rdma_alloc_tagset(struct nvme_ctrl *nctrl,
-		bool admin)
-{
-	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
-	struct blk_mq_tag_set *set;
-	int ret;
-
-	if (admin) {
-		set = &ctrl->admin_tag_set;
-		memset(set, 0, sizeof(*set));
-		set->ops = &nvme_rdma_admin_mq_ops;
-		set->queue_depth = NVME_RDMA_AQ_BLKMQ_DEPTH;
-		set->reserved_tags = 2; /* connect + keep-alive */
-		set->numa_node = NUMA_NO_NODE;
-		set->cmd_size = sizeof(struct nvme_rdma_request) +
-			SG_CHUNK_SIZE * sizeof(struct scatterlist);
-		set->driver_data = ctrl;
-		set->nr_hw_queues = 1;
-		set->timeout = ADMIN_TIMEOUT;
-	} else {
-		set = &ctrl->tag_set;
-		memset(set, 0, sizeof(*set));
-		set->ops = &nvme_rdma_mq_ops;
-		set->queue_depth = nctrl->opts->queue_size;
-		set->reserved_tags = 1; /* fabric connect */
-		set->numa_node = NUMA_NO_NODE;
-		set->flags = BLK_MQ_F_SHOULD_MERGE;
-		set->cmd_size = sizeof(struct nvme_rdma_request) +
-			SG_CHUNK_SIZE * sizeof(struct scatterlist);
-		set->driver_data = ctrl;
-		set->nr_hw_queues = nctrl->queue_count - 1;
-		set->timeout = NVME_IO_TIMEOUT;
-	}
-
-	ret = blk_mq_alloc_tag_set(set);
-	if (ret)
-		goto out;
-
-	/*
-	 * We need a reference on the device as long as the tag_set is alive,
-	 * as the MRs in the request structures need a valid ib_device.
-	 */
-	ret = nvme_rdma_dev_get(ctrl->device);
-	if (!ret) {
-		ret = -EINVAL;
-		goto out_free_tagset;
-	}
-
-	return set;
-
-out_free_tagset:
-	blk_mq_free_tag_set(set);
-out:
-	return ERR_PTR(ret);
-}
-
-static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl,
-		bool remove)
-{
-	nvme_rdma_stop_queue(&ctrl->queues[0]);
-	if (remove) {
-		blk_cleanup_queue(ctrl->ctrl.admin_q);
-		nvme_rdma_free_tagset(&ctrl->ctrl, true);
-	}
-	nvme_rdma_free_queue(&ctrl->queues[0]);
-}
-
-static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
-		bool new)
-{
-	int error;
-
-	error = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
-	if (error)
-		return error;
-
-	ctrl->device = ctrl->queues[0].device;
-
-	ctrl->max_fr_pages = min_t(u32, NVME_RDMA_MAX_SEGMENTS,
-		ctrl->device->dev->attrs.max_fast_reg_page_list_len);
-
-	if (new) {
-		ctrl->ctrl.admin_tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, true);
-		if (IS_ERR(ctrl->ctrl.admin_tagset)) {
-			error = PTR_ERR(ctrl->ctrl.admin_tagset);
-			goto out_free_queue;
-		}
-
-		ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
-		if (IS_ERR(ctrl->ctrl.admin_q)) {
-			error = PTR_ERR(ctrl->ctrl.admin_q);
-			goto out_free_tagset;
-		}
-	} else {
-		error = blk_mq_reinit_tagset(&ctrl->admin_tag_set,
-					     nvme_rdma_reinit_request);
-		if (error)
-			goto out_free_queue;
-	}
-
-	error = nvme_rdma_start_queue(ctrl, 0);
-	if (error)
-		goto out_cleanup_queue;
-
-	error = ctrl->ctrl.ops->reg_read64(&ctrl->ctrl, NVME_REG_CAP,
-			&ctrl->ctrl.cap);
-	if (error) {
-		dev_err(ctrl->ctrl.device,
-			"prop_get NVME_REG_CAP failed\n");
-		goto out_stop_queue;
-	}
-
-	ctrl->ctrl.sqsize =
-		min_t(int, NVME_CAP_MQES(ctrl->ctrl.cap), ctrl->ctrl.sqsize);
-
-	error = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
-	if (error)
-		goto out_stop_queue;
-
-	ctrl->ctrl.max_hw_sectors =
-		(ctrl->max_fr_pages - 1) << (ilog2(SZ_4K) - 9);
-
-	error = nvme_init_identify(&ctrl->ctrl);
-	if (error)
-		goto out_stop_queue;
-
-	error = nvme_rdma_alloc_qe(ctrl->queues[0].device->dev,
-			&ctrl->async_event_sqe, sizeof(struct nvme_command),
-			DMA_TO_DEVICE);
-	if (error)
-		goto out_stop_queue;
-
-	return 0;
-
-out_stop_queue:
-	nvme_rdma_stop_queue(&ctrl->queues[0]);
-out_cleanup_queue:
-	if (new)
-		blk_cleanup_queue(ctrl->ctrl.admin_q);
-out_free_tagset:
-	if (new)
-		nvme_rdma_free_tagset(&ctrl->ctrl, true);
-out_free_queue:
-	nvme_rdma_free_queue(&ctrl->queues[0]);
-	return error;
-}
-
-static void nvme_rdma_destroy_io_queues(struct nvme_rdma_ctrl *ctrl,
-		bool remove)
-{
-	nvme_rdma_stop_io_queues(ctrl);
-	if (remove) {
-		blk_cleanup_queue(ctrl->ctrl.connect_q);
-		nvme_rdma_free_tagset(&ctrl->ctrl, false);
-	}
-	nvme_rdma_free_io_queues(ctrl);
-}
-
-static int nvme_rdma_configure_io_queues(struct nvme_rdma_ctrl *ctrl, bool new)
-{
-	int ret;
-
-	ret = nvme_rdma_alloc_io_queues(ctrl);
-	if (ret)
-		return ret;
-
-	if (new) {
-		ctrl->ctrl.tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, false);
-		if (IS_ERR(ctrl->ctrl.tagset)) {
-			ret = PTR_ERR(ctrl->ctrl.tagset);
-			goto out_free_io_queues;
-		}
-
-		ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
-		if (IS_ERR(ctrl->ctrl.connect_q)) {
-			ret = PTR_ERR(ctrl->ctrl.connect_q);
-			goto out_free_tag_set;
-		}
-	} else {
-		ret = blk_mq_reinit_tagset(&ctrl->tag_set,
-					   nvme_rdma_reinit_request);
-		if (ret)
-			goto out_free_io_queues;
-
-		blk_mq_update_nr_hw_queues(&ctrl->tag_set,
-			ctrl->ctrl.queue_count - 1);
-	}
-
-	ret = nvme_rdma_start_io_queues(ctrl);
-	if (ret)
-		goto out_cleanup_connect_q;
-
-	return 0;
-
-out_cleanup_connect_q:
-	if (new)
-		blk_cleanup_queue(ctrl->ctrl.connect_q);
-out_free_tag_set:
-	if (new)
-		nvme_rdma_free_tagset(&ctrl->ctrl, false);
-out_free_io_queues:
-	nvme_rdma_free_io_queues(ctrl);
-	return ret;
-}
-
-static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
-{
-	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
-
-	if (list_empty(&ctrl->list))
-		goto free_ctrl;
-
-	mutex_lock(&nvme_rdma_ctrl_mutex);
-	list_del(&ctrl->list);
-	mutex_unlock(&nvme_rdma_ctrl_mutex);
-
-	kfree(ctrl->queues);
-	nvmf_free_options(nctrl->opts);
-free_ctrl:
-	kfree(ctrl);
-}
-
-static void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)
-{
-	/* If we are resetting/deleting then do nothing */
-	if (ctrl->ctrl.state != NVME_CTRL_RECONNECTING) {
-		WARN_ON_ONCE(ctrl->ctrl.state == NVME_CTRL_NEW ||
-			ctrl->ctrl.state == NVME_CTRL_LIVE);
-		return;
-	}
-
-	if (nvmf_should_reconnect(&ctrl->ctrl)) {
-		dev_info(ctrl->ctrl.device, "Reconnecting in %d seconds...\n",
-			ctrl->ctrl.opts->reconnect_delay);
-		queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
-				ctrl->ctrl.opts->reconnect_delay * HZ);
-	} else {
-		dev_info(ctrl->ctrl.device, "Removing controller...\n");
-		queue_work(nvme_wq, &ctrl->delete_work);
-	}
-}
-
-static void nvme_rdma_reconnect_ctrl_work(struct work_struct *work)
-{
-	struct nvme_rdma_ctrl *ctrl = container_of(to_delayed_work(work),
-			struct nvme_rdma_ctrl, reconnect_work);
-	bool changed;
-	int ret;
-
-	++ctrl->ctrl.nr_reconnects;
-
-	if (ctrl->ctrl.queue_count > 1)
-		nvme_rdma_destroy_io_queues(ctrl, false);
-
-	nvme_rdma_destroy_admin_queue(ctrl, false);
-	ret = nvme_rdma_configure_admin_queue(ctrl, false);
-	if (ret)
-		goto requeue;
-
-	if (ctrl->ctrl.queue_count > 1) {
-		ret = nvme_rdma_configure_io_queues(ctrl, false);
-		if (ret)
-			goto requeue;
-	}
-
-	changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
-	if (!changed) {
-		/* state change failure is ok if we're in DELETING state */
-		WARN_ON_ONCE(ctrl->ctrl.state != NVME_CTRL_DELETING);
-		return;
-	}
-
-	ctrl->ctrl.nr_reconnects = 0;
-
-	nvme_start_ctrl(&ctrl->ctrl);
-
-	dev_info(ctrl->ctrl.device, "Successfully reconnected\n");
-
-	return;
-
-requeue:
-	dev_info(ctrl->ctrl.device, "Failed reconnect attempt %d\n",
-			ctrl->ctrl.nr_reconnects);
-	nvme_rdma_reconnect_or_remove(ctrl);
-}
-
-static void nvme_rdma_error_recovery_work(struct work_struct *work)
-{
-	struct nvme_rdma_ctrl *ctrl = container_of(work,
-			struct nvme_rdma_ctrl, err_work);
-
-	nvme_stop_keep_alive(&ctrl->ctrl);
-
-	if (ctrl->ctrl.queue_count > 1) {
-		nvme_stop_queues(&ctrl->ctrl);
-		nvme_rdma_stop_io_queues(ctrl);
-	}
-	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
-	nvme_rdma_stop_queue(&ctrl->queues[0]);
-
-	/* We must take care of fastfail/requeue all our inflight requests */
-	if (ctrl->ctrl.queue_count > 1)
-		blk_mq_tagset_busy_iter(&ctrl->tag_set,
-					nvme_cancel_request, &ctrl->ctrl);
-	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
-				nvme_cancel_request, &ctrl->ctrl);
-
-	/*
-	 * queues are not a live anymore, so restart the queues to fail fast
-	 * new IO
-	 */
-	blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
-	nvme_start_queues(&ctrl->ctrl);
-
-	nvme_rdma_reconnect_or_remove(ctrl);
-}
-
-static void nvme_rdma_error_recovery(struct nvme_rdma_ctrl *ctrl)
-{
-	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RECONNECTING))
-		return;
-
-	queue_work(nvme_wq, &ctrl->err_work);
-}
-
-static void nvme_rdma_wr_error(struct ib_cq *cq, struct ib_wc *wc,
-		const char *op)
-{
-	struct nvme_rdma_queue *queue = cq->cq_context;
-	struct nvme_rdma_ctrl *ctrl = queue->ctrl;
-
-	if (ctrl->ctrl.state == NVME_CTRL_LIVE)
-		dev_info(ctrl->ctrl.device,
-			     "%s for CQE 0x%p failed with status %s (%d)\n",
-			     op, wc->wr_cqe,
-			     ib_wc_status_msg(wc->status), wc->status);
-	nvme_rdma_error_recovery(ctrl);
-}
-
-static void nvme_rdma_memreg_done(struct ib_cq *cq, struct ib_wc *wc)
-{
-	if (unlikely(wc->status != IB_WC_SUCCESS))
-		nvme_rdma_wr_error(cq, wc, "MEMREG");
-}
-
-static void nvme_rdma_inv_rkey_done(struct ib_cq *cq, struct ib_wc *wc)
-{
-	if (unlikely(wc->status != IB_WC_SUCCESS))
-		nvme_rdma_wr_error(cq, wc, "LOCAL_INV");
-}
-
-static int nvme_rdma_inv_rkey(struct nvme_rdma_queue *queue,
-		struct nvme_rdma_request *req)
-{
-	struct ib_send_wr *bad_wr;
-	struct ib_send_wr wr = {
-		.opcode		    = IB_WR_LOCAL_INV,
-		.next		    = NULL,
-		.num_sge	    = 0,
-		.send_flags	    = 0,
-		.ex.invalidate_rkey = req->mr->rkey,
-	};
-
-	req->reg_cqe.done = nvme_rdma_inv_rkey_done;
-	wr.wr_cqe = &req->reg_cqe;
-
-	return ib_post_send(queue->qp, &wr, &bad_wr);
-}
-
-static void nvme_rdma_unmap_data(struct nvme_rdma_queue *queue,
-		struct request *rq)
-{
-	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
-	struct nvme_rdma_ctrl *ctrl = queue->ctrl;
-	struct nvme_rdma_device *dev = queue->device;
-	struct ib_device *ibdev = dev->dev;
-	int res;
-
-	if (!blk_rq_bytes(rq))
-		return;
-
-	if (req->mr->need_inval) {
-		res = nvme_rdma_inv_rkey(queue, req);
-		if (unlikely(res < 0)) {
-			dev_err(ctrl->ctrl.device,
-				"Queueing INV WR for rkey %#x failed (%d)\n",
-				req->mr->rkey, res);
-			nvme_rdma_error_recovery(queue->ctrl);
-		}
-	}
-
-	ib_dma_unmap_sg(ibdev, req->sg_table.sgl,
-			req->nents, rq_data_dir(rq) ==
-				    WRITE ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
-
-	nvme_cleanup_cmd(rq);
-	sg_free_table_chained(&req->sg_table, true);
-}
-
-static int nvme_rdma_set_sg_null(struct nvme_command *c)
-{
-	struct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;
-
-	sg->addr = 0;
-	put_unaligned_le24(0, sg->length);
-	put_unaligned_le32(0, sg->key);
-	sg->type = NVME_KEY_SGL_FMT_DATA_DESC << 4;
-	return 0;
-}
-
-static int nvme_rdma_map_sg_inline(struct nvme_rdma_queue *queue,
-		struct nvme_rdma_request *req, struct nvme_command *c)
-{
-	struct nvme_sgl_desc *sg = &c->common.dptr.sgl;
-
-	req->sge[1].addr = sg_dma_address(req->sg_table.sgl);
-	req->sge[1].length = sg_dma_len(req->sg_table.sgl);
-	req->sge[1].lkey = queue->device->pd->local_dma_lkey;
-
-	sg->addr = cpu_to_le64(queue->ctrl->ctrl.icdoff);
-	sg->length = cpu_to_le32(sg_dma_len(req->sg_table.sgl));
-	sg->type = (NVME_SGL_FMT_DATA_DESC << 4) | NVME_SGL_FMT_OFFSET;
-
-	req->inline_data = true;
-	req->num_sge++;
-	return 0;
-}
-
-static int nvme_rdma_map_sg_single(struct nvme_rdma_queue *queue,
-		struct nvme_rdma_request *req, struct nvme_command *c)
-{
-	struct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;
-
-	sg->addr = cpu_to_le64(sg_dma_address(req->sg_table.sgl));
-	put_unaligned_le24(sg_dma_len(req->sg_table.sgl), sg->length);
-	put_unaligned_le32(queue->device->pd->unsafe_global_rkey, sg->key);
-	sg->type = NVME_KEY_SGL_FMT_DATA_DESC << 4;
-	return 0;
-}
-
-static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
-		struct nvme_rdma_request *req, struct nvme_command *c,
-		int count)
-{
-	struct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;
-	int nr;
-
-	/*
-	 * Align the MR to a 4K page size to match the ctrl page size and
-	 * the block virtual boundary.
-	 */
-	nr = ib_map_mr_sg(req->mr, req->sg_table.sgl, count, NULL, SZ_4K);
-	if (unlikely(nr < count)) {
-		if (nr < 0)
-			return nr;
-		return -EINVAL;
-	}
-
-	ib_update_fast_reg_key(req->mr, ib_inc_rkey(req->mr->rkey));
-
-	req->reg_cqe.done = nvme_rdma_memreg_done;
-	memset(&req->reg_wr, 0, sizeof(req->reg_wr));
-	req->reg_wr.wr.opcode = IB_WR_REG_MR;
-	req->reg_wr.wr.wr_cqe = &req->reg_cqe;
-	req->reg_wr.wr.num_sge = 0;
-	req->reg_wr.mr = req->mr;
-	req->reg_wr.key = req->mr->rkey;
-	req->reg_wr.access = IB_ACCESS_LOCAL_WRITE |
-			     IB_ACCESS_REMOTE_READ |
-			     IB_ACCESS_REMOTE_WRITE;
-
-	req->mr->need_inval = true;
-
-	sg->addr = cpu_to_le64(req->mr->iova);
-	put_unaligned_le24(req->mr->length, sg->length);
-	put_unaligned_le32(req->mr->rkey, sg->key);
-	sg->type = (NVME_KEY_SGL_FMT_DATA_DESC << 4) |
-			NVME_SGL_FMT_INVALIDATE;
-
-	return 0;
-}
-
-static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
-		struct request *rq, struct nvme_command *c)
-{
-	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
-	struct nvme_rdma_device *dev = queue->device;
-	struct ib_device *ibdev = dev->dev;
-	int count, ret;
-
-	req->num_sge = 1;
-	req->inline_data = false;
-	req->mr->need_inval = false;
-	refcount_set(&req->ref, 2); /* send and recv completions */
-
-	c->common.flags |= NVME_CMD_SGL_METABUF;
-
-	if (!blk_rq_bytes(rq))
-		return nvme_rdma_set_sg_null(c);
-
-	req->sg_table.sgl = req->first_sgl;
-	ret = sg_alloc_table_chained(&req->sg_table,
-			blk_rq_nr_phys_segments(rq), req->sg_table.sgl);
-	if (ret)
-		return -ENOMEM;
-
-	req->nents = blk_rq_map_sg(rq->q, rq, req->sg_table.sgl);
-
-	count = ib_dma_map_sg(ibdev, req->sg_table.sgl, req->nents,
-		    rq_data_dir(rq) == WRITE ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
-	if (unlikely(count <= 0)) {
-		sg_free_table_chained(&req->sg_table, true);
-		return -EIO;
-	}
-
-	if (count == 1) {
-		if (rq_data_dir(rq) == WRITE && nvme_rdma_queue_idx(queue) &&
-		    blk_rq_payload_bytes(rq) <=
-				nvme_rdma_inline_data_size(queue))
-			return nvme_rdma_map_sg_inline(queue, req, c);
-
-		if (dev->pd->flags & IB_PD_UNSAFE_GLOBAL_RKEY)
-			return nvme_rdma_map_sg_single(queue, req, c);
-	}
-
-	return nvme_rdma_map_sg_fr(queue, req, c, count);
-}
-
-static void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)
-{
-	struct nvme_rdma_qe *qe =
-		container_of(wc->wr_cqe, struct nvme_rdma_qe, cqe);
-	struct nvme_rdma_request *req =
-		container_of(qe, struct nvme_rdma_request, sqe);
-	struct request *rq = blk_mq_rq_from_pdu(req);
-
-	if (unlikely(wc->status != IB_WC_SUCCESS)) {
-		nvme_rdma_wr_error(cq, wc, "SEND");
-		return;
-	}
-
-	if (refcount_dec_and_test(&req->ref))
-		nvme_end_request(rq, req->status, req->result);
-}
-
-static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
-		struct nvme_rdma_qe *qe, struct ib_sge *sge, u32 num_sge,
-		struct ib_send_wr *first)
-{
-	struct ib_send_wr wr, *bad_wr;
-	int ret;
-
-	sge->addr   = qe->dma;
-	sge->length = sizeof(struct nvme_command),
-	sge->lkey   = queue->device->pd->local_dma_lkey;
-
-	wr.next       = NULL;
-	wr.wr_cqe     = &qe->cqe;
-	wr.sg_list    = sge;
-	wr.num_sge    = num_sge;
-	wr.opcode     = IB_WR_SEND;
-	wr.send_flags = IB_SEND_SIGNALED;
-
-	if (first)
-		first->next = &wr;
-	else
-		first = &wr;
-
-	ret = ib_post_send(queue->qp, first, &bad_wr);
-	if (unlikely(ret)) {
-		dev_err(queue->ctrl->ctrl.device,
-			     "%s failed with error code %d\n", __func__, ret);
-	}
-	return ret;
-}
-
-static int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,
-		struct nvme_rdma_qe *qe)
-{
-	struct ib_recv_wr wr, *bad_wr;
-	struct ib_sge list;
-	int ret;
-
-	list.addr   = qe->dma;
-	list.length = sizeof(struct nvme_completion);
-	list.lkey   = queue->device->pd->local_dma_lkey;
-
-	qe->cqe.done = nvme_rdma_recv_done;
-
-	wr.next     = NULL;
-	wr.wr_cqe   = &qe->cqe;
-	wr.sg_list  = &list;
-	wr.num_sge  = 1;
-
-	ret = ib_post_recv(queue->qp, &wr, &bad_wr);
-	if (unlikely(ret)) {
-		dev_err(queue->ctrl->ctrl.device,
-			"%s failed with error code %d\n", __func__, ret);
-	}
-	return ret;
-}
-
-static struct blk_mq_tags *nvme_rdma_tagset(struct nvme_rdma_queue *queue)
-{
-	u32 queue_idx = nvme_rdma_queue_idx(queue);
-
-	if (queue_idx == 0)
-		return queue->ctrl->admin_tag_set.tags[queue_idx];
-	return queue->ctrl->tag_set.tags[queue_idx - 1];
-}
-
-static void nvme_rdma_async_done(struct ib_cq *cq, struct ib_wc *wc)
-{
-	if (unlikely(wc->status != IB_WC_SUCCESS))
-		nvme_rdma_wr_error(cq, wc, "ASYNC");
-}
-
-static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg, int aer_idx)
-{
-	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
-	struct nvme_rdma_queue *queue = &ctrl->queues[0];
-	struct ib_device *dev = queue->device->dev;
-	struct nvme_rdma_qe *sqe = &ctrl->async_event_sqe;
-	struct nvme_command *cmd = sqe->data;
-	struct ib_sge sge;
-	int ret;
-
-	if (WARN_ON_ONCE(aer_idx != 0))
-		return;
-
-	ib_dma_sync_single_for_cpu(dev, sqe->dma, sizeof(*cmd), DMA_TO_DEVICE);
-
-	memset(cmd, 0, sizeof(*cmd));
-	cmd->common.opcode = nvme_admin_async_event;
-	cmd->common.command_id = NVME_RDMA_AQ_BLKMQ_DEPTH;
-	cmd->common.flags |= NVME_CMD_SGL_METABUF;
-	nvme_rdma_set_sg_null(cmd);
-
-	sqe->cqe.done = nvme_rdma_async_done;
-
-	ib_dma_sync_single_for_device(dev, sqe->dma, sizeof(*cmd),
-			DMA_TO_DEVICE);
-
-	ret = nvme_rdma_post_send(queue, sqe, &sge, 1, NULL);
-	WARN_ON_ONCE(ret);
-}
-
-static int nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
-		struct nvme_completion *cqe, struct ib_wc *wc, int tag)
-{
-	struct request *rq;
-	struct nvme_rdma_request *req;
-	int ret = 0;
-
-	rq = blk_mq_tag_to_rq(nvme_rdma_tagset(queue), cqe->command_id);
-	if (!rq) {
-		dev_err(queue->ctrl->ctrl.device,
-			"tag 0x%x on QP %#x not found\n",
-			cqe->command_id, queue->qp->qp_num);
-		nvme_rdma_error_recovery(queue->ctrl);
-		return ret;
-	}
-	req = blk_mq_rq_to_pdu(rq);
-
-	req->status = cqe->status;
-	req->result = cqe->result;
-
-	if ((wc->wc_flags & IB_WC_WITH_INVALIDATE) &&
-	    wc->ex.invalidate_rkey == req->mr->rkey)
-		req->mr->need_inval = false;
-
-	if (refcount_dec_and_test(&req->ref)) {
-		if (rq->tag == tag)
-			ret = 1;
-		nvme_end_request(rq, req->status, req->result);
-	}
-
-	return ret;
-}
-
-static int __nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc, int tag)
-{
-	struct nvme_rdma_qe *qe =
-		container_of(wc->wr_cqe, struct nvme_rdma_qe, cqe);
-	struct nvme_rdma_queue *queue = cq->cq_context;
-	struct ib_device *ibdev = queue->device->dev;
-	struct nvme_completion *cqe = qe->data;
-	const size_t len = sizeof(struct nvme_completion);
-	int ret = 0;
-
-	if (unlikely(wc->status != IB_WC_SUCCESS)) {
-		nvme_rdma_wr_error(cq, wc, "RECV");
-		return 0;
-	}
-
-	ib_dma_sync_single_for_cpu(ibdev, qe->dma, len, DMA_FROM_DEVICE);
-	/*
-	 * AEN requests are special as they don't time out and can
-	 * survive any kind of queue freeze and often don't respond to
-	 * aborts.  We don't even bother to allocate a struct request
-	 * for them but rather special case them here.
-	 */
-	if (unlikely(nvme_rdma_queue_idx(queue) == 0 &&
-			cqe->command_id >= NVME_RDMA_AQ_BLKMQ_DEPTH))
-		nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
-				&cqe->result);
-	else
-		ret = nvme_rdma_process_nvme_rsp(queue, cqe, wc, tag);
-	ib_dma_sync_single_for_device(ibdev, qe->dma, len, DMA_FROM_DEVICE);
-
-	nvme_rdma_post_recv(queue, qe);
-	return ret;
-}
-
-static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
-{
-	__nvme_rdma_recv_done(cq, wc, -1);
-}
-
-static int nvme_rdma_conn_established(struct nvme_rdma_queue *queue)
-{
-	int ret, i;
-
-	for (i = 0; i < queue->queue_size; i++) {
-		ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
-		if (ret)
-			goto out_destroy_queue_ib;
-	}
-
-	return 0;
-
-out_destroy_queue_ib:
-	nvme_rdma_destroy_queue_ib(queue);
-	return ret;
-}
-
-static int nvme_rdma_conn_rejected(struct nvme_rdma_queue *queue,
-		struct rdma_cm_event *ev)
-{
-	struct rdma_cm_id *cm_id = queue->cm_id;
-	int status = ev->status;
-	const char *rej_msg;
-	const struct nvme_rdma_cm_rej *rej_data;
-	u8 rej_data_len;
-
-	rej_msg = rdma_reject_msg(cm_id, status);
-	rej_data = rdma_consumer_reject_data(cm_id, ev, &rej_data_len);
-
-	if (rej_data && rej_data_len >= sizeof(u16)) {
-		u16 sts = le16_to_cpu(rej_data->sts);
-
-		dev_err(queue->ctrl->ctrl.device,
-		      "Connect rejected: status %d (%s) nvme status %d (%s).\n",
-		      status, rej_msg, sts, nvme_rdma_cm_msg(sts));
-	} else {
-		dev_err(queue->ctrl->ctrl.device,
-			"Connect rejected: status %d (%s).\n", status, rej_msg);
-	}
-
-	return -ECONNRESET;
-}
-
-static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
-{
-	int ret;
-
-	ret = nvme_rdma_create_queue_ib(queue);
-	if (ret)
-		return ret;
-
-	ret = rdma_resolve_route(queue->cm_id, NVME_RDMA_CONNECT_TIMEOUT_MS);
-	if (ret) {
-		dev_err(queue->ctrl->ctrl.device,
-			"rdma_resolve_route failed (%d).\n",
-			queue->cm_error);
-		goto out_destroy_queue;
-	}
-
-	return 0;
-
-out_destroy_queue:
-	nvme_rdma_destroy_queue_ib(queue);
-	return ret;
-}
-
-static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
-{
-	struct nvme_rdma_ctrl *ctrl = queue->ctrl;
-	struct rdma_conn_param param = { };
-	struct nvme_rdma_cm_req priv = { };
-	int ret;
-
-	param.qp_num = queue->qp->qp_num;
-	param.flow_control = 1;
-
-	param.responder_resources = queue->device->dev->attrs.max_qp_rd_atom;
-	/* maximum retry count */
-	param.retry_count = 7;
-	param.rnr_retry_count = 7;
-	param.private_data = &priv;
-	param.private_data_len = sizeof(priv);
-
-	priv.recfmt = cpu_to_le16(NVME_RDMA_CM_FMT_1_0);
-	priv.qid = cpu_to_le16(nvme_rdma_queue_idx(queue));
-	/*
-	 * set the admin queue depth to the minimum size
-	 * specified by the Fabrics standard.
-	 */
-	if (priv.qid == 0) {
-		priv.hrqsize = cpu_to_le16(NVME_AQ_DEPTH);
-		priv.hsqsize = cpu_to_le16(NVME_AQ_DEPTH - 1);
-	} else {
-		/*
-		 * current interpretation of the fabrics spec
-		 * is at minimum you make hrqsize sqsize+1, or a
-		 * 1's based representation of sqsize.
-		 */
-		priv.hrqsize = cpu_to_le16(queue->queue_size);
-		priv.hsqsize = cpu_to_le16(queue->ctrl->ctrl.sqsize);
-	}
-
-	ret = rdma_connect(queue->cm_id, &param);
-	if (ret) {
-		dev_err(ctrl->ctrl.device,
-			"rdma_connect failed (%d).\n", ret);
-		goto out_destroy_queue_ib;
-	}
-
-	return 0;
-
-out_destroy_queue_ib:
-	nvme_rdma_destroy_queue_ib(queue);
-	return ret;
-}
-
-static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
-		struct rdma_cm_event *ev)
-{
-	struct nvme_rdma_queue *queue = cm_id->context;
-	int cm_error = 0;
-
-	dev_dbg(queue->ctrl->ctrl.device, "%s (%d): status %d id %p\n",
-		rdma_event_msg(ev->event), ev->event,
-		ev->status, cm_id);
-
-	switch (ev->event) {
-	case RDMA_CM_EVENT_ADDR_RESOLVED:
-		cm_error = nvme_rdma_addr_resolved(queue);
-		break;
-	case RDMA_CM_EVENT_ROUTE_RESOLVED:
-		cm_error = nvme_rdma_route_resolved(queue);
-		break;
-	case RDMA_CM_EVENT_ESTABLISHED:
-		queue->cm_error = nvme_rdma_conn_established(queue);
-		/* complete cm_done regardless of success/failure */
-		complete(&queue->cm_done);
-		return 0;
-	case RDMA_CM_EVENT_REJECTED:
-		nvme_rdma_destroy_queue_ib(queue);
-		cm_error = nvme_rdma_conn_rejected(queue, ev);
-		break;
-	case RDMA_CM_EVENT_ROUTE_ERROR:
-	case RDMA_CM_EVENT_CONNECT_ERROR:
-	case RDMA_CM_EVENT_UNREACHABLE:
-		nvme_rdma_destroy_queue_ib(queue);
-	case RDMA_CM_EVENT_ADDR_ERROR:
-		dev_dbg(queue->ctrl->ctrl.device,
-			"CM error event %d\n", ev->event);
-		cm_error = -ECONNRESET;
-		break;
-	case RDMA_CM_EVENT_DISCONNECTED:
-	case RDMA_CM_EVENT_ADDR_CHANGE:
-	case RDMA_CM_EVENT_TIMEWAIT_EXIT:
-		dev_dbg(queue->ctrl->ctrl.device,
-			"disconnect received - connection closed\n");
-		nvme_rdma_error_recovery(queue->ctrl);
-		break;
-	case RDMA_CM_EVENT_DEVICE_REMOVAL:
-		/* device removal is handled via the ib_client API */
-		break;
-	default:
-		dev_err(queue->ctrl->ctrl.device,
-			"Unexpected RDMA CM event (%d)\n", ev->event);
-		nvme_rdma_error_recovery(queue->ctrl);
-		break;
-	}
-
-	if (cm_error) {
-		queue->cm_error = cm_error;
-		complete(&queue->cm_done);
-	}
-
-	return 0;
-}
-
-static enum blk_eh_timer_return
-nvme_rdma_timeout(struct request *rq, bool reserved)
-{
-	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
-
-	/* queue error recovery */
-	nvme_rdma_error_recovery(req->queue->ctrl);
-
-	/* fail with DNR on cmd timeout */
-	nvme_req(rq)->status = NVME_SC_ABORT_REQ | NVME_SC_DNR;
-
-	return BLK_EH_HANDLED;
-}
-
-/*
- * We cannot accept any other command until the Connect command has completed.
- */
-static inline blk_status_t
-nvme_rdma_is_ready(struct nvme_rdma_queue *queue, struct request *rq)
-{
-	if (unlikely(!test_bit(NVME_RDMA_Q_LIVE, &queue->flags)))
-		return nvmf_check_init_req(&queue->ctrl->ctrl, rq);
-	return BLK_STS_OK;
-}
-
-static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
-		const struct blk_mq_queue_data *bd)
-{
-	struct nvme_ns *ns = hctx->queue->queuedata;
-	struct nvme_rdma_queue *queue = hctx->driver_data;
-	struct request *rq = bd->rq;
-	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
-	struct nvme_rdma_qe *sqe = &req->sqe;
-	struct nvme_command *c = sqe->data;
-	struct ib_device *dev;
-	blk_status_t ret;
-	int err;
-
-	WARN_ON_ONCE(rq->tag < 0);
-
-	ret = nvme_rdma_is_ready(queue, rq);
-	if (unlikely(ret))
-		return ret;
-
-	dev = queue->device->dev;
-	ib_dma_sync_single_for_cpu(dev, sqe->dma,
-			sizeof(struct nvme_command), DMA_TO_DEVICE);
-
-	ret = nvme_setup_cmd(ns, rq, c);
-	if (ret)
-		return ret;
-
-	blk_mq_start_request(rq);
-
-	err = nvme_rdma_map_data(queue, rq, c);
-	if (unlikely(err < 0)) {
-		dev_err(queue->ctrl->ctrl.device,
-			     "Failed to map data (%d)\n", err);
-		nvme_cleanup_cmd(rq);
-		goto err;
-	}
-
-	sqe->cqe.done = nvme_rdma_send_done;
-
-	ib_dma_sync_single_for_device(dev, sqe->dma,
-			sizeof(struct nvme_command), DMA_TO_DEVICE);
-
-	err = nvme_rdma_post_send(queue, sqe, req->sge, req->num_sge,
-			req->mr->need_inval ? &req->reg_wr.wr : NULL);
-	if (unlikely(err)) {
-		nvme_rdma_unmap_data(queue, rq);
-		goto err;
-	}
-
-	return BLK_STS_OK;
-err:
-	if (err == -ENOMEM || err == -EAGAIN)
-		return BLK_STS_RESOURCE;
-	return BLK_STS_IOERR;
-}
-
-static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx, unsigned int tag)
-{
-	struct nvme_rdma_queue *queue = hctx->driver_data;
-	struct ib_cq *cq = queue->ib_cq;
-	struct ib_wc wc;
-	int found = 0;
-
-	while (ib_poll_cq(cq, 1, &wc) > 0) {
-		struct ib_cqe *cqe = wc.wr_cqe;
-
-		if (cqe) {
-			if (cqe->done == nvme_rdma_recv_done)
-				found |= __nvme_rdma_recv_done(cq, &wc, tag);
-			else
-				cqe->done(cq, &wc);
-		}
-	}
-
-	return found;
-}
-
-static void nvme_rdma_complete_rq(struct request *rq)
-{
-	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
-
-	nvme_rdma_unmap_data(req->queue, rq);
-	nvme_complete_rq(rq);
-}
-
-static int nvme_rdma_map_queues(struct blk_mq_tag_set *set)
-{
-	struct nvme_rdma_ctrl *ctrl = set->driver_data;
-
-	return blk_mq_rdma_map_queues(set, ctrl->device->dev, 0);
-}
-
-static const struct blk_mq_ops nvme_rdma_mq_ops = {
-	.queue_rq	= nvme_rdma_queue_rq,
-	.complete	= nvme_rdma_complete_rq,
-	.init_request	= nvme_rdma_init_request,
-	.exit_request	= nvme_rdma_exit_request,
-	.init_hctx	= nvme_rdma_init_hctx,
-	.poll		= nvme_rdma_poll,
-	.timeout	= nvme_rdma_timeout,
-	.map_queues	= nvme_rdma_map_queues,
-};
-
-static const struct blk_mq_ops nvme_rdma_admin_mq_ops = {
-	.queue_rq	= nvme_rdma_queue_rq,
-	.complete	= nvme_rdma_complete_rq,
-	.init_request	= nvme_rdma_init_request,
-	.exit_request	= nvme_rdma_exit_request,
-	.init_hctx	= nvme_rdma_init_admin_hctx,
-	.timeout	= nvme_rdma_timeout,
-};
-
-static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
-{
-	cancel_work_sync(&ctrl->err_work);
-	cancel_delayed_work_sync(&ctrl->reconnect_work);
-
-	if (ctrl->ctrl.queue_count > 1) {
-		nvme_stop_queues(&ctrl->ctrl);
-		blk_mq_tagset_busy_iter(&ctrl->tag_set,
-					nvme_cancel_request, &ctrl->ctrl);
-		if (shutdown)
-			nvme_start_queues(&ctrl->ctrl);
-		nvme_rdma_destroy_io_queues(ctrl, shutdown);
-	}
-
-	if (shutdown)
-		nvme_shutdown_ctrl(&ctrl->ctrl);
-	else
-		nvme_disable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
-
-	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
-	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
-				nvme_cancel_request, &ctrl->ctrl);
-	blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
-	nvme_rdma_destroy_admin_queue(ctrl, shutdown);
-}
-
-static void nvme_rdma_remove_ctrl(struct nvme_rdma_ctrl *ctrl)
-{
-	nvme_remove_namespaces(&ctrl->ctrl);
-	nvme_rdma_shutdown_ctrl(ctrl, true);
-	nvme_uninit_ctrl(&ctrl->ctrl);
-	nvme_put_ctrl(&ctrl->ctrl);
-}
-
-static void nvme_rdma_del_ctrl_work(struct work_struct *work)
-{
-	struct nvme_rdma_ctrl *ctrl = container_of(work,
-				struct nvme_rdma_ctrl, delete_work);
-
-	nvme_stop_ctrl(&ctrl->ctrl);
-	nvme_rdma_remove_ctrl(ctrl);
-}
-
-static int __nvme_rdma_del_ctrl(struct nvme_rdma_ctrl *ctrl)
-{
-	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_DELETING))
-		return -EBUSY;
-
-	if (!queue_work(nvme_wq, &ctrl->delete_work))
-		return -EBUSY;
-
-	return 0;
-}
-
-static int nvme_rdma_del_ctrl(struct nvme_ctrl *nctrl)
-{
-	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
-	int ret = 0;
-
-	/*
-	 * Keep a reference until all work is flushed since
-	 * __nvme_rdma_del_ctrl can free the ctrl mem
-	 */
-	if (!kref_get_unless_zero(&ctrl->ctrl.kref))
-		return -EBUSY;
-	ret = __nvme_rdma_del_ctrl(ctrl);
-	if (!ret)
-		flush_work(&ctrl->delete_work);
-	nvme_put_ctrl(&ctrl->ctrl);
-	return ret;
-}
-
-static void nvme_rdma_reset_ctrl_work(struct work_struct *work)
-{
-	struct nvme_rdma_ctrl *ctrl =
-		container_of(work, struct nvme_rdma_ctrl, ctrl.reset_work);
-	int ret;
-	bool changed;
-
-	nvme_stop_ctrl(&ctrl->ctrl);
-	nvme_rdma_shutdown_ctrl(ctrl, false);
-
-	ret = nvme_rdma_configure_admin_queue(ctrl, false);
-	if (ret)
-		goto out_fail;
-
-	if (ctrl->ctrl.queue_count > 1) {
-		ret = nvme_rdma_configure_io_queues(ctrl, false);
-		if (ret)
-			goto out_fail;
-	}
-
-	changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
-	WARN_ON_ONCE(!changed);
-
-	nvme_start_ctrl(&ctrl->ctrl);
-
-	return;
-
-out_fail:
-	dev_warn(ctrl->ctrl.device, "Removing after reset failure\n");
-	nvme_rdma_remove_ctrl(ctrl);
-}
-
-static const struct nvme_ctrl_ops nvme_rdma_ctrl_ops = {
-	.name			= "rdma",
-	.module			= THIS_MODULE,
-	.flags			= NVME_F_FABRICS,
-	.reg_read32		= nvmf_reg_read32,
-	.reg_read64		= nvmf_reg_read64,
-	.reg_write32		= nvmf_reg_write32,
-	.free_ctrl		= nvme_rdma_free_ctrl,
-	.submit_async_event	= nvme_rdma_submit_async_event,
-	.delete_ctrl		= nvme_rdma_del_ctrl,
-	.get_address		= nvmf_get_address,
-};
-
-static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
-		struct nvmf_ctrl_options *opts)
-{
-	struct nvme_rdma_ctrl *ctrl;
-	int ret;
-	bool changed;
-	char *port;
-
-	ctrl = kzalloc(sizeof(*ctrl), GFP_KERNEL);
-	if (!ctrl)
-		return ERR_PTR(-ENOMEM);
-	ctrl->ctrl.opts = opts;
-	INIT_LIST_HEAD(&ctrl->list);
-
-	if (opts->mask & NVMF_OPT_TRSVCID)
-		port = opts->trsvcid;
-	else
-		port = __stringify(NVME_RDMA_IP_PORT);
-
-	ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
-			opts->traddr, port, &ctrl->addr);
-	if (ret) {
-		pr_err("malformed address passed: %s:%s\n", opts->traddr, port);
-		goto out_free_ctrl;
-	}
-
-	if (opts->mask & NVMF_OPT_HOST_TRADDR) {
-		ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
-			opts->host_traddr, NULL, &ctrl->src_addr);
-		if (ret) {
-			pr_err("malformed src address passed: %s\n",
-			       opts->host_traddr);
-			goto out_free_ctrl;
-		}
-	}
-
-	ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,
-				0 /* no quirks, we're perfect! */);
-	if (ret)
-		goto out_free_ctrl;
-
-	INIT_DELAYED_WORK(&ctrl->reconnect_work,
-			nvme_rdma_reconnect_ctrl_work);
-	INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
-	INIT_WORK(&ctrl->delete_work, nvme_rdma_del_ctrl_work);
-	INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
-
-	ctrl->ctrl.queue_count = opts->nr_io_queues + 1; /* +1 for admin queue */
-	ctrl->ctrl.sqsize = opts->queue_size - 1;
-	ctrl->ctrl.kato = opts->kato;
-
-	ret = -ENOMEM;
-	ctrl->queues = kcalloc(ctrl->ctrl.queue_count, sizeof(*ctrl->queues),
-				GFP_KERNEL);
-	if (!ctrl->queues)
-		goto out_uninit_ctrl;
-
-	ret = nvme_rdma_configure_admin_queue(ctrl, true);
-	if (ret)
-		goto out_kfree_queues;
-
-	/* sanity check icdoff */
-	if (ctrl->ctrl.icdoff) {
-		dev_err(ctrl->ctrl.device, "icdoff is not supported!\n");
-		ret = -EINVAL;
-		goto out_remove_admin_queue;
-	}
-
-	/* sanity check keyed sgls */
-	if (!(ctrl->ctrl.sgls & (1 << 20))) {
-		dev_err(ctrl->ctrl.device, "Mandatory keyed sgls are not support\n");
-		ret = -EINVAL;
-		goto out_remove_admin_queue;
-	}
-
-	if (opts->queue_size > ctrl->ctrl.maxcmd) {
-		/* warn if maxcmd is lower than queue_size */
-		dev_warn(ctrl->ctrl.device,
-			"queue_size %zu > ctrl maxcmd %u, clamping down\n",
-			opts->queue_size, ctrl->ctrl.maxcmd);
-		opts->queue_size = ctrl->ctrl.maxcmd;
-	}
-
-	if (opts->queue_size > ctrl->ctrl.sqsize + 1) {
-		/* warn if sqsize is lower than queue_size */
-		dev_warn(ctrl->ctrl.device,
-			"queue_size %zu > ctrl sqsize %u, clamping down\n",
-			opts->queue_size, ctrl->ctrl.sqsize + 1);
-		opts->queue_size = ctrl->ctrl.sqsize + 1;
-	}
-
-	if (opts->nr_io_queues) {
-		ret = nvme_rdma_configure_io_queues(ctrl, true);
-		if (ret)
-			goto out_remove_admin_queue;
-	}
-
-	changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
-	WARN_ON_ONCE(!changed);
-
-	dev_info(ctrl->ctrl.device, "new ctrl: NQN \"%s\", addr %pISpcs\n",
-		ctrl->ctrl.opts->subsysnqn, &ctrl->addr);
-
-	kref_get(&ctrl->ctrl.kref);
-
-	mutex_lock(&nvme_rdma_ctrl_mutex);
-	list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
-	mutex_unlock(&nvme_rdma_ctrl_mutex);
-
-	nvme_start_ctrl(&ctrl->ctrl);
-
-	return &ctrl->ctrl;
-
-out_remove_admin_queue:
-	nvme_rdma_destroy_admin_queue(ctrl, true);
-out_kfree_queues:
-	kfree(ctrl->queues);
-out_uninit_ctrl:
-	nvme_uninit_ctrl(&ctrl->ctrl);
-	nvme_put_ctrl(&ctrl->ctrl);
-	if (ret > 0)
-		ret = -EIO;
-	return ERR_PTR(ret);
-out_free_ctrl:
-	kfree(ctrl);
-	return ERR_PTR(ret);
-}
-
-static struct nvmf_transport_ops nvme_rdma_transport = {
-	.name		= "rdma",
-	.required_opts	= NVMF_OPT_TRADDR,
-	.allowed_opts	= NVMF_OPT_TRSVCID | NVMF_OPT_RECONNECT_DELAY |
-			  NVMF_OPT_HOST_TRADDR | NVMF_OPT_CTRL_LOSS_TMO,
-	.create_ctrl	= nvme_rdma_create_ctrl,
-};
-
-static void nvme_rdma_remove_one(struct ib_device *ib_device, void *client_data)
-{
-	struct nvme_rdma_ctrl *ctrl;
-
-	/* Delete all controllers using this device */
-	mutex_lock(&nvme_rdma_ctrl_mutex);
-	list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
-		if (ctrl->device->dev != ib_device)
-			continue;
-		dev_info(ctrl->ctrl.device,
-			"Removing ctrl: NQN \"%s\", addr %pISp\n",
-			ctrl->ctrl.opts->subsysnqn, &ctrl->addr);
-		__nvme_rdma_del_ctrl(ctrl);
-	}
-	mutex_unlock(&nvme_rdma_ctrl_mutex);
-
-	flush_workqueue(nvme_wq);
-}
-
-static struct ib_client nvme_rdma_ib_client = {
-	.name   = "nvme_rdma",
-	.remove = nvme_rdma_remove_one
-};
-
-static int __init nvme_rdma_init_module(void)
-{
-	int ret;
-
-	ret = ib_register_client(&nvme_rdma_ib_client);
-	if (ret)
-		return ret;
-
-	ret = nvmf_register_transport(&nvme_rdma_transport);
-	if (ret)
-		goto err_unreg_client;
-
-	return 0;
-
-err_unreg_client:
-	ib_unregister_client(&nvme_rdma_ib_client);
-	return ret;
-}
-
-static void __exit nvme_rdma_cleanup_module(void)
-{
-	nvmf_unregister_transport(&nvme_rdma_transport);
-	ib_unregister_client(&nvme_rdma_ib_client);
-}
-
-module_init(nvme_rdma_init_module);
-module_exit(nvme_rdma_cleanup_module);
-
-MODULE_LICENSE("GPL v2");
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/re_insmod.sh b/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/re_insmod.sh
deleted file mode 100644
index 9ad4d23..0000000
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.14.81/re_insmod.sh
+++ /dev/null
@@ -1,7 +0,0 @@
-#!/bin/bash
-
-sudo rmmod nvme
-sudo rmmod nvme_core
-
-sudo insmod nvme-core.ko
-sudo insmod nvme.ko
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.15.18-041518-ubuntu-18_04/core.c b/PDK/driver/PCIe/kernel_driver/kernel_v4.15.18-041518-ubuntu-18_04/core.c
index d31d7c0..5a80214 100644
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.15.18-041518-ubuntu-18_04/core.c
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v4.15.18-041518-ubuntu-18_04/core.c
@@ -1319,8 +1319,8 @@ int __nvme_submit_kv_user_cmd(struct request_queue *q, struct nvme_command *cmd,
             kernel_ctx = get_aio_user_ctx(kv_data, bufflen, true);
             if (kernel_ctx) {
                 if (is_kv_store_cmd(cmd->common.opcode) || is_kv_append_cmd(cmd->common.opcode)) {
-                    (void)sg_copy_to_buffer(user_ctx->sg, user_ctx->nents,
-                         kv_data, user_ctx->len);
+			        (void)sg_copy_to_buffer(user_ctx->sg, user_ctx->nents,
+					    kv_data, user_ctx->len);
 #if 0
                     pr_err("copied data %c:%c:%c:%c: %c:%c:%c:%c.\n",
                             kv_data[0], kv_data[1], kv_data[2], kv_data[3],
@@ -1402,7 +1402,7 @@ int __nvme_submit_kv_user_cmd(struct request_queue *q, struct nvme_command *cmd,
 	} else {
 		blk_execute_rq(req->q, disk, req, 0);
         if (nvme_req(req)->flags & NVME_REQ_CANCELLED)
-            ret = -EINTR;
+            ret = EINTR;
         else
 		    ret = nvme_req(req)->status;
 
@@ -1500,18 +1500,18 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
 //    }
 
 	switch(cmd.opcode) {
-        case nvme_cmd_kv_store:
+		case nvme_cmd_kv_store:
         case nvme_cmd_kv_append:
             option = cpu_to_le32(cmd.cdw4);
-            c.kv_store.offset = cpu_to_le32(cmd.cdw5);
-            /* validate key length */
+			c.kv_store.offset = cpu_to_le32(cmd.cdw5);
+			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
                     cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            c.kv_store.key_len = cpu_to_le32(cmd.key_length -1); /* key len -1 */
+			c.kv_store.key_len = cpu_to_le32(cmd.key_length -1); /* key len -1 */
             c.kv_store.option = (option & 0xff);
             /* set value size */
             if (cmd.data_length % 4) {
@@ -1521,90 +1521,90 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
                 c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2));
             }
 
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user*)cmd.key_addr;
-                meta_len = cmd.key_length;
-            } else {
-                memcpy(c.kv_store.key, cmd.key, cmd.key_length);
-            }
-            break;
-        case nvme_cmd_kv_retrieve:
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user*)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_store.key, cmd.key, cmd.key_length);
+			}
+		break;
+		case nvme_cmd_kv_retrieve:
             option = cpu_to_le32(cmd.cdw4);
-            c.kv_retrieve.offset = cpu_to_le32(cmd.cdw5);
-            /* validate key length */
+			c.kv_retrieve.offset = cpu_to_le32(cmd.cdw5);
+			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
                     cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            c.kv_retrieve.key_len = cpu_to_le32(cmd.key_length -1); /* key len - 1 */
+			c.kv_retrieve.key_len = cpu_to_le32(cmd.key_length -1); /* key len - 1 */
             c.kv_retrieve.option = (option & 0xff);
-            c.kv_retrieve.value_len = cpu_to_le32((cmd.data_length >> 2));
+			c.kv_retrieve.value_len = cpu_to_le32((cmd.data_length >> 2));
 
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user*)cmd.key_addr;
-                meta_len = cmd.key_length;
-            } else {
-                memcpy(c.kv_retrieve.key, cmd.key, cmd.key_length);
-            }
-            break;
-        case nvme_cmd_kv_delete:
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user*)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_retrieve.key, cmd.key, cmd.key_length);
+			}
+		break;
+		case nvme_cmd_kv_delete:
             option = cpu_to_le32(cmd.cdw4);
-            /* validate key length */
+			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
                     cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            c.kv_delete.key_len = cpu_to_le32(cmd.key_length -1);
-            c.kv_delete.option = (option & 0xff);
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user *)cmd.key_addr;
-                meta_len = cmd.key_length;
-            } else {
-                memcpy(c.kv_delete.key, cmd.key, cmd.key_length);
-            }
-            break;
-        case nvme_cmd_kv_exist:
+			c.kv_delete.key_len = cpu_to_le32(cmd.key_length -1);
+		    c.kv_delete.option = (option & 0xff);
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user *)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_delete.key, cmd.key, cmd.key_length);
+			}
+			break;
+		case nvme_cmd_kv_exist:
             option = cpu_to_le32(cmd.cdw4);
-            /* validate key length */
+			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
                     cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
-            }
-            c.kv_exist.key_len = cpu_to_le32(cmd.key_length -1);
-            c.kv_exist.option = (option & 0xff);
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user *)cmd.key_addr;
-                meta_len = cmd.key_length;
-                } else {
-                memcpy(c.kv_exist.key, cmd.key, cmd.key_length);
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            break;
-        case nvme_cmd_kv_iter_req:
+			c.kv_exist.key_len = cpu_to_le32(cmd.key_length -1);
+		    c.kv_exist.option = (option & 0xff);
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user *)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_exist.key, cmd.key, cmd.key_length);
+			}
+			break;
+		case nvme_cmd_kv_iter_req:
             option = cpu_to_le32(cmd.cdw4);
             iter_handle = cpu_to_le32(cmd.cdw5);
             c.kv_iter_req.iter_handle = iter_handle & 0xff;
             c.kv_iter_req.option = option & 0xff;
             c.kv_iter_req.iter_val = cpu_to_le32(cmd.cdw12);
             c.kv_iter_req.iter_bitmask = cpu_to_le32(cmd.cdw13);
-            break;
-        case nvme_cmd_kv_iter_read:
+			break;
+		case nvme_cmd_kv_iter_read:
             option = cpu_to_le32(cmd.cdw4);
             iter_handle = cpu_to_le32(cmd.cdw5);
             c.kv_iter_read.iter_handle = iter_handle & 0xff;
             c.kv_iter_read.option = option & 0xff;
-            c.kv_iter_read.value_len = cpu_to_le32((cmd.data_length >> 2));
-            break;
-        default:
-            cmd.result = KVS_ERR_IO;
-            status = -EINVAL;
-            goto exit;
-}
+			c.kv_iter_read.value_len = cpu_to_le32((cmd.data_length >> 2));
+		break;
+		default:
+				cmd.result = KVS_ERR_IO;
+				status = -EINVAL;
+				goto exit;
+	}
 
 //    if (cmd.data_addr) {
 //        u32 *c_data = c.common.cdw2;
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.4.0-141-ubuntu-16_04/core.c b/PDK/driver/PCIe/kernel_driver/kernel_v4.4.0-141-ubuntu-16_04/core.c
index 80500a6..cba76c2 100644
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.4.0-141-ubuntu-16_04/core.c
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v4.4.0-141-ubuntu-16_04/core.c
@@ -1064,7 +1064,7 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
 			c.kv_store.offset = cpu_to_le32(cmd.cdw5);
 			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-                cmd.key_length < KVCMD_MIN_KEY_SIZE) {
+                    cmd.key_length < KVCMD_MIN_KEY_SIZE) {
 				cmd.result = KVS_ERR_VALUE;
 				status = -EINVAL;
 				goto exit;
@@ -1090,7 +1090,7 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
 			c.kv_retrieve.offset = cpu_to_le32(cmd.cdw5);
 			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-                cmd.key_length < KVCMD_MIN_KEY_SIZE) {
+                    cmd.key_length < KVCMD_MIN_KEY_SIZE) {
 				cmd.result = KVS_ERR_VALUE;
 				status = -EINVAL;
 				goto exit;
@@ -1116,7 +1116,7 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
 		case nvme_cmd_kv_delete:
             option = cpu_to_le32(cmd.cdw4);
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-                cmd.key_length < KVCMD_MIN_KEY_SIZE) {
+                    cmd.key_length < KVCMD_MIN_KEY_SIZE) {
 				cmd.result = KVS_ERR_VALUE;
 				status = -EINVAL;
 				goto exit;
@@ -1133,7 +1133,7 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
 		case nvme_cmd_kv_exist:
             option = cpu_to_le32(cmd.cdw4);
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-                cmd.key_length < KVCMD_MIN_KEY_SIZE) {
+                    cmd.key_length < KVCMD_MIN_KEY_SIZE) {
 				cmd.result = KVS_ERR_VALUE;
 				status = -EINVAL;
 				goto exit;
@@ -1162,11 +1162,11 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
             c.kv_iter_read.iter_handle = iter_handle & 0xff;
             c.kv_iter_read.option = option & 0xff;
 			c.kv_iter_read.value_len = cpu_to_le32((cmd.data_length >> 2));
-			break;
+		break;
 		default:
-			cmd.result = KVS_ERR_IO;
-			status = -EINVAL;
-			goto exit;
+				cmd.result = KVS_ERR_IO;
+				status = -EINVAL;
+				goto exit;
 	}
 
 //    if (cmd.data_addr) {
@@ -1500,7 +1500,6 @@ static int nvme_user_cmd(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
 
     if (cmd.timeout_ms)
         timeout = msecs_to_jiffies(cmd.timeout_ms);
-
 #if 1
     if (ns == NULL && cmd.opcode == nvme_admin_format_nvm)
         timeout = (120 * HZ);
@@ -2789,6 +2788,11 @@ int __init nvme_core_init(void)
 	else if (result > 0)
 		nvme_char_major = result;
 
+	nvme_class = class_create(THIS_MODULE, "nvme");
+	if (IS_ERR(nvme_class)) {
+		result = PTR_ERR(nvme_class);
+		goto unregister_chrdev;
+	}
 #if 1
 	result = aio_service_init();
 	if (result)
@@ -2798,22 +2802,14 @@ int __init nvme_core_init(void)
     if (result)
 		goto unregister_aio_service;
 #endif
-
-	nvme_class = class_create(THIS_MODULE, "nvme");
-	if (IS_ERR(nvme_class)) {
-		result = PTR_ERR(nvme_class);
-		goto unregister_aio_worker;
-	}
 	return 0;
 #if 1
-unregister_aio_worker:
-	aio_worker_exit();
-unregister_aio_service:
+ unregister_aio_service:
     aio_service_exit();
 #endif
-unregister_chrdev:
+ unregister_chrdev:
 	__unregister_chrdev(nvme_char_major, 0, NVME_MINORS, "nvme");
-unregister_blkdev:
+ unregister_blkdev:
 	unregister_blkdev(nvme_major, "nvme");
 	return result;
 }
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.4.0-98-ubuntu-16_04/core.c b/PDK/driver/PCIe/kernel_driver/kernel_v4.4.0-98-ubuntu-16_04/core.c
index 56e0d94..df8bae9 100644
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.4.0-98-ubuntu-16_04/core.c
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v4.4.0-98-ubuntu-16_04/core.c
@@ -1045,53 +1045,53 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
 //    }
 
 	switch(cmd.opcode) {
-        case nvme_cmd_kv_store:
+		case nvme_cmd_kv_store:
         case nvme_cmd_kv_append:
             option = cpu_to_le32(cmd.cdw4);
-            c.kv_store.offset = cpu_to_le32(cmd.cdw5);
-            /* validate key length */
+			c.kv_store.offset = cpu_to_le32(cmd.cdw5);
+			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-                cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
+                    cmd.key_length < KVCMD_MIN_KEY_SIZE) {
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            c.kv_store.key_len = cpu_to_le32(cmd.key_length -1); /* key len -1 */
+			c.kv_store.key_len = cpu_to_le32(cmd.key_length -1); /* key len -1 */
             c.kv_store.option = (option & 0xff);
             /* set value size */
             if (cmd.data_length % 4) {
-                c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2) + 1);
+			    c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2) + 1);
                 c.kv_store.invalid_byte = 4 - (cmd.data_length % 4);
             } else {
-                c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2));
-            }
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user*)cmd.key_addr;
-                meta_len = cmd.key_length;
-            } else {
-                memcpy(c.kv_store.key, cmd.key, cmd.key_length);
+			    c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2));
             }
-        break;
-        case nvme_cmd_kv_retrieve:
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user*)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_store.key, cmd.key, cmd.key_length);
+			}
+		break;
+		case nvme_cmd_kv_retrieve:
             option = cpu_to_le32(cmd.cdw4);
-            c.kv_retrieve.offset = cpu_to_le32(cmd.cdw5);
-            /* validate key length */
+			c.kv_retrieve.offset = cpu_to_le32(cmd.cdw5);
+			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-                cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
+                    cmd.key_length < KVCMD_MIN_KEY_SIZE) {
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            c.kv_retrieve.key_len = cpu_to_le32(cmd.key_length -1); /* key len - 1 */
+			c.kv_retrieve.key_len = cpu_to_le32(cmd.key_length -1); /* key len - 1 */
             c.kv_retrieve.option = (option & 0xff);
-            c.kv_retrieve.value_len = cpu_to_le32((cmd.data_length >> 2));
+			c.kv_retrieve.value_len = cpu_to_le32((cmd.data_length >> 2));
 
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user*)cmd.key_addr;
-                meta_len = cmd.key_length;
-            } else {
-                memcpy(c.kv_retrieve.key, cmd.key, cmd.key_length);
-            }
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user*)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_retrieve.key, cmd.key, cmd.key_length);
+			}
 #if 0
             if (aio && offset_in_page(cmd.data_addr)) {
                 /* aio does not support unaligned memory*/
@@ -1099,11 +1099,11 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
 				goto exit;
             }
 #endif
-        break;
-        case nvme_cmd_kv_delete:
+		break;
+		case nvme_cmd_kv_delete:
             option = cpu_to_le32(cmd.cdw4);
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-                cmd.key_length < KVCMD_MIN_KEY_SIZE) {
+                    cmd.key_length < KVCMD_MIN_KEY_SIZE) {
 				cmd.result = KVS_ERR_VALUE;
 				status = -EINVAL;
 				goto exit;
@@ -1119,7 +1119,7 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
 			break;
 		case nvme_cmd_kv_exist:
             option = cpu_to_le32(cmd.cdw4);
-            if (cmd.key_length > KVCMD_MAX_KEY_SIZE ||
+            if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
                     cmd.key_length < KVCMD_MIN_KEY_SIZE) {
 				cmd.result = KVS_ERR_VALUE;
 				status = -EINVAL;
@@ -1149,11 +1149,11 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
             c.kv_iter_read.iter_handle = iter_handle & 0xff;
             c.kv_iter_read.option = option & 0xff;
 			c.kv_iter_read.value_len = cpu_to_le32((cmd.data_length >> 2));
-			break;
+		break;
 		default:
-			cmd.result = KVS_ERR_IO;
-			status = -EINVAL;
-			goto exit;
+				cmd.result = KVS_ERR_IO;
+				status = -EINVAL;
+				goto exit;
 	}
 
 //    if (cmd.data_addr) {
@@ -2546,6 +2546,11 @@ int __init nvme_core_init(void)
 	else if (result > 0)
 		nvme_char_major = result;
 
+	nvme_class = class_create(THIS_MODULE, "nvme");
+	if (IS_ERR(nvme_class)) {
+		result = PTR_ERR(nvme_class);
+		goto unregister_chrdev;
+	}
 #if 1
 	result = aio_service_init();
 	if (result)
@@ -2555,22 +2560,14 @@ int __init nvme_core_init(void)
     if (result)
 		goto unregister_aio_service;
 #endif
-
-	nvme_class = class_create(THIS_MODULE, "nvme");
-	if (IS_ERR(nvme_class)) {
-		result = PTR_ERR(nvme_class);
-		goto unregister_aio_worker;
-	}
 	return 0;
 #if 1
-unregister_aio_worker:
-	aio_worker_exit();
-unregister_aio_service:
+ unregister_aio_service:
     aio_service_exit();
 #endif
-unregister_chrdev:
+ unregister_chrdev:
 	__unregister_chrdev(nvme_char_major, 0, NVME_MINORS, "nvme");
-unregister_blkdev:
+ unregister_blkdev:
 	unregister_blkdev(nvme_major, "nvme");
 	return result;
 }
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.4.0-98-ubuntu-16_04/linux_nvme.h b/PDK/driver/PCIe/kernel_driver/kernel_v4.4.0-98-ubuntu-16_04/linux_nvme.h
index c880bd5..52585dd 100644
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.4.0-98-ubuntu-16_04/linux_nvme.h
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v4.4.0-98-ubuntu-16_04/linux_nvme.h
@@ -1022,9 +1022,9 @@ struct nvme_command {
 		struct nvme_kv_store_command kv_store;
 		struct nvme_kv_retrieve_command kv_retrieve;
 		struct nvme_kv_delete_command kv_delete;
-		struct nvme_kv_append_command kv_append;
-		struct nvme_kv_iter_req_command kv_iter_req;
-		struct nvme_kv_iter_read_command kv_iter_read;
+        struct nvme_kv_append_command kv_append;
+        struct nvme_kv_iter_req_command kv_iter_req;
+        struct nvme_kv_iter_read_command kv_iter_read;
 		struct nvme_kv_exist_command kv_exist;
 #endif
 	};
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.9.5/core.c b/PDK/driver/PCIe/kernel_driver/kernel_v4.9.5/core.c
index 460e7ba..68cc549 100644
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.9.5/core.c
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v4.9.5/core.c
@@ -1626,53 +1626,53 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
 //    }
 
 	switch(cmd.opcode) {
-        case nvme_cmd_kv_store:
+		case nvme_cmd_kv_store:
         case nvme_cmd_kv_append:
             option = cpu_to_le32(cmd.cdw4);
-            c.kv_store.offset = cpu_to_le32(cmd.cdw5);
-            /* validate key length */
+			c.kv_store.offset = cpu_to_le32(cmd.cdw5);
+			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-                cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
+                    cmd.key_length < KVCMD_MIN_KEY_SIZE) {
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            c.kv_store.key_len = cpu_to_le32(cmd.key_length -1); /* key len -1 */
+			c.kv_store.key_len = cpu_to_le32(cmd.key_length -1); /* key len -1 */
             c.kv_store.option = (option & 0xff);
             /* set value size */
             if (cmd.data_length % 4) {
-                c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2) + 1);
+			    c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2) + 1);
                 c.kv_store.invalid_byte = 4 - (cmd.data_length % 4);
             } else {
-                c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2));
+			    c.kv_store.value_len = cpu_to_le32((cmd.data_length >> 2));
             }
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user*)cmd.key_addr;
-                meta_len = cmd.key_length;
-            } else {
-                memcpy(c.kv_store.key, cmd.key, cmd.key_length);
-            }
-        break;
-        case nvme_cmd_kv_retrieve:
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user*)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_store.key, cmd.key, cmd.key_length);
+			}
+		break;
+		case nvme_cmd_kv_retrieve:
             option = cpu_to_le32(cmd.cdw4);
-            c.kv_retrieve.offset = cpu_to_le32(cmd.cdw5);
-            /* validate key length */
+			c.kv_retrieve.offset = cpu_to_le32(cmd.cdw5);
+			/* validate key length */
             if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
-                cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
+                    cmd.key_length < KVCMD_MIN_KEY_SIZE) {
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            c.kv_retrieve.key_len = cpu_to_le32(cmd.key_length -1); /* key len - 1 */
+			c.kv_retrieve.key_len = cpu_to_le32(cmd.key_length -1); /* key len - 1 */
             c.kv_retrieve.option = (option & 0xff);
-            c.kv_retrieve.value_len = cpu_to_le32((cmd.data_length >> 2));
+			c.kv_retrieve.value_len = cpu_to_le32((cmd.data_length >> 2));
 
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user*)cmd.key_addr;
-                meta_len = cmd.key_length;
-            } else {
-                memcpy(c.kv_retrieve.key, cmd.key, cmd.key_length);
-            }
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user*)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_retrieve.key, cmd.key, cmd.key_length);
+			}
 #if 0
             if (aio && offset_in_page(cmd.data_addr)) {
                 /* aio does not support unaligned memory*/
@@ -1680,62 +1680,62 @@ static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl,
 				goto exit;
             }
 #endif
-            break;
-        case nvme_cmd_kv_delete:
+		break;
+		case nvme_cmd_kv_delete:
             option = cpu_to_le32(cmd.cdw4);
-            if (cmd.key_length > KVCMD_MAX_KEY_SIZE ||
-                cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
-            }
-            c.kv_delete.key_len = cpu_to_le32(cmd.key_length -1);
-            c.kv_delete.option = (option & 0xff);	
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user *)cmd.key_addr;
-                meta_len = cmd.key_length;
-            } else {
-                memcpy(c.kv_delete.key, cmd.key, cmd.key_length);
+            if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
+                    cmd.key_length < KVCMD_MIN_KEY_SIZE) {
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            break;
-        case nvme_cmd_kv_exist:
+			c.kv_delete.key_len = cpu_to_le32(cmd.key_length -1);
+		    c.kv_delete.option = (option & 0xff);	
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user *)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_delete.key, cmd.key, cmd.key_length);
+			}
+			break;
+		case nvme_cmd_kv_exist:
             option = cpu_to_le32(cmd.cdw4);
-            if (cmd.key_length > KVCMD_MAX_KEY_SIZE ||
-                cmd.key_length < KVCMD_MIN_KEY_SIZE) {
-                cmd.result = KVS_ERR_VALUE;
-                status = -EINVAL;
-                goto exit;
-            }
-            c.kv_exist.key_len = cpu_to_le32(cmd.key_length -1);
-            c.kv_exist.option = (option & 0xff);	
-            if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
-                metadata = (void __user *)cmd.key_addr;
-                meta_len = cmd.key_length;
-            } else {
-                memcpy(c.kv_exist.key, cmd.key, cmd.key_length);
+            if (cmd.key_length >  KVCMD_MAX_KEY_SIZE ||
+                    cmd.key_length < KVCMD_MIN_KEY_SIZE) {
+				cmd.result = KVS_ERR_VALUE;
+				status = -EINVAL;
+				goto exit;
             }
-            break;
+			c.kv_exist.key_len = cpu_to_le32(cmd.key_length -1);
+		    c.kv_exist.option = (option & 0xff);	
+			if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+				metadata = (void __user *)cmd.key_addr;
+				meta_len = cmd.key_length;
+			} else {
+				memcpy(c.kv_exist.key, cmd.key, cmd.key_length);
+			}
+			break;
 
-        case nvme_cmd_kv_iter_req:
+		case nvme_cmd_kv_iter_req:
             option = cpu_to_le32(cmd.cdw4);
             iter_handle = cpu_to_le32(cmd.cdw5);
             c.kv_iter_req.iter_handle = iter_handle & 0xff;
             c.kv_iter_req.option = option & 0xff;
             c.kv_iter_req.iter_val = cpu_to_le32(cmd.cdw12);
             c.kv_iter_req.iter_bitmask = cpu_to_le32(cmd.cdw13);
-            break;
-        case nvme_cmd_kv_iter_read:
+			break;
+		case nvme_cmd_kv_iter_read:
             option = cpu_to_le32(cmd.cdw4);
             iter_handle = cpu_to_le32(cmd.cdw5);
             c.kv_iter_read.iter_handle = iter_handle & 0xff;
             c.kv_iter_read.option = option & 0xff;
-            c.kv_iter_read.value_len = cpu_to_le32((cmd.data_length >> 2));
-            break;
-        default:
-            cmd.result = KVS_ERR_IO;
-            status = -EINVAL;
-            goto exit;
-        }
+			c.kv_iter_read.value_len = cpu_to_le32((cmd.data_length >> 2));
+		break;
+		default:
+				cmd.result = KVS_ERR_IO;
+				status = -EINVAL;
+				goto exit;
+	}
 
 //    if (cmd.data_addr) {
 //        u32 *c_data = c.common.cdw2;
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v4.9.5/linux_nvme.h b/PDK/driver/PCIe/kernel_driver/kernel_v4.9.5/linux_nvme.h
index 9ed9e12..f1853d5 100644
--- a/PDK/driver/PCIe/kernel_driver/kernel_v4.9.5/linux_nvme.h
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v4.9.5/linux_nvme.h
@@ -1036,8 +1036,8 @@ struct nvme_command {
 		struct nvme_kv_retrieve_command kv_retrieve;
 		struct nvme_kv_delete_command kv_delete;
         struct nvme_kv_append_command kv_append;
-		struct nvme_kv_iter_req_command kv_iter_req;
-		struct nvme_kv_iter_read_command kv_iter_read;
+        struct nvme_kv_iter_req_command kv_iter_req;
+        struct nvme_kv_iter_read_command kv_iter_read;
 		struct nvme_kv_exist_command kv_exist;
 	};
 };
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/Makefile b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/Makefile
new file mode 100644
index 0000000..31dc710
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/Makefile
@@ -0,0 +1,34 @@
+# SPDX-License-Identifier: GPL-2.0
+
+GIT_COMMIT := "$(shell git describe --abbrev=4 --always --tags)"
+
+ccflags-y				+= -I$(src) -g -DGITVER=\"$(GIT_COMMIT)\"
+
+obj-$(CONFIG_NVME_CORE)			+= nvme-core.o
+obj-$(CONFIG_BLK_DEV_NVME)		+= nvme.o
+obj-$(CONFIG_NVME_FABRICS)		+= nvme-fabrics.o
+obj-$(CONFIG_NVME_RDMA)			+= nvme-rdma.o
+obj-$(CONFIG_NVME_FC)			+= nvme-fc.o
+obj-$(CONFIG_NVME_TCP)			+= nvme-tcp.o
+
+nvme-core-y				:= core.o
+nvme-core-$(CONFIG_TRACING)		+= trace.o
+nvme-core-$(CONFIG_NVME_MULTIPATH)	+= multipath.o
+nvme-core-$(CONFIG_NVM)			+= lightnvm.o
+nvme-core-$(CONFIG_FAULT_INJECTION_DEBUG_FS)	+= fault_inject.o
+
+nvme-y					+= pci.o
+
+nvme-fabrics-y				+= fabrics.o
+
+nvme-rdma-y				+= rdma.o
+
+nvme-fc-y				+= fc.o
+
+nvme-tcp-y				+= tcp.o
+
+all:
+	make -C /lib/modules/`uname -r`/build M=`pwd` modules
+clean:
+	make -C /lib/modules/`uname -r`/build M=`pwd` clean
+
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/README b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/README
new file mode 100644
index 0000000..f075564
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/README
@@ -0,0 +1,24 @@
+This 5.1 kernel driver supports RDMA and TCP fabric
+
+Build Instructions
+==================
+copy ./linux_nvme.h as nvme.h to your 5.1 headers directory /usr/src/linux-headers-xxx or /usr/src/kernels/5.1.0/include/linux/
+	cp ./linux_nvme.h /usr/src/kernels/5.1.0/include/linux/nvme.h 
+copy ./linux_nvme_ioctl.h as nvme_ioctl.h to your 5.1 headers directory /usr/src/linux-headers-xxx or /usr/src/kernels/5.1.0/include/uapi/linux/
+	cp ./linux_nvme_ioctl.h /usr/src/kernels/5.1.0/include/uapi/linux/nvme_ioctl.h 
+make clean
+make VERBOSE=1
+
+Installation
+============
+nvme disconnect-all
+rmmod nvme-tcp;sudo rmmod nvme-fabrics;sudo rmmod nvme;sudo rmmod nvme-core
+
+insmod ./nvme-core.ko;insmod ./nvme-fabrics.ko;insmod ./nvme-tcp.ko;insmod ./nvme-rdma.ko
+
+Testing Sample
+==============
+Build and install latest nvme-cli
+nvme discover -t tcp -a 101.100.20.4(OSS target IP) -s 1023(OSS target port)
+nvme connect -t tcp -a 101.100.20.6(OSS target IP) -s 1023(OSS target port) -n nqn.2019-10.io:msl-ssg-mp06-data1(subsystem got discovered by above step)
+
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/core.c b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/core.c
new file mode 100644
index 0000000..e0dde8d
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/core.c
@@ -0,0 +1,5639 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * NVM Express device driver
+ * Copyright (c) 2011-2014, Intel Corporation.
+ */
+
+#include <linux/blkdev.h>
+#include <linux/blk-mq.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/hdreg.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/list_sort.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/pr.h>
+#include <linux/ptrace.h>
+#include <linux/nvme_ioctl.h>
+#include <linux/t10-pi.h>
+#include <linux/pm_qos.h>
+#include <asm/unaligned.h>
+
+#define CREATE_TRACE_POINTS
+#include "trace.h"
+
+#include "nvme.h"
+#include "fabrics.h"
+
+#define NVME_MINORS		(1U << MINORBITS)
+
+#include <linux/file.h>
+#include <linux/fdtable.h>
+#include <linux/eventfd.h>
+#include <linux/kref.h>
+#include <linux/bio.h>
+#include <linux/freezer.h>
+#include <linux/wait.h>
+#include <linux/kthread.h>
+#include <linux/task_io_accounting_ops.h>
+
+#include <linux/crash_dump.h>
+#include <linux/kernel.h>
+
+#define P2ROUNDUP(x, a)		(-(-(x) & -(__typeof__(x))(a)))
+#define P2NPHASE(x, a)		(-(x) & ((a) - 1))
+
+unsigned int admin_timeout = 60;
+module_param(admin_timeout, uint, 0644);
+MODULE_PARM_DESC(admin_timeout, "timeout in seconds for admin commands");
+EXPORT_SYMBOL_GPL(admin_timeout);
+
+unsigned int nvme_io_timeout = 30;
+module_param_named(io_timeout, nvme_io_timeout, uint, 0644);
+MODULE_PARM_DESC(io_timeout, "timeout in seconds for I/O");
+EXPORT_SYMBOL_GPL(nvme_io_timeout);
+
+static unsigned char shutdown_timeout = 5;
+module_param(shutdown_timeout, byte, 0644);
+MODULE_PARM_DESC(shutdown_timeout, "timeout in seconds for controller shutdown");
+
+static u8 nvme_max_retries = 5;
+module_param_named(max_retries, nvme_max_retries, byte, 0644);
+MODULE_PARM_DESC(max_retries, "max number of retries a command may have");
+
+static unsigned long default_ps_max_latency_us = 100000;
+module_param(default_ps_max_latency_us, ulong, 0644);
+MODULE_PARM_DESC(default_ps_max_latency_us,
+		 "max power saving latency for new devices; use PM QOS to change per device");
+
+static bool force_apst;
+module_param(force_apst, bool, 0644);
+MODULE_PARM_DESC(force_apst, "allow APST for newly enumerated devices even if quirked off");
+
+static bool streams;
+module_param(streams, bool, 0644);
+MODULE_PARM_DESC(streams, "turn on support for Streams write directives");
+
+unsigned int nvme_mem_align = 512;
+module_param_named(mem_align, nvme_mem_align, uint, 0644);
+MODULE_PARM_DESC(mem_align, "Userspace memory alignment");
+EXPORT_SYMBOL_GPL(nvme_mem_align);
+
+/*
+ * nvme_wq - hosts nvme related works that are not reset or delete
+ * nvme_reset_wq - hosts nvme reset works
+ * nvme_delete_wq - hosts nvme delete works
+ *
+ * nvme_wq will host works such are scan, aen handling, fw activation,
+ * keep-alive error recovery, periodic reconnects etc. nvme_reset_wq
+ * runs reset works which also flush works hosted on nvme_wq for
+ * serialization purposes. nvme_delete_wq host controller deletion
+ * works which flush reset works for serialization.
+ */
+struct workqueue_struct *nvme_wq;
+EXPORT_SYMBOL_GPL(nvme_wq);
+
+struct workqueue_struct *nvme_reset_wq;
+EXPORT_SYMBOL_GPL(nvme_reset_wq);
+
+struct workqueue_struct *nvme_delete_wq;
+EXPORT_SYMBOL_GPL(nvme_delete_wq);
+
+static DEFINE_IDA(nvme_subsystems_ida);
+static LIST_HEAD(nvme_subsystems);
+static DEFINE_MUTEX(nvme_subsystems_lock);
+
+static DEFINE_IDA(nvme_instance_ida);
+static dev_t nvme_chr_devt;
+static struct class *nvme_class;
+static struct class *nvme_subsys_class;
+
+static int nvme_revalidate_disk(struct gendisk *disk);
+static void nvme_put_subsystem(struct nvme_subsystem *subsys);
+static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
+					   unsigned nsid);
+
+/*
+ * PORT AIO Support logic from Kvepic.
+ */
+
+// AIO data structures
+static struct kmem_cache        *kaioctx_cachep = 0;
+static struct kmem_cache        *kaiocb_cachep = 0;
+static mempool_t *kaiocb_mempool = 0;
+static mempool_t *kaioctx_mempool = 0;
+
+static __u32 aio_context_id;
+
+#define AIOCTX_MAX 1024
+#define AIOCB_MAX (1024*64)
+
+typedef struct kv_map_data {
+	struct sg_table table;
+} kv_map_data_t;
+
+struct nvme_kaioctx
+{
+	struct nvme_aioctx	c_uctx;
+	struct eventfd_ctx	*c_ectx;
+	struct list_head	c_done_list;
+	spinlock_t		c_lock;
+	struct kref		c_ref;
+};
+
+static struct nvme_kaioctx **g_kaioctx_tb = NULL;
+static spinlock_t g_kaioctx_tb_spinlock;
+
+struct aio_user_ctx {
+	int nents;
+	int len;
+	//struct page ** pages;
+	struct scatterlist sg[];
+	//char data[1];
+	//char data[];
+};
+
+struct nvme_kaiocb {
+	struct list_head	list;
+	struct nvme_aioevent	event;
+	struct nvme_command	cmd;
+	struct gendisk		*disk;
+	unsigned long		start_time;
+	struct scatterlist	meta_sg;
+	bool			use_meta;
+	void			*meta;
+	struct aio_user_ctx	*user_ctx;
+	struct aio_user_ctx	*kernel_ctx;
+	struct request		*req;
+	struct bio		*bio;
+};
+
+/* context for aio worker.*/
+typedef struct aio_worker {
+	spinlock_t 		w_lock;
+	struct list_head 	w_todo_list;
+	wait_queue_head_t 	w_wq;
+	struct task_struct	*w_task;
+	int			w_cpu;
+} aio_worker_t;
+
+/* percpu aio worker context pointer */
+aio_worker_t * __percpu percpu_worker;
+
+static void remove_kaioctx(struct nvme_kaioctx * ctx)
+{
+	struct nvme_kaiocb *kcb;
+	struct list_head *pos, *next;
+	unsigned long flags;
+
+	if (ctx) {
+		spin_lock_irqsave(&ctx->c_lock, flags);
+		list_for_each_safe(pos, next, &ctx->c_done_list) {
+			kcb = list_entry(pos, struct nvme_kaiocb, list);
+			list_del(pos);
+			mempool_free(kcb, kaiocb_mempool);
+   	    	}
+        	spin_unlock_irqrestore(&ctx->c_lock, flags);
+
+		eventfd_ctx_put(ctx->c_ectx);
+		mempool_free(ctx, kaioctx_mempool);
+	}
+}
+
+static void cleanup_kaioctx(struct kref *kref)
+{
+	struct nvme_kaioctx *ctx = container_of(kref, struct nvme_kaioctx, c_ref);
+	remove_kaioctx(ctx);
+}
+
+static void ref_kaioctx(struct nvme_kaioctx *ctx)
+{
+	kref_get(&ctx->c_ref);
+}
+
+static void deref_kaioctx(struct nvme_kaioctx *ctx)
+{
+	kref_put(&ctx->c_ref, cleanup_kaioctx);
+}
+
+/* destroy memory pools
+ */
+static void destroy_aio_mempool(void)
+{
+    int i = 0;
+    if (g_kaioctx_tb) {
+        for (i = 0; i < AIOCTX_MAX; i++) {
+            if (g_kaioctx_tb[i]) {
+               remove_kaioctx(g_kaioctx_tb[i]);
+               g_kaioctx_tb[i] = NULL;
+            }
+        }
+        kfree(g_kaioctx_tb);
+        g_kaioctx_tb = NULL;
+    }
+    if (kaiocb_mempool)
+        mempool_destroy(kaiocb_mempool);
+    if (kaioctx_mempool)
+        mempool_destroy(kaioctx_mempool);
+    if (kaioctx_cachep)
+        kmem_cache_destroy(kaioctx_cachep);
+    if (kaiocb_cachep)
+        kmem_cache_destroy(kaiocb_cachep);
+}
+
+/* prepare basic data structures
+ * to support aio context and requests
+ */
+static int aio_service_init(void)
+{
+	size_t size = sizeof(struct nvme_kaioctx *) * AIOCTX_MAX;
+
+	g_kaioctx_tb = (struct nvme_kaioctx **)
+		           kmalloc(size, GFP_KERNEL);
+	if (!g_kaioctx_tb)
+		goto fail;
+
+	memset(g_kaioctx_tb, 0, size);
+	// slap allocator and memory pool
+	kaioctx_cachep = kmem_cache_create("nvme_kaioctx",
+			        	    sizeof(struct nvme_kaioctx), 0, 0, NULL);
+	if (!kaioctx_cachep)
+		goto fail;
+
+	kaiocb_cachep = kmem_cache_create("nvme_kaiocb",
+			                   sizeof(struct nvme_kaiocb), 0, 0, NULL);
+	if (!kaiocb_cachep)
+		goto fail;
+#if 0
+	nvme_cmd_cachep = kmem_cache_create("nvme_cmd_async",
+			                   sizeof(struct nvme_command), 64, 0, NULL);
+	if (!kaiocb_cachep)
+		goto fail;
+#endif
+	kaioctx_mempool = mempool_create_slab_pool(AIOCTX_MAX, kaioctx_cachep);
+	if (!kaioctx_mempool)
+		goto fail;
+
+	kaiocb_mempool = mempool_create_slab_pool(AIOCB_MAX, kaiocb_cachep);
+	if (!kaiocb_mempool)
+		goto fail;
+
+	// context id 0 is reserved for normal I/O operations (synchronous)
+	aio_context_id = 1;
+	spin_lock_init(&g_kaioctx_tb_spinlock);
+	return 0;
+
+fail:
+	destroy_aio_mempool();
+	return -ENOMEM;
+}
+
+/*
+ * release memory before exit
+ */
+static int aio_service_exit(void)
+{
+	destroy_aio_mempool();
+	printk(KERN_DEBUG "nvme-aio: unloaded\n");
+	return 0;
+}
+
+//TODO: Potential race: there should be find-and-ref
+// operation, so that while kcb are being handled and
+// inserted into the per-ctx list, ctx slot table should
+// remain unchanged so no kcb is dropped or inserted
+// into a newly created ctx
+static inline struct nvme_kaioctx *find_kaioctx(__u32 ctxid)
+{
+	struct nvme_kaioctx *tmp = NULL;
+
+	tmp = g_kaioctx_tb[ctxid];
+	if (tmp)
+		ref_kaioctx(tmp);
+	else
+		pr_err("%s: ctxid %u doesn't exist\n", __FUNCTION__, ctxid);
+
+	return tmp;
+}
+
+/*
+ * find an aio context with a given id
+ */
+static int set_aioctx_event(struct nvme_kaiocb *kcb)
+{
+	unsigned long flags;
+	struct nvme_kaioctx *ctx;
+	__u32 ctxid = kcb->event.ctxid;
+
+	ctx = find_kaioctx(ctxid);
+	if (ctx) {
+		spin_lock_irqsave(&ctx->c_lock, flags);
+		list_add_tail(&kcb->list, &ctx->c_done_list);
+		spin_unlock_irqrestore(&ctx->c_lock, flags);
+
+		//TODO
+		eventfd_signal(ctx->c_ectx, 1);
+		deref_kaioctx(ctx);
+
+		return 0;
+	}
+
+	return -1;
+}
+
+/*
+ * delete an aio context
+ * it will release any resources allocated for this context
+ */
+static int nvme_del_aioctx(struct nvme_aioctx __user *uctx )
+{
+	unsigned long flags;
+	struct nvme_kaioctx ctx, **kpp;
+
+	if (copy_from_user(&ctx, uctx, sizeof(struct nvme_aioctx)))
+		return -EFAULT;
+
+	kpp = &g_kaioctx_tb[ctx.c_uctx.ctxid];
+	spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
+	if (*kpp) {
+		deref_kaioctx(*kpp);
+		*kpp = NULL;
+    	}
+	spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
+
+	return 0;
+}
+
+/*
+ * set up an aio context
+ * allocate a new context with given parameters and prepare a eventfd_context
+ */
+static int nvme_set_aioctx(struct nvme_aioctx __user *uctx)
+{
+	int ret = 0;
+	int i = 0;
+	struct nvme_kaioctx *ctx;
+	struct fd efile;
+	struct eventfd_ctx *eventfd_ctx = NULL;
+	unsigned long flags;
+
+	if (unlikely(!capable(CAP_SYS_ADMIN)))
+		return -EACCES;
+
+	//ctx = kmem_cache_zalloc(kaioctx_cachep, GFP_KERNEL);
+	ctx = mempool_alloc(kaioctx_mempool, GFP_NOIO);
+	if (!ctx)
+		return -ENOMEM;
+
+	if (copy_from_user(ctx, uctx, sizeof(struct nvme_aioctx)))
+        	return -EFAULT;
+
+	efile = fdget(ctx->c_uctx.eventfd);
+	if (!efile.file) {
+		pr_err("nvme_set_aioctx: failed to get efile for efd %d.\n",
+			ctx->c_uctx.eventfd);
+		ret = -EBADF;
+		goto exit;
+	}
+
+	eventfd_ctx = eventfd_ctx_fileget(efile.file);
+	if (IS_ERR(eventfd_ctx)) {
+		pr_err("nvme_set_aioctx: failed to get eventfd_ctx for efd %d.\n",
+			ctx->c_uctx.eventfd);
+		ret = PTR_ERR(eventfd_ctx);
+		goto put_efile;
+	}
+
+	// set context id
+	spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
+	if (g_kaioctx_tb[aio_context_id]) {
+		for(i = 0; i < AIOCTX_MAX; i++) {
+			if(g_kaioctx_tb[i] == NULL) {
+				aio_context_id = i;
+				break;
+			}
+		}
+		if (i >= AIOCTX_MAX) {
+			pr_err("nvme_set_aioctx: too many aioctx open.\n");
+			ret = -EMFILE;
+			goto put_event_fd;
+		}
+	}
+
+	ctx->c_uctx.ctxid = aio_context_id++;
+    	if (aio_context_id == AIOCTX_MAX)
+        	aio_context_id = 0;
+	ctx->c_ectx = eventfd_ctx;
+	spin_lock_init(&ctx->c_lock);
+	INIT_LIST_HEAD(&ctx->c_done_list);
+	kref_init(&ctx->c_ref);
+	g_kaioctx_tb[ctx->c_uctx.ctxid] = ctx;
+	spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
+
+	if (copy_to_user(&uctx->ctxid, &ctx->c_uctx.ctxid, sizeof(ctx->c_uctx.ctxid))) {
+		pr_err("nvme_set_aioctx: failed to copy context id %d to user.\n",
+			ctx->c_uctx.ctxid);
+        	ret =  -EINVAL;
+		goto cleanup;
+	}
+
+	eventfd_ctx = NULL;
+	fdput(efile);
+
+	return 0;
+
+cleanup:
+	spin_lock_irqsave(&g_kaioctx_tb_spinlock, flags);
+	g_kaioctx_tb[ctx->c_uctx.ctxid - 1] = NULL;
+	spin_unlock_irqrestore(&g_kaioctx_tb_spinlock, flags);
+	mempool_free(ctx, kaiocb_mempool);
+put_event_fd:
+	eventfd_ctx_put(eventfd_ctx);
+put_efile:
+	fdput(efile);
+exit:
+	return ret;
+}
+
+static int nvme_get_ioevents(struct nvme_aioevents __user *uevents)
+{
+	struct list_head *pos, *next;
+	struct nvme_kaiocb *kcb;
+	struct nvme_kaioctx *ctx;
+	unsigned long flags, left;
+	LIST_HEAD(batch);
+
+	__u16 count =0;
+	__u16 nr = 0;
+	__u32 ctxid = 0;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	if (get_user(nr, &uevents->nr) < 0)
+		return -EINVAL;
+
+	if (nr == 0 || nr > 128)
+		return -EINVAL;
+
+	if (get_user(ctxid, &uevents->ctxid) < 0)
+		return -EINVAL;
+
+	ctx = find_kaioctx(ctxid);
+	if (ctx) {
+		spin_lock_irqsave(&ctx->c_lock, flags);
+		list_for_each_safe(pos, next, &ctx->c_done_list) {
+    	    		list_del_init(pos);
+            		list_add(pos, &batch);
+            		count++;
+			if (nr == count)
+				break;
+		}
+        	spin_unlock_irqrestore(&ctx->c_lock, flags);
+
+        	deref_kaioctx(ctx);
+		count = 0;
+        	list_for_each_safe(pos, next, &batch) {
+    	    		list_del(pos);
+    	    		kcb = list_entry(pos, struct nvme_kaiocb, list);
+			// TODO: can we do the sequence in one go, and is there benefits?
+            		left = copy_to_user(&uevents->events[count], &kcb->event,
+					    sizeof(struct nvme_aioevent));
+			if (left)
+				pr_debug("%s: failed to copy event back to user-land\n",
+					 __FUNCTION__);
+
+			/* To prevent double-insert during nvme_cancel_request() */
+			INIT_LIST_HEAD(&kcb->list);
+
+			mempool_free(kcb, kaiocb_mempool);
+			count++;
+        	}
+   	}
+
+	if (put_user(count, &uevents->nr) < 0)
+		return -EINVAL;
+
+	return 0;
+}
+
+static inline int my_fault(char __user *uaddr, int size)
+{
+	char __user *end = uaddr + size - 1;
+
+	if (unlikely(size == 0))
+		return 0;
+
+	if (unlikely(uaddr > end))
+		return -EFAULT;
+	/*
+	 * Writing zeroes into userspace here is OK, because we know that if
+	 * the zero gets there, we'll be overwriting it.
+	 */
+	do {
+		if (unlikely(__put_user(0, uaddr) != 0)) {
+			pr_err("failed to fault addr %px\n", uaddr);
+			return -EFAULT;
+		}
+		uaddr += PAGE_SIZE;
+	} while (uaddr <= end);
+
+	/* Check whether the range spilled into the next page. */
+	if (((unsigned long)uaddr & PAGE_MASK) ==
+			((unsigned long)end & PAGE_MASK))
+		return __put_user(0, end);
+
+	return 0;
+}
+
+static void
+kv_free_map_data(kv_map_data_t *kmd)
+{
+	kfree(kmd->table.sgl);
+	kfree(kmd);
+}
+
+static kv_map_data_t *
+kv_alloc_map_data(uint8_t *addr, size_t len, gfp_t gfp_mask)
+{
+	kv_map_data_t *kmd;
+	struct page **pages;
+	int mapped;
+	size_t off = offset_in_page(addr);
+	size_t pc = PFN_UP(off + len);
+	int i, ret = 0;
+	//int locked;
+
+	kmd = kzalloc(sizeof (kv_map_data_t), gfp_mask);
+	if (!kmd)
+		return ERR_PTR(-ENOMEM);
+
+	pages = kzalloc(pc * sizeof (struct page *), GFP_KERNEL);
+	if (!pages) {
+		kfree(kmd);
+		return ERR_PTR(-ENOMEM);
+	}
+#if 0
+	/* TODO: consider switching back to get_user_pages_fast */
+	locked = 1;
+   	down_read(&current->mm->mmap_sem);
+	mapped = get_user_pages_locked((unsigned long)addr, pc, FOLL_GET | FOLL_WRITE, pages, &locked);
+	if (locked)
+		up_read(&current->mm->mmap_sem);
+#endif
+
+	mapped = get_user_pages_fast((unsigned long)addr, pc, 1, pages);
+	if (mapped != pc) {
+		pr_err("get_user_pages_fast mapped %d, asked %zu\n", mapped, pc);
+		while (mapped > 0)
+			if (pages[--mapped])
+				put_page(pages[--mapped]);
+		kfree(pages);
+		kfree(kmd);
+		return ERR_PTR((mapped < 0) ? mapped : -EFAULT);
+	}
+#if 0
+	/* This kernel API seems unstable and produce len=0
+	 * for the last entry sometimes. Maybe try again later */
+	ret = sg_alloc_table_from_pages(&kmd->table, pages, pc,
+					off, len, gfp_mask);
+#endif
+
+	kmd->table.sgl = kzalloc(pc * sizeof (struct scatterlist), gfp_mask);
+	kmd->table.nents = pc;
+	sg_init_table(kmd->table.sgl, pc);
+	for (i=0; i<pc; i++) {
+		//pr_debug("%s: user page %px off 0x%zx len 0x%x\n",
+		//		__FUNCTION__, pages[i], off,
+		//		min_t(unsigned, len, PAGE_SIZE - off));
+		sg_set_page(kmd->table.sgl+i, pages[i],
+                	min_t(unsigned, len, PAGE_SIZE - off),
+			    	off);
+		len -= min_t(unsigned, len, PAGE_SIZE - off);
+		off = 0;
+	}
+	sg_mark_end(kmd->table.sgl + i - 1);
+
+	kfree(pages);
+
+	return (ret < 0) ? ERR_PTR(ret) : kmd;
+}
+
+
+static int kv_rq_unmap_key(struct bio *bio)
+{
+	int i, ret = 0;
+	struct bio_vec *bvec;
+	struct bvec_iter_all iter_all;
+
+	if (!bio)
+		return 0;
+
+	if (bio_flagged(bio, BIO_USER_MAPPED))
+		bio_for_each_segment_all(bvec, bio, i, iter_all)
+			put_page(bvec->bv_page);
+	else
+		bio_free_pages(bio);
+
+	bio_put(bio);
+
+	return ret;
+}
+
+static inline unsigned
+kv_bvec_value(struct bio *bio)
+{
+	char *p;
+	unsigned v;
+	struct bio_vec *vec = bio->bi_io_vec;
+	unsigned off = vec->bv_offset;
+
+	p = kmap(vec->bv_page);
+	v =  *(unsigned *)(p + off);
+	kunmap(vec->bv_page);
+
+	return v;
+}
+
+static inline void
+kv_kcb_print(struct nvme_kaiocb *kcb, const char *name)
+{
+	struct bio *bio = kcb->bio;
+	struct bio_vec *vec = bio->bi_io_vec;
+	unsigned off = vec->bv_offset;
+
+	pr_debug("%s(%u): tag %u off 0x%x value:0x%04x\n",
+		__FUNCTION__, smp_processor_id(),
+		kcb->req->tag, off, kv_bvec_value(bio));
+}
+
+static int kv_rq_uncopy_user(struct nvme_kaiocb *kcb)
+{
+	int i;
+	struct bio *bio = kcb->bio;
+	kv_map_data_t *kmd = bio->bi_private;
+	struct sg_table *sg_tbl = &kmd->table;
+
+	struct bvec_iter_all iter;
+	struct bio_vec *bvec;
+
+	size_t soff = 0, boff = 0;
+	size_t sg_len = 0, bv_len = 0;
+	size_t sg_ent = 0, copy, left;
+	void *kaddr, *from, *uaddr, *to;
+
+	if (kcb->event.status) {
+		for (; sg_ent < sg_tbl->nents; sg_ent++)
+			put_page(sg_page(&sg_tbl->sgl[sg_ent]));
+		kv_free_map_data(kmd);
+		return 0;
+	}
+
+	kv_kcb_print(kcb, __FUNCTION__);
+
+	bio_for_each_segment_all(bvec, bio, i, iter) {
+		kaddr = kmap(bvec->bv_page);
+		bv_len = bvec->bv_len;
+
+		while (sg_ent < sg_tbl->nents) {
+			if (!soff) {
+				uaddr = kmap(sg_page(&sg_tbl->sgl[sg_ent]));
+				sg_len = sg_tbl->sgl[sg_ent].length;
+			}
+
+			to = uaddr + sg_tbl->sgl[sg_ent].offset + soff;
+			from = kaddr + bvec->bv_offset + boff;
+			copy = min(sg_len - soff, bv_len - boff);
+
+			//pr_debug("Copy from kernel page %px to user page %px off 0x%zx len 0x%zx",
+			//	bvec->bv_page, sg_page(&sg_tbl->sgl[sg_ent]), to - uaddr, copy);
+
+			if (access_ok(to, copy)) {
+				/* raw_copy_to_user doesn't allow fault to happen while copy */
+				my_fault(to, copy);
+				kasan_check_read(from, copy);
+				left = raw_copy_to_user(to, from, copy);
+			} else {
+				pr_err("%s: addr %px + len %zx out of bound\n", __FUNCTION__, to, copy);
+				return -EFAULT;
+			}
+
+			if (left) {
+				pr_err("%s: asked to copy %zu, left %zu\n", __FUNCTION__, copy, left);
+				return -EFAULT;
+			}
+
+			copy -= left;
+			soff += copy;
+			boff += copy;
+
+			if (soff == sg_len) {
+				kunmap(sg_page(&sg_tbl->sgl[sg_ent]));
+				/* get_user_pages_fast */
+				put_page(sg_page(&sg_tbl->sgl[sg_ent]));
+				soff = 0;
+				sg_ent++;
+			}
+
+			if (boff == bv_len)
+				break;
+		}
+
+		boff = 0;
+		kunmap(bvec->bv_page);
+	}
+
+	kv_free_map_data(kmd);
+	return 0;
+}
+
+/* blk_rq_unmap_user can only be called within the original user context */
+static int kv_rq_unmap_user(struct nvme_kaiocb *kcb)
+{
+	int ret = 0;
+	struct bio *bio = kcb->bio;
+
+	if (bio_flagged(bio, BIO_USER_MAPPED)) {
+		//blk_rq_unmap_user(bio);
+		struct bio_vec *bvec;
+		int i;
+		struct bvec_iter_all iter_all;
+
+		/*
+	 	 * make sure we dirty pages we wrote to
+	 	 */
+		bio_for_each_segment_all(bvec, bio, i, iter_all) {
+			if (bio_data_dir(bio) == READ)
+				set_page_dirty_lock(bvec->bv_page);
+
+			put_page(bvec->bv_page);
+		}
+	} else {
+		if (bio_data_dir(bio) == READ)
+			 ret = kv_rq_uncopy_user(kcb);
+
+		bio_free_pages(bio);
+	}
+
+	bio_put(bio);
+
+	return ret;
+}
+
+static int kv_iter2bio(struct bio *bio, struct iov_iter *iter)
+{
+	int i;
+	struct bvec_iter_all iter_all;
+	struct bio_vec *bvec;
+
+	bio_for_each_segment_all(bvec, bio, i, iter_all) {
+		ssize_t ret;
+
+		ret = copy_page_from_iter(bvec->bv_page,
+					  bvec->bv_offset,
+					  bvec->bv_len,
+					  iter);
+
+		if (!iov_iter_count(iter))
+			break;
+
+		if (ret < bvec->bv_len)
+			return -EFAULT;
+	}
+
+	return 0;
+}
+
+static int kv_bio_map_key(struct request *rq, struct iov_iter *iter,
+			    	bool copy, gfp_t gfp_mask)
+{
+	int ret, j, i = 0;
+	struct request_queue *q = rq->q;
+	struct bio *bio;
+	uint8_t __user *addr;
+	size_t len, off, left;
+	size_t pagec;
+	struct page *page_table[2];
+	struct page **pages = (struct page **)page_table;
+
+	addr = iter->iov->iov_base + iter->iov_offset;
+	len = iter->count;
+	off = copy ? 0 : offset_in_page(addr);
+	pagec = PFN_UP(off + len);
+
+	/* Double check */
+	BUG_ON(len > KVCMD_MAX_KEY_SIZE);
+	BUG_ON(pagec > 2);
+
+	bio = bio_kmalloc(gfp_mask, pagec);
+	if (!bio)
+		return -ENOMEM;
+
+	if (copy) {
+		pages[0] = alloc_page(q->bounce_gfp);
+		if (!pages[0]) {
+			ret = -ENOMEM;
+			goto ERR_FREE_PAGE;
+		}
+	} else {
+		int mapped;
+
+		bio_set_flag(bio, BIO_USER_MAPPED);
+
+		mapped = get_user_pages_fast((unsigned long)addr, pagec, false, pages);
+		if (mapped != pagec) {
+			pr_err("get_user_pages_fast mapped %d, asked %zu\n", mapped, pagec);
+			/* TODO: maybe free pages actually allocated */
+			ret = (mapped < 0) ? mapped : (i=mapped, -ENOMEM);
+			goto ERR_FREE_PAGE;
+		}
+		i = pagec;
+	}
+
+	left = len;
+	for (j=0; j<pagec; j++) {
+		size_t l = min_t(unsigned, left, PAGE_SIZE - off);
+
+		if (bio_add_pc_page(q, bio, pages[j],
+				    l, off) < l) {
+			pr_err("bio_add_pc_page error\n");
+			ret = -EFAULT;
+			goto ERR_FREE_PAGE;
+		}
+
+		//bio_add_page(bio, pages[j], l, off);
+
+		left -= l;
+		off = 0;
+	}
+
+	if (copy) {
+		ret = kv_iter2bio(bio, iter);
+		if (ret)
+			goto ERR_FREE_PAGE;
+	} else {
+		/* TODO: Revisit */
+		iov_iter_advance(iter, bio->bi_iter.bi_size);
+	}
+
+	bio->bi_opf &= ~REQ_OP_MASK;
+	bio->bi_opf |= REQ_OP_DRV_OUT; /* Key is always written out to target */
+
+	kv_req(rq)->key_bio = bio;
+
+	pr_debug("%s: rq %px key bio %px sz %u\n",
+		 __FUNCTION__, rq, bio, bio->bi_iter.bi_size);
+
+	return 0;
+
+ERR_FREE_PAGE:
+	if (copy)
+		__free_page(pages[0]);
+	else
+		while (i-- > 0)
+			put_page(pages[i]);
+
+	return ret;
+}
+
+
+static int kv_bio_map_large(struct request *rq, struct iov_iter *iter,
+			    bool read, bool copy, gfp_t gfp_mask)
+{
+	int ret, i, j;
+	kv_map_data_t *kmd = NULL;
+	struct request_queue *q = rq->q;
+	size_t pagec = 0;
+	struct page **pages = NULL;
+	struct bio *bio, *orig_bio;
+	uint8_t *addr;
+	size_t len, off, left;
+
+MORE_BIO:
+	addr = iter->iov->iov_base + iter->iov_offset;
+	len = iter->count;
+	off = (copy) ? 0 : offset_in_page(addr);
+	pagec = PFN_UP(off + len);
+
+	pr_debug("%s: cid %u addr %px len 0x%zx pagec %zu\n",
+		__FUNCTION__, rq->tag, addr, len, pagec);
+
+#if 0
+	// There are some background to read, but the max is ignored now
+	// https://lwn.net/Articles/758397/
+	if (pagec > BIO_MAX_PAGES) {
+		pagec = BIO_MAX_PAGES;
+		len = pagec * PAGE_SIZE - off;
+	}
+#endif
+	bio = bio_kmalloc(gfp_mask, pagec);
+	if (!bio)
+		return -ENOMEM;
+
+	/* TODO */
+	pages = kzalloc(pagec * sizeof (struct page *), GFP_KERNEL);
+
+	if (copy) {
+		left = len;
+		i = 0;
+		while (left) {
+			unsigned bytes = min_t(unsigned, left, PAGE_SIZE);
+
+			pages[i] = alloc_page(q->bounce_gfp | gfp_mask);
+			if (!pages[i]) {
+				ret = -ENOMEM;
+				goto ERR_FREE_PAGE;
+			}
+
+			left -= bytes;
+			++i;
+		}
+
+		if (read) {
+			kmd = kv_alloc_map_data(addr, len, gfp_mask);
+			if (IS_ERR(kmd)) {
+				ret = PTR_ERR(kmd);
+				kmd = NULL;
+				goto ERR_FREE_PAGE;
+			}
+		}
+	} else {
+		int mapped;
+
+		bio_set_flag(bio, BIO_USER_MAPPED);
+
+		mapped = get_user_pages_fast((unsigned long)addr, pagec, read, pages);
+		if (mapped != pagec) {
+			pr_err("get_user_pages_fast mapped %d, asked %zu\n", mapped, pagec);
+			/* TODO: maybe free pages actually allocated */
+			ret = (mapped < 0) ? mapped : (i=mapped, -ENOMEM);
+			goto ERR_FREE_PAGE;
+		}
+		i = pagec;
+	}
+
+	pr_debug("%s: queue_max_hw_sectors 0x%x", __FUNCTION__, queue_max_hw_sectors(q));
+	left = len;
+	for (j=0; j<pagec; j++) {
+		size_t l = min_t(unsigned, left, PAGE_SIZE - off);
+
+		if (bio_add_pc_page(q, bio, pages[j],
+				    l, off) < l) {
+			pr_err("%s: bio_add_pc_page page %u error\n", __FUNCTION__, j);
+			ret = -EFAULT;
+			goto ERR_FREE_PAGE;
+		}
+
+		left -= l;
+		off = 0;
+	}
+
+	if (copy && !read) {
+		ret = kv_iter2bio(bio, iter);
+		if (ret)
+			goto ERR_FREE_PAGE;
+	} else {
+		//if (copy)
+		//	zero_fill_bio(bio);
+		/* TODO: Revisit */
+		iov_iter_advance(iter, bio->bi_iter.bi_size);
+	}
+
+	bio->bi_private = kmd;
+	bio->bi_opf &= ~REQ_OP_MASK;
+	bio->bi_opf |= req_op(rq);
+
+	orig_bio = bio;
+
+	/*
+	 * We link the bounce buffer in and could have to traverse it
+	 * later so we have to get a ref to prevent it from being freed
+	 */
+	ret = blk_rq_append_bio(rq, &bio);
+	if (ret) {
+		pr_err("%s: blk_rq_append_bio failed ret %d\n", __FUNCTION__, ret);
+		bio = orig_bio;
+		goto ERR_FREE_PAGE;
+	}
+
+	if (iter->count)
+		goto MORE_BIO;
+
+	kfree(pages);
+
+	pr_debug("%s: rq %px bio %px __bi_cnt %d data_len %u %s\n",
+		__FUNCTION__, rq, orig_bio, bio->__bi_cnt.counter, rq->__data_len,
+		copy ? "memcpy" : "user pinned");
+
+	return 0;
+
+ERR_FREE_PAGE:
+	if (copy) {
+		if (kmd)
+			kv_free_map_data(kmd);
+		while (i-- > 0)
+			__free_page(pages[i]);
+	} else {
+		while (i-- > 0)
+			put_page(pages[i]);
+	}
+
+	kfree(pages);
+	bio_put(bio);
+
+	return ret;
+}
+
+static int kv_rq_map_user_iov(struct request *req, struct iov_iter *iter,
+			      bool read, bool key,  bool copy, gfp_t gfp_mask)
+{
+	int ret = 0;
+
+//	if (len <= (PAGE_SIZE >> 2))
+//		ret = kv_bio_map_small(req, iter, read, copy, gfp_mask);
+//	else
+
+	if (key)
+		ret = kv_bio_map_key(req, iter, copy, gfp_mask | GFP_DMA);
+	else
+		ret = kv_bio_map_large(req, iter, read, copy, gfp_mask);
+
+	/* Assume iov_iter_advance has updated iov_len in the
+	 * last duration */
+	if (unlikely(!ret))
+		BUG_ON(iter->count);
+
+	return ret;
+}
+
+inline
+static bool kv_buf_aligned(struct request *rq, struct iov_iter *i)
+{
+	struct request_queue __maybe_unused *q = rq->q;
+	const struct iovec *iov = i->iov;
+	//unsigned long align	= queue_dma_alignment(q);
+	unsigned long align	= nvme_mem_align - 1;
+
+	return (uintptr_t)iov->iov_base & align;
+}
+
+static int kv_rq_map_user(struct request *rq, struct iov_iter *i,
+			  bool read, bool key, gfp_t gfp_mask)
+{
+	int ret = 0;
+	bool copy = true;
+
+	BUG_ON(!rq || !i);
+
+	if (i->count == 0)
+		return ret;
+
+	copy = kv_buf_aligned(rq, i);
+	ret = kv_rq_map_user_iov(rq, i, read, key, copy, gfp_mask);
+
+	return ret;
+}
+
+static void kv_complete_ccb(struct nvme_kaiocb *kcb)
+{
+	int res;
+	struct request *req = kcb->req;
+	struct bio *bio = kcb->bio;
+
+	if (likely(bio)) {
+		res = kv_rq_unmap_user(kcb);
+		if (res) {
+			pr_err("kv_rq_unmap_user failed with err %d\n",
+			       res);
+			kcb->event.status = res;
+		}
+	}
+
+	kv_rq_unmap_key(kv_req(req)->key_bio);
+	/* TODO */
+	kv_req(req)->ptr = NULL;
+
+    	if (kcb->use_meta)
+        	put_page(sg_page(&kcb->meta_sg));
+    	if (kcb->meta)
+		kfree(kcb->meta);
+	if (kcb->user_ctx) {
+		kfree(kcb->user_ctx);
+        	if (is_kv_store_cmd(kcb->cmd.common.opcode) ||
+		    is_kv_append_cmd(kcb->cmd.common.opcode))
+            		generic_end_io_acct(req->q, WRITE, &kcb->disk->part0,
+					    kcb->start_time);
+        	else
+            		generic_end_io_acct(req->q, READ, &kcb->disk->part0,
+					    kcb->start_time);
+	}
+
+	kfree(kv_req(req)->req.cmd);
+	blk_mq_free_request(req);
+
+	if (set_aioctx_event(kcb))
+		mempool_free(kcb, kaiocb_mempool);
+}
+
+static int kv_aio_worker_fn(void *arg)
+{
+	unsigned long flags;
+	struct list_head *pos, *next, batch;
+	aio_worker_t *w = (aio_worker_t *) arg;
+	struct nvme_kaiocb *kcb;
+
+	pr_debug("start aio worker %u\n", w->w_cpu);
+
+	while (!kthread_should_stop() || !list_empty(&w->w_todo_list)) {
+
+		long ret;
+            	ret = wait_event_interruptible_timeout(w->w_wq,
+               					       !list_empty(&w->w_todo_list),
+						       HZ/10);
+		if (!ret)
+			continue;
+
+		INIT_LIST_HEAD(&batch);
+
+		spin_lock_irqsave(&w->w_lock, flags);
+		list_splice(&w->w_todo_list, &batch);
+		INIT_LIST_HEAD(&w->w_todo_list);
+		spin_unlock_irqrestore(&w->w_lock, flags);
+
+		list_for_each_safe(pos, next, &batch) {
+        		kcb = list_entry(pos, struct nvme_kaiocb, list);
+               		list_del_init(pos);
+			kv_complete_ccb(kcb);
+        	}
+	}
+
+    	return 0;
+}
+
+static void kv_cmd_done(struct request *req, blk_status_t status)
+{
+	unsigned long flags;
+	struct nvme_kaiocb *kcb = req->end_io_data;
+	int cpu = smp_processor_id();
+	aio_worker_t *w = per_cpu_ptr(percpu_worker, cpu);
+
+	BUG_ON(kcb->req != req);
+
+	spin_lock_irqsave(&w->w_lock, flags);
+	if (unlikely(!list_empty(&kcb->list))) {
+		pr_err("%s: req %px already on list\n", __FUNCTION__, kcb->req);
+	} else {
+		pr_debug("%s:kcb %p status %x result %x\n", __FUNCTION__,
+			kcb, kcb->event.status, kcb->event.result);
+
+		kcb->event.result = nvme_req(req)->result.u32;
+		kcb->event.status = nvme_req(req)->status;
+
+		list_add_tail(&kcb->list, &w->w_todo_list);
+	}
+	spin_unlock_irqrestore(&w->w_lock, flags);
+
+	wake_up(&w->w_wq);
+
+	return;
+}
+
+
+static void kv_aio_worker_fini(void)
+{
+	int i = 0;
+	aio_worker_t *w = NULL;
+
+	for_each_online_cpu(i) {
+		w = per_cpu_ptr(percpu_worker, i);
+		if (w->w_task)
+			kthread_stop(w->w_task);
+	}
+
+	free_percpu(percpu_worker);
+
+	return;
+}
+
+
+static int kv_aio_worker_init(void)
+{
+	int i = 0;
+	aio_worker_t *w = NULL;
+
+	percpu_worker = alloc_percpu(aio_worker_t);
+	if (!percpu_worker) {
+		pr_err("Failed to alloc percpu_worker!\n");
+		return -ENOMEM;
+    	}
+
+	for_each_online_cpu(i) {
+		w = per_cpu_ptr(percpu_worker, i);
+		w->w_cpu = i;
+		spin_lock_init(&w->w_lock);
+		INIT_LIST_HEAD(&w->w_todo_list);
+		init_waitqueue_head(&w->w_wq);
+		w->w_task = kthread_create_on_node(kv_aio_worker_fn,
+					   	 w, cpu_to_node(i),
+						 "aio_worker/%u", i);
+		if (!w->w_task)
+        		goto ERR_THREAD;
+
+		kthread_bind(w->w_task, i);
+		wake_up_process(w->w_task);
+	}
+
+	return 0;
+
+ERR_THREAD:
+	kv_aio_worker_fini();
+	return -EINVAL;
+}
+
+static void nvme_set_queue_dying(struct nvme_ns *ns)
+{
+	/*
+	 * Revalidating a dead namespace sets capacity to 0. This will end
+	 * buffered writers dirtying pages that can't be synced.
+	 */
+	if (!ns->disk || test_and_set_bit(NVME_NS_DEAD, &ns->flags))
+		return;
+	revalidate_disk(ns->disk);
+	blk_set_queue_dying(ns->queue);
+	/* Forcibly unquiesce queues to avoid blocking dispatch */
+	blk_mq_unquiesce_queue(ns->queue);
+}
+
+static void nvme_queue_scan(struct nvme_ctrl *ctrl)
+{
+	/*
+	 * Only new queue scan work when admin and IO queues are both alive
+	 */
+	if (ctrl->state == NVME_CTRL_LIVE) {
+//		dev_info(ctrl->dev, "%s\n", __FUNCTION__);
+//		dump_stack();
+		queue_work(nvme_wq, &ctrl->scan_work);
+	}
+}
+
+int nvme_reset_ctrl(struct nvme_ctrl *ctrl)
+{
+	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING))
+		return -EBUSY;
+	if (!queue_work(nvme_reset_wq, &ctrl->reset_work))
+		return -EBUSY;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(nvme_reset_ctrl);
+
+int nvme_reset_ctrl_sync(struct nvme_ctrl *ctrl)
+{
+	int ret;
+
+	ret = nvme_reset_ctrl(ctrl);
+	if (!ret) {
+		flush_work(&ctrl->reset_work);
+		if (ctrl->state != NVME_CTRL_LIVE &&
+		    ctrl->state != NVME_CTRL_ADMIN_ONLY)
+			ret = -ENETRESET;
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvme_reset_ctrl_sync);
+
+static void nvme_do_delete_ctrl(struct nvme_ctrl *ctrl)
+{
+	dev_info(ctrl->device,
+		 "Removing ctrl: NQN \"%s\"\n", ctrl->opts->subsysnqn);
+
+	flush_work(&ctrl->reset_work);
+	nvme_stop_ctrl(ctrl);
+	nvme_remove_namespaces(ctrl);
+	ctrl->ops->delete_ctrl(ctrl);
+	nvme_uninit_ctrl(ctrl);
+	nvme_put_ctrl(ctrl);
+}
+
+static void nvme_delete_ctrl_work(struct work_struct *work)
+{
+	struct nvme_ctrl *ctrl =
+		container_of(work, struct nvme_ctrl, delete_work);
+
+	nvme_do_delete_ctrl(ctrl);
+}
+
+int nvme_delete_ctrl(struct nvme_ctrl *ctrl)
+{
+	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_DELETING))
+		return -EBUSY;
+	if (!queue_work(nvme_delete_wq, &ctrl->delete_work))
+		return -EBUSY;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(nvme_delete_ctrl);
+
+static int nvme_delete_ctrl_sync(struct nvme_ctrl *ctrl)
+{
+	int ret = 0;
+
+	/*
+	 * Keep a reference until nvme_do_delete_ctrl() complete,
+	 * since ->delete_ctrl can free the controller.
+	 */
+	nvme_get_ctrl(ctrl);
+	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_DELETING))
+		ret = -EBUSY;
+	if (!ret)
+		nvme_do_delete_ctrl(ctrl);
+	nvme_put_ctrl(ctrl);
+	return ret;
+}
+
+static inline bool nvme_ns_has_pi(struct nvme_ns *ns)
+{
+	return ns->pi_type && ns->ms == sizeof(struct t10_pi_tuple);
+}
+
+static blk_status_t nvme_error_status(struct request *req)
+{
+	switch (nvme_req(req)->status & 0x7ff) {
+	case NVME_SC_SUCCESS:
+		return BLK_STS_OK;
+	case NVME_SC_CAP_EXCEEDED:
+		return BLK_STS_NOSPC;
+	case NVME_SC_LBA_RANGE:
+		return BLK_STS_TARGET;
+	case NVME_SC_BAD_ATTRIBUTES:
+	case NVME_SC_ONCS_NOT_SUPPORTED:
+	case NVME_SC_INVALID_OPCODE:
+	case NVME_SC_INVALID_FIELD:
+	case NVME_SC_INVALID_NS:
+		return BLK_STS_NOTSUPP;
+	case NVME_SC_WRITE_FAULT:
+	case NVME_SC_READ_ERROR:
+	case NVME_SC_UNWRITTEN_BLOCK:
+	case NVME_SC_ACCESS_DENIED:
+	case NVME_SC_READ_ONLY:
+	case NVME_SC_COMPARE_FAILED:
+		return BLK_STS_MEDIUM;
+	case NVME_SC_GUARD_CHECK:
+	case NVME_SC_APPTAG_CHECK:
+	case NVME_SC_REFTAG_CHECK:
+	case NVME_SC_INVALID_PI:
+		return BLK_STS_PROTECTION;
+	case NVME_SC_RESERVATION_CONFLICT:
+		return BLK_STS_NEXUS;
+	default:
+		return BLK_STS_IOERR;
+	}
+}
+
+static inline bool nvme_req_needs_retry(struct request *req)
+{
+	if (blk_noretry_request(req))
+		return false;
+	if (nvme_req(req)->status & NVME_SC_DNR)
+		return false;
+	if (nvme_req(req)->retries >= nvme_max_retries)
+		return false;
+	return true;
+}
+
+static void nvme_retry_req(struct request *req)
+{
+	struct nvme_ns *ns = req->q->queuedata;
+	unsigned long delay = 0;
+	u16 crd;
+
+	/* The mask and shift result must be <= 3 */
+	crd = (nvme_req(req)->status & NVME_SC_CRD) >> 11;
+	if (ns && crd)
+		delay = ns->ctrl->crdt[crd - 1] * 100;
+
+	nvme_req(req)->retries++;
+	blk_mq_requeue_request(req, false);
+	blk_mq_delay_kick_requeue_list(req->q, delay);
+}
+
+void nvme_complete_rq(struct request *req)
+{
+
+	blk_status_t status = nvme_error_status(req);
+
+	pr_debug("%s: req(%px) status 0x%x\n", __FUNCTION__, req, nvme_req(req)->status);
+
+	trace_nvme_complete_rq(req);
+
+	if (nvme_req(req)->ctrl->kas)
+		nvme_req(req)->ctrl->comp_seen = true;
+
+	if (unlikely(status != BLK_STS_OK && nvme_req_needs_retry(req))) {
+		pr_err("Retry...\n");
+		if ((req->cmd_flags & REQ_NVME_MPATH) &&
+		    blk_path_error(status)) {
+			nvme_failover_req(req);
+			return;
+		}
+
+		if (!blk_queue_dying(req->q)) {
+			nvme_retry_req(req);
+			return;
+		}
+	}
+#if 0
+	if (nvme_req(req)->cmd)
+		pr_debug("%s: cid %u opcode %x result %llu\n", __FUNCTION__,
+			 nvme_req(req)->cmd->common.command_id,
+			 nvme_req(req)->cmd->common.opcode,
+			 nvme_req(req)->result.u64);
+#endif
+	blk_mq_end_request(req, status);
+}
+EXPORT_SYMBOL_GPL(nvme_complete_rq);
+
+bool nvme_cancel_request(struct request *req, void *data, bool reserved)
+{
+	dev_dbg_ratelimited(((struct nvme_ctrl *) data)->device,
+				"Cancelling I/O %d", req->tag);
+
+	pr_err("%s: req %px\n", __FUNCTION__, req);
+
+	nvme_req(req)->status = NVME_SC_ABORT_REQ;
+	blk_mq_complete_request_sync(req);
+	return true;
+}
+EXPORT_SYMBOL_GPL(nvme_cancel_request);
+
+bool nvme_change_ctrl_state(struct nvme_ctrl *ctrl,
+		enum nvme_ctrl_state new_state)
+{
+	enum nvme_ctrl_state old_state;
+	unsigned long flags;
+	bool changed = false;
+
+	pr_debug("%s: new state %u\n", __FUNCTION__, new_state);
+	spin_lock_irqsave(&ctrl->lock, flags);
+
+	old_state = ctrl->state;
+	switch (new_state) {
+	case NVME_CTRL_ADMIN_ONLY:
+		switch (old_state) {
+		case NVME_CTRL_CONNECTING:
+			changed = true;
+			/* FALLTHRU */
+		default:
+			break;
+		}
+		break;
+	case NVME_CTRL_LIVE:
+		switch (old_state) {
+		case NVME_CTRL_NEW:
+		case NVME_CTRL_RESETTING:
+		case NVME_CTRL_CONNECTING:
+			changed = true;
+			/* FALLTHRU */
+		default:
+			break;
+		}
+		break;
+	case NVME_CTRL_RESETTING:
+		switch (old_state) {
+		case NVME_CTRL_NEW:
+		case NVME_CTRL_LIVE:
+		case NVME_CTRL_ADMIN_ONLY:
+			changed = true;
+			/* FALLTHRU */
+		default:
+			break;
+		}
+		break;
+	case NVME_CTRL_CONNECTING:
+		switch (old_state) {
+		case NVME_CTRL_NEW:
+		case NVME_CTRL_RESETTING:
+			changed = true;
+			/* FALLTHRU */
+		default:
+			break;
+		}
+		break;
+	case NVME_CTRL_DELETING:
+		switch (old_state) {
+		case NVME_CTRL_LIVE:
+		case NVME_CTRL_ADMIN_ONLY:
+		case NVME_CTRL_RESETTING:
+		case NVME_CTRL_CONNECTING:
+			changed = true;
+			/* FALLTHRU */
+		default:
+			break;
+		}
+		break;
+	case NVME_CTRL_DEAD:
+		switch (old_state) {
+		case NVME_CTRL_DELETING:
+			changed = true;
+			/* FALLTHRU */
+		default:
+			break;
+		}
+		break;
+	default:
+		break;
+	}
+
+	if (changed)
+		ctrl->state = new_state;
+
+	spin_unlock_irqrestore(&ctrl->lock, flags);
+	if (changed && ctrl->state == NVME_CTRL_LIVE)
+		nvme_kick_requeue_lists(ctrl);
+	return changed;
+}
+EXPORT_SYMBOL_GPL(nvme_change_ctrl_state);
+
+static void nvme_free_ns_head(struct kref *ref)
+{
+	struct nvme_ns_head *head =
+		container_of(ref, struct nvme_ns_head, ref);
+
+	nvme_mpath_remove_disk(head);
+	ida_simple_remove(&head->subsys->ns_ida, head->instance);
+	list_del_init(&head->entry);
+	cleanup_srcu_struct_quiesced(&head->srcu);
+	nvme_put_subsystem(head->subsys);
+	kfree(head);
+}
+
+static void nvme_put_ns_head(struct nvme_ns_head *head)
+{
+	kref_put(&head->ref, nvme_free_ns_head);
+}
+
+static void nvme_free_ns(struct kref *kref)
+{
+	struct nvme_ns *ns = container_of(kref, struct nvme_ns, kref);
+
+	if (ns->ndev)
+		nvme_nvm_unregister(ns);
+
+	put_disk(ns->disk);
+	nvme_put_ns_head(ns->head);
+	nvme_put_ctrl(ns->ctrl);
+	kfree(ns);
+}
+
+static void nvme_put_ns(struct nvme_ns *ns)
+{
+	kref_put(&ns->kref, nvme_free_ns);
+}
+
+static inline void nvme_clear_nvme_request(struct request *req)
+{
+	if (!(req->rq_flags & RQF_DONTPREP)) {
+		nvme_req(req)->retries = 0;
+		nvme_req(req)->flags = 0;
+		req->rq_flags |= RQF_DONTPREP;
+	}
+}
+
+struct request *nvme_alloc_request(struct request_queue *q,
+		struct nvme_command *cmd, blk_mq_req_flags_t flags, int qid)
+{
+	unsigned op = nvme_is_write(cmd) ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN;
+	struct request *req;
+
+	if (qid == NVME_QID_ANY) {
+		req = blk_mq_alloc_request(q, op, flags);
+	} else {
+		req = blk_mq_alloc_request_hctx(q, op, flags,
+				qid ? qid - 1 : 0);
+	}
+	if (IS_ERR(req))
+		return req;
+
+	req->cmd_flags |= REQ_FAILFAST_DRIVER;
+	nvme_clear_nvme_request(req);
+	nvme_req(req)->cmd = cmd;
+
+	return req;
+}
+EXPORT_SYMBOL_GPL(nvme_alloc_request);
+
+static int nvme_toggle_streams(struct nvme_ctrl *ctrl, bool enable)
+{
+	struct nvme_command c;
+
+	memset(&c, 0, sizeof(c));
+
+	c.directive.opcode = nvme_admin_directive_send;
+	c.directive.nsid = cpu_to_le32(NVME_NSID_ALL);
+	c.directive.doper = NVME_DIR_SND_ID_OP_ENABLE;
+	c.directive.dtype = NVME_DIR_IDENTIFY;
+	c.directive.tdtype = NVME_DIR_STREAMS;
+	c.directive.endir = enable ? NVME_DIR_ENDIR : 0;
+
+	return nvme_submit_sync_cmd(ctrl->admin_q, &c, NULL, 0);
+}
+
+static int nvme_disable_streams(struct nvme_ctrl *ctrl)
+{
+	return nvme_toggle_streams(ctrl, false);
+}
+
+static int nvme_enable_streams(struct nvme_ctrl *ctrl)
+{
+	return nvme_toggle_streams(ctrl, true);
+}
+
+static int nvme_get_stream_params(struct nvme_ctrl *ctrl,
+				  struct streams_directive_params *s, u32 nsid)
+{
+	struct nvme_command c;
+
+	memset(&c, 0, sizeof(c));
+	memset(s, 0, sizeof(*s));
+
+	c.directive.opcode = nvme_admin_directive_recv;
+	c.directive.nsid = cpu_to_le32(nsid);
+	c.directive.numd = cpu_to_le32((sizeof(*s) >> 2) - 1);
+	c.directive.doper = NVME_DIR_RCV_ST_OP_PARAM;
+	c.directive.dtype = NVME_DIR_STREAMS;
+
+	return nvme_submit_sync_cmd(ctrl->admin_q, &c, s, sizeof(*s));
+}
+
+static int nvme_configure_directives(struct nvme_ctrl *ctrl)
+{
+	struct streams_directive_params s;
+	int ret;
+
+	if (!(ctrl->oacs & NVME_CTRL_OACS_DIRECTIVES))
+		return 0;
+	if (!streams)
+		return 0;
+
+	ret = nvme_enable_streams(ctrl);
+	if (ret)
+		return ret;
+
+	ret = nvme_get_stream_params(ctrl, &s, NVME_NSID_ALL);
+	if (ret)
+		return ret;
+
+	ctrl->nssa = le16_to_cpu(s.nssa);
+	if (ctrl->nssa < BLK_MAX_WRITE_HINTS - 1) {
+		dev_info(ctrl->device, "too few streams (%u) available\n",
+					ctrl->nssa);
+		nvme_disable_streams(ctrl);
+		return 0;
+	}
+
+	ctrl->nr_streams = min_t(unsigned, ctrl->nssa, BLK_MAX_WRITE_HINTS - 1);
+	dev_info(ctrl->device, "Using %u streams\n", ctrl->nr_streams);
+	return 0;
+}
+
+/*
+ * Check if 'req' has a write hint associated with it. If it does, assign
+ * a valid namespace stream to the write.
+ */
+static void nvme_assign_write_stream(struct nvme_ctrl *ctrl,
+				     struct request *req, u16 *control,
+				     u32 *dsmgmt)
+{
+	enum rw_hint streamid = req->write_hint;
+
+	if (streamid == WRITE_LIFE_NOT_SET || streamid == WRITE_LIFE_NONE)
+		streamid = 0;
+	else {
+		streamid--;
+		if (WARN_ON_ONCE(streamid > ctrl->nr_streams))
+			return;
+
+		*control |= NVME_RW_DTYPE_STREAMS;
+		*dsmgmt |= streamid << 16;
+	}
+
+	if (streamid < ARRAY_SIZE(req->q->write_hints))
+		req->q->write_hints[streamid] += blk_rq_bytes(req) >> 9;
+}
+
+static inline void nvme_setup_flush(struct nvme_ns *ns,
+		struct nvme_command *cmnd)
+{
+	cmnd->common.opcode = nvme_cmd_flush;
+	cmnd->common.nsid = cpu_to_le32(ns->head->ns_id);
+}
+
+static blk_status_t nvme_setup_discard(struct nvme_ns *ns, struct request *req,
+		struct nvme_command *cmnd)
+{
+	unsigned short segments = blk_rq_nr_discard_segments(req), n = 0;
+	struct nvme_dsm_range *range;
+	struct bio *bio;
+
+	range = kmalloc_array(segments, sizeof(*range),
+				GFP_ATOMIC | __GFP_NOWARN);
+	if (!range) {
+		/*
+		 * If we fail allocation our range, fallback to the controller
+		 * discard page. If that's also busy, it's safe to return
+		 * busy, as we know we can make progress once that's freed.
+		 */
+		if (test_and_set_bit_lock(0, &ns->ctrl->discard_page_busy))
+			return BLK_STS_RESOURCE;
+
+		range = page_address(ns->ctrl->discard_page);
+	}
+
+	__rq_for_each_bio(bio, req) {
+		u64 slba = nvme_block_nr(ns, bio->bi_iter.bi_sector);
+		u32 nlb = bio->bi_iter.bi_size >> ns->lba_shift;
+
+		if (n < segments) {
+			range[n].cattr = cpu_to_le32(0);
+			range[n].nlb = cpu_to_le32(nlb);
+			range[n].slba = cpu_to_le64(slba);
+		}
+		n++;
+	}
+
+	if (WARN_ON_ONCE(n != segments)) {
+		if (virt_to_page(range) == ns->ctrl->discard_page)
+			clear_bit_unlock(0, &ns->ctrl->discard_page_busy);
+		else
+			kfree(range);
+		return BLK_STS_IOERR;
+	}
+
+	cmnd->dsm.opcode = nvme_cmd_dsm;
+	cmnd->dsm.nsid = cpu_to_le32(ns->head->ns_id);
+	cmnd->dsm.nr = cpu_to_le32(segments - 1);
+	cmnd->dsm.attributes = cpu_to_le32(NVME_DSMGMT_AD);
+
+	req->special_vec.bv_page = virt_to_page(range);
+	req->special_vec.bv_offset = offset_in_page(range);
+	req->special_vec.bv_len = sizeof(*range) * segments;
+	req->rq_flags |= RQF_SPECIAL_PAYLOAD;
+
+	return BLK_STS_OK;
+}
+
+static inline blk_status_t nvme_setup_write_zeroes(struct nvme_ns *ns,
+		struct request *req, struct nvme_command *cmnd)
+{
+	if (ns->ctrl->quirks & NVME_QUIRK_DEALLOCATE_ZEROES)
+		return nvme_setup_discard(ns, req, cmnd);
+
+	cmnd->write_zeroes.opcode = nvme_cmd_write_zeroes;
+	cmnd->write_zeroes.nsid = cpu_to_le32(ns->head->ns_id);
+	cmnd->write_zeroes.slba =
+		cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
+	cmnd->write_zeroes.length =
+		cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);
+	cmnd->write_zeroes.control = 0;
+	return BLK_STS_OK;
+}
+
+static inline blk_status_t nvme_setup_rw(struct nvme_ns *ns,
+		struct request *req, struct nvme_command *cmnd)
+{
+	struct nvme_ctrl *ctrl = ns->ctrl;
+	u16 control = 0;
+	u32 dsmgmt = 0;
+
+	if (req->cmd_flags & REQ_FUA)
+		control |= NVME_RW_FUA;
+	if (req->cmd_flags & (REQ_FAILFAST_DEV | REQ_RAHEAD))
+		control |= NVME_RW_LR;
+
+	if (req->cmd_flags & REQ_RAHEAD)
+		dsmgmt |= NVME_RW_DSM_FREQ_PREFETCH;
+
+	cmnd->rw.opcode = (rq_data_dir(req) ? nvme_cmd_write : nvme_cmd_read);
+	cmnd->rw.nsid = cpu_to_le32(ns->head->ns_id);
+	cmnd->rw.slba = cpu_to_le64(nvme_block_nr(ns, blk_rq_pos(req)));
+	cmnd->rw.length = cpu_to_le16((blk_rq_bytes(req) >> ns->lba_shift) - 1);
+
+	if (req_op(req) == REQ_OP_WRITE && ctrl->nr_streams)
+		nvme_assign_write_stream(ctrl, req, &control, &dsmgmt);
+
+	if (ns->ms) {
+		/*
+		 * If formated with metadata, the block layer always provides a
+		 * metadata buffer if CONFIG_BLK_DEV_INTEGRITY is enabled.  Else
+		 * we enable the PRACT bit for protection information or set the
+		 * namespace capacity to zero to prevent any I/O.
+		 */
+		if (!blk_integrity_rq(req)) {
+			if (WARN_ON_ONCE(!nvme_ns_has_pi(ns)))
+				return BLK_STS_NOTSUPP;
+			control |= NVME_RW_PRINFO_PRACT;
+		} else if (req_op(req) == REQ_OP_WRITE) {
+			t10_pi_prepare(req, ns->pi_type);
+		}
+
+		switch (ns->pi_type) {
+		case NVME_NS_DPS_PI_TYPE3:
+			control |= NVME_RW_PRINFO_PRCHK_GUARD;
+			break;
+		case NVME_NS_DPS_PI_TYPE1:
+		case NVME_NS_DPS_PI_TYPE2:
+			control |= NVME_RW_PRINFO_PRCHK_GUARD |
+					NVME_RW_PRINFO_PRCHK_REF;
+			cmnd->rw.reftag = cpu_to_le32(t10_pi_ref_tag(req));
+			break;
+		}
+	}
+
+	cmnd->rw.control = cpu_to_le16(control);
+	cmnd->rw.dsmgmt = cpu_to_le32(dsmgmt);
+	return 0;
+}
+
+void nvme_cleanup_cmd(struct request *req)
+{
+	if (blk_integrity_rq(req) && req_op(req) == REQ_OP_READ &&
+	    nvme_req(req)->status == 0) {
+		struct nvme_ns *ns = req->rq_disk->private_data;
+
+		t10_pi_complete(req, ns->pi_type,
+				blk_rq_bytes(req) >> ns->lba_shift);
+	}
+	if (req->rq_flags & RQF_SPECIAL_PAYLOAD) {
+		struct nvme_ns *ns = req->rq_disk->private_data;
+		struct page *page = req->special_vec.bv_page;
+
+		if (page == ns->ctrl->discard_page)
+			clear_bit_unlock(0, &ns->ctrl->discard_page_busy);
+		else
+			kfree(page_address(page) + req->special_vec.bv_offset);
+	}
+}
+EXPORT_SYMBOL_GPL(nvme_cleanup_cmd);
+
+blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
+		struct nvme_command *cmd)
+{
+	blk_status_t ret = BLK_STS_OK;
+
+	nvme_clear_nvme_request(req);
+
+	memset(cmd, 0, sizeof(*cmd));
+	switch (req_op(req)) {
+	case REQ_OP_DRV_IN:
+	case REQ_OP_DRV_OUT:
+		memcpy(cmd, nvme_req(req)->cmd, sizeof(*cmd));
+		break;
+	case REQ_OP_FLUSH:
+		nvme_setup_flush(ns, cmd);
+		break;
+	case REQ_OP_WRITE_ZEROES:
+		ret = nvme_setup_write_zeroes(ns, req, cmd);
+		break;
+	case REQ_OP_DISCARD:
+		ret = nvme_setup_discard(ns, req, cmd);
+		break;
+	case REQ_OP_READ:
+	case REQ_OP_WRITE:
+		ret = nvme_setup_rw(ns, req, cmd);
+		break;
+	default:
+		WARN_ON_ONCE(1);
+		return BLK_STS_IOERR;
+	}
+
+	cmd->common.command_id = req->tag;
+	trace_nvme_setup_cmd(req, cmd);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvme_setup_cmd);
+
+static void nvme_end_sync_rq(struct request *rq, blk_status_t error)
+{
+	struct completion *waiting = rq->end_io_data;
+
+	rq->end_io_data = NULL;
+	complete(waiting);
+}
+
+static void nvme_execute_rq_polled(struct request_queue *q,
+		struct gendisk *bd_disk, struct request *rq, int at_head)
+{
+	DECLARE_COMPLETION_ONSTACK(wait);
+
+	WARN_ON_ONCE(!test_bit(QUEUE_FLAG_POLL, &q->queue_flags));
+
+	rq->cmd_flags |= REQ_HIPRI;
+	rq->end_io_data = &wait;
+	blk_execute_rq_nowait(q, bd_disk, rq, at_head, nvme_end_sync_rq);
+
+	while (!completion_done(&wait)) {
+		blk_poll(q, request_to_qc_t(rq->mq_hctx, rq), true);
+		cond_resched();
+	}
+}
+
+
+static struct nvme_kaiocb *
+kv_alloc_ccb(struct nvme_passthru_kv_cmd *cmd, struct nvme_command *c,
+	     struct gendisk *disk)
+{
+	struct nvme_kaiocb *kcb;
+
+	if (unlikely(!disk))
+		return ERR_PTR(-EFAULT);
+
+	/* TODO: study GFP_NOIO */
+	kcb = mempool_alloc(kaiocb_mempool, GFP_NOIO);
+	if (!kcb)
+		return ERR_PTR(-ENOMEM);
+
+	memset(kcb, 0, sizeof (*kcb));
+	INIT_LIST_HEAD(&kcb->list);
+
+	kcb->event.reqid = cmd->reqid;
+	kcb->event.ctxid = cmd->ctxid;
+	kcb->cmd = *c;
+	kcb->disk = disk;
+
+	return kcb;
+}
+
+static struct request *
+kv_alloc_request(struct request_queue *q, struct nvme_command *cmd,
+		 blk_mq_req_flags_t flags, int qid)
+{
+	struct request *req;
+	uint8_t *data;
+
+	req = nvme_alloc_request(q, cmd, 0, NVME_QID_ANY);
+	if (IS_ERR(req))
+		return req;
+
+	kv_req(req)->kv_data_sg_ptr = NULL;
+	kv_req(req)->ptr = NULL;
+	kv_req(req)->kv_data_nents = 0;
+	kv_req(req)->kv_data_len = 0;
+
+	data = kmalloc(sizeof (*cmd), GFP_KERNEL);
+	if (!data)
+		return NULL;
+
+	memcpy(data, cmd, sizeof (*cmd));
+
+	kv_req(req)->req.cmd = (void *)data;
+
+	return req;
+}
+
+int kv_import_user_buf(int rw, void __user *kbuf, size_t klen,
+			void __user *vbuf, size_t vlen,
+			struct iovec *iov, struct iov_iter *i)
+{
+
+	size_t len = klen + vlen;
+
+	if (unlikely(!access_ok(kbuf, klen) ||
+		     !access_ok(vbuf, vlen) ))
+		return -EFAULT;
+
+	iov->iov_base = kbuf;
+	iov->iov_len = klen;
+	iov++;
+	iov->iov_base = vbuf;
+	iov->iov_len = vlen;
+
+	iov_iter_init(i, rw, iov, 2, len);
+
+	return 0;
+}
+
+static int
+kv_submit_async_cmd(struct request_queue *q, struct nvme_command *cmd,
+		struct nvme_passthru_kv_cmd *pthr_cmd,
+		void __user *ubuf, unsigned ulen,
+		void __user *key_ubuf, unsigned key_ulen, u32 meta_seed,
+		u32 *result, u32 *status, unsigned timeout)
+{
+	struct nvme_ns *ns = q->queuedata;
+	struct gendisk *disk = ns ? ns->disk : NULL;
+	struct request *req;
+	struct nvme_kaiocb *kcb = NULL;
+	struct iov_iter i;
+ 	struct iovec iov;   /* 0=key 1=value */
+	int ret;
+
+	req = kv_alloc_request(q, cmd, 0, NVME_QID_ANY);
+	if (IS_ERR(req))
+		return PTR_ERR(req);
+
+	kcb = kv_alloc_ccb(pthr_cmd, cmd, disk);
+	if (IS_ERR(kcb)) {
+		ret = PTR_ERR(kcb);
+		goto OUT_REQ;
+	}
+
+	//Check if the command is RDMA Data Direct and set flag
+	if (pthr_cmd->type == NVME_SGL_TYPE_VENDOR_SPECIFIC && \
+				pthr_cmd->rdd_chandle != 0) {
+		kv_req(req)->kv_rddpt = 1;
+	} else {
+		kv_req(req)->kv_rddpt = 0;
+	}
+
+	pr_debug("%s: request_queue %px req %px opc 0x%x tag %u"
+		 "kbuf 0x%px key_ulen %u vbuf 0x%px ulen %u\n",
+		 __FUNCTION__, q, req, pthr_cmd->opcode, req->tag,
+		 key_ubuf, key_ulen, ubuf, ulen);
+
+	req->timeout = timeout ? timeout : ADMIN_TIMEOUT;
+
+	if (key_ubuf) {
+		ret = import_single_range(rq_data_dir(req), key_ubuf, key_ulen,
+					  &iov, &i);
+		if (ret)
+			goto OUT_KCB;
+
+		ret = kv_rq_map_user(req, &i, false, true, GFP_KERNEL);
+		if (ret)
+			goto OUT_KCB;
+	}
+
+	ret = import_single_range(rq_data_dir(req), ubuf, ulen,
+				   &iov, &i);
+	if (ret)
+		goto OUT_KCB;
+
+	if(kv_req(req)->kv_rddpt == 1)
+		goto Q_RDD;
+
+	ret = kv_rq_map_user(req, &i, !rq_data_dir(req), false, GFP_KERNEL);
+	if (ret)
+		goto OUT_KCB;
+#if 0
+	if (req->bio) {
+		size_t i;
+		struct bvec_iter_all iter;
+		struct bio *bio = req->bio;
+		struct bio_vec *bvec;
+
+		bio_for_each_segment_all(bvec, bio, i, iter) {
+			pr_debug("page(%zu) %px pfn 0x%lx off 0x%x/0x%x\n",
+			       i, bvec->bv_page, __page_to_pfn(bvec->bv_page),
+			       bvec->bv_offset, bvec->bv_len);
+		}
+	}
+#endif
+//RDMA Data Direct skips data mapping
+Q_RDD:
+	pr_debug("%s: cpu %u enqueue ctxid %u req 0x%px tag %u\n",
+		__FUNCTION__, smp_processor_id(), pthr_cmd->ctxid, req, req->tag);
+
+	kcb->req = req;
+	kcb->bio = req->bio;
+	req->end_io_data = kcb;
+	blk_execute_rq_nowait(req->q, disk, req, 0, kv_cmd_done);
+
+	return 0;
+
+OUT_KCB:
+	mempool_free(kcb, kaiocb_mempool);
+OUT_REQ:
+	blk_mq_free_request(req);
+
+	return ret;
+}
+
+static inline int
+kv_key_len_check(__u64 key, size_t len)
+{
+	if (!key && !len)
+		goto SUCCESS;
+
+	/* logic XOR */
+	if (!key != !len)
+		goto ERROR;
+
+       	if (len > KVCMD_MAX_KEY_SIZE ||
+            len < KVCMD_MIN_KEY_SIZE)
+		goto ERROR;
+SUCCESS:
+	return 0;
+ERROR:
+	return -1;
+}
+
+static inline int
+kv_value_len_check(__u64 data, size_t len, bool is_rdd)
+{
+    size_t max_len = KVCMD_MAX_VALUE_SIZE;
+
+    if(is_rdd) max_len = KVCMD_MAX_RDD_VALUE_SIZE;
+
+	if (!data && !len)
+		goto SUCCESS;
+
+	if (!data != !len)
+		goto ERROR;
+
+    if (len > max_len)
+	{
+		pr_err("Failed Value size limit max allowed [%x] got [%x]\n", max_len, len);
+		goto ERROR;
+	}
+SUCCESS:
+	return 0;
+ERROR:
+	return -1;
+}
+
+static inline int
+kv_cmd_sanity_check(struct nvme_passthru_kv_cmd *cmd)
+{
+	size_t data_len;
+
+	if (!is_kv_cmd(cmd->opcode))
+		return -1;
+	else if (kv_key_len_check(cmd->key_addr, cmd->key_length))
+		return -1;
+	else {
+		if (cmd->opcode == nvme_cmd_kv_retrieve || cmd->opcode == nvme_cmd_kv_store) {
+			if (cmd->type == NVME_SGL_TYPE_VENDOR_SPECIFIC && cmd->rdd_chandle != 0) {
+				data_len = cmd->cdw10;
+				//Check 3 byte value length for RDMA direct request
+				if (kv_value_len_check(cmd->data_addr, data_len, true))
+					return -1;
+			} else {
+				if (kv_value_len_check(cmd->data_addr, cmd->data_length, false))
+					return -1;
+			}
+		} else {
+			if (kv_value_len_check(cmd->data_addr, cmd->data_length, false))
+				return -1;
+		}
+	}
+
+	return 0;
+}
+
+static int nvme_user_kv_cmd(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
+			struct nvme_passthru_kv_cmd __user *ucmd, bool aio)
+{
+	int status;
+	unsigned timeout = 0;
+	unsigned meta_len = 0;
+	unsigned iter_handle = 0;
+	void __user *metadata = NULL;
+	struct nvme_passthru_kv_cmd cmd;
+	struct nvme_command c;
+	unsigned data_len = 0;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+	if (copy_from_user(&cmd, ucmd, sizeof(cmd)))
+		return -EFAULT;
+	if (cmd.flags)
+		return -EINVAL;
+
+	/* Sanity check */
+	if (kv_cmd_sanity_check(&cmd)) {
+		pr_err("%s: Invalid passthru cmd:opc 0x%x key_addr %llu key_len %u "
+			"data_addr %x data_len %x \n", __FUNCTION__, cmd.opcode,
+			cmd.key_addr, cmd.key_length, cmd.data_addr, cmd.data_length);
+		status = -EINVAL;
+		goto exit;
+	}
+
+	data_len = cmd.data_length;
+
+	memset(&c, 0, sizeof(c));
+	c.common.opcode = cmd.opcode;
+	c.common.flags = cmd.flags;
+#ifdef KSID_SUPPORT
+	c.common.nsid = cmd.cdw3;
+#else
+	c.common.nsid = cpu_to_le32(cmd.nsid);
+#endif
+	timeout = msecs_to_jiffies(cmd.timeout_ms);
+
+	switch(cmd.opcode) {
+	case nvme_cmd_kv_store:
+        case nvme_cmd_kv_append:
+		c.kv_store.offset = cpu_to_le32(cmd.cdw5);
+		c.kv_store.key_len = cpu_to_le32(cmd.key_length - 1); /* key len -1 */
+		c.kv_store.option = cpu_to_le32(cmd.cdw4) & 0xff;
+		if (cmd.opcode == nvme_cmd_kv_store && cmd.type == NVME_SGL_TYPE_VENDOR_SPECIFIC && cmd.rdd_chandle != 0) {//RDMA Data direct Store
+			data_len = cmd.cdw10;
+			c.kv_store.rdd_chandle = cmd.rdd_chandle;//Copy before changing option/invalid_bytes
+
+			c.kv_store.value_len = P2ROUNDUP(cpu_to_le32(data_len), 4) >> 2;
+			c.kv_store.invalid_byte = P2NPHASE(cpu_to_le32(data_len), 4);
+
+
+			c.kv_store.dptr.ksgl.addr = cmd.data_addr;
+
+			c.kv_store.dptr.ksgl.length[0] = cmd.length[0];
+			c.kv_store.dptr.ksgl.length[1] = cmd.length[1];
+			c.kv_store.dptr.ksgl.length[2] = cmd.length[2];
+
+			c.kv_store.dptr.ksgl.key[0] = cmd.rkey[0];
+			c.kv_store.dptr.ksgl.key[1] = cmd.rkey[1];
+			c.kv_store.dptr.ksgl.key[2] = cmd.rkey[2];
+			c.kv_store.dptr.ksgl.key[3] = cmd.rkey[3];
+
+			//SGL Vendor Specific
+			c.kv_store.dptr.ksgl.type = (NVME_SGL_TYPE_VENDOR_SPECIFIC << 4);;
+
+			pr_debug("Data direct store data length %x value len %x\n", data_len, c.kv_retrieve.value_len);
+		} else {
+			c.kv_store.value_len = P2ROUNDUP(cpu_to_le32(cmd.data_length), 4) >> 2;
+			c.kv_store.invalid_byte = P2NPHASE(cpu_to_le32(cmd.data_length), 4);
+		}
+		/* The prev max key len the key len come from cmd.key_length. For key len [256, 1024],
+		    openMPDK uses cmd.key_length_large field to convey new key len and the old key_length is fixed to
+		    255 */
+		if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+			metadata = (void __user *) cmd.key_addr;
+			meta_len = cmd.key_length;
+		} else {
+			memcpy(c.kv_store.key, cmd.key, cmd.key_length);
+		}
+		break;
+        case nvme_cmd_kv_list:
+                c.kv_list.key_len = cmd.key_length -1; /* key len - 1 */
+                c.kv_list.rsvd = cmd.cdw2 & 0xf;
+                c.kv_list.value_len = P2ROUNDUP(cpu_to_le32(cmd.data_length), 4) >> 2;
+                c.kv_list.key_offset = cmd.list_key_offset;
+                c.kv_list.max_keys = cmd.list_max_keys;
+
+                if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+                        metadata = (void __user*) cmd.key_addr;
+                        meta_len = cmd.key_length;
+                } else {
+                        memcpy(c.kv_list.key, cmd.key, cmd.key_length);
+                }
+                break;
+        case nvme_cmd_kv_lock:
+        case nvme_cmd_kv_unlock:
+                c.kv_lock.key_len = (cmd.cdw11 & 0xFF) -1; /* key len - 1 */
+				c.kv_lock.priority = (cmd.cdw11 >> 8) & 0x3 ;
+				c.kv_lock.writer = (cmd.cdw11 >> 10) & 0x1;
+				c.kv_lock.blocking = (cmd.cdw11 >> 11) * 0x1;
+				c.kv_lock.uuid = cpu_to_le64(((uint64_t)cmd.cdw3) << 32 | cmd.cdw2);
+				c.kv_lock.duration = cpu_to_le32(cmd.cdw10);
+
+                if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+                        metadata = (void __user*) cmd.key_addr;
+                        meta_len = cmd.key_length;
+                } else {
+                        memcpy(c.kv_list.key, cmd.key, cmd.key_length);
+                }
+                break;
+        case nvme_cmd_kv_retrieve:
+		c.kv_retrieve.offset = cpu_to_le32(cmd.cdw5);
+		c.kv_retrieve.key_len = cpu_to_le32(cmd.key_length - 1); /* key len - 1 */
+		c.kv_retrieve.option = cpu_to_le32(cmd.cdw4) & 0xff;
+		if (cmd.type == NVME_SGL_TYPE_VENDOR_SPECIFIC && cmd.rdd_chandle != 0) {//RDMA Data direct Get
+			data_len = cmd.cdw10;
+			c.kv_retrieve.value_len = P2ROUNDUP(cpu_to_le32(data_len), 4) >> 2;
+
+			c.kv_retrieve.rdd_chandle = cmd.rdd_chandle;
+
+			c.kv_retrieve.dptr.ksgl.addr = cmd.data_addr;
+
+			c.kv_retrieve.dptr.ksgl.length[0] = cmd.length[0];
+			c.kv_retrieve.dptr.ksgl.length[1] = cmd.length[1];
+			c.kv_retrieve.dptr.ksgl.length[2] = cmd.length[2];
+
+			c.kv_retrieve.dptr.ksgl.key[0] = cmd.rkey[0];
+			c.kv_retrieve.dptr.ksgl.key[1] = cmd.rkey[1];
+			c.kv_retrieve.dptr.ksgl.key[2] = cmd.rkey[2];
+			c.kv_retrieve.dptr.ksgl.key[3] = cmd.rkey[3];
+
+			//SGL Vendor Specific
+			c.kv_retrieve.dptr.ksgl.type = (NVME_SGL_TYPE_VENDOR_SPECIFIC << 4);;
+
+			pr_debug("Data direct retrieve data length %x value len %x\n", data_len, c.kv_retrieve.value_len);
+		} else {
+			c.kv_retrieve.value_len = P2ROUNDUP(cpu_to_le32(cmd.data_length), 4) >> 2;
+		}
+
+		if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+			metadata = (void __user*) cmd.key_addr;
+			meta_len = cmd.key_length;
+		} else {
+			memcpy(c.kv_retrieve.key, cmd.key, cmd.key_length);
+		}
+		break;
+	case nvme_cmd_kv_delete:
+		c.kv_delete.key_len = cpu_to_le32(cmd.key_length - 1);
+		c.kv_delete.option = cpu_to_le32(cmd.cdw4) & 0xff;
+
+		if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+			metadata = (void __user *) cmd.key_addr;
+			meta_len = cmd.key_length;
+		} else {
+			memcpy(c.kv_delete.key, cmd.key, cmd.key_length);
+		}
+		break;
+	case nvme_cmd_kv_exist:
+		c.kv_exist.key_len = cpu_to_le32(cmd.key_length - 1);
+		c.kv_exist.option = (cpu_to_le32(cmd.cdw4) & 0xff);
+
+		if (cmd.key_length > KVCMD_INLINE_KEY_MAX) {
+			metadata = (void __user *) cmd.key_addr;
+			meta_len = cmd.key_length;
+
+		} else {
+			memcpy(c.kv_exist.key, cmd.key, cmd.key_length);
+		}
+		break;
+	case nvme_cmd_kv_iter_req:
+		iter_handle = cpu_to_le32(cmd.cdw5);
+		c.kv_iter_req.iter_handle = iter_handle & 0xff;
+		c.kv_iter_req.option = cpu_to_le32(cmd.cdw4) & 0xff;
+		c.kv_iter_req.iter_val = cpu_to_le32(cmd.cdw12);
+		c.kv_iter_req.iter_bitmask = cpu_to_le32(cmd.cdw13);
+		break;
+	case nvme_cmd_kv_iter_read:
+		iter_handle = cpu_to_le32(cmd.cdw5);
+		c.kv_iter_read.iter_handle = iter_handle & 0xff;
+		c.kv_iter_read.option = cpu_to_le32(cmd.cdw4) & 0xff;
+		c.kv_iter_read.value_len = cpu_to_le32((cmd.data_length >> 2));
+		break;
+	default:
+		cmd.result = KVS_ERR_IO;
+		status = -EINVAL;
+		goto exit;
+	}
+
+	if (likely(aio))
+		status = kv_submit_async_cmd(ns->queue, &c, &cmd,
+					     (void __user *) cmd.data_addr,
+					     data_len, metadata, meta_len, 0,
+					     &cmd.result, &cmd.status, timeout);
+#if 0
+	else
+		/* TODO: ns is always valid */
+		status = _nvme_submit_kv_user_cmd(ns ? ns->queue : ctrl->admin_q,
+						  &c, &cmd,
+						  (void __user *) cmd.data_addr,
+						  cmd.data_length, metadata, meta_len, 0,
+						  &cmd.result, &cmd.status, timeout, aio);
+#endif
+exit:
+	if (!aio) {
+		if (put_user(cmd.result, &ucmd->result))
+			return -EFAULT;
+		if (put_user(cmd.status, &ucmd->status))
+			return -EFAULT;
+	}
+
+	return status;
+}
+
+
+/*
+ * Returns 0 on success.  If the result is negative, it's a Linux error code;
+ * if the result is positive, it's an NVM Express status code
+ */
+int __nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
+		union nvme_result *result, void *buffer, unsigned bufflen,
+		unsigned timeout, int qid, int at_head,
+		blk_mq_req_flags_t flags, bool poll)
+{
+	struct request *req;
+	int ret;
+
+	req = nvme_alloc_request(q, cmd, flags, qid);
+	if (IS_ERR(req))
+		return PTR_ERR(req);
+
+	req->timeout = timeout ? timeout : ADMIN_TIMEOUT;
+
+	if (buffer && bufflen) {
+		ret = blk_rq_map_kern(q, req, buffer, bufflen, GFP_KERNEL);
+		if (ret)
+			goto out;
+	}
+
+	pr_debug("%s req %px tag %d opcode %u\n", __FUNCTION__, req, req->tag, cmd->common.opcode);
+
+	if (poll)
+		nvme_execute_rq_polled(req->q, NULL, req, at_head);
+	else
+		blk_execute_rq(req->q, NULL, req, at_head);
+	if (result)
+		*result = nvme_req(req)->result;
+	if (nvme_req(req)->flags & NVME_REQ_CANCELLED)
+		ret = -EINTR;
+	else
+		ret = nvme_req(req)->status;
+ out:
+	blk_mq_free_request(req);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(__nvme_submit_sync_cmd);
+
+int nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
+		void *buffer, unsigned bufflen)
+{
+	return __nvme_submit_sync_cmd(q, cmd, NULL, buffer, bufflen, 0,
+			NVME_QID_ANY, 0, 0, false);
+}
+EXPORT_SYMBOL_GPL(nvme_submit_sync_cmd);
+
+static void *nvme_add_user_metadata(struct bio *bio, void __user *ubuf,
+		unsigned len, u32 seed, bool write)
+{
+	struct bio_integrity_payload *bip;
+	int ret = -ENOMEM;
+	void *buf;
+
+	buf = kmalloc(len, GFP_KERNEL);
+	if (!buf)
+		goto out;
+
+	ret = -EFAULT;
+	if (write && copy_from_user(buf, ubuf, len))
+		goto out_free_meta;
+
+	bip = bio_integrity_alloc(bio, GFP_KERNEL, 1);
+	if (IS_ERR(bip)) {
+		ret = PTR_ERR(bip);
+		goto out_free_meta;
+	}
+
+	bip->bip_iter.bi_size = len;
+	bip->bip_iter.bi_sector = seed;
+	ret = bio_integrity_add_page(bio, virt_to_page(buf), len,
+			offset_in_page(buf));
+	if (ret == len)
+		return buf;
+	ret = -ENOMEM;
+out_free_meta:
+	kfree(buf);
+out:
+	return ERR_PTR(ret);
+}
+
+static int nvme_submit_user_cmd(struct request_queue *q,
+		struct nvme_command *cmd, void __user *ubuffer,
+		unsigned bufflen, void __user *meta_buffer, unsigned meta_len,
+		u32 meta_seed, u32 *result, unsigned timeout)
+{
+	bool write = nvme_is_write(cmd);
+	struct nvme_ns *ns = q->queuedata;
+	struct gendisk *disk = ns ? ns->disk : NULL;
+	struct request *req;
+	struct bio *bio = NULL;
+	void *meta = NULL;
+	int ret;
+
+	req = nvme_alloc_request(q, cmd, 0, NVME_QID_ANY);
+	if (IS_ERR(req))
+		return PTR_ERR(req);
+
+	req->timeout = timeout ? timeout : ADMIN_TIMEOUT;
+	nvme_req(req)->flags |= NVME_REQ_USERCMD;
+
+	if (ubuffer && bufflen) {
+		ret = blk_rq_map_user(q, req, NULL, ubuffer, bufflen,
+				GFP_KERNEL);
+		if (ret)
+			goto out;
+		bio = req->bio;
+		bio->bi_disk = disk;
+		if (disk && meta_buffer && meta_len) {
+			meta = nvme_add_user_metadata(bio, meta_buffer, meta_len,
+					meta_seed, write);
+			if (IS_ERR(meta)) {
+				ret = PTR_ERR(meta);
+				goto out_unmap;
+			}
+			req->cmd_flags |= REQ_INTEGRITY;
+		}
+	}
+
+	blk_execute_rq(req->q, disk, req, 0);
+	if (nvme_req(req)->flags & NVME_REQ_CANCELLED) {
+		ret = -EINTR;
+		pr_err("%s:cancelled ret=%d\n", __FUNCTION__, ret);
+	} else {
+		ret = nvme_req(req)->status;
+		pr_err("%s:ret=%d\n", __FUNCTION__, ret);
+	}
+	if (result)
+		*result = le32_to_cpu(nvme_req(req)->result.u32);
+	if (meta && !ret && !write) {
+		if (copy_to_user(meta_buffer, meta, meta_len))
+			ret = -EFAULT;
+	}
+	kfree(meta);
+out_unmap:
+	if (bio)
+		blk_rq_unmap_user(bio);
+out:
+	blk_mq_free_request(req);
+	return ret;
+}
+
+static void nvme_keep_alive_end_io(struct request *rq, blk_status_t status)
+{
+	struct nvme_ctrl *ctrl = rq->end_io_data;
+	unsigned long flags;
+	bool startka = false;
+
+	blk_mq_free_request(rq);
+
+	if (status) {
+		dev_err(ctrl->device,
+			"failed nvme_keep_alive_end_io error=%d\n",
+				status);
+		return;
+	}
+
+	ctrl->comp_seen = false;
+	spin_lock_irqsave(&ctrl->lock, flags);
+	if (ctrl->state == NVME_CTRL_LIVE ||
+	    ctrl->state == NVME_CTRL_CONNECTING)
+		startka = true;
+	spin_unlock_irqrestore(&ctrl->lock, flags);
+	if (startka)
+		schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+}
+
+static int nvme_keep_alive(struct nvme_ctrl *ctrl)
+{
+	struct request *rq;
+
+	rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd, BLK_MQ_REQ_RESERVED,
+			NVME_QID_ANY);
+	if (IS_ERR(rq))
+		return PTR_ERR(rq);
+
+	rq->timeout = ctrl->kato * HZ;
+	rq->end_io_data = ctrl;
+
+	blk_execute_rq_nowait(rq->q, NULL, rq, 0, nvme_keep_alive_end_io);
+
+	return 0;
+}
+
+static void nvme_keep_alive_work(struct work_struct *work)
+{
+	struct nvme_ctrl *ctrl = container_of(to_delayed_work(work),
+			struct nvme_ctrl, ka_work);
+	bool comp_seen = ctrl->comp_seen;
+
+	if ((ctrl->ctratt & NVME_CTRL_ATTR_TBKAS) && comp_seen) {
+		dev_dbg(ctrl->device,
+			"reschedule traffic based keep-alive timer\n");
+		ctrl->comp_seen = false;
+		schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+		return;
+	}
+
+	if (nvme_keep_alive(ctrl)) {
+		/* allocation failure, reset the controller */
+		dev_err(ctrl->device, "keep-alive failed\n");
+		nvme_reset_ctrl(ctrl);
+		return;
+	}
+}
+
+static void nvme_start_keep_alive(struct nvme_ctrl *ctrl)
+{
+	if (unlikely(ctrl->kato == 0))
+		return;
+
+	schedule_delayed_work(&ctrl->ka_work, ctrl->kato * HZ);
+}
+
+void nvme_stop_keep_alive(struct nvme_ctrl *ctrl)
+{
+	if (unlikely(ctrl->kato == 0))
+		return;
+
+	cancel_delayed_work_sync(&ctrl->ka_work);
+}
+EXPORT_SYMBOL_GPL(nvme_stop_keep_alive);
+
+static int nvme_identify_ctrl(struct nvme_ctrl *dev, struct nvme_id_ctrl **id)
+{
+	struct nvme_command c = { };
+	int error;
+
+	/* gcc-4.4.4 (at least) has issues with initializers and anon unions */
+	c.identify.opcode = nvme_admin_identify;
+	c.identify.cns = NVME_ID_CNS_CTRL;
+
+	*id = kmalloc(sizeof(struct nvme_id_ctrl), GFP_KERNEL);
+	if (!*id)
+		return -ENOMEM;
+
+	error = nvme_submit_sync_cmd(dev->admin_q, &c, *id,
+			sizeof(struct nvme_id_ctrl));
+	if (error)
+		kfree(*id);
+	return error;
+}
+
+static int nvme_identify_ns_descs(struct nvme_ctrl *ctrl, unsigned nsid,
+		struct nvme_ns_ids *ids)
+{
+	struct nvme_command c = { };
+	int status;
+	void *data;
+	int pos;
+	int len;
+
+	c.identify.opcode = nvme_admin_identify;
+	c.identify.nsid = cpu_to_le32(nsid);
+	c.identify.cns = NVME_ID_CNS_NS_DESC_LIST;
+
+	data = kzalloc(NVME_IDENTIFY_DATA_SIZE, GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	status = nvme_submit_sync_cmd(ctrl->admin_q, &c, data,
+				      NVME_IDENTIFY_DATA_SIZE);
+	if (status)
+		goto free_data;
+
+	for (pos = 0; pos < NVME_IDENTIFY_DATA_SIZE; pos += len) {
+		struct nvme_ns_id_desc *cur = data + pos;
+
+		if (cur->nidl == 0)
+			break;
+
+		switch (cur->nidt) {
+		case NVME_NIDT_EUI64:
+			if (cur->nidl != NVME_NIDT_EUI64_LEN) {
+				dev_warn(ctrl->device,
+					 "ctrl returned bogus length: %d for NVME_NIDT_EUI64\n",
+					 cur->nidl);
+				goto free_data;
+			}
+			len = NVME_NIDT_EUI64_LEN;
+			memcpy(ids->eui64, data + pos + sizeof(*cur), len);
+			break;
+		case NVME_NIDT_NGUID:
+			if (cur->nidl != NVME_NIDT_NGUID_LEN) {
+				dev_warn(ctrl->device,
+					 "ctrl returned bogus length: %d for NVME_NIDT_NGUID\n",
+					 cur->nidl);
+				goto free_data;
+			}
+			len = NVME_NIDT_NGUID_LEN;
+			memcpy(ids->nguid, data + pos + sizeof(*cur), len);
+			break;
+		case NVME_NIDT_UUID:
+			if (cur->nidl != NVME_NIDT_UUID_LEN) {
+				dev_warn(ctrl->device,
+					 "ctrl returned bogus length: %d for NVME_NIDT_UUID\n",
+					 cur->nidl);
+				goto free_data;
+			}
+			len = NVME_NIDT_UUID_LEN;
+			uuid_copy(&ids->uuid, data + pos + sizeof(*cur));
+			break;
+		default:
+			/* Skip unknown types */
+			len = cur->nidl;
+			break;
+		}
+
+		len += sizeof(*cur);
+	}
+free_data:
+	kfree(data);
+	return status;
+}
+
+static int nvme_identify_ns_list(struct nvme_ctrl *dev, unsigned nsid, __le32 *ns_list)
+{
+	struct nvme_command c = { };
+
+	c.identify.opcode = nvme_admin_identify;
+	c.identify.cns = NVME_ID_CNS_NS_ACTIVE_LIST;
+	c.identify.nsid = cpu_to_le32(nsid);
+	return nvme_submit_sync_cmd(dev->admin_q, &c, ns_list,
+				    NVME_IDENTIFY_DATA_SIZE);
+}
+
+static struct nvme_id_ns *nvme_identify_ns(struct nvme_ctrl *ctrl,
+		unsigned nsid)
+{
+	struct nvme_id_ns *id;
+	struct nvme_command c = { };
+	int error;
+
+	/* gcc-4.4.4 (at least) has issues with initializers and anon unions */
+	c.identify.opcode = nvme_admin_identify;
+	c.identify.nsid = cpu_to_le32(nsid);
+	c.identify.cns = NVME_ID_CNS_NS;
+
+	id = kmalloc(sizeof(*id), GFP_KERNEL);
+	if (!id)
+		return NULL;
+
+	error = nvme_submit_sync_cmd(ctrl->admin_q, &c, id, sizeof(*id));
+	if (error) {
+		dev_warn(ctrl->device, "Identify namespace failed\n");
+		kfree(id);
+		return NULL;
+	}
+
+	return id;
+}
+
+static int nvme_set_features(struct nvme_ctrl *dev, unsigned fid, unsigned dword11,
+		      void *buffer, size_t buflen, u32 *result)
+{
+	struct nvme_command c;
+	union nvme_result res;
+	int ret;
+
+	memset(&c, 0, sizeof(c));
+	c.features.opcode = nvme_admin_set_features;
+	c.features.fid = cpu_to_le32(fid);
+	c.features.dword11 = cpu_to_le32(dword11);
+
+	ret = __nvme_submit_sync_cmd(dev->admin_q, &c, &res,
+			buffer, buflen, 0, NVME_QID_ANY, 0, 0, false);
+	if (ret >= 0 && result)
+		*result = le32_to_cpu(res.u32);
+	return ret;
+}
+
+int nvme_set_queue_count(struct nvme_ctrl *ctrl, int *count)
+{
+	u32 q_count = (*count - 1) | ((*count - 1) << 16);
+	u32 result;
+	int status, nr_io_queues;
+
+	status = nvme_set_features(ctrl, NVME_FEAT_NUM_QUEUES, q_count, NULL, 0,
+			&result);
+	if (status < 0)
+		return status;
+
+	/*
+	 * Degraded controllers might return an error when setting the queue
+	 * count.  We still want to be able to bring them online and offer
+	 * access to the admin queue, as that might be only way to fix them up.
+	 */
+	if (status > 0) {
+		dev_err(ctrl->device, "Could not set queue count (%d)\n", status);
+		*count = 0;
+	} else {
+		nr_io_queues = min(result & 0xffff, result >> 16) + 1;
+		*count = min(*count, nr_io_queues);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(nvme_set_queue_count);
+
+#define NVME_AEN_SUPPORTED \
+	(NVME_AEN_CFG_NS_ATTR | NVME_AEN_CFG_FW_ACT | NVME_AEN_CFG_ANA_CHANGE)
+
+static void nvme_enable_aen(struct nvme_ctrl *ctrl)
+{
+	u32 result, supported_aens = ctrl->oaes & NVME_AEN_SUPPORTED;
+	int status;
+
+	if (!supported_aens)
+		return;
+
+	status = nvme_set_features(ctrl, NVME_FEAT_ASYNC_EVENT, supported_aens,
+			NULL, 0, &result);
+	if (status)
+		dev_warn(ctrl->device, "Failed to configure AEN (cfg %x)\n",
+			 supported_aens);
+}
+
+static int nvme_submit_io(struct nvme_ns *ns, struct nvme_user_io __user *uio)
+{
+	struct nvme_user_io io;
+	struct nvme_command c;
+	unsigned length, meta_len;
+	void __user *metadata;
+
+	if (copy_from_user(&io, uio, sizeof(io)))
+		return -EFAULT;
+	if (io.flags)
+		return -EINVAL;
+
+	switch (io.opcode) {
+	case nvme_cmd_write:
+	case nvme_cmd_read:
+	case nvme_cmd_compare:
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	length = (io.nblocks + 1) << ns->lba_shift;
+	meta_len = (io.nblocks + 1) * ns->ms;
+	metadata = (void __user *)(uintptr_t)io.metadata;
+
+	if (ns->ext) {
+		length += meta_len;
+		meta_len = 0;
+	} else if (meta_len) {
+		if ((io.metadata & 3) || !io.metadata)
+			return -EINVAL;
+	}
+
+	memset(&c, 0, sizeof(c));
+	c.rw.opcode = io.opcode;
+	c.rw.flags = io.flags;
+	c.rw.nsid = cpu_to_le32(ns->head->ns_id);
+	c.rw.slba = cpu_to_le64(io.slba);
+	c.rw.length = cpu_to_le16(io.nblocks);
+	c.rw.control = cpu_to_le16(io.control);
+	c.rw.dsmgmt = cpu_to_le32(io.dsmgmt);
+	c.rw.reftag = cpu_to_le32(io.reftag);
+	c.rw.apptag = cpu_to_le16(io.apptag);
+	c.rw.appmask = cpu_to_le16(io.appmask);
+
+	return nvme_submit_user_cmd(ns->queue, &c,
+			(void __user *)(uintptr_t)io.addr, length,
+			metadata, meta_len, lower_32_bits(io.slba), NULL, 0);
+}
+
+static u32 nvme_known_admin_effects(u8 opcode)
+{
+	switch (opcode) {
+	case nvme_admin_format_nvm:
+		return NVME_CMD_EFFECTS_CSUPP | NVME_CMD_EFFECTS_LBCC |
+					NVME_CMD_EFFECTS_CSE_MASK;
+	case nvme_admin_sanitize_nvm:
+		return NVME_CMD_EFFECTS_CSE_MASK;
+	default:
+		break;
+	}
+	return 0;
+}
+
+static u32 nvme_passthru_start(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
+								u8 opcode)
+{
+	u32 effects = 0;
+
+	if (ns) {
+		if (ctrl->effects)
+			effects = le32_to_cpu(ctrl->effects->iocs[opcode]);
+		if (effects & ~(NVME_CMD_EFFECTS_CSUPP | NVME_CMD_EFFECTS_LBCC))
+			dev_warn(ctrl->device,
+				 "IO command:%02x has unhandled effects:%08x\n",
+				 opcode, effects);
+		return 0;
+	}
+
+	if (ctrl->effects)
+		effects = le32_to_cpu(ctrl->effects->acs[opcode]);
+	else
+		effects = nvme_known_admin_effects(opcode);
+
+	/*
+	 * For simplicity, IO to all namespaces is quiesced even if the command
+	 * effects say only one namespace is affected.
+	 */
+	if (effects & (NVME_CMD_EFFECTS_LBCC | NVME_CMD_EFFECTS_CSE_MASK)) {
+		dev_info(ctrl->device, "%s: opcode %u freeze ctrl\n", __FUNCTION__, opcode);
+		mutex_lock(&ctrl->scan_lock);
+		nvme_start_freeze(ctrl);
+		nvme_wait_freeze(ctrl);
+	}
+	return effects;
+}
+
+static void nvme_update_formats(struct nvme_ctrl *ctrl)
+{
+	struct nvme_ns *ns;
+
+	down_read(&ctrl->namespaces_rwsem);
+	list_for_each_entry(ns, &ctrl->namespaces, list)
+		if (ns->disk && nvme_revalidate_disk(ns->disk))
+			nvme_set_queue_dying(ns);
+	up_read(&ctrl->namespaces_rwsem);
+
+	nvme_remove_invalid_namespaces(ctrl, NVME_NSID_ALL);
+}
+
+static void nvme_passthru_end(struct nvme_ctrl *ctrl, u32 effects)
+{
+	/*
+	 * Revalidate LBA changes prior to unfreezing. This is necessary to
+	 * prevent memory corruption if a logical block size was changed by
+	 * this command.
+	 */
+	if (effects & NVME_CMD_EFFECTS_LBCC)
+		nvme_update_formats(ctrl);
+	if (effects & (NVME_CMD_EFFECTS_LBCC | NVME_CMD_EFFECTS_CSE_MASK)) {
+		nvme_unfreeze(ctrl);
+		mutex_unlock(&ctrl->scan_lock);
+	}
+	if (effects & NVME_CMD_EFFECTS_CCC)
+		nvme_init_identify(ctrl);
+	if (effects & (NVME_CMD_EFFECTS_NIC | NVME_CMD_EFFECTS_NCC))
+		nvme_queue_scan(ctrl);
+}
+
+static int nvme_user_cmd(struct nvme_ctrl *ctrl, struct nvme_ns *ns,
+			struct nvme_passthru_cmd __user *ucmd)
+{
+	struct nvme_passthru_cmd cmd;
+	struct nvme_command c;
+	unsigned timeout = 0;
+	u32 effects;
+	int status;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+	if (copy_from_user(&cmd, ucmd, sizeof(cmd)))
+		return -EFAULT;
+	if (cmd.flags)
+		return -EINVAL;
+
+	memset(&c, 0, sizeof(c));
+	c.common.opcode = cmd.opcode;
+	c.common.flags = cmd.flags;
+	c.common.nsid = cpu_to_le32(cmd.nsid);
+	c.common.cdw2[0] = cpu_to_le32(cmd.cdw2);
+	c.common.cdw2[1] = cpu_to_le32(cmd.cdw3);
+	c.common.cdw10 = cpu_to_le32(cmd.cdw10);
+	c.common.cdw11 = cpu_to_le32(cmd.cdw11);
+	c.common.cdw12 = cpu_to_le32(cmd.cdw12);
+	c.common.cdw13 = cpu_to_le32(cmd.cdw13);
+	c.common.cdw14 = cpu_to_le32(cmd.cdw14);
+	c.common.cdw15 = cpu_to_le32(cmd.cdw15);
+
+	if (cmd.timeout_ms)
+		timeout = msecs_to_jiffies(cmd.timeout_ms);
+
+	effects = nvme_passthru_start(ctrl, ns, cmd.opcode);
+	status = nvme_submit_user_cmd(ns ? ns->queue : ctrl->admin_q, &c,
+			(void __user *)(uintptr_t)cmd.addr, cmd.data_len,
+			(void __user *)(uintptr_t)cmd.metadata, cmd.metadata_len,
+			0, &cmd.result, timeout);
+//	dev_info(ctrl->device, "%s: opcode %u 0x%x st=%d\n", __FUNCTION__, cmd.opcode,
+//								effects, status);
+	nvme_passthru_end(ctrl, effects);
+
+	if (status >= 0) {
+		if (put_user(cmd.result, &ucmd->result))
+			return -EFAULT;
+	}
+
+	return status;
+}
+
+/*
+ * Issue ioctl requests on the first available path.  Note that unlike normal
+ * block layer requests we will not retry failed request on another controller.
+ */
+static struct nvme_ns *nvme_get_ns_from_disk(struct gendisk *disk,
+		struct nvme_ns_head **head, int *srcu_idx)
+{
+#ifdef CONFIG_NVME_MULTIPATH
+	if (disk->fops == &nvme_ns_head_ops) {
+		*head = disk->private_data;
+		*srcu_idx = srcu_read_lock(&(*head)->srcu);
+		return nvme_find_path(*head);
+	}
+#endif
+	*head = NULL;
+	*srcu_idx = -1;
+	return disk->private_data;
+}
+
+static void nvme_put_ns_from_disk(struct nvme_ns_head *head, int idx)
+{
+	if (head)
+		srcu_read_unlock(&head->srcu, idx);
+}
+
+static int nvme_ns_ioctl(struct nvme_ns *ns, unsigned cmd, unsigned long arg)
+{
+	switch (cmd) {
+	case NVME_IOCTL_ID:
+		force_successful_syscall_return();
+		return ns->head->ns_id;
+	case NVME_IOCTL_ADMIN_CMD:
+		return nvme_user_cmd(ns->ctrl, NULL, (void __user *)arg);
+	case NVME_IOCTL_IO_CMD:
+		return nvme_user_cmd(ns->ctrl, ns, (void __user *)arg);
+	case NVME_IOCTL_SUBMIT_IO:
+		return nvme_submit_io(ns, (void __user *)arg);
+	case NVME_IOCTL_IO_KV_CMD:
+		return nvme_user_kv_cmd(ns->ctrl, ns, (void __user *)arg, false);
+	case NVME_IOCTL_AIO_CMD:
+		return nvme_user_kv_cmd(ns->ctrl, ns, (void __user *)arg, true);
+	case NVME_IOCTL_SET_AIOCTX:
+		return nvme_set_aioctx((void __user*)arg);
+	case NVME_IOCTL_DEL_AIOCTX:
+		return nvme_del_aioctx((void __user*)arg);
+	case NVME_IOCTL_GET_AIOEVENT:
+		return nvme_get_ioevents((void __user*)arg);
+	default:
+#ifdef CONFIG_NVM
+		if (ns->ndev)
+			return nvme_nvm_ioctl(ns, cmd, arg);
+#endif
+		if (is_sed_ioctl(cmd))
+			return sed_ioctl(ns->ctrl->opal_dev, cmd,
+					 (void __user *) arg);
+		return -ENOTTY;
+	}
+}
+
+static int nvme_ioctl(struct block_device *bdev, fmode_t mode,
+		unsigned int cmd, unsigned long arg)
+{
+	struct nvme_ns_head *head = NULL;
+	struct nvme_ns *ns;
+	int srcu_idx, ret;
+
+	ns = nvme_get_ns_from_disk(bdev->bd_disk, &head, &srcu_idx);
+	if (unlikely(!ns))
+		ret = -EWOULDBLOCK;
+	else
+		ret = nvme_ns_ioctl(ns, cmd, arg);
+	nvme_put_ns_from_disk(head, srcu_idx);
+	return ret;
+}
+
+static int nvme_open(struct block_device *bdev, fmode_t mode)
+{
+	struct nvme_ns *ns = bdev->bd_disk->private_data;
+
+#ifdef CONFIG_NVME_MULTIPATH
+	/* should never be called due to GENHD_FL_HIDDEN */
+	if (WARN_ON_ONCE(ns->head->disk))
+		goto fail;
+#endif
+	if (!kref_get_unless_zero(&ns->kref))
+		goto fail;
+	if (!try_module_get(ns->ctrl->ops->module))
+		goto fail_put_ns;
+
+	return 0;
+
+fail_put_ns:
+	nvme_put_ns(ns);
+fail:
+	return -ENXIO;
+}
+
+static void nvme_release(struct gendisk *disk, fmode_t mode)
+{
+	struct nvme_ns *ns = disk->private_data;
+
+	module_put(ns->ctrl->ops->module);
+	nvme_put_ns(ns);
+}
+
+static int nvme_getgeo(struct block_device *bdev, struct hd_geometry *geo)
+{
+	/* some standard values */
+	geo->heads = 1 << 6;
+	geo->sectors = 1 << 5;
+	geo->cylinders = get_capacity(bdev->bd_disk) >> 11;
+	return 0;
+}
+
+#ifdef CONFIG_BLK_DEV_INTEGRITY
+static void nvme_init_integrity(struct gendisk *disk, u16 ms, u8 pi_type)
+{
+	struct blk_integrity integrity;
+
+	memset(&integrity, 0, sizeof(integrity));
+	switch (pi_type) {
+	case NVME_NS_DPS_PI_TYPE3:
+		integrity.profile = &t10_pi_type3_crc;
+		integrity.tag_size = sizeof(u16) + sizeof(u32);
+		integrity.flags |= BLK_INTEGRITY_DEVICE_CAPABLE;
+		break;
+	case NVME_NS_DPS_PI_TYPE1:
+	case NVME_NS_DPS_PI_TYPE2:
+		integrity.profile = &t10_pi_type1_crc;
+		integrity.tag_size = sizeof(u16);
+		integrity.flags |= BLK_INTEGRITY_DEVICE_CAPABLE;
+		break;
+	default:
+		integrity.profile = NULL;
+		break;
+	}
+	integrity.tuple_size = ms;
+	blk_integrity_register(disk, &integrity);
+	blk_queue_max_integrity_segments(disk->queue, 1);
+}
+#else
+static void nvme_init_integrity(struct gendisk *disk, u16 ms, u8 pi_type)
+{
+}
+#endif /* CONFIG_BLK_DEV_INTEGRITY */
+
+static void nvme_set_chunk_size(struct nvme_ns *ns)
+{
+	u32 chunk_size = (((u32)ns->noiob) << (ns->lba_shift - 9));
+	blk_queue_chunk_sectors(ns->queue, rounddown_pow_of_two(chunk_size));
+}
+
+static void nvme_config_discard(struct gendisk *disk, struct nvme_ns *ns)
+{
+	struct nvme_ctrl *ctrl = ns->ctrl;
+	struct request_queue *queue = disk->queue;
+	u32 size = queue_logical_block_size(queue);
+
+	if (!(ctrl->oncs & NVME_CTRL_ONCS_DSM)) {
+		blk_queue_flag_clear(QUEUE_FLAG_DISCARD, queue);
+		return;
+	}
+
+	if (ctrl->nr_streams && ns->sws && ns->sgs)
+		size *= ns->sws * ns->sgs;
+
+	BUILD_BUG_ON(PAGE_SIZE / sizeof(struct nvme_dsm_range) <
+			NVME_DSM_MAX_RANGES);
+
+	queue->limits.discard_alignment = 0;
+	queue->limits.discard_granularity = size;
+
+	/* If discard is already enabled, don't reset queue limits */
+	if (blk_queue_flag_test_and_set(QUEUE_FLAG_DISCARD, queue))
+		return;
+
+	blk_queue_max_discard_sectors(queue, UINT_MAX);
+	blk_queue_max_discard_segments(queue, NVME_DSM_MAX_RANGES);
+
+	if (ctrl->quirks & NVME_QUIRK_DEALLOCATE_ZEROES)
+		blk_queue_max_write_zeroes_sectors(queue, UINT_MAX);
+}
+
+static void nvme_config_write_zeroes(struct gendisk *disk, struct nvme_ns *ns)
+{
+	u32 max_sectors;
+	unsigned short bs = 1 << ns->lba_shift;
+
+	if (!(ns->ctrl->oncs & NVME_CTRL_ONCS_WRITE_ZEROES) ||
+	    (ns->ctrl->quirks & NVME_QUIRK_DISABLE_WRITE_ZEROES))
+		return;
+	/*
+	 * Even though NVMe spec explicitly states that MDTS is not
+	 * applicable to the write-zeroes:- "The restriction does not apply to
+	 * commands that do not transfer data between the host and the
+	 * controller (e.g., Write Uncorrectable ro Write Zeroes command).".
+	 * In order to be more cautious use controller's max_hw_sectors value
+	 * to configure the maximum sectors for the write-zeroes which is
+	 * configured based on the controller's MDTS field in the
+	 * nvme_init_identify() if available.
+	 */
+	if (ns->ctrl->max_hw_sectors == UINT_MAX)
+		max_sectors = ((u32)(USHRT_MAX + 1) * bs) >> 9;
+	else
+		max_sectors = ((u32)(ns->ctrl->max_hw_sectors + 1) * bs) >> 9;
+
+	blk_queue_max_write_zeroes_sectors(disk->queue, max_sectors);
+}
+
+static void nvme_report_ns_ids(struct nvme_ctrl *ctrl, unsigned int nsid,
+		struct nvme_id_ns *id, struct nvme_ns_ids *ids)
+{
+	memset(ids, 0, sizeof(*ids));
+
+	if (ctrl->vs >= NVME_VS(1, 1, 0))
+		memcpy(ids->eui64, id->eui64, sizeof(id->eui64));
+	if (ctrl->vs >= NVME_VS(1, 2, 0))
+		memcpy(ids->nguid, id->nguid, sizeof(id->nguid));
+	if (ctrl->vs >= NVME_VS(1, 3, 0)) {
+		 /* Don't treat error as fatal we potentially
+		  * already have a NGUID or EUI-64
+		  */
+		if (nvme_identify_ns_descs(ctrl, nsid, ids))
+			dev_warn(ctrl->device,
+				 "%s: Identify Descriptors failed\n", __func__);
+	}
+}
+
+static bool nvme_ns_ids_valid(struct nvme_ns_ids *ids)
+{
+	return !uuid_is_null(&ids->uuid) ||
+		memchr_inv(ids->nguid, 0, sizeof(ids->nguid)) ||
+		memchr_inv(ids->eui64, 0, sizeof(ids->eui64));
+}
+
+static bool nvme_ns_ids_equal(struct nvme_ns_ids *a, struct nvme_ns_ids *b)
+{
+	return uuid_equal(&a->uuid, &b->uuid) &&
+		memcmp(&a->nguid, &b->nguid, sizeof(a->nguid)) == 0 &&
+		memcmp(&a->eui64, &b->eui64, sizeof(a->eui64)) == 0;
+}
+
+static void nvme_update_disk_info(struct gendisk *disk,
+		struct nvme_ns *ns, struct nvme_id_ns *id)
+{
+	sector_t capacity = le64_to_cpup(&id->nsze) << (ns->lba_shift - 9);
+	unsigned short bs = 1 << ns->lba_shift;
+
+	blk_mq_freeze_queue(disk->queue);
+	blk_integrity_unregister(disk);
+
+	blk_queue_logical_block_size(disk->queue, bs);
+	blk_queue_physical_block_size(disk->queue, bs);
+	blk_queue_io_min(disk->queue, bs);
+
+	if (ns->ms && !ns->ext &&
+	    (ns->ctrl->ops->flags & NVME_F_METADATA_SUPPORTED))
+		nvme_init_integrity(disk, ns->ms, ns->pi_type);
+	if (ns->ms && !nvme_ns_has_pi(ns) && !blk_get_integrity(disk))
+		capacity = 0;
+
+	set_capacity(disk, capacity);
+
+	nvme_config_discard(disk, ns);
+	nvme_config_write_zeroes(disk, ns);
+
+	if (id->nsattr & (1 << 0))
+		set_disk_ro(disk, true);
+	else
+		set_disk_ro(disk, false);
+
+	blk_mq_unfreeze_queue(disk->queue);
+}
+
+static void __nvme_revalidate_disk(struct gendisk *disk, struct nvme_id_ns *id)
+{
+	struct nvme_ns *ns = disk->private_data;
+
+	/*
+	 * If identify namespace failed, use default 512 byte block size so
+	 * block layer can use before failing read/write for 0 capacity.
+	 */
+	ns->lba_shift = id->lbaf[id->flbas & NVME_NS_FLBAS_LBA_MASK].ds;
+	if (ns->lba_shift == 0)
+		ns->lba_shift = 9;
+	ns->noiob = le16_to_cpu(id->noiob);
+	ns->ms = le16_to_cpu(id->lbaf[id->flbas & NVME_NS_FLBAS_LBA_MASK].ms);
+	ns->ext = ns->ms && (id->flbas & NVME_NS_FLBAS_META_EXT);
+	/* the PI implementation requires metadata equal t10 pi tuple size */
+	if (ns->ms == sizeof(struct t10_pi_tuple))
+		ns->pi_type = id->dps & NVME_NS_DPS_PI_MASK;
+	else
+		ns->pi_type = 0;
+
+	if (ns->noiob)
+		nvme_set_chunk_size(ns);
+	nvme_update_disk_info(disk, ns, id);
+#ifdef CONFIG_NVME_MULTIPATH
+	if (ns->head->disk) {
+		nvme_update_disk_info(ns->head->disk, ns, id);
+		blk_queue_stack_limits(ns->head->disk->queue, ns->queue);
+	}
+#endif
+}
+
+static int nvme_revalidate_disk(struct gendisk *disk)
+{
+	struct nvme_ns *ns = disk->private_data;
+	struct nvme_ctrl *ctrl = ns->ctrl;
+	struct nvme_id_ns *id;
+	struct nvme_ns_ids ids;
+	int ret = 0;
+
+	if (test_bit(NVME_NS_DEAD, &ns->flags)) {
+		set_capacity(disk, 0);
+		return -ENODEV;
+	}
+
+	id = nvme_identify_ns(ctrl, ns->head->ns_id);
+	if (!id)
+		return -ENODEV;
+
+	if (id->ncap == 0) {
+		ret = -ENODEV;
+		goto out;
+	}
+
+	__nvme_revalidate_disk(disk, id);
+	nvme_report_ns_ids(ctrl, ns->head->ns_id, id, &ids);
+	if (!nvme_ns_ids_equal(&ns->head->ids, &ids)) {
+		dev_err(ctrl->device,
+			"identifiers changed for nsid %d\n", ns->head->ns_id);
+		ret = -ENODEV;
+	}
+
+out:
+	kfree(id);
+	return ret;
+}
+
+static char nvme_pr_type(enum pr_type type)
+{
+	switch (type) {
+	case PR_WRITE_EXCLUSIVE:
+		return 1;
+	case PR_EXCLUSIVE_ACCESS:
+		return 2;
+	case PR_WRITE_EXCLUSIVE_REG_ONLY:
+		return 3;
+	case PR_EXCLUSIVE_ACCESS_REG_ONLY:
+		return 4;
+	case PR_WRITE_EXCLUSIVE_ALL_REGS:
+		return 5;
+	case PR_EXCLUSIVE_ACCESS_ALL_REGS:
+		return 6;
+	default:
+		return 0;
+	}
+};
+
+static int nvme_pr_command(struct block_device *bdev, u32 cdw10,
+				u64 key, u64 sa_key, u8 op)
+{
+	struct nvme_ns_head *head = NULL;
+	struct nvme_ns *ns;
+	struct nvme_command c;
+	int srcu_idx, ret;
+	u8 data[16] = { 0, };
+
+	ns = nvme_get_ns_from_disk(bdev->bd_disk, &head, &srcu_idx);
+	if (unlikely(!ns))
+		return -EWOULDBLOCK;
+
+	put_unaligned_le64(key, &data[0]);
+	put_unaligned_le64(sa_key, &data[8]);
+
+	memset(&c, 0, sizeof(c));
+	c.common.opcode = op;
+	c.common.nsid = cpu_to_le32(ns->head->ns_id);
+	c.common.cdw10 = cpu_to_le32(cdw10);
+
+	ret = nvme_submit_sync_cmd(ns->queue, &c, data, 16);
+	nvme_put_ns_from_disk(head, srcu_idx);
+	return ret;
+}
+
+static int nvme_pr_register(struct block_device *bdev, u64 old,
+		u64 new, unsigned flags)
+{
+	u32 cdw10;
+
+	if (flags & ~PR_FL_IGNORE_KEY)
+		return -EOPNOTSUPP;
+
+	cdw10 = old ? 2 : 0;
+	cdw10 |= (flags & PR_FL_IGNORE_KEY) ? 1 << 3 : 0;
+	cdw10 |= (1 << 30) | (1 << 31); /* PTPL=1 */
+	return nvme_pr_command(bdev, cdw10, old, new, nvme_cmd_resv_register);
+}
+
+static int nvme_pr_reserve(struct block_device *bdev, u64 key,
+		enum pr_type type, unsigned flags)
+{
+	u32 cdw10;
+
+	if (flags & ~PR_FL_IGNORE_KEY)
+		return -EOPNOTSUPP;
+
+	cdw10 = nvme_pr_type(type) << 8;
+	cdw10 |= ((flags & PR_FL_IGNORE_KEY) ? 1 << 3 : 0);
+	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_acquire);
+}
+
+static int nvme_pr_preempt(struct block_device *bdev, u64 old, u64 new,
+		enum pr_type type, bool abort)
+{
+	u32 cdw10 = nvme_pr_type(type) << 8 | (abort ? 2 : 1);
+	return nvme_pr_command(bdev, cdw10, old, new, nvme_cmd_resv_acquire);
+}
+
+static int nvme_pr_clear(struct block_device *bdev, u64 key)
+{
+	u32 cdw10 = 1 | (key ? 1 << 3 : 0);
+	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_register);
+}
+
+static int nvme_pr_release(struct block_device *bdev, u64 key, enum pr_type type)
+{
+	u32 cdw10 = nvme_pr_type(type) << 8 | (key ? 1 << 3 : 0);
+	return nvme_pr_command(bdev, cdw10, key, 0, nvme_cmd_resv_release);
+}
+
+static const struct pr_ops nvme_pr_ops = {
+	.pr_register	= nvme_pr_register,
+	.pr_reserve	= nvme_pr_reserve,
+	.pr_release	= nvme_pr_release,
+	.pr_preempt	= nvme_pr_preempt,
+	.pr_clear	= nvme_pr_clear,
+};
+
+#ifdef CONFIG_BLK_SED_OPAL
+int nvme_sec_submit(void *data, u16 spsp, u8 secp, void *buffer, size_t len,
+		bool send)
+{
+	struct nvme_ctrl *ctrl = data;
+	struct nvme_command cmd;
+
+	memset(&cmd, 0, sizeof(cmd));
+	if (send)
+		cmd.common.opcode = nvme_admin_security_send;
+	else
+		cmd.common.opcode = nvme_admin_security_recv;
+	cmd.common.nsid = 0;
+	cmd.common.cdw10 = cpu_to_le32(((u32)secp) << 24 | ((u32)spsp) << 8);
+	cmd.common.cdw11 = cpu_to_le32(len);
+
+	return __nvme_submit_sync_cmd(ctrl->admin_q, &cmd, NULL, buffer, len,
+				      ADMIN_TIMEOUT, NVME_QID_ANY, 1, 0, false);
+}
+EXPORT_SYMBOL_GPL(nvme_sec_submit);
+#endif /* CONFIG_BLK_SED_OPAL */
+
+static const struct block_device_operations nvme_fops = {
+	.owner		= THIS_MODULE,
+	.ioctl		= nvme_ioctl,
+	.compat_ioctl	= nvme_ioctl,
+	.open		= nvme_open,
+	.release	= nvme_release,
+	.getgeo		= nvme_getgeo,
+	.revalidate_disk= nvme_revalidate_disk,
+	.pr_ops		= &nvme_pr_ops,
+};
+
+#ifdef CONFIG_NVME_MULTIPATH
+static int nvme_ns_head_open(struct block_device *bdev, fmode_t mode)
+{
+	struct nvme_ns_head *head = bdev->bd_disk->private_data;
+
+	if (!kref_get_unless_zero(&head->ref))
+		return -ENXIO;
+	return 0;
+}
+
+static void nvme_ns_head_release(struct gendisk *disk, fmode_t mode)
+{
+	nvme_put_ns_head(disk->private_data);
+}
+
+const struct block_device_operations nvme_ns_head_ops = {
+	.owner		= THIS_MODULE,
+	.open		= nvme_ns_head_open,
+	.release	= nvme_ns_head_release,
+	.ioctl		= nvme_ioctl,
+	.compat_ioctl	= nvme_ioctl,
+	.getgeo		= nvme_getgeo,
+	.pr_ops		= &nvme_pr_ops,
+};
+#endif /* CONFIG_NVME_MULTIPATH */
+
+static int nvme_wait_ready(struct nvme_ctrl *ctrl, u64 cap, bool enabled)
+{
+	unsigned long timeout =
+		((NVME_CAP_TIMEOUT(cap) + 1) * HZ / 2) + jiffies;
+	u32 csts, bit = enabled ? NVME_CSTS_RDY : 0;
+	int ret;
+
+	while ((ret = ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts)) == 0) {
+		if (csts == ~0)
+			return -ENODEV;
+		if ((csts & NVME_CSTS_RDY) == bit)
+			break;
+
+		msleep(100);
+		if (fatal_signal_pending(current))
+			return -EINTR;
+		if (time_after(jiffies, timeout)) {
+			dev_err(ctrl->device,
+				"Device not ready; aborting %s\n", enabled ?
+						"initialisation" : "reset");
+			return -ENODEV;
+		}
+	}
+
+	return ret;
+}
+
+/*
+ * If the device has been passed off to us in an enabled state, just clear
+ * the enabled bit.  The spec says we should set the 'shutdown notification
+ * bits', but doing so may cause the device to complete commands to the
+ * admin queue ... and we don't know what memory that might be pointing at!
+ */
+int nvme_disable_ctrl(struct nvme_ctrl *ctrl, u64 cap)
+{
+	int ret;
+
+	ctrl->ctrl_config &= ~NVME_CC_SHN_MASK;
+	ctrl->ctrl_config &= ~NVME_CC_ENABLE;
+
+	ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	if (ret)
+		return ret;
+
+	if (ctrl->quirks & NVME_QUIRK_DELAY_BEFORE_CHK_RDY)
+		msleep(NVME_QUIRK_DELAY_AMOUNT);
+
+	return nvme_wait_ready(ctrl, cap, false);
+}
+EXPORT_SYMBOL_GPL(nvme_disable_ctrl);
+
+int nvme_enable_ctrl(struct nvme_ctrl *ctrl, u64 cap)
+{
+	/*
+	 * Default to a 4K page size, with the intention to update this
+	 * path in the future to accomodate architectures with differing
+	 * kernel and IO page sizes.
+	 */
+	unsigned dev_page_min = NVME_CAP_MPSMIN(cap) + 12, page_shift = 12;
+	int ret;
+
+	if (page_shift < dev_page_min) {
+		dev_err(ctrl->device,
+			"Minimum device page size %u too large for host (%u)\n",
+			1 << dev_page_min, 1 << page_shift);
+		return -ENODEV;
+	}
+
+	ctrl->page_size = 1 << page_shift;
+
+	ctrl->ctrl_config = NVME_CC_CSS_NVM;
+	ctrl->ctrl_config |= (page_shift - 12) << NVME_CC_MPS_SHIFT;
+	ctrl->ctrl_config |= NVME_CC_AMS_RR | NVME_CC_SHN_NONE;
+	ctrl->ctrl_config |= NVME_CC_IOSQES | NVME_CC_IOCQES;
+	ctrl->ctrl_config |= NVME_CC_ENABLE;
+
+	ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	if (ret)
+		return ret;
+	return nvme_wait_ready(ctrl, cap, true);
+}
+EXPORT_SYMBOL_GPL(nvme_enable_ctrl);
+
+int nvme_shutdown_ctrl(struct nvme_ctrl *ctrl)
+{
+	unsigned long timeout = jiffies + (ctrl->shutdown_timeout * HZ);
+	u32 csts;
+	int ret;
+
+	ctrl->ctrl_config &= ~NVME_CC_SHN_MASK;
+	ctrl->ctrl_config |= NVME_CC_SHN_NORMAL;
+
+	ret = ctrl->ops->reg_write32(ctrl, NVME_REG_CC, ctrl->ctrl_config);
+	if (ret)
+		return ret;
+
+	while ((ret = ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts)) == 0) {
+		if ((csts & NVME_CSTS_SHST_MASK) == NVME_CSTS_SHST_CMPLT)
+			break;
+
+		msleep(100);
+		if (fatal_signal_pending(current))
+			return -EINTR;
+		if (time_after(jiffies, timeout)) {
+			dev_err(ctrl->device,
+				"Device shutdown incomplete; abort shutdown\n");
+			return -ENODEV;
+		}
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvme_shutdown_ctrl);
+
+static void nvme_set_queue_limits(struct nvme_ctrl *ctrl,
+		struct request_queue *q)
+{
+	bool vwc = false;
+
+	if (ctrl->max_hw_sectors) {
+		u32 max_segments =
+			(ctrl->max_hw_sectors / (ctrl->page_size >> 9)) + 1;
+
+		max_segments = min_not_zero(max_segments, ctrl->max_segments);
+		pr_debug("%s: max hw sectors %u\n", __FUNCTION__, ctrl->max_hw_sectors);
+		blk_queue_max_hw_sectors(q, ctrl->max_hw_sectors);
+		blk_queue_max_segments(q, min_t(u32, max_segments, USHRT_MAX));
+	}
+	if ((ctrl->quirks & NVME_QUIRK_STRIPE_SIZE) &&
+	    is_power_of_2(ctrl->max_hw_sectors))
+		blk_queue_chunk_sectors(q, ctrl->max_hw_sectors);
+	blk_queue_virt_boundary(q, ctrl->page_size - 1);
+	if (ctrl->vwc & NVME_CTRL_VWC_PRESENT)
+		vwc = true;
+	blk_queue_write_cache(q, vwc, vwc);
+}
+
+static int nvme_configure_timestamp(struct nvme_ctrl *ctrl)
+{
+	__le64 ts;
+	int ret;
+
+	if (!(ctrl->oncs & NVME_CTRL_ONCS_TIMESTAMP))
+		return 0;
+
+	ts = cpu_to_le64(ktime_to_ms(ktime_get_real()));
+	ret = nvme_set_features(ctrl, NVME_FEAT_TIMESTAMP, 0, &ts, sizeof(ts),
+			NULL);
+	if (ret)
+		dev_warn_once(ctrl->device,
+			"could not set timestamp (%d)\n", ret);
+	return ret;
+}
+
+static int nvme_configure_acre(struct nvme_ctrl *ctrl)
+{
+	struct nvme_feat_host_behavior *host;
+	int ret;
+
+	/* Don't bother enabling the feature if retry delay is not reported */
+	if (!ctrl->crdt[0])
+		return 0;
+
+	host = kzalloc(sizeof(*host), GFP_KERNEL);
+	if (!host)
+		return 0;
+
+	host->acre = NVME_ENABLE_ACRE;
+	ret = nvme_set_features(ctrl, NVME_FEAT_HOST_BEHAVIOR, 0,
+				host, sizeof(*host), NULL);
+	kfree(host);
+	return ret;
+}
+
+static int nvme_configure_apst(struct nvme_ctrl *ctrl)
+{
+	/*
+	 * APST (Autonomous Power State Transition) lets us program a
+	 * table of power state transitions that the controller will
+	 * perform automatically.  We configure it with a simple
+	 * heuristic: we are willing to spend at most 2% of the time
+	 * transitioning between power states.  Therefore, when running
+	 * in any given state, we will enter the next lower-power
+	 * non-operational state after waiting 50 * (enlat + exlat)
+	 * microseconds, as long as that state's exit latency is under
+	 * the requested maximum latency.
+	 *
+	 * We will not autonomously enter any non-operational state for
+	 * which the total latency exceeds ps_max_latency_us.  Users
+	 * can set ps_max_latency_us to zero to turn off APST.
+	 */
+
+	unsigned apste;
+	struct nvme_feat_auto_pst *table;
+	u64 max_lat_us = 0;
+	int max_ps = -1;
+	int ret;
+
+	/*
+	 * If APST isn't supported or if we haven't been initialized yet,
+	 * then don't do anything.
+	 */
+	if (!ctrl->apsta)
+		return 0;
+
+	if (ctrl->npss > 31) {
+		dev_warn(ctrl->device, "NPSS is invalid; not using APST\n");
+		return 0;
+	}
+
+	table = kzalloc(sizeof(*table), GFP_KERNEL);
+	if (!table)
+		return 0;
+
+	if (!ctrl->apst_enabled || ctrl->ps_max_latency_us == 0) {
+		/* Turn off APST. */
+		apste = 0;
+		dev_dbg(ctrl->device, "APST disabled\n");
+	} else {
+		__le64 target = cpu_to_le64(0);
+		int state;
+
+		/*
+		 * Walk through all states from lowest- to highest-power.
+		 * According to the spec, lower-numbered states use more
+		 * power.  NPSS, despite the name, is the index of the
+		 * lowest-power state, not the number of states.
+		 */
+		for (state = (int)ctrl->npss; state >= 0; state--) {
+			u64 total_latency_us, exit_latency_us, transition_ms;
+
+			if (target)
+				table->entries[state] = target;
+
+			/*
+			 * Don't allow transitions to the deepest state
+			 * if it's quirked off.
+			 */
+			if (state == ctrl->npss &&
+			    (ctrl->quirks & NVME_QUIRK_NO_DEEPEST_PS))
+				continue;
+
+			/*
+			 * Is this state a useful non-operational state for
+			 * higher-power states to autonomously transition to?
+			 */
+			if (!(ctrl->psd[state].flags &
+			      NVME_PS_FLAGS_NON_OP_STATE))
+				continue;
+
+			exit_latency_us =
+				(u64)le32_to_cpu(ctrl->psd[state].exit_lat);
+			if (exit_latency_us > ctrl->ps_max_latency_us)
+				continue;
+
+			total_latency_us =
+				exit_latency_us +
+				le32_to_cpu(ctrl->psd[state].entry_lat);
+
+			/*
+			 * This state is good.  Use it as the APST idle
+			 * target for higher power states.
+			 */
+			transition_ms = total_latency_us + 19;
+			do_div(transition_ms, 20);
+			if (transition_ms > (1 << 24) - 1)
+				transition_ms = (1 << 24) - 1;
+
+			target = cpu_to_le64((state << 3) |
+					     (transition_ms << 8));
+
+			if (max_ps == -1)
+				max_ps = state;
+
+			if (total_latency_us > max_lat_us)
+				max_lat_us = total_latency_us;
+		}
+
+		apste = 1;
+
+		if (max_ps == -1) {
+			dev_dbg(ctrl->device, "APST enabled but no non-operational states are available\n");
+		} else {
+			dev_dbg(ctrl->device, "APST enabled: max PS = %d, max round-trip latency = %lluus, table = %*phN\n",
+				max_ps, max_lat_us, (int)sizeof(*table), table);
+		}
+	}
+
+	ret = nvme_set_features(ctrl, NVME_FEAT_AUTO_PST, apste,
+				table, sizeof(*table), NULL);
+	if (ret)
+		dev_err(ctrl->device, "failed to set APST feature (%d)\n", ret);
+
+	kfree(table);
+	return ret;
+}
+
+static void nvme_set_latency_tolerance(struct device *dev, s32 val)
+{
+	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
+	u64 latency;
+
+	switch (val) {
+	case PM_QOS_LATENCY_TOLERANCE_NO_CONSTRAINT:
+	case PM_QOS_LATENCY_ANY:
+		latency = U64_MAX;
+		break;
+
+	default:
+		latency = val;
+	}
+
+	if (ctrl->ps_max_latency_us != latency) {
+		ctrl->ps_max_latency_us = latency;
+		nvme_configure_apst(ctrl);
+	}
+}
+
+struct nvme_core_quirk_entry {
+	/*
+	 * NVMe model and firmware strings are padded with spaces.  For
+	 * simplicity, strings in the quirk table are padded with NULLs
+	 * instead.
+	 */
+	u16 vid;
+	const char *mn;
+	const char *fr;
+	unsigned long quirks;
+};
+
+static const struct nvme_core_quirk_entry core_quirks[] = {
+	{
+		/*
+		 * This Toshiba device seems to die using any APST states.  See:
+		 * https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1678184/comments/11
+		 */
+		.vid = 0x1179,
+		.mn = "THNSF5256GPUK TOSHIBA",
+		.quirks = NVME_QUIRK_NO_APST,
+	}
+};
+
+/* match is null-terminated but idstr is space-padded. */
+static bool string_matches(const char *idstr, const char *match, size_t len)
+{
+	size_t matchlen;
+
+	if (!match)
+		return true;
+
+	matchlen = strlen(match);
+	WARN_ON_ONCE(matchlen > len);
+
+	if (memcmp(idstr, match, matchlen))
+		return false;
+
+	for (; matchlen < len; matchlen++)
+		if (idstr[matchlen] != ' ')
+			return false;
+
+	return true;
+}
+
+static bool quirk_matches(const struct nvme_id_ctrl *id,
+			  const struct nvme_core_quirk_entry *q)
+{
+	return q->vid == le16_to_cpu(id->vid) &&
+		string_matches(id->mn, q->mn, sizeof(id->mn)) &&
+		string_matches(id->fr, q->fr, sizeof(id->fr));
+}
+
+static void nvme_init_subnqn(struct nvme_subsystem *subsys, struct nvme_ctrl *ctrl,
+		struct nvme_id_ctrl *id)
+{
+	size_t nqnlen;
+	int off;
+
+	if(!(ctrl->quirks & NVME_QUIRK_IGNORE_DEV_SUBNQN)) {
+		nqnlen = strnlen(id->subnqn, NVMF_NQN_SIZE);
+		if (nqnlen > 0 && nqnlen < NVMF_NQN_SIZE) {
+			strlcpy(subsys->subnqn, id->subnqn, NVMF_NQN_SIZE);
+			return;
+		}
+
+		if (ctrl->vs >= NVME_VS(1, 2, 1))
+			dev_warn(ctrl->device, "missing or invalid SUBNQN field.\n");
+	}
+
+	/* Generate a "fake" NQN per Figure 254 in NVMe 1.3 + ECN 001 */
+	off = snprintf(subsys->subnqn, NVMF_NQN_SIZE,
+			"nqn.2014.08.org.nvmexpress:%04x%04x",
+			le16_to_cpu(id->vid), le16_to_cpu(id->ssvid));
+	memcpy(subsys->subnqn + off, id->sn, sizeof(id->sn));
+	off += sizeof(id->sn);
+	memcpy(subsys->subnqn + off, id->mn, sizeof(id->mn));
+	off += sizeof(id->mn);
+	memset(subsys->subnqn + off, 0, sizeof(subsys->subnqn) - off);
+}
+
+static void __nvme_release_subsystem(struct nvme_subsystem *subsys)
+{
+	ida_simple_remove(&nvme_subsystems_ida, subsys->instance);
+	kfree(subsys);
+}
+
+static void nvme_release_subsystem(struct device *dev)
+{
+	__nvme_release_subsystem(container_of(dev, struct nvme_subsystem, dev));
+}
+
+static void nvme_destroy_subsystem(struct kref *ref)
+{
+	struct nvme_subsystem *subsys =
+			container_of(ref, struct nvme_subsystem, ref);
+
+	mutex_lock(&nvme_subsystems_lock);
+	list_del(&subsys->entry);
+	mutex_unlock(&nvme_subsystems_lock);
+
+	ida_destroy(&subsys->ns_ida);
+	device_del(&subsys->dev);
+	put_device(&subsys->dev);
+}
+
+static void nvme_put_subsystem(struct nvme_subsystem *subsys)
+{
+	kref_put(&subsys->ref, nvme_destroy_subsystem);
+}
+
+static struct nvme_subsystem *__nvme_find_get_subsystem(const char *subsysnqn)
+{
+	struct nvme_subsystem *subsys;
+
+	lockdep_assert_held(&nvme_subsystems_lock);
+
+	list_for_each_entry(subsys, &nvme_subsystems, entry) {
+		if (strcmp(subsys->subnqn, subsysnqn))
+			continue;
+		if (!kref_get_unless_zero(&subsys->ref))
+			continue;
+		return subsys;
+	}
+
+	return NULL;
+}
+
+#define SUBSYS_ATTR_RO(_name, _mode, _show)			\
+	struct device_attribute subsys_attr_##_name = \
+		__ATTR(_name, _mode, _show, NULL)
+
+static ssize_t nvme_subsys_show_nqn(struct device *dev,
+				    struct device_attribute *attr,
+				    char *buf)
+{
+	struct nvme_subsystem *subsys =
+		container_of(dev, struct nvme_subsystem, dev);
+
+	return snprintf(buf, PAGE_SIZE, "%s\n", subsys->subnqn);
+}
+static SUBSYS_ATTR_RO(subsysnqn, S_IRUGO, nvme_subsys_show_nqn);
+
+#define nvme_subsys_show_str_function(field)				\
+static ssize_t subsys_##field##_show(struct device *dev,		\
+			    struct device_attribute *attr, char *buf)	\
+{									\
+	struct nvme_subsystem *subsys =					\
+		container_of(dev, struct nvme_subsystem, dev);		\
+	return sprintf(buf, "%.*s\n",					\
+		       (int)sizeof(subsys->field), subsys->field);	\
+}									\
+static SUBSYS_ATTR_RO(field, S_IRUGO, subsys_##field##_show);
+
+nvme_subsys_show_str_function(model);
+nvme_subsys_show_str_function(serial);
+nvme_subsys_show_str_function(firmware_rev);
+
+static struct attribute *nvme_subsys_attrs[] = {
+	&subsys_attr_model.attr,
+	&subsys_attr_serial.attr,
+	&subsys_attr_firmware_rev.attr,
+	&subsys_attr_subsysnqn.attr,
+#ifdef CONFIG_NVME_MULTIPATH
+	&subsys_attr_iopolicy.attr,
+#endif
+	NULL,
+};
+
+static struct attribute_group nvme_subsys_attrs_group = {
+	.attrs = nvme_subsys_attrs,
+};
+
+static const struct attribute_group *nvme_subsys_attrs_groups[] = {
+	&nvme_subsys_attrs_group,
+	NULL,
+};
+
+static int nvme_active_ctrls(struct nvme_subsystem *subsys)
+{
+	int count = 0;
+	struct nvme_ctrl *ctrl;
+
+	mutex_lock(&subsys->lock);
+	list_for_each_entry(ctrl, &subsys->ctrls, subsys_entry) {
+		if (ctrl->state != NVME_CTRL_DELETING &&
+		    ctrl->state != NVME_CTRL_DEAD)
+			count++;
+	}
+	mutex_unlock(&subsys->lock);
+
+	return count;
+}
+
+static int nvme_init_subsystem(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
+{
+	struct nvme_subsystem *subsys, *found;
+	int ret;
+
+	subsys = kzalloc(sizeof(*subsys), GFP_KERNEL);
+	if (!subsys)
+		return -ENOMEM;
+	ret = ida_simple_get(&nvme_subsystems_ida, 0, 0, GFP_KERNEL);
+	if (ret < 0) {
+		kfree(subsys);
+		return ret;
+	}
+	subsys->instance = ret;
+	mutex_init(&subsys->lock);
+	kref_init(&subsys->ref);
+	INIT_LIST_HEAD(&subsys->ctrls);
+	INIT_LIST_HEAD(&subsys->nsheads);
+	nvme_init_subnqn(subsys, ctrl, id);
+	memcpy(subsys->serial, id->sn, sizeof(subsys->serial));
+	memcpy(subsys->model, id->mn, sizeof(subsys->model));
+	memcpy(subsys->firmware_rev, id->fr, sizeof(subsys->firmware_rev));
+	subsys->vendor_id = le16_to_cpu(id->vid);
+	subsys->cmic = id->cmic;
+#ifdef CONFIG_NVME_MULTIPATH
+	subsys->iopolicy = NVME_IOPOLICY_NUMA;
+#endif
+
+	subsys->dev.class = nvme_subsys_class;
+	subsys->dev.release = nvme_release_subsystem;
+	subsys->dev.groups = nvme_subsys_attrs_groups;
+	dev_set_name(&subsys->dev, "nvme-subsys%d", subsys->instance);
+	device_initialize(&subsys->dev);
+
+	mutex_lock(&nvme_subsystems_lock);
+	found = __nvme_find_get_subsystem(subsys->subnqn);
+	if (found) {
+		/*
+		 * Verify that the subsystem actually supports multiple
+		 * controllers, else bail out.
+		 */
+		if (!(ctrl->opts && ctrl->opts->discovery_nqn) &&
+		    nvme_active_ctrls(found) && !(id->cmic & (1 << 1))) {
+			dev_err(ctrl->device,
+				"ignoring ctrl due to duplicate subnqn (%s).\n",
+				found->subnqn);
+			nvme_put_subsystem(found);
+			ret = -EINVAL;
+			goto out_unlock;
+		}
+
+		__nvme_release_subsystem(subsys);
+		subsys = found;
+	} else {
+		ret = device_add(&subsys->dev);
+		if (ret) {
+			dev_err(ctrl->device,
+				"failed to register subsystem device.\n");
+			goto out_unlock;
+		}
+		ida_init(&subsys->ns_ida);
+		list_add_tail(&subsys->entry, &nvme_subsystems);
+	}
+
+	ctrl->subsys = subsys;
+	mutex_unlock(&nvme_subsystems_lock);
+
+	if (sysfs_create_link(&subsys->dev.kobj, &ctrl->device->kobj,
+			dev_name(ctrl->device))) {
+		dev_err(ctrl->device,
+			"failed to create sysfs link from subsystem.\n");
+		/* the transport driver will eventually put the subsystem */
+		return -EINVAL;
+	}
+
+	mutex_lock(&subsys->lock);
+	list_add_tail(&ctrl->subsys_entry, &subsys->ctrls);
+	mutex_unlock(&subsys->lock);
+
+	return 0;
+
+out_unlock:
+	mutex_unlock(&nvme_subsystems_lock);
+	put_device(&subsys->dev);
+	return ret;
+}
+
+int nvme_get_log(struct nvme_ctrl *ctrl, u32 nsid, u8 log_page, u8 lsp,
+		void *log, size_t size, u64 offset)
+{
+	struct nvme_command c = { };
+	unsigned long dwlen = size / 4 - 1;
+
+	c.get_log_page.opcode = nvme_admin_get_log_page;
+	c.get_log_page.nsid = cpu_to_le32(nsid);
+	c.get_log_page.lid = log_page;
+	c.get_log_page.lsp = lsp;
+	c.get_log_page.numdl = cpu_to_le16(dwlen & ((1 << 16) - 1));
+	c.get_log_page.numdu = cpu_to_le16(dwlen >> 16);
+	c.get_log_page.lpol = cpu_to_le32(lower_32_bits(offset));
+	c.get_log_page.lpou = cpu_to_le32(upper_32_bits(offset));
+
+	return nvme_submit_sync_cmd(ctrl->admin_q, &c, log, size);
+}
+
+static int nvme_get_effects_log(struct nvme_ctrl *ctrl)
+{
+	int ret;
+
+	if (!ctrl->effects)
+		ctrl->effects = kzalloc(sizeof(*ctrl->effects), GFP_KERNEL);
+
+	if (!ctrl->effects)
+		return 0;
+
+	ret = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_CMD_EFFECTS, 0,
+			ctrl->effects, sizeof(*ctrl->effects), 0);
+	if (ret) {
+		kfree(ctrl->effects);
+		ctrl->effects = NULL;
+	}
+	return ret;
+}
+
+/*
+ * Initialize the cached copies of the Identify data and various controller
+ * register in our nvme_ctrl structure.  This should be called as soon as
+ * the admin queue is fully up and running.
+ */
+int nvme_init_identify(struct nvme_ctrl *ctrl)
+{
+	struct nvme_id_ctrl *id;
+	u64 cap;
+	int ret, page_shift;
+	u32 max_hw_sectors;
+	bool prev_apst_enabled;
+
+	ret = ctrl->ops->reg_read32(ctrl, NVME_REG_VS, &ctrl->vs);
+	if (ret) {
+		dev_err(ctrl->device, "Reading VS failed (%d)\n", ret);
+		return ret;
+	}
+
+	ret = ctrl->ops->reg_read64(ctrl, NVME_REG_CAP, &cap);
+	if (ret) {
+		dev_err(ctrl->device, "Reading CAP failed (%d)\n", ret);
+		return ret;
+	}
+	page_shift = NVME_CAP_MPSMIN(cap) + 12;
+
+	if (ctrl->vs >= NVME_VS(1, 1, 0))
+		ctrl->subsystem = NVME_CAP_NSSRC(cap);
+
+	ret = nvme_identify_ctrl(ctrl, &id);
+	if (ret) {
+		dev_err(ctrl->device, "Identify Controller failed (%d)\n", ret);
+		return -EIO;
+	}
+
+	if (id->lpa & NVME_CTRL_LPA_CMD_EFFECTS_LOG) {
+		ret = nvme_get_effects_log(ctrl);
+		if (ret < 0)
+			goto out_free;
+	}
+
+	if (!ctrl->identified) {
+		int i;
+
+		ret = nvme_init_subsystem(ctrl, id);
+		if (ret)
+			goto out_free;
+
+		/*
+		 * Check for quirks.  Quirk can depend on firmware version,
+		 * so, in principle, the set of quirks present can change
+		 * across a reset.  As a possible future enhancement, we
+		 * could re-scan for quirks every time we reinitialize
+		 * the device, but we'd have to make sure that the driver
+		 * behaves intelligently if the quirks change.
+		 */
+		for (i = 0; i < ARRAY_SIZE(core_quirks); i++) {
+			if (quirk_matches(id, &core_quirks[i]))
+				ctrl->quirks |= core_quirks[i].quirks;
+		}
+	}
+
+	if (force_apst && (ctrl->quirks & NVME_QUIRK_NO_DEEPEST_PS)) {
+		dev_warn(ctrl->device, "forcibly allowing all power states due to nvme_core.force_apst -- use at your own risk\n");
+		ctrl->quirks &= ~NVME_QUIRK_NO_DEEPEST_PS;
+	}
+
+	ctrl->crdt[0] = le16_to_cpu(id->crdt1);
+	ctrl->crdt[1] = le16_to_cpu(id->crdt2);
+	ctrl->crdt[2] = le16_to_cpu(id->crdt3);
+
+	ctrl->oacs = le16_to_cpu(id->oacs);
+	ctrl->oncs = le16_to_cpup(&id->oncs);
+	ctrl->oaes = le32_to_cpu(id->oaes);
+	atomic_set(&ctrl->abort_limit, id->acl + 1);
+	ctrl->vwc = id->vwc;
+
+	if (id->mdts)
+		max_hw_sectors = 1 << (id->mdts + page_shift - 9);
+	else
+		max_hw_sectors = UINT_MAX;
+	ctrl->max_hw_sectors =
+		min_not_zero(ctrl->max_hw_sectors, max_hw_sectors);
+
+	pr_err("%s: nvme ctrl(%px) Max Data Transfer Size(MDTS) 0x%x\n",
+		__FUNCTION__, ctrl, 1 << (id->mdts + page_shift));
+
+	nvme_set_queue_limits(ctrl, ctrl->admin_q);
+	ctrl->sgls = le32_to_cpu(id->sgls);
+	ctrl->kas = le16_to_cpu(id->kas);
+	ctrl->max_namespaces = le32_to_cpu(id->mnan);
+	ctrl->ctratt = le32_to_cpu(id->ctratt);
+
+	if (id->rtd3e) {
+		/* us -> s */
+		u32 transition_time = le32_to_cpu(id->rtd3e) / 1000000;
+
+		ctrl->shutdown_timeout = clamp_t(unsigned int, transition_time,
+						 shutdown_timeout, 60);
+
+		if (ctrl->shutdown_timeout != shutdown_timeout)
+			dev_info(ctrl->device,
+				 "Shutdown timeout set to %u seconds\n",
+				 ctrl->shutdown_timeout);
+	} else
+		ctrl->shutdown_timeout = shutdown_timeout;
+
+	ctrl->npss = id->npss;
+	ctrl->apsta = id->apsta;
+	prev_apst_enabled = ctrl->apst_enabled;
+	if (ctrl->quirks & NVME_QUIRK_NO_APST) {
+		if (force_apst && id->apsta) {
+			dev_warn(ctrl->device, "forcibly allowing APST due to nvme_core.force_apst -- use at your own risk\n");
+			ctrl->apst_enabled = true;
+		} else {
+			ctrl->apst_enabled = false;
+		}
+	} else {
+		ctrl->apst_enabled = id->apsta;
+	}
+	memcpy(ctrl->psd, id->psd, sizeof(ctrl->psd));
+
+	if (ctrl->ops->flags & NVME_F_FABRICS) {
+		/* TCP doesn't use icdoff */
+		ctrl->icdoff = le16_to_cpu(id->icdoff);
+		ctrl->ioccsz = le32_to_cpu(id->ioccsz);
+		ctrl->iorcsz = le32_to_cpu(id->iorcsz);
+		ctrl->maxcmd = le16_to_cpu(id->maxcmd);
+
+		/*
+		 * In fabrics we need to verify the cntlid matches the
+		 * admin connect
+		 */
+		if (ctrl->cntlid != le16_to_cpu(id->cntlid)) {
+			ret = -EINVAL;
+			goto out_free;
+		}
+
+		if (!ctrl->opts->discovery_nqn && !ctrl->kas) {
+			dev_err(ctrl->device,
+				"keep-alive support is mandatory for fabrics\n");
+			ret = -EINVAL;
+			goto out_free;
+		}
+	} else {
+		ctrl->cntlid = le16_to_cpu(id->cntlid);
+		ctrl->hmpre = le32_to_cpu(id->hmpre);
+		ctrl->hmmin = le32_to_cpu(id->hmmin);
+		ctrl->hmminds = le32_to_cpu(id->hmminds);
+		ctrl->hmmaxd = le16_to_cpu(id->hmmaxd);
+	}
+
+	ret = nvme_mpath_init(ctrl, id);
+	kfree(id);
+
+	if (ret < 0)
+		return ret;
+
+	if (ctrl->apst_enabled && !prev_apst_enabled)
+		dev_pm_qos_expose_latency_tolerance(ctrl->device);
+	else if (!ctrl->apst_enabled && prev_apst_enabled)
+		dev_pm_qos_hide_latency_tolerance(ctrl->device);
+
+	ret = nvme_configure_apst(ctrl);
+	if (ret < 0)
+		return ret;
+	
+	ret = nvme_configure_timestamp(ctrl);
+	if (ret < 0)
+		return ret;
+
+	ret = nvme_configure_directives(ctrl);
+	if (ret < 0)
+		return ret;
+
+	ret = nvme_configure_acre(ctrl);
+	if (ret < 0)
+		return ret;
+
+	ctrl->identified = true;
+
+	return 0;
+
+out_free:
+	kfree(id);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvme_init_identify);
+
+static int nvme_dev_open(struct inode *inode, struct file *file)
+{
+	struct nvme_ctrl *ctrl =
+		container_of(inode->i_cdev, struct nvme_ctrl, cdev);
+
+	switch (ctrl->state) {
+	case NVME_CTRL_LIVE:
+	case NVME_CTRL_ADMIN_ONLY:
+		break;
+	default:
+		return -EWOULDBLOCK;
+	}
+
+	file->private_data = ctrl;
+	return 0;
+}
+
+static int nvme_dev_user_cmd(struct nvme_ctrl *ctrl, void __user *argp)
+{
+	struct nvme_ns *ns;
+	int ret;
+
+	down_read(&ctrl->namespaces_rwsem);
+	if (list_empty(&ctrl->namespaces)) {
+		ret = -ENOTTY;
+		goto out_unlock;
+	}
+
+	ns = list_first_entry(&ctrl->namespaces, struct nvme_ns, list);
+	if (ns != list_last_entry(&ctrl->namespaces, struct nvme_ns, list)) {
+		dev_warn(ctrl->device,
+			"NVME_IOCTL_IO_CMD not supported when multiple namespaces present!\n");
+		ret = -EINVAL;
+		goto out_unlock;
+	}
+
+	dev_warn(ctrl->device,
+		"using deprecated NVME_IOCTL_IO_CMD ioctl on the char device!\n");
+	kref_get(&ns->kref);
+	up_read(&ctrl->namespaces_rwsem);
+
+	ret = nvme_user_cmd(ctrl, ns, argp);
+	nvme_put_ns(ns);
+	return ret;
+
+out_unlock:
+	up_read(&ctrl->namespaces_rwsem);
+	return ret;
+}
+
+static long nvme_dev_ioctl(struct file *file, unsigned int cmd,
+		unsigned long arg)
+{
+	struct nvme_ctrl *ctrl = file->private_data;
+	void __user *argp = (void __user *)arg;
+
+	switch (cmd) {
+	case NVME_IOCTL_ADMIN_CMD:
+		return nvme_user_cmd(ctrl, NULL, argp);
+	case NVME_IOCTL_IO_CMD:
+		return nvme_dev_user_cmd(ctrl, argp);
+	case NVME_IOCTL_RESET:
+		dev_warn(ctrl->device, "resetting controller\n");
+		return nvme_reset_ctrl_sync(ctrl);
+	case NVME_IOCTL_SUBSYS_RESET:
+		return nvme_reset_subsystem(ctrl);
+	case NVME_IOCTL_RESCAN:
+		nvme_queue_scan(ctrl);
+		return 0;
+	default:
+		return -ENOTTY;
+	}
+}
+
+static const struct file_operations nvme_dev_fops = {
+	.owner		= THIS_MODULE,
+	.open		= nvme_dev_open,
+	.unlocked_ioctl	= nvme_dev_ioctl,
+	.compat_ioctl	= nvme_dev_ioctl,
+};
+
+static ssize_t nvme_sysfs_reset(struct device *dev,
+				struct device_attribute *attr, const char *buf,
+				size_t count)
+{
+	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
+	int ret;
+
+	ret = nvme_reset_ctrl_sync(ctrl);
+	if (ret < 0)
+		return ret;
+	return count;
+}
+static DEVICE_ATTR(reset_controller, S_IWUSR, NULL, nvme_sysfs_reset);
+
+static ssize_t nvme_sysfs_rescan(struct device *dev,
+				struct device_attribute *attr, const char *buf,
+				size_t count)
+{
+	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
+
+	nvme_queue_scan(ctrl);
+	return count;
+}
+static DEVICE_ATTR(rescan_controller, S_IWUSR, NULL, nvme_sysfs_rescan);
+
+static inline struct nvme_ns_head *dev_to_ns_head(struct device *dev)
+{
+	struct gendisk *disk = dev_to_disk(dev);
+
+	if (disk->fops == &nvme_fops)
+		return nvme_get_ns_from_dev(dev)->head;
+	else
+		return disk->private_data;
+}
+
+static ssize_t wwid_show(struct device *dev, struct device_attribute *attr,
+		char *buf)
+{
+	struct nvme_ns_head *head = dev_to_ns_head(dev);
+	struct nvme_ns_ids *ids = &head->ids;
+	struct nvme_subsystem *subsys = head->subsys;
+	int serial_len = sizeof(subsys->serial);
+	int model_len = sizeof(subsys->model);
+
+	if (!uuid_is_null(&ids->uuid))
+		return sprintf(buf, "uuid.%pU\n", &ids->uuid);
+
+	if (memchr_inv(ids->nguid, 0, sizeof(ids->nguid)))
+		return sprintf(buf, "eui.%16phN\n", ids->nguid);
+
+	if (memchr_inv(ids->eui64, 0, sizeof(ids->eui64)))
+		return sprintf(buf, "eui.%8phN\n", ids->eui64);
+
+	while (serial_len > 0 && (subsys->serial[serial_len - 1] == ' ' ||
+				  subsys->serial[serial_len - 1] == '\0'))
+		serial_len--;
+	while (model_len > 0 && (subsys->model[model_len - 1] == ' ' ||
+				 subsys->model[model_len - 1] == '\0'))
+		model_len--;
+
+	return sprintf(buf, "nvme.%04x-%*phN-%*phN-%08x\n", subsys->vendor_id,
+		serial_len, subsys->serial, model_len, subsys->model,
+		head->ns_id);
+}
+static DEVICE_ATTR_RO(wwid);
+
+static ssize_t nguid_show(struct device *dev, struct device_attribute *attr,
+		char *buf)
+{
+	return sprintf(buf, "%pU\n", dev_to_ns_head(dev)->ids.nguid);
+}
+static DEVICE_ATTR_RO(nguid);
+
+static ssize_t uuid_show(struct device *dev, struct device_attribute *attr,
+		char *buf)
+{
+	struct nvme_ns_ids *ids = &dev_to_ns_head(dev)->ids;
+
+	/* For backward compatibility expose the NGUID to userspace if
+	 * we have no UUID set
+	 */
+	if (uuid_is_null(&ids->uuid)) {
+		printk_ratelimited(KERN_WARNING
+				   "No UUID available providing old NGUID\n");
+		return sprintf(buf, "%pU\n", ids->nguid);
+	}
+	return sprintf(buf, "%pU\n", &ids->uuid);
+}
+static DEVICE_ATTR_RO(uuid);
+
+static ssize_t eui_show(struct device *dev, struct device_attribute *attr,
+		char *buf)
+{
+	return sprintf(buf, "%8ph\n", dev_to_ns_head(dev)->ids.eui64);
+}
+static DEVICE_ATTR_RO(eui);
+
+static ssize_t nsid_show(struct device *dev, struct device_attribute *attr,
+		char *buf)
+{
+	return sprintf(buf, "%d\n", dev_to_ns_head(dev)->ns_id);
+}
+static DEVICE_ATTR_RO(nsid);
+
+static struct attribute *nvme_ns_id_attrs[] = {
+	&dev_attr_wwid.attr,
+	&dev_attr_uuid.attr,
+	&dev_attr_nguid.attr,
+	&dev_attr_eui.attr,
+	&dev_attr_nsid.attr,
+#ifdef CONFIG_NVME_MULTIPATH
+	&dev_attr_ana_grpid.attr,
+	&dev_attr_ana_state.attr,
+#endif
+	NULL,
+};
+
+static umode_t nvme_ns_id_attrs_are_visible(struct kobject *kobj,
+		struct attribute *a, int n)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct nvme_ns_ids *ids = &dev_to_ns_head(dev)->ids;
+
+	if (a == &dev_attr_uuid.attr) {
+		if (uuid_is_null(&ids->uuid) &&
+		    !memchr_inv(ids->nguid, 0, sizeof(ids->nguid)))
+			return 0;
+	}
+	if (a == &dev_attr_nguid.attr) {
+		if (!memchr_inv(ids->nguid, 0, sizeof(ids->nguid)))
+			return 0;
+	}
+	if (a == &dev_attr_eui.attr) {
+		if (!memchr_inv(ids->eui64, 0, sizeof(ids->eui64)))
+			return 0;
+	}
+#ifdef CONFIG_NVME_MULTIPATH
+	if (a == &dev_attr_ana_grpid.attr || a == &dev_attr_ana_state.attr) {
+		if (dev_to_disk(dev)->fops != &nvme_fops) /* per-path attr */
+			return 0;
+		if (!nvme_ctrl_use_ana(nvme_get_ns_from_dev(dev)->ctrl))
+			return 0;
+	}
+#endif
+	return a->mode;
+}
+
+static const struct attribute_group nvme_ns_id_attr_group = {
+	.attrs		= nvme_ns_id_attrs,
+	.is_visible	= nvme_ns_id_attrs_are_visible,
+};
+
+const struct attribute_group *nvme_ns_id_attr_groups[] = {
+	&nvme_ns_id_attr_group,
+#ifdef CONFIG_NVM
+	&nvme_nvm_attr_group,
+#endif
+	NULL,
+};
+
+#define nvme_show_str_function(field)						\
+static ssize_t  field##_show(struct device *dev,				\
+			    struct device_attribute *attr, char *buf)		\
+{										\
+        struct nvme_ctrl *ctrl = dev_get_drvdata(dev);				\
+        return sprintf(buf, "%.*s\n",						\
+		(int)sizeof(ctrl->subsys->field), ctrl->subsys->field);		\
+}										\
+static DEVICE_ATTR(field, S_IRUGO, field##_show, NULL);
+
+nvme_show_str_function(model);
+nvme_show_str_function(serial);
+nvme_show_str_function(firmware_rev);
+
+#define nvme_show_int_function(field)						\
+static ssize_t  field##_show(struct device *dev,				\
+			    struct device_attribute *attr, char *buf)		\
+{										\
+        struct nvme_ctrl *ctrl = dev_get_drvdata(dev);				\
+        return sprintf(buf, "%d\n", ctrl->field);	\
+}										\
+static DEVICE_ATTR(field, S_IRUGO, field##_show, NULL);
+
+nvme_show_int_function(cntlid);
+nvme_show_int_function(numa_node);
+
+static ssize_t nvme_sysfs_delete(struct device *dev,
+				struct device_attribute *attr, const char *buf,
+				size_t count)
+{
+	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
+
+	if (device_remove_file_self(dev, attr))
+		nvme_delete_ctrl_sync(ctrl);
+	return count;
+}
+static DEVICE_ATTR(delete_controller, S_IWUSR, NULL, nvme_sysfs_delete);
+
+static ssize_t nvme_sysfs_show_transport(struct device *dev,
+					 struct device_attribute *attr,
+					 char *buf)
+{
+	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
+
+	return snprintf(buf, PAGE_SIZE, "%s\n", ctrl->ops->name);
+}
+static DEVICE_ATTR(transport, S_IRUGO, nvme_sysfs_show_transport, NULL);
+
+static ssize_t nvme_sysfs_show_state(struct device *dev,
+				     struct device_attribute *attr,
+				     char *buf)
+{
+	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
+	static const char *const state_name[] = {
+		[NVME_CTRL_NEW]		= "new",
+		[NVME_CTRL_LIVE]	= "live",
+		[NVME_CTRL_ADMIN_ONLY]	= "only-admin",
+		[NVME_CTRL_RESETTING]	= "resetting",
+		[NVME_CTRL_CONNECTING]	= "connecting",
+		[NVME_CTRL_DELETING]	= "deleting",
+		[NVME_CTRL_DEAD]	= "dead",
+	};
+
+	if ((unsigned)ctrl->state < ARRAY_SIZE(state_name) &&
+	    state_name[ctrl->state])
+		return sprintf(buf, "%s\n", state_name[ctrl->state]);
+
+	return sprintf(buf, "unknown state\n");
+}
+
+static DEVICE_ATTR(state, S_IRUGO, nvme_sysfs_show_state, NULL);
+
+static ssize_t nvme_sysfs_show_subsysnqn(struct device *dev,
+					 struct device_attribute *attr,
+					 char *buf)
+{
+	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
+
+	return snprintf(buf, PAGE_SIZE, "%s\n", ctrl->subsys->subnqn);
+}
+static DEVICE_ATTR(subsysnqn, S_IRUGO, nvme_sysfs_show_subsysnqn, NULL);
+
+static ssize_t nvme_sysfs_show_address(struct device *dev,
+					 struct device_attribute *attr,
+					 char *buf)
+{
+	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
+
+	return ctrl->ops->get_address(ctrl, buf, PAGE_SIZE);
+}
+static DEVICE_ATTR(address, S_IRUGO, nvme_sysfs_show_address, NULL);
+
+static struct attribute *nvme_dev_attrs[] = {
+	&dev_attr_reset_controller.attr,
+	&dev_attr_rescan_controller.attr,
+	&dev_attr_model.attr,
+	&dev_attr_serial.attr,
+	&dev_attr_firmware_rev.attr,
+	&dev_attr_cntlid.attr,
+	&dev_attr_delete_controller.attr,
+	&dev_attr_transport.attr,
+	&dev_attr_subsysnqn.attr,
+	&dev_attr_address.attr,
+	&dev_attr_state.attr,
+	&dev_attr_numa_node.attr,
+	NULL
+};
+
+static umode_t nvme_dev_attrs_are_visible(struct kobject *kobj,
+		struct attribute *a, int n)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct nvme_ctrl *ctrl = dev_get_drvdata(dev);
+
+	if (a == &dev_attr_delete_controller.attr && !ctrl->ops->delete_ctrl)
+		return 0;
+	if (a == &dev_attr_address.attr && !ctrl->ops->get_address)
+		return 0;
+
+	return a->mode;
+}
+
+static struct attribute_group nvme_dev_attrs_group = {
+	.attrs		= nvme_dev_attrs,
+	.is_visible	= nvme_dev_attrs_are_visible,
+};
+
+static const struct attribute_group *nvme_dev_attr_groups[] = {
+	&nvme_dev_attrs_group,
+	NULL,
+};
+
+static struct nvme_ns_head *__nvme_find_ns_head(struct nvme_subsystem *subsys,
+		unsigned nsid)
+{
+	struct nvme_ns_head *h;
+
+	lockdep_assert_held(&subsys->lock);
+
+	list_for_each_entry(h, &subsys->nsheads, entry) {
+		if (h->ns_id == nsid && kref_get_unless_zero(&h->ref))
+			return h;
+	}
+
+	return NULL;
+}
+
+static int __nvme_check_ids(struct nvme_subsystem *subsys,
+		struct nvme_ns_head *new)
+{
+	struct nvme_ns_head *h;
+
+	lockdep_assert_held(&subsys->lock);
+
+	list_for_each_entry(h, &subsys->nsheads, entry) {
+		if (nvme_ns_ids_valid(&new->ids) &&
+		    !list_empty(&h->list) &&
+		    nvme_ns_ids_equal(&new->ids, &h->ids))
+			return -EINVAL;
+	}
+
+	return 0;
+}
+
+static struct nvme_ns_head *nvme_alloc_ns_head(struct nvme_ctrl *ctrl,
+		unsigned nsid, struct nvme_id_ns *id)
+{
+	struct nvme_ns_head *head;
+	size_t size = sizeof(*head);
+	int ret = -ENOMEM;
+
+#ifdef CONFIG_NVME_MULTIPATH
+	size += num_possible_nodes() * sizeof(struct nvme_ns *);
+#endif
+
+	head = kzalloc(size, GFP_KERNEL);
+	if (!head)
+		goto out;
+	ret = ida_simple_get(&ctrl->subsys->ns_ida, 1, 0, GFP_KERNEL);
+	if (ret < 0)
+		goto out_free_head;
+	head->instance = ret;
+	INIT_LIST_HEAD(&head->list);
+	ret = init_srcu_struct(&head->srcu);
+	if (ret)
+		goto out_ida_remove;
+	head->subsys = ctrl->subsys;
+	head->ns_id = nsid;
+	kref_init(&head->ref);
+
+	nvme_report_ns_ids(ctrl, nsid, id, &head->ids);
+
+	ret = __nvme_check_ids(ctrl->subsys, head);
+	if (ret) {
+		dev_err(ctrl->device,
+			"duplicate IDs for nsid %d\n", nsid);
+		goto out_cleanup_srcu;
+	}
+
+	ret = nvme_mpath_alloc_disk(ctrl, head);
+	if (ret)
+		goto out_cleanup_srcu;
+
+	list_add_tail(&head->entry, &ctrl->subsys->nsheads);
+
+	kref_get(&ctrl->subsys->ref);
+
+	return head;
+out_cleanup_srcu:
+	cleanup_srcu_struct(&head->srcu);
+out_ida_remove:
+	ida_simple_remove(&ctrl->subsys->ns_ida, head->instance);
+out_free_head:
+	kfree(head);
+out:
+	return ERR_PTR(ret);
+}
+
+static int nvme_init_ns_head(struct nvme_ns *ns, unsigned nsid,
+		struct nvme_id_ns *id)
+{
+	struct nvme_ctrl *ctrl = ns->ctrl;
+	bool is_shared = id->nmic & (1 << 0);
+	struct nvme_ns_head *head = NULL;
+	int ret = 0;
+
+	mutex_lock(&ctrl->subsys->lock);
+	if (is_shared)
+		head = __nvme_find_ns_head(ctrl->subsys, nsid);
+	if (!head) {
+		head = nvme_alloc_ns_head(ctrl, nsid, id);
+		if (IS_ERR(head)) {
+			ret = PTR_ERR(head);
+			goto out_unlock;
+		}
+	} else {
+		struct nvme_ns_ids ids;
+
+		nvme_report_ns_ids(ctrl, nsid, id, &ids);
+		if (!nvme_ns_ids_equal(&head->ids, &ids)) {
+			dev_err(ctrl->device,
+				"IDs don't match for shared namespace %d\n",
+					nsid);
+			ret = -EINVAL;
+			goto out_unlock;
+		}
+	}
+
+	list_add_tail(&ns->siblings, &head->list);
+	ns->head = head;
+
+out_unlock:
+	mutex_unlock(&ctrl->subsys->lock);
+	return ret;
+}
+
+static int ns_cmp(void *priv, struct list_head *a, struct list_head *b)
+{
+	struct nvme_ns *nsa = container_of(a, struct nvme_ns, list);
+	struct nvme_ns *nsb = container_of(b, struct nvme_ns, list);
+
+	return nsa->head->ns_id - nsb->head->ns_id;
+}
+
+static struct nvme_ns *nvme_find_get_ns(struct nvme_ctrl *ctrl, unsigned nsid)
+{
+	struct nvme_ns *ns, *ret = NULL;
+
+	down_read(&ctrl->namespaces_rwsem);
+	list_for_each_entry(ns, &ctrl->namespaces, list) {
+		if (ns->head->ns_id == nsid) {
+			if (!kref_get_unless_zero(&ns->kref))
+				continue;
+			ret = ns;
+			break;
+		}
+		if (ns->head->ns_id > nsid)
+			break;
+	}
+	up_read(&ctrl->namespaces_rwsem);
+	return ret;
+}
+
+static int nvme_setup_streams_ns(struct nvme_ctrl *ctrl, struct nvme_ns *ns)
+{
+	struct streams_directive_params s;
+	int ret;
+
+	if (!ctrl->nr_streams)
+		return 0;
+
+	ret = nvme_get_stream_params(ctrl, &s, ns->head->ns_id);
+	if (ret)
+		return ret;
+
+	ns->sws = le32_to_cpu(s.sws);
+	ns->sgs = le16_to_cpu(s.sgs);
+
+	if (ns->sws) {
+		unsigned int bs = 1 << ns->lba_shift;
+
+		blk_queue_io_min(ns->queue, bs * ns->sws);
+		if (ns->sgs)
+			blk_queue_io_opt(ns->queue, bs * ns->sws * ns->sgs);
+	}
+
+	return 0;
+}
+
+static int nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid)
+{
+	struct nvme_ns *ns;
+	struct gendisk *disk;
+	struct nvme_id_ns *id;
+	char disk_name[DISK_NAME_LEN];
+	int node = ctrl->numa_node, flags = GENHD_FL_EXT_DEVT, ret;
+
+	ns = kzalloc_node(sizeof(*ns), GFP_KERNEL, node);
+	if (!ns)
+		return -ENOMEM;
+
+	dev_info(ctrl->dev, "%s: ctrl %px nsid %u ns %px\n",
+		    __FUNCTION__, ctrl, nsid, ns);
+
+	ns->queue = blk_mq_init_queue(ctrl->tagset);
+	if (IS_ERR(ns->queue)) {
+		ret = PTR_ERR(ns->queue);
+		goto out_free_ns;
+	}
+
+	blk_queue_flag_set(QUEUE_FLAG_NONROT, ns->queue);
+	if (ctrl->ops->flags & NVME_F_PCI_P2PDMA)
+		blk_queue_flag_set(QUEUE_FLAG_PCI_P2PDMA, ns->queue);
+
+	ns->queue->queuedata = ns;
+	ns->ctrl = ctrl;
+
+	kref_init(&ns->kref);
+	ns->lba_shift = 9; /* set to a default value for 512 until disk is validated */
+
+	blk_queue_logical_block_size(ns->queue, 1 << ns->lba_shift);
+	nvme_set_queue_limits(ctrl, ns->queue);
+
+	id = nvme_identify_ns(ctrl, nsid);
+	if (!id) {
+		ret = -EIO;
+		goto out_free_queue;
+	}
+
+	if (id->ncap == 0) {
+		ret = -EINVAL;
+		goto out_free_id;
+	}
+
+	ret = nvme_init_ns_head(ns, nsid, id);
+	if (ret)
+		goto out_free_id;
+	nvme_setup_streams_ns(ctrl, ns);
+	nvme_set_disk_name(disk_name, ns, ctrl, &flags);
+
+	disk = alloc_disk_node(0, node);
+	if (!disk) {
+		ret = -ENOMEM;
+		goto out_unlink_ns;
+	}
+
+	disk->fops = &nvme_fops;
+	disk->private_data = ns;
+	disk->queue = ns->queue;
+	disk->flags = flags;
+	memcpy(disk->disk_name, disk_name, DISK_NAME_LEN);
+	ns->disk = disk;
+
+	__nvme_revalidate_disk(disk, id);
+
+	if ((ctrl->quirks & NVME_QUIRK_LIGHTNVM) && id->vs[0] == 0x1) {
+		ret = nvme_nvm_register(ns, disk_name, node);
+		if (ret) {
+			dev_warn(ctrl->device, "LightNVM init failure\n");
+			goto out_put_disk;
+		}
+	}
+
+	down_write(&ctrl->namespaces_rwsem);
+	list_add_tail(&ns->list, &ctrl->namespaces);
+	up_write(&ctrl->namespaces_rwsem);
+
+	nvme_get_ctrl(ctrl);
+
+	device_add_disk(ctrl->device, ns->disk, nvme_ns_id_attr_groups);
+
+	nvme_mpath_add_disk(ns, id);
+	nvme_fault_inject_init(ns);
+	kfree(id);
+
+	return 0;
+ out_put_disk:
+	put_disk(ns->disk);
+ out_unlink_ns:
+	mutex_lock(&ctrl->subsys->lock);
+	list_del_rcu(&ns->siblings);
+	mutex_unlock(&ctrl->subsys->lock);
+	nvme_put_ns_head(ns->head);
+ out_free_id:
+	kfree(id);
+ out_free_queue:
+	blk_cleanup_queue(ns->queue);
+ out_free_ns:
+	kfree(ns);
+	return ret;
+}
+
+static void nvme_ns_remove(struct nvme_ns *ns)
+{
+	if (test_and_set_bit(NVME_NS_REMOVING, &ns->flags))
+		return;
+
+	nvme_fault_inject_fini(ns);
+	if (ns->disk && ns->disk->flags & GENHD_FL_UP) {
+		del_gendisk(ns->disk);
+		blk_cleanup_queue(ns->queue);
+		if (blk_get_integrity(ns->disk))
+			blk_integrity_unregister(ns->disk);
+	}
+
+	mutex_lock(&ns->ctrl->subsys->lock);
+	list_del_rcu(&ns->siblings);
+	nvme_mpath_clear_current_path(ns);
+	mutex_unlock(&ns->ctrl->subsys->lock);
+
+	down_write(&ns->ctrl->namespaces_rwsem);
+	list_del_init(&ns->list);
+	up_write(&ns->ctrl->namespaces_rwsem);
+
+	synchronize_srcu(&ns->head->srcu);
+	nvme_mpath_check_last_path(ns);
+	nvme_put_ns(ns);
+}
+
+static void nvme_validate_ns(struct nvme_ctrl *ctrl, unsigned nsid)
+{
+	struct nvme_ns *ns;
+
+	dev_info(ctrl->dev, "%s: ctrl %px validate nsid %u\n",
+			     __FUNCTION__, ctrl, nsid);
+
+	ns = nvme_find_get_ns(ctrl, nsid);
+	if (ns) {
+		if (ns->disk && revalidate_disk(ns->disk))
+			nvme_ns_remove(ns);
+		nvme_put_ns(ns);
+	} else
+		nvme_alloc_ns(ctrl, nsid);
+}
+
+static void nvme_remove_invalid_namespaces(struct nvme_ctrl *ctrl,
+					unsigned nsid)
+{
+	struct nvme_ns *ns, *next;
+	LIST_HEAD(rm_list);
+
+	down_write(&ctrl->namespaces_rwsem);
+	list_for_each_entry_safe(ns, next, &ctrl->namespaces, list) {
+		if (ns->head->ns_id > nsid || test_bit(NVME_NS_DEAD, &ns->flags))
+			list_move_tail(&ns->list, &rm_list);
+	}
+	up_write(&ctrl->namespaces_rwsem);
+
+	list_for_each_entry_safe(ns, next, &rm_list, list)
+		nvme_ns_remove(ns);
+
+}
+
+static int nvme_scan_ns_list(struct nvme_ctrl *ctrl, unsigned nn)
+{
+	struct nvme_ns *ns;
+	__le32 *ns_list;
+	unsigned i, j, nsid, prev = 0, num_lists = DIV_ROUND_UP(nn, 1024);
+	int ret = 0;
+
+	ns_list = kzalloc(NVME_IDENTIFY_DATA_SIZE, GFP_KERNEL);
+	if (!ns_list)
+		return -ENOMEM;
+
+	for (i = 0; i < num_lists; i++) {
+		ret = nvme_identify_ns_list(ctrl, prev, ns_list);
+		if (ret)
+			goto free;
+
+		for (j = 0; j < min(nn, 1024U); j++) {
+			nsid = le32_to_cpu(ns_list[j]);
+			if (!nsid)
+				goto out;
+
+			nvme_validate_ns(ctrl, nsid);
+
+			while (++prev < nsid) {
+				ns = nvme_find_get_ns(ctrl, prev);
+				if (ns) {
+					nvme_ns_remove(ns);
+					nvme_put_ns(ns);
+				}
+			}
+		}
+		nn -= j;
+	}
+ out:
+	nvme_remove_invalid_namespaces(ctrl, prev);
+ free:
+	kfree(ns_list);
+	return ret;
+}
+
+static void nvme_scan_ns_sequential(struct nvme_ctrl *ctrl, unsigned nn)
+{
+	unsigned i;
+
+	for (i = 1; i <= nn; i++)
+		nvme_validate_ns(ctrl, i);
+
+	nvme_remove_invalid_namespaces(ctrl, nn);
+}
+
+static void nvme_clear_changed_ns_log(struct nvme_ctrl *ctrl)
+{
+	size_t log_size = NVME_MAX_CHANGED_NAMESPACES * sizeof(__le32);
+	__le32 *log;
+	int error;
+
+	log = kzalloc(log_size, GFP_KERNEL);
+	if (!log)
+		return;
+
+	/*
+	 * We need to read the log to clear the AEN, but we don't want to rely
+	 * on it for the changed namespace information as userspace could have
+	 * raced with us in reading the log page, which could cause us to miss
+	 * updates.
+	 */
+	error = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_CHANGED_NS, 0, log,
+			log_size, 0);
+	if (error)
+		dev_warn(ctrl->device,
+			"reading changed ns log failed: %d\n", error);
+
+	kfree(log);
+}
+
+static void nvme_scan_work(struct work_struct *work)
+{
+	struct nvme_ctrl *ctrl =
+		container_of(work, struct nvme_ctrl, scan_work);
+	struct nvme_id_ctrl *id;
+	unsigned nn;
+
+	if (ctrl->state != NVME_CTRL_LIVE)
+		return;
+
+	WARN_ON_ONCE(!ctrl->tagset);
+
+	if (test_and_clear_bit(NVME_AER_NOTICE_NS_CHANGED, &ctrl->events)) {
+		dev_info(ctrl->device, "rescanning namespaces.\n");
+		nvme_clear_changed_ns_log(ctrl);
+	}
+
+	if (nvme_identify_ctrl(ctrl, &id))
+		return;
+
+	mutex_lock(&ctrl->scan_lock);
+	/* Number of namespaces */
+	nn = le32_to_cpu(id->nn);
+	pr_debug("%s: %s nn %u ver 0x%x quirks 0x%lx\n",
+		   __FUNCTION__, ctrl->name, id->nn, ctrl->vs, ctrl->quirks);
+	if (ctrl->vs >= NVME_VS(1, 1, 0) &&
+	    !(ctrl->quirks & NVME_QUIRK_IDENTIFY_CNS)) {
+		if (!nvme_scan_ns_list(ctrl, nn))
+			goto out_free_id;
+	}
+	nvme_scan_ns_sequential(ctrl, nn);
+out_free_id:
+	mutex_unlock(&ctrl->scan_lock);
+	kfree(id);
+	down_write(&ctrl->namespaces_rwsem);
+	list_sort(NULL, &ctrl->namespaces, ns_cmp);
+	up_write(&ctrl->namespaces_rwsem);
+}
+
+/*
+ * This function iterates the namespace list unlocked to allow recovery from
+ * controller failure. It is up to the caller to ensure the namespace list is
+ * not modified by scan work while this function is executing.
+ */
+void nvme_remove_namespaces(struct nvme_ctrl *ctrl)
+{
+	struct nvme_ns *ns, *next;
+	LIST_HEAD(ns_list);
+
+	dev_info(ctrl->dev, "%s\n", __FUNCTION__);
+
+	/* prevent racing with ns scanning */
+	flush_work(&ctrl->scan_work);
+
+	/*
+	 * The dead states indicates the controller was not gracefully
+	 * disconnected. In that case, we won't be able to flush any data while
+	 * removing the namespaces' disks; fail all the queues now to avoid
+	 * potentially having to clean up the failed sync later.
+	 */
+	if (ctrl->state == NVME_CTRL_DEAD)
+		nvme_kill_queues(ctrl);
+
+	down_write(&ctrl->namespaces_rwsem);
+	list_splice_init(&ctrl->namespaces, &ns_list);
+	up_write(&ctrl->namespaces_rwsem);
+
+	list_for_each_entry_safe(ns, next, &ns_list, list)
+		nvme_ns_remove(ns);
+}
+EXPORT_SYMBOL_GPL(nvme_remove_namespaces);
+
+static void nvme_aen_uevent(struct nvme_ctrl *ctrl)
+{
+	char *envp[2] = { NULL, NULL };
+	u32 aen_result = ctrl->aen_result;
+
+	ctrl->aen_result = 0;
+	if (!aen_result)
+		return;
+
+	envp[0] = kasprintf(GFP_KERNEL, "NVME_AEN=%#08x", aen_result);
+	if (!envp[0])
+		return;
+	kobject_uevent_env(&ctrl->device->kobj, KOBJ_CHANGE, envp);
+	kfree(envp[0]);
+}
+
+static void nvme_async_event_work(struct work_struct *work)
+{
+	struct nvme_ctrl *ctrl =
+		container_of(work, struct nvme_ctrl, async_event_work);
+
+	nvme_aen_uevent(ctrl);
+	ctrl->ops->submit_async_event(ctrl);
+}
+
+static bool nvme_ctrl_pp_status(struct nvme_ctrl *ctrl)
+{
+
+	u32 csts;
+
+	if (ctrl->ops->reg_read32(ctrl, NVME_REG_CSTS, &csts))
+		return false;
+
+	if (csts == ~0)
+		return false;
+
+	return ((ctrl->ctrl_config & NVME_CC_ENABLE) && (csts & NVME_CSTS_PP));
+}
+
+static void nvme_get_fw_slot_info(struct nvme_ctrl *ctrl)
+{
+	struct nvme_fw_slot_info_log *log;
+
+	log = kmalloc(sizeof(*log), GFP_KERNEL);
+	if (!log)
+		return;
+
+	if (nvme_get_log(ctrl, NVME_NSID_ALL, 0, NVME_LOG_FW_SLOT, log,
+			sizeof(*log), 0))
+		dev_warn(ctrl->device, "Get FW SLOT INFO log error\n");
+	kfree(log);
+}
+
+static void nvme_fw_act_work(struct work_struct *work)
+{
+	struct nvme_ctrl *ctrl = container_of(work,
+				struct nvme_ctrl, fw_act_work);
+	unsigned long fw_act_timeout;
+
+	if (ctrl->mtfa)
+		fw_act_timeout = jiffies +
+				msecs_to_jiffies(ctrl->mtfa * 100);
+	else
+		fw_act_timeout = jiffies +
+				msecs_to_jiffies(admin_timeout * 1000);
+
+	nvme_stop_queues(ctrl);
+	while (nvme_ctrl_pp_status(ctrl)) {
+		if (time_after(jiffies, fw_act_timeout)) {
+			dev_warn(ctrl->device,
+				"Fw activation timeout, reset controller\n");
+			nvme_reset_ctrl(ctrl);
+			break;
+		}
+		msleep(100);
+	}
+
+	if (ctrl->state != NVME_CTRL_LIVE)
+		return;
+
+	nvme_start_queues(ctrl);
+	/* read FW slot information to clear the AER */
+	nvme_get_fw_slot_info(ctrl);
+}
+
+static void nvme_handle_aen_notice(struct nvme_ctrl *ctrl, u32 result)
+{
+	u32 aer_notice_type = (result & 0xff00) >> 8;
+
+	switch (aer_notice_type) {
+	case NVME_AER_NOTICE_NS_CHANGED:
+		trace_nvme_async_event(ctrl, aer_notice_type);
+		set_bit(NVME_AER_NOTICE_NS_CHANGED, &ctrl->events);
+		nvme_queue_scan(ctrl);
+		break;
+	case NVME_AER_NOTICE_FW_ACT_STARTING:
+		trace_nvme_async_event(ctrl, aer_notice_type);
+		queue_work(nvme_wq, &ctrl->fw_act_work);
+		break;
+#ifdef CONFIG_NVME_MULTIPATH
+	case NVME_AER_NOTICE_ANA:
+		trace_nvme_async_event(ctrl, aer_notice_type);
+		if (!ctrl->ana_log_buf)
+			break;
+		queue_work(nvme_wq, &ctrl->ana_work);
+		break;
+#endif
+	default:
+		dev_warn(ctrl->device, "async event result %08x\n", result);
+	}
+}
+
+void nvme_complete_async_event(struct nvme_ctrl *ctrl, __le16 status,
+		volatile union nvme_result *res)
+{
+	u32 result = le32_to_cpu(res->u32);
+	u32 aer_type = result & 0x07;
+
+	if (le16_to_cpu(status) >> 1 != NVME_SC_SUCCESS)
+		return;
+
+	switch (aer_type) {
+	case NVME_AER_NOTICE:
+		nvme_handle_aen_notice(ctrl, result);
+		break;
+	case NVME_AER_ERROR:
+	case NVME_AER_SMART:
+	case NVME_AER_CSS:
+	case NVME_AER_VS:
+		trace_nvme_async_event(ctrl, aer_type);
+		ctrl->aen_result = result;
+		break;
+	default:
+		break;
+	}
+	queue_work(nvme_wq, &ctrl->async_event_work);
+}
+EXPORT_SYMBOL_GPL(nvme_complete_async_event);
+
+void nvme_stop_ctrl(struct nvme_ctrl *ctrl)
+{
+	nvme_mpath_stop(ctrl);
+	nvme_stop_keep_alive(ctrl);
+	flush_work(&ctrl->async_event_work);
+	cancel_work_sync(&ctrl->fw_act_work);
+}
+EXPORT_SYMBOL_GPL(nvme_stop_ctrl);
+
+void nvme_start_ctrl(struct nvme_ctrl *ctrl)
+{
+	if (ctrl->kato)
+		nvme_start_keep_alive(ctrl);
+
+	if (ctrl->queue_count > 1) {
+		nvme_queue_scan(ctrl);
+		nvme_enable_aen(ctrl);
+		queue_work(nvme_wq, &ctrl->async_event_work);
+		nvme_start_queues(ctrl);
+	}
+}
+EXPORT_SYMBOL_GPL(nvme_start_ctrl);
+
+void nvme_uninit_ctrl(struct nvme_ctrl *ctrl)
+{
+	cdev_device_del(&ctrl->cdev, ctrl->device);
+}
+EXPORT_SYMBOL_GPL(nvme_uninit_ctrl);
+
+static void nvme_free_ctrl(struct device *dev)
+{
+	struct nvme_ctrl *ctrl =
+		container_of(dev, struct nvme_ctrl, ctrl_device);
+	struct nvme_subsystem *subsys = ctrl->subsys;
+
+	ida_simple_remove(&nvme_instance_ida, ctrl->instance);
+	kfree(ctrl->effects);
+	nvme_mpath_uninit(ctrl);
+	__free_page(ctrl->discard_page);
+
+	if (subsys) {
+		mutex_lock(&subsys->lock);
+		list_del(&ctrl->subsys_entry);
+		mutex_unlock(&subsys->lock);
+		sysfs_remove_link(&subsys->dev.kobj, dev_name(ctrl->device));
+	}
+
+	ctrl->ops->free_ctrl(ctrl);
+
+	if (subsys)
+		nvme_put_subsystem(subsys);
+}
+
+/*
+ * Initialize a NVMe controller structures.  This needs to be called during
+ * earliest initialization so that we have the initialized structured around
+ * during probing.
+ */
+int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
+		const struct nvme_ctrl_ops *ops, unsigned long quirks)
+{
+	int ret;
+
+	ctrl->state = NVME_CTRL_NEW;
+	spin_lock_init(&ctrl->lock);
+	mutex_init(&ctrl->scan_lock);
+	INIT_LIST_HEAD(&ctrl->namespaces);
+	init_rwsem(&ctrl->namespaces_rwsem);
+	ctrl->dev = dev;
+	ctrl->ops = ops;
+	ctrl->quirks = quirks;
+	INIT_WORK(&ctrl->scan_work, nvme_scan_work);
+	INIT_WORK(&ctrl->async_event_work, nvme_async_event_work);
+	INIT_WORK(&ctrl->fw_act_work, nvme_fw_act_work);
+	INIT_WORK(&ctrl->delete_work, nvme_delete_ctrl_work);
+
+	INIT_DELAYED_WORK(&ctrl->ka_work, nvme_keep_alive_work);
+	memset(&ctrl->ka_cmd, 0, sizeof(ctrl->ka_cmd));
+	ctrl->ka_cmd.common.opcode = nvme_admin_keep_alive;
+
+	BUILD_BUG_ON(NVME_DSM_MAX_RANGES * sizeof(struct nvme_dsm_range) >
+			PAGE_SIZE);
+	ctrl->discard_page = alloc_page(GFP_KERNEL);
+	if (!ctrl->discard_page) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	ret = ida_simple_get(&nvme_instance_ida, 0, 0, GFP_KERNEL);
+	if (ret < 0)
+		goto out;
+	ctrl->instance = ret;
+
+	device_initialize(&ctrl->ctrl_device);
+	ctrl->device = &ctrl->ctrl_device;
+	ctrl->device->devt = MKDEV(MAJOR(nvme_chr_devt), ctrl->instance);
+	ctrl->device->class = nvme_class;
+	ctrl->device->parent = ctrl->dev;
+	ctrl->device->groups = nvme_dev_attr_groups;
+	ctrl->device->release = nvme_free_ctrl;
+	dev_set_drvdata(ctrl->device, ctrl);
+	ret = dev_set_name(ctrl->device, "nvme%d", ctrl->instance);
+	if (ret)
+		goto out_release_instance;
+
+	cdev_init(&ctrl->cdev, &nvme_dev_fops);
+	ctrl->cdev.owner = ops->module;
+	ret = cdev_device_add(&ctrl->cdev, ctrl->device);
+	if (ret)
+		goto out_free_name;
+
+	/*
+	 * Initialize latency tolerance controls.  The sysfs files won't
+	 * be visible to userspace unless the device actually supports APST.
+	 */
+	ctrl->device->power.set_latency_tolerance = nvme_set_latency_tolerance;
+	dev_pm_qos_update_user_latency_tolerance(ctrl->device,
+		min(default_ps_max_latency_us, (unsigned long)S32_MAX));
+
+	return 0;
+out_free_name:
+	kfree_const(ctrl->device->kobj.name);
+out_release_instance:
+	ida_simple_remove(&nvme_instance_ida, ctrl->instance);
+out:
+	if (ctrl->discard_page)
+		__free_page(ctrl->discard_page);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvme_init_ctrl);
+
+/**
+ * nvme_kill_queues(): Ends all namespace queues
+ * @ctrl: the dead controller that needs to end
+ *
+ * Call this function when the driver determines it is unable to get the
+ * controller in a state capable of servicing IO.
+ */
+void nvme_kill_queues(struct nvme_ctrl *ctrl)
+{
+	struct nvme_ns *ns;
+
+	down_read(&ctrl->namespaces_rwsem);
+
+	/* Forcibly unquiesce queues to avoid blocking dispatch */
+	if (ctrl->admin_q && !blk_queue_dying(ctrl->admin_q))
+		blk_mq_unquiesce_queue(ctrl->admin_q);
+
+	list_for_each_entry(ns, &ctrl->namespaces, list)
+		nvme_set_queue_dying(ns);
+
+	up_read(&ctrl->namespaces_rwsem);
+}
+EXPORT_SYMBOL_GPL(nvme_kill_queues);
+
+void nvme_unfreeze(struct nvme_ctrl *ctrl)
+{
+	struct nvme_ns *ns;
+
+	down_read(&ctrl->namespaces_rwsem);
+	list_for_each_entry(ns, &ctrl->namespaces, list)
+		blk_mq_unfreeze_queue(ns->queue);
+	up_read(&ctrl->namespaces_rwsem);
+}
+EXPORT_SYMBOL_GPL(nvme_unfreeze);
+
+void nvme_wait_freeze_timeout(struct nvme_ctrl *ctrl, long timeout)
+{
+	struct nvme_ns *ns;
+
+	down_read(&ctrl->namespaces_rwsem);
+	list_for_each_entry(ns, &ctrl->namespaces, list) {
+		timeout = blk_mq_freeze_queue_wait_timeout(ns->queue, timeout);
+		if (timeout <= 0)
+			break;
+	}
+	up_read(&ctrl->namespaces_rwsem);
+}
+EXPORT_SYMBOL_GPL(nvme_wait_freeze_timeout);
+
+void nvme_wait_freeze(struct nvme_ctrl *ctrl)
+{
+	struct nvme_ns *ns;
+
+	down_read(&ctrl->namespaces_rwsem);
+	list_for_each_entry(ns, &ctrl->namespaces, list)
+		blk_mq_freeze_queue_wait(ns->queue);
+	up_read(&ctrl->namespaces_rwsem);
+}
+EXPORT_SYMBOL_GPL(nvme_wait_freeze);
+
+void nvme_start_freeze(struct nvme_ctrl *ctrl)
+{
+	struct nvme_ns *ns;
+
+	down_read(&ctrl->namespaces_rwsem);
+	list_for_each_entry(ns, &ctrl->namespaces, list)
+		blk_freeze_queue_start(ns->queue);
+	up_read(&ctrl->namespaces_rwsem);
+}
+EXPORT_SYMBOL_GPL(nvme_start_freeze);
+
+void nvme_stop_queues(struct nvme_ctrl *ctrl)
+{
+	struct nvme_ns *ns;
+
+	down_read(&ctrl->namespaces_rwsem);
+	list_for_each_entry(ns, &ctrl->namespaces, list)
+		blk_mq_quiesce_queue(ns->queue);
+	up_read(&ctrl->namespaces_rwsem);
+}
+EXPORT_SYMBOL_GPL(nvme_stop_queues);
+
+void nvme_start_queues(struct nvme_ctrl *ctrl)
+{
+	struct nvme_ns *ns;
+
+	down_read(&ctrl->namespaces_rwsem);
+	list_for_each_entry(ns, &ctrl->namespaces, list)
+		blk_mq_unquiesce_queue(ns->queue);
+	up_read(&ctrl->namespaces_rwsem);
+}
+EXPORT_SYMBOL_GPL(nvme_start_queues);
+
+int __init nvme_core_init(void)
+{
+	int result = -EINVAL;
+
+	if (is_kdump_kernel()) {
+		pr_err("Skipping nvme-core module\n");
+		goto out;
+	}
+
+        if (!is_power_of_2(nvme_mem_align)) {
+                pr_err("mem_align = %d is not power of 2. Exitting... \n", nvme_mem_align);
+                goto out;
+        }
+
+        pr_err("DSS-Version: Host-Driver : %s\n", GITVER);
+	pr_err("Samsung Key-Value NVMeoF driver ver. %s\n", KV_DRV_VERSION);
+        pr_err("Driver settings:\n");
+	pr_err("    BIO_MAX_PAGES = %d\n", BIO_MAX_PAGES);
+	pr_err("    mem_align     = %d\n", nvme_mem_align);
+
+	nvme_wq = alloc_workqueue("nvme-wq",
+			WQ_UNBOUND | WQ_MEM_RECLAIM | WQ_SYSFS, 0);
+	if (!nvme_wq)
+		goto out;
+
+	nvme_reset_wq = alloc_workqueue("nvme-reset-wq",
+			WQ_UNBOUND | WQ_MEM_RECLAIM | WQ_SYSFS, 0);
+	if (!nvme_reset_wq)
+		goto destroy_wq;
+
+	nvme_delete_wq = alloc_workqueue("nvme-delete-wq",
+			WQ_UNBOUND | WQ_MEM_RECLAIM | WQ_SYSFS, 0);
+	if (!nvme_delete_wq)
+		goto destroy_reset_wq;
+
+	result = alloc_chrdev_region(&nvme_chr_devt, 0, NVME_MINORS, "nvme");
+	if (result < 0)
+		goto destroy_delete_wq;
+
+	nvme_class = class_create(THIS_MODULE, "nvme");
+	if (IS_ERR(nvme_class)) {
+		result = PTR_ERR(nvme_class);
+		goto unregister_chrdev;
+	}
+
+	nvme_subsys_class = class_create(THIS_MODULE, "nvme-subsystem");
+	if (IS_ERR(nvme_subsys_class)) {
+		result = PTR_ERR(nvme_subsys_class);
+		goto destroy_class;
+	}
+
+	result = aio_service_init();
+	if (result)
+		goto destroy_class;
+
+	result = kv_aio_worker_init();
+	if (result)
+		goto destroy_aio_service;
+
+	return 0;
+destroy_aio_service:
+    	aio_service_exit();
+destroy_class:
+	class_destroy(nvme_class);
+unregister_chrdev:
+	unregister_chrdev_region(nvme_chr_devt, NVME_MINORS);
+destroy_delete_wq:
+	destroy_workqueue(nvme_delete_wq);
+destroy_reset_wq:
+	destroy_workqueue(nvme_reset_wq);
+destroy_wq:
+	destroy_workqueue(nvme_wq);
+out:
+	return result;
+}
+
+void __exit nvme_core_exit(void)
+{
+	ida_destroy(&nvme_subsystems_ida);
+	class_destroy(nvme_subsys_class);
+	class_destroy(nvme_class);
+	unregister_chrdev_region(nvme_chr_devt, NVME_MINORS);
+	destroy_workqueue(nvme_delete_wq);
+	destroy_workqueue(nvme_reset_wq);
+	destroy_workqueue(nvme_wq);
+	kv_aio_worker_fini();
+	aio_service_exit();
+}
+
+MODULE_LICENSE("GPL");
+MODULE_VERSION("1.0");
+module_init(nvme_core_init);
+module_exit(nvme_core_exit);
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/fabrics.c b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/fabrics.c
new file mode 100644
index 0000000..e80abf7
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/fabrics.c
@@ -0,0 +1,1205 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * NVMe over Fabrics common host code.
+ * Copyright (c) 2015-2016 HGST, a Western Digital Company.
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+#include <linux/init.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/parser.h>
+#include <linux/seq_file.h>
+#include "nvme.h"
+#include "fabrics.h"
+
+static LIST_HEAD(nvmf_transports);
+static DECLARE_RWSEM(nvmf_transports_rwsem);
+
+static LIST_HEAD(nvmf_hosts);
+static DEFINE_MUTEX(nvmf_hosts_mutex);
+
+static struct nvmf_host *nvmf_default_host;
+
+static struct nvmf_host *__nvmf_host_find(const char *hostnqn)
+{
+	struct nvmf_host *host;
+
+	list_for_each_entry(host, &nvmf_hosts, list) {
+		if (!strcmp(host->nqn, hostnqn))
+			return host;
+	}
+
+	return NULL;
+}
+
+static struct nvmf_host *nvmf_host_add(const char *hostnqn)
+{
+	struct nvmf_host *host;
+
+	mutex_lock(&nvmf_hosts_mutex);
+	host = __nvmf_host_find(hostnqn);
+	if (host) {
+		kref_get(&host->ref);
+		goto out_unlock;
+	}
+
+	host = kmalloc(sizeof(*host), GFP_KERNEL);
+	if (!host)
+		goto out_unlock;
+
+	kref_init(&host->ref);
+	strlcpy(host->nqn, hostnqn, NVMF_NQN_SIZE);
+
+	list_add_tail(&host->list, &nvmf_hosts);
+out_unlock:
+	mutex_unlock(&nvmf_hosts_mutex);
+	return host;
+}
+
+static struct nvmf_host *nvmf_host_default(void)
+{
+	struct nvmf_host *host;
+
+	host = kmalloc(sizeof(*host), GFP_KERNEL);
+	if (!host)
+		return NULL;
+
+	kref_init(&host->ref);
+	uuid_gen(&host->id);
+	snprintf(host->nqn, NVMF_NQN_SIZE,
+		"nqn.2014-08.org.nvmexpress:uuid:%pUb", &host->id);
+
+	mutex_lock(&nvmf_hosts_mutex);
+	list_add_tail(&host->list, &nvmf_hosts);
+	mutex_unlock(&nvmf_hosts_mutex);
+
+	return host;
+}
+
+static void nvmf_host_destroy(struct kref *ref)
+{
+	struct nvmf_host *host = container_of(ref, struct nvmf_host, ref);
+
+	mutex_lock(&nvmf_hosts_mutex);
+	list_del(&host->list);
+	mutex_unlock(&nvmf_hosts_mutex);
+
+	kfree(host);
+}
+
+static void nvmf_host_put(struct nvmf_host *host)
+{
+	if (host)
+		kref_put(&host->ref, nvmf_host_destroy);
+}
+
+/**
+ * nvmf_get_address() -  Get address/port
+ * @ctrl:	Host NVMe controller instance which we got the address
+ * @buf:	OUTPUT parameter that will contain the address/port
+ * @size:	buffer size
+ */
+int nvmf_get_address(struct nvme_ctrl *ctrl, char *buf, int size)
+{
+	int len = 0;
+
+	if (ctrl->opts->mask & NVMF_OPT_TRADDR)
+		len += snprintf(buf, size, "traddr=%s", ctrl->opts->traddr);
+	if (ctrl->opts->mask & NVMF_OPT_TRSVCID)
+		len += snprintf(buf + len, size - len, "%strsvcid=%s",
+				(len) ? "," : "", ctrl->opts->trsvcid);
+	if (ctrl->opts->mask & NVMF_OPT_HOST_TRADDR)
+		len += snprintf(buf + len, size - len, "%shost_traddr=%s",
+				(len) ? "," : "", ctrl->opts->host_traddr);
+	len += snprintf(buf + len, size - len, "\n");
+
+	return len;
+}
+EXPORT_SYMBOL_GPL(nvmf_get_address);
+
+/**
+ * nvmf_reg_read32() -  NVMe Fabrics "Property Get" API function.
+ * @ctrl:	Host NVMe controller instance maintaining the admin
+ *		queue used to submit the property read command to
+ *		the allocated NVMe controller resource on the target system.
+ * @off:	Starting offset value of the targeted property
+ *		register (see the fabrics section of the NVMe standard).
+ * @val:	OUTPUT parameter that will contain the value of
+ *		the property after a successful read.
+ *
+ * Used by the host system to retrieve a 32-bit capsule property value
+ * from an NVMe controller on the target system.
+ *
+ * ("Capsule property" is an "PCIe register concept" applied to the
+ * NVMe fabrics space.)
+ *
+ * Return:
+ *	0: successful read
+ *	> 0: NVMe error status code
+ *	< 0: Linux errno error code
+ */
+int nvmf_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val)
+{
+	struct nvme_command cmd;
+	union nvme_result res;
+	int ret;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd.prop_get.opcode = nvme_fabrics_command;
+	cmd.prop_get.fctype = nvme_fabrics_type_property_get;
+	cmd.prop_get.offset = cpu_to_le32(off);
+
+	ret = __nvme_submit_sync_cmd(ctrl->admin_q, &cmd, &res, NULL, 0, 0,
+			NVME_QID_ANY, 0, 0, false);
+
+	if (ret >= 0)
+		*val = le64_to_cpu(res.u64);
+	if (unlikely(ret != 0))
+		dev_err(ctrl->device,
+			"Property Get error: %d, offset %#x\n",
+			ret > 0 ? ret & ~NVME_SC_DNR : ret, off);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvmf_reg_read32);
+
+/**
+ * nvmf_reg_read64() -  NVMe Fabrics "Property Get" API function.
+ * @ctrl:	Host NVMe controller instance maintaining the admin
+ *		queue used to submit the property read command to
+ *		the allocated controller resource on the target system.
+ * @off:	Starting offset value of the targeted property
+ *		register (see the fabrics section of the NVMe standard).
+ * @val:	OUTPUT parameter that will contain the value of
+ *		the property after a successful read.
+ *
+ * Used by the host system to retrieve a 64-bit capsule property value
+ * from an NVMe controller on the target system.
+ *
+ * ("Capsule property" is an "PCIe register concept" applied to the
+ * NVMe fabrics space.)
+ *
+ * Return:
+ *	0: successful read
+ *	> 0: NVMe error status code
+ *	< 0: Linux errno error code
+ */
+int nvmf_reg_read64(struct nvme_ctrl *ctrl, u32 off, u64 *val)
+{
+	struct nvme_command cmd;
+	union nvme_result res;
+	int ret;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd.prop_get.opcode = nvme_fabrics_command;
+	cmd.prop_get.fctype = nvme_fabrics_type_property_get;
+	cmd.prop_get.attrib = 1;
+	cmd.prop_get.offset = cpu_to_le32(off);
+
+	ret = __nvme_submit_sync_cmd(ctrl->admin_q, &cmd, &res, NULL, 0, 0,
+			NVME_QID_ANY, 0, 0, false);
+
+	if (ret >= 0)
+		*val = le64_to_cpu(res.u64);
+	if (unlikely(ret != 0))
+		dev_err(ctrl->device,
+			"Property Get error: %d, offset %#x\n",
+			ret > 0 ? ret & ~NVME_SC_DNR : ret, off);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvmf_reg_read64);
+
+/**
+ * nvmf_reg_write32() -  NVMe Fabrics "Property Write" API function.
+ * @ctrl:	Host NVMe controller instance maintaining the admin
+ *		queue used to submit the property read command to
+ *		the allocated NVMe controller resource on the target system.
+ * @off:	Starting offset value of the targeted property
+ *		register (see the fabrics section of the NVMe standard).
+ * @val:	Input parameter that contains the value to be
+ *		written to the property.
+ *
+ * Used by the NVMe host system to write a 32-bit capsule property value
+ * to an NVMe controller on the target system.
+ *
+ * ("Capsule property" is an "PCIe register concept" applied to the
+ * NVMe fabrics space.)
+ *
+ * Return:
+ *	0: successful write
+ *	> 0: NVMe error status code
+ *	< 0: Linux errno error code
+ */
+int nvmf_reg_write32(struct nvme_ctrl *ctrl, u32 off, u32 val)
+{
+	struct nvme_command cmd;
+	int ret;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd.prop_set.opcode = nvme_fabrics_command;
+	cmd.prop_set.fctype = nvme_fabrics_type_property_set;
+	cmd.prop_set.attrib = 0;
+	cmd.prop_set.offset = cpu_to_le32(off);
+	cmd.prop_set.value = cpu_to_le64(val);
+
+	ret = __nvme_submit_sync_cmd(ctrl->admin_q, &cmd, NULL, NULL, 0, 0,
+			NVME_QID_ANY, 0, 0, false);
+	if (unlikely(ret))
+		dev_err(ctrl->device,
+			"Property Set error: %d, offset %#x\n",
+			ret > 0 ? ret & ~NVME_SC_DNR : ret, off);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvmf_reg_write32);
+
+/**
+ * nvmf_log_connect_error() - Error-parsing-diagnostic print
+ * out function for connect() errors.
+ *
+ * @ctrl: the specific /dev/nvmeX device that had the error.
+ *
+ * @errval: Error code to be decoded in a more human-friendly
+ *	    printout.
+ *
+ * @offset: For use with the NVMe error code NVME_SC_CONNECT_INVALID_PARAM.
+ *
+ * @cmd: This is the SQE portion of a submission capsule.
+ *
+ * @data: This is the "Data" portion of a submission capsule.
+ */
+static void nvmf_log_connect_error(struct nvme_ctrl *ctrl,
+		int errval, int offset, struct nvme_command *cmd,
+		struct nvmf_connect_data *data)
+{
+	int err_sctype = errval & (~NVME_SC_DNR);
+
+	switch (err_sctype) {
+
+	case (NVME_SC_CONNECT_INVALID_PARAM):
+		if (offset >> 16) {
+			char *inv_data = "Connect Invalid Data Parameter";
+
+			switch (offset & 0xffff) {
+			case (offsetof(struct nvmf_connect_data, cntlid)):
+				dev_err(ctrl->device,
+					"%s, cntlid: %d\n",
+					inv_data, data->cntlid);
+				break;
+			case (offsetof(struct nvmf_connect_data, hostnqn)):
+				dev_err(ctrl->device,
+					"%s, hostnqn \"%s\"\n",
+					inv_data, data->hostnqn);
+				break;
+			case (offsetof(struct nvmf_connect_data, subsysnqn)):
+				dev_err(ctrl->device,
+					"%s, subsysnqn \"%s\"\n",
+					inv_data, data->subsysnqn);
+				break;
+			default:
+				dev_err(ctrl->device,
+					"%s, starting byte offset: %d\n",
+				       inv_data, offset & 0xffff);
+				break;
+			}
+		} else {
+			char *inv_sqe = "Connect Invalid SQE Parameter";
+
+			switch (offset) {
+			case (offsetof(struct nvmf_connect_command, qid)):
+				dev_err(ctrl->device,
+				       "%s, qid %d\n",
+					inv_sqe, cmd->connect.qid);
+				break;
+			default:
+				dev_err(ctrl->device,
+					"%s, starting byte offset: %d\n",
+					inv_sqe, offset);
+			}
+		}
+		break;
+
+	case NVME_SC_CONNECT_INVALID_HOST:
+		dev_err(ctrl->device,
+			"Connect for subsystem %s is not allowed, hostnqn: %s\n",
+			data->subsysnqn, data->hostnqn);
+		break;
+
+	case NVME_SC_CONNECT_CTRL_BUSY:
+		dev_err(ctrl->device,
+			"Connect command failed: controller is busy or not available\n");
+		break;
+
+	case NVME_SC_CONNECT_FORMAT:
+		dev_err(ctrl->device,
+			"Connect incompatible format: %d",
+			cmd->connect.recfmt);
+		break;
+
+	default:
+		dev_err(ctrl->device,
+			"Connect command failed, error wo/DNR bit: %d\n",
+			err_sctype);
+		break;
+	} /* switch (err_sctype) */
+}
+
+/**
+ * nvmf_connect_admin_queue() - NVMe Fabrics Admin Queue "Connect"
+ *				API function.
+ * @ctrl:	Host nvme controller instance used to request
+ *              a new NVMe controller allocation on the target
+ *              system and  establish an NVMe Admin connection to
+ *              that controller.
+ *
+ * This function enables an NVMe host device to request a new allocation of
+ * an NVMe controller resource on a target system as well establish a
+ * fabrics-protocol connection of the NVMe Admin queue between the
+ * host system device and the allocated NVMe controller on the
+ * target system via a NVMe Fabrics "Connect" command.
+ *
+ * Return:
+ *	0: success
+ *	> 0: NVMe error status code
+ *	< 0: Linux errno error code
+ *
+ */
+int nvmf_connect_admin_queue(struct nvme_ctrl *ctrl)
+{
+	struct nvme_command cmd;
+	union nvme_result res;
+	struct nvmf_connect_data *data;
+	int ret;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd.connect.opcode = nvme_fabrics_command;
+	cmd.connect.fctype = nvme_fabrics_type_connect;
+	cmd.connect.qid = 0;
+	cmd.connect.sqsize = cpu_to_le16(NVME_AQ_DEPTH - 1);
+
+	/*
+	 * Set keep-alive timeout in seconds granularity (ms * 1000)
+	 * and add a grace period for controller kato enforcement
+	 */
+	cmd.connect.kato = ctrl->opts->discovery_nqn ? 0 :
+		cpu_to_le32((ctrl->kato + NVME_KATO_GRACE) * 1000);
+
+	if (ctrl->opts->disable_sqflow)
+		cmd.connect.cattr |= NVME_CONNECT_DISABLE_SQFLOW;
+
+	//data = (struct nvmf_connect_data *)__get_free_page(GFP_KERNEL);
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	uuid_copy(&data->hostid, &ctrl->opts->host->id);
+	data->cntlid = cpu_to_le16(0xffff);
+	strncpy(data->subsysnqn, ctrl->opts->subsysnqn, NVMF_NQN_SIZE);
+	strncpy(data->hostnqn, ctrl->opts->host->nqn, NVMF_NQN_SIZE);
+
+	ret = __nvme_submit_sync_cmd(ctrl->admin_q, &cmd, &res,
+			data, sizeof(*data), 0, NVME_QID_ANY, 1,
+			BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, false);
+	if (ret) {
+		nvmf_log_connect_error(ctrl, ret, le32_to_cpu(res.u32),
+				       &cmd, data);
+		goto out_free_data;
+	}
+
+	ctrl->cntlid = le16_to_cpu(res.u16);
+
+out_free_data:
+	//free_page((unsigned long)data);
+	kfree(data);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvmf_connect_admin_queue);
+
+/**
+ * nvmf_connect_io_queue() - NVMe Fabrics I/O Queue "Connect"
+ *			     API function.
+ * @ctrl:	Host nvme controller instance used to establish an
+ *		NVMe I/O queue connection to the already allocated NVMe
+ *		controller on the target system.
+ * @qid:	NVMe I/O queue number for the new I/O connection between
+ *		host and target (note qid == 0 is illegal as this is
+ *		the Admin queue, per NVMe standard).
+ * @poll:	Whether or not to poll for the completion of the connect cmd.
+ *
+ * This function issues a fabrics-protocol connection
+ * of a NVMe I/O queue (via NVMe Fabrics "Connect" command)
+ * between the host system device and the allocated NVMe controller
+ * on the target system.
+ *
+ * Return:
+ *	0: success
+ *	> 0: NVMe error status code
+ *	< 0: Linux errno error code
+ */
+int nvmf_connect_io_queue(struct nvme_ctrl *ctrl, u16 qid, bool poll)
+{
+	struct nvme_command cmd;
+	struct nvmf_connect_data *data;
+	union nvme_result res;
+	int ret;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd.connect.opcode = nvme_fabrics_command;
+	cmd.connect.fctype = nvme_fabrics_type_connect;
+	cmd.connect.qid = cpu_to_le16(qid);
+	cmd.connect.sqsize = cpu_to_le16(ctrl->sqsize);
+
+	if (ctrl->opts->disable_sqflow)
+		cmd.connect.cattr |= NVME_CONNECT_DISABLE_SQFLOW;
+
+	//data = (struct nvmf_connect_data *)__get_free_page(GFP_KERNEL);
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+
+	uuid_copy(&data->hostid, &ctrl->opts->host->id);
+	data->cntlid = cpu_to_le16(ctrl->cntlid);
+	strncpy(data->subsysnqn, ctrl->opts->subsysnqn, NVMF_NQN_SIZE);
+	strncpy(data->hostnqn, ctrl->opts->host->nqn, NVMF_NQN_SIZE);
+
+	ret = __nvme_submit_sync_cmd(ctrl->connect_q, &cmd, &res,
+			data, sizeof(*data), 0, qid, 1,
+			BLK_MQ_REQ_RESERVED | BLK_MQ_REQ_NOWAIT, poll);
+	if (ret) {
+		nvmf_log_connect_error(ctrl, ret, le32_to_cpu(res.u32),
+				       &cmd, data);
+	}
+	//free_page((unsigned long)data);
+	kfree(data);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvmf_connect_io_queue);
+
+bool nvmf_should_reconnect(struct nvme_ctrl *ctrl)
+{
+	if (ctrl->opts->max_reconnects == -1 ||
+	    ctrl->nr_reconnects < ctrl->opts->max_reconnects)
+		return true;
+
+	return false;
+}
+EXPORT_SYMBOL_GPL(nvmf_should_reconnect);
+
+/**
+ * nvmf_register_transport() - NVMe Fabrics Library registration function.
+ * @ops:	Transport ops instance to be registered to the
+ *		common fabrics library.
+ *
+ * API function that registers the type of specific transport fabric
+ * being implemented to the common NVMe fabrics library. Part of
+ * the overall init sequence of starting up a fabrics driver.
+ */
+int nvmf_register_transport(struct nvmf_transport_ops *ops)
+{
+	if (!ops->create_ctrl)
+		return -EINVAL;
+
+	down_write(&nvmf_transports_rwsem);
+	list_add_tail(&ops->entry, &nvmf_transports);
+	up_write(&nvmf_transports_rwsem);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(nvmf_register_transport);
+
+/**
+ * nvmf_unregister_transport() - NVMe Fabrics Library unregistration function.
+ * @ops:	Transport ops instance to be unregistered from the
+ *		common fabrics library.
+ *
+ * Fabrics API function that unregisters the type of specific transport
+ * fabric being implemented from the common NVMe fabrics library.
+ * Part of the overall exit sequence of unloading the implemented driver.
+ */
+void nvmf_unregister_transport(struct nvmf_transport_ops *ops)
+{
+	down_write(&nvmf_transports_rwsem);
+	list_del(&ops->entry);
+	up_write(&nvmf_transports_rwsem);
+}
+EXPORT_SYMBOL_GPL(nvmf_unregister_transport);
+
+static struct nvmf_transport_ops *nvmf_lookup_transport(
+		struct nvmf_ctrl_options *opts)
+{
+	struct nvmf_transport_ops *ops;
+
+	lockdep_assert_held(&nvmf_transports_rwsem);
+
+	list_for_each_entry(ops, &nvmf_transports, entry) {
+		if (strcmp(ops->name, opts->transport) == 0)
+			return ops;
+	}
+
+	return NULL;
+}
+
+/*
+ * For something we're not in a state to send to the device the default action
+ * is to busy it and retry it after the controller state is recovered.  However,
+ * if the controller is deleting or if anything is marked for failfast or
+ * nvme multipath it is immediately failed.
+ *
+ * Note: commands used to initialize the controller will be marked for failfast.
+ * Note: nvme cli/ioctl commands are marked for failfast.
+ */
+blk_status_t nvmf_fail_nonready_command(struct nvme_ctrl *ctrl,
+		struct request *rq)
+{
+	if (ctrl->state != NVME_CTRL_DELETING &&
+	    ctrl->state != NVME_CTRL_DEAD &&
+	    !blk_noretry_request(rq) && !(rq->cmd_flags & REQ_NVME_MPATH))
+		return BLK_STS_RESOURCE;
+
+	nvme_req(rq)->status = NVME_SC_HOST_PATH_ERROR;
+	blk_mq_start_request(rq);
+	nvme_complete_rq(rq);
+	return BLK_STS_OK;
+}
+EXPORT_SYMBOL_GPL(nvmf_fail_nonready_command);
+
+bool __nvmf_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
+		bool queue_live)
+{
+	struct nvme_request *req = nvme_req(rq);
+
+	/*
+	 * If we are in some state of setup or teardown only allow
+	 * internally generated commands.
+	 */
+	if (!blk_rq_is_passthrough(rq) || (req->flags & NVME_REQ_USERCMD))
+		return false;
+
+	/*
+	 * Only allow commands on a live queue, except for the connect command,
+	 * which is require to set the queue live in the appropinquate states.
+	 */
+	switch (ctrl->state) {
+	case NVME_CTRL_NEW:
+	case NVME_CTRL_CONNECTING:
+		if (req->cmd->common.opcode == nvme_fabrics_command &&
+		    req->cmd->fabrics.fctype == nvme_fabrics_type_connect)
+			return true;
+		break;
+	default:
+		break;
+	case NVME_CTRL_DEAD:
+		return false;
+	}
+
+	return queue_live;
+}
+EXPORT_SYMBOL_GPL(__nvmf_check_ready);
+
+static const match_table_t opt_tokens = {
+	{ NVMF_OPT_TRANSPORT,		"transport=%s"		},
+	{ NVMF_OPT_TRADDR,		"traddr=%s"		},
+	{ NVMF_OPT_TRSVCID,		"trsvcid=%s"		},
+	{ NVMF_OPT_NQN,			"nqn=%s"		},
+	{ NVMF_OPT_QUEUE_SIZE,		"queue_size=%d"		},
+	{ NVMF_OPT_NR_IO_QUEUES,	"nr_io_queues=%d"	},
+	{ NVMF_OPT_RECONNECT_DELAY,	"reconnect_delay=%d"	},
+	{ NVMF_OPT_CTRL_LOSS_TMO,	"ctrl_loss_tmo=%d"	},
+	{ NVMF_OPT_KATO,		"keep_alive_tmo=%d"	},
+	{ NVMF_OPT_HOSTNQN,		"hostnqn=%s"		},
+	{ NVMF_OPT_HOST_TRADDR,		"host_traddr=%s"	},
+	{ NVMF_OPT_HOST_ID,		"hostid=%s"		},
+	{ NVMF_OPT_DUP_CONNECT,		"duplicate_connect"	},
+	{ NVMF_OPT_DISABLE_SQFLOW,	"disable_sqflow"	},
+	{ NVMF_OPT_HDR_DIGEST,		"hdr_digest"		},
+	{ NVMF_OPT_DATA_DIGEST,		"data_digest"		},
+	{ NVMF_OPT_NR_WRITE_QUEUES,	"nr_write_queues=%d"	},
+	{ NVMF_OPT_NR_POLL_QUEUES,	"nr_poll_queues=%d"	},
+	{ NVMF_OPT_ERR,			NULL			}
+};
+
+static int nvmf_parse_options(struct nvmf_ctrl_options *opts,
+		const char *buf)
+{
+	substring_t args[MAX_OPT_ARGS];
+	char *options, *o, *p;
+	int token, ret = 0;
+	size_t nqnlen  = 0;
+	int ctrl_loss_tmo = NVMF_DEF_CTRL_LOSS_TMO;
+	uuid_t hostid;
+
+	/* Set defaults */
+	opts->queue_size = NVMF_DEF_QUEUE_SIZE;
+	// TODO: Limit the # of IO queues for now
+	opts->nr_io_queues = num_online_cpus();
+	opts->reconnect_delay = NVMF_DEF_RECONNECT_DELAY;
+	opts->kato = NVME_DEFAULT_KATO;
+	opts->duplicate_connect = false;
+	opts->hdr_digest = false;
+	opts->data_digest = false;
+
+	options = o = kstrdup(buf, GFP_KERNEL);
+	if (!options)
+		return -ENOMEM;
+
+	uuid_gen(&hostid);
+
+	while ((p = strsep(&o, ",\n")) != NULL) {
+		if (!*p)
+			continue;
+
+		token = match_token(p, opt_tokens, args);
+		opts->mask |= token;
+		switch (token) {
+		case NVMF_OPT_TRANSPORT:
+			p = match_strdup(args);
+			if (!p) {
+				ret = -ENOMEM;
+				goto out;
+			}
+			kfree(opts->transport);
+			opts->transport = p;
+			break;
+		case NVMF_OPT_NQN:
+			p = match_strdup(args);
+			if (!p) {
+				ret = -ENOMEM;
+				goto out;
+			}
+			kfree(opts->subsysnqn);
+			opts->subsysnqn = p;
+			nqnlen = strlen(opts->subsysnqn);
+			if (nqnlen >= NVMF_NQN_SIZE) {
+				pr_err("%s needs to be < %d bytes\n",
+					opts->subsysnqn, NVMF_NQN_SIZE);
+				ret = -EINVAL;
+				goto out;
+			}
+			opts->discovery_nqn =
+				!(strcmp(opts->subsysnqn,
+					 NVME_DISC_SUBSYS_NAME));
+			break;
+		case NVMF_OPT_TRADDR:
+			p = match_strdup(args);
+			if (!p) {
+				ret = -ENOMEM;
+				goto out;
+			}
+			kfree(opts->traddr);
+			opts->traddr = p;
+			break;
+		case NVMF_OPT_TRSVCID:
+			p = match_strdup(args);
+			if (!p) {
+				ret = -ENOMEM;
+				goto out;
+			}
+			kfree(opts->trsvcid);
+			opts->trsvcid = p;
+			break;
+		case NVMF_OPT_QUEUE_SIZE:
+			if (match_int(args, &token)) {
+				ret = -EINVAL;
+				goto out;
+			}
+			if (token < NVMF_MIN_QUEUE_SIZE ||
+			    token > NVMF_MAX_QUEUE_SIZE) {
+				pr_err("Invalid queue_size %d\n", token);
+				ret = -EINVAL;
+				goto out;
+			}
+			opts->queue_size = token;
+			break;
+		case NVMF_OPT_NR_IO_QUEUES:
+			if (match_int(args, &token)) {
+				ret = -EINVAL;
+				goto out;
+			}
+			if (token <= 0) {
+				pr_err("Invalid number of IOQs %d\n", token);
+				ret = -EINVAL;
+				goto out;
+			}
+			if (opts->discovery_nqn) {
+				pr_debug("Ignoring nr_io_queues value for discovery controller\n");
+				break;
+			}
+
+			opts->nr_io_queues = min_t(unsigned int,
+					num_online_cpus(), token);
+			break;
+		case NVMF_OPT_KATO:
+			if (match_int(args, &token)) {
+				ret = -EINVAL;
+				goto out;
+			}
+
+			if (token < 0) {
+				pr_err("Invalid keep_alive_tmo %d\n", token);
+				ret = -EINVAL;
+				goto out;
+			} else if (token == 0 && !opts->discovery_nqn) {
+				/* Allowed for debug */
+				pr_warn("keep_alive_tmo 0 won't execute keep alives!!!\n");
+			}
+			opts->kato = token;
+
+			if (opts->discovery_nqn && opts->kato) {
+				pr_err("Discovery controllers cannot accept KATO != 0\n");
+				ret = -EINVAL;
+				goto out;
+			}
+
+			break;
+		case NVMF_OPT_CTRL_LOSS_TMO:
+			if (match_int(args, &token)) {
+				ret = -EINVAL;
+				goto out;
+			}
+
+			if (token < 0)
+				pr_warn("ctrl_loss_tmo < 0 will reconnect forever\n");
+			ctrl_loss_tmo = token;
+			break;
+		case NVMF_OPT_HOSTNQN:
+			if (opts->host) {
+				pr_err("hostnqn already user-assigned: %s\n",
+				       opts->host->nqn);
+				ret = -EADDRINUSE;
+				goto out;
+			}
+			p = match_strdup(args);
+			if (!p) {
+				ret = -ENOMEM;
+				goto out;
+			}
+			nqnlen = strlen(p);
+			if (nqnlen >= NVMF_NQN_SIZE) {
+				pr_err("%s needs to be < %d bytes\n",
+					p, NVMF_NQN_SIZE);
+				kfree(p);
+				ret = -EINVAL;
+				goto out;
+			}
+			nvmf_host_put(opts->host);
+			opts->host = nvmf_host_add(p);
+			kfree(p);
+			if (!opts->host) {
+				ret = -ENOMEM;
+				goto out;
+			}
+			break;
+		case NVMF_OPT_RECONNECT_DELAY:
+			if (match_int(args, &token)) {
+				ret = -EINVAL;
+				goto out;
+			}
+			if (token <= 0) {
+				pr_err("Invalid reconnect_delay %d\n", token);
+				ret = -EINVAL;
+				goto out;
+			}
+			opts->reconnect_delay = token;
+			break;
+		case NVMF_OPT_HOST_TRADDR:
+			p = match_strdup(args);
+			if (!p) {
+				ret = -ENOMEM;
+				goto out;
+			}
+			kfree(opts->host_traddr);
+			opts->host_traddr = p;
+			break;
+		case NVMF_OPT_HOST_ID:
+			p = match_strdup(args);
+			if (!p) {
+				ret = -ENOMEM;
+				goto out;
+			}
+			ret = uuid_parse(p, &hostid);
+			if (ret) {
+				pr_err("Invalid hostid %s\n", p);
+				ret = -EINVAL;
+				kfree(p);
+				goto out;
+			}
+			kfree(p);
+			break;
+		case NVMF_OPT_DUP_CONNECT:
+			opts->duplicate_connect = true;
+			break;
+		case NVMF_OPT_DISABLE_SQFLOW:
+			opts->disable_sqflow = true;
+			break;
+		case NVMF_OPT_HDR_DIGEST:
+			opts->hdr_digest = true;
+			break;
+		case NVMF_OPT_DATA_DIGEST:
+			opts->data_digest = true;
+			break;
+		case NVMF_OPT_NR_WRITE_QUEUES:
+			if (match_int(args, &token)) {
+				ret = -EINVAL;
+				goto out;
+			}
+			if (token <= 0) {
+				pr_err("Invalid nr_write_queues %d\n", token);
+				ret = -EINVAL;
+				goto out;
+			}
+			opts->nr_write_queues = token;
+			break;
+		case NVMF_OPT_NR_POLL_QUEUES:
+			if (match_int(args, &token)) {
+				ret = -EINVAL;
+				goto out;
+			}
+			if (token <= 0) {
+				pr_err("Invalid nr_poll_queues %d\n", token);
+				ret = -EINVAL;
+				goto out;
+			}
+			opts->nr_poll_queues = token;
+			break;
+		default:
+			pr_warn("unknown parameter or missing value '%s' in ctrl creation request\n",
+				p);
+			ret = -EINVAL;
+			goto out;
+		}
+	}
+
+	if (opts->discovery_nqn) {
+		opts->kato = 0;
+		opts->nr_io_queues = 0;
+		opts->nr_write_queues = 0;
+		opts->nr_poll_queues = 0;
+		opts->duplicate_connect = true;
+	}
+	if (ctrl_loss_tmo < 0)
+		opts->max_reconnects = -1;
+	else
+		opts->max_reconnects = DIV_ROUND_UP(ctrl_loss_tmo,
+						opts->reconnect_delay);
+
+	if (!opts->host) {
+		kref_get(&nvmf_default_host->ref);
+		opts->host = nvmf_default_host;
+	}
+
+	uuid_copy(&opts->host->id, &hostid);
+
+out:
+	kfree(options);
+	return ret;
+}
+
+static int nvmf_check_required_opts(struct nvmf_ctrl_options *opts,
+		unsigned int required_opts)
+{
+	if ((opts->mask & required_opts) != required_opts) {
+		int i;
+
+		for (i = 0; i < ARRAY_SIZE(opt_tokens); i++) {
+			if ((opt_tokens[i].token & required_opts) &&
+			    !(opt_tokens[i].token & opts->mask)) {
+				pr_warn("missing parameter '%s'\n",
+					opt_tokens[i].pattern);
+			}
+		}
+
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+bool nvmf_ip_options_match(struct nvme_ctrl *ctrl,
+		struct nvmf_ctrl_options *opts)
+{
+	if (!nvmf_ctlr_matches_baseopts(ctrl, opts) ||
+	    strcmp(opts->traddr, ctrl->opts->traddr) ||
+	    strcmp(opts->trsvcid, ctrl->opts->trsvcid))
+		return false;
+
+	/*
+	 * Checking the local address is rough. In most cases, none is specified
+	 * and the host port is selected by the stack.
+	 *
+	 * Assume no match if:
+	 * -  local address is specified and address is not the same
+	 * -  local address is not specified but remote is, or vice versa
+	 *    (admin using specific host_traddr when it matters).
+	 */
+	if ((opts->mask & NVMF_OPT_HOST_TRADDR) &&
+	    (ctrl->opts->mask & NVMF_OPT_HOST_TRADDR)) {
+		if (strcmp(opts->host_traddr, ctrl->opts->host_traddr))
+			return false;
+	} else if ((opts->mask & NVMF_OPT_HOST_TRADDR) ||
+		   (ctrl->opts->mask & NVMF_OPT_HOST_TRADDR)) {
+		return false;
+	}
+
+	return true;
+}
+EXPORT_SYMBOL_GPL(nvmf_ip_options_match);
+
+static int nvmf_check_allowed_opts(struct nvmf_ctrl_options *opts,
+		unsigned int allowed_opts)
+{
+	if (opts->mask & ~allowed_opts) {
+		int i;
+
+		for (i = 0; i < ARRAY_SIZE(opt_tokens); i++) {
+			if ((opt_tokens[i].token & opts->mask) &&
+			    (opt_tokens[i].token & ~allowed_opts)) {
+				pr_warn("invalid parameter '%s'\n",
+					opt_tokens[i].pattern);
+			}
+		}
+
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+void nvmf_free_options(struct nvmf_ctrl_options *opts)
+{
+	nvmf_host_put(opts->host);
+	kfree(opts->transport);
+	kfree(opts->traddr);
+	kfree(opts->trsvcid);
+	kfree(opts->subsysnqn);
+	kfree(opts->host_traddr);
+	kfree(opts);
+}
+EXPORT_SYMBOL_GPL(nvmf_free_options);
+
+#define NVMF_REQUIRED_OPTS	(NVMF_OPT_TRANSPORT | NVMF_OPT_NQN)
+#define NVMF_ALLOWED_OPTS	(NVMF_OPT_QUEUE_SIZE | NVMF_OPT_NR_IO_QUEUES | \
+				 NVMF_OPT_KATO | NVMF_OPT_HOSTNQN | \
+				 NVMF_OPT_HOST_ID | NVMF_OPT_DUP_CONNECT |\
+				 NVMF_OPT_DISABLE_SQFLOW)
+
+static struct nvme_ctrl *
+nvmf_create_ctrl(struct device *dev, const char *buf, size_t count)
+{
+	struct nvmf_ctrl_options *opts;
+	struct nvmf_transport_ops *ops;
+	struct nvme_ctrl *ctrl;
+	int ret;
+
+	opts = kzalloc(sizeof(*opts), GFP_KERNEL);
+	if (!opts)
+		return ERR_PTR(-ENOMEM);
+
+	ret = nvmf_parse_options(opts, buf);
+	if (ret)
+		goto out_free_opts;
+
+
+	request_module("nvme-%s", opts->transport);
+
+	/*
+	 * Check the generic options first as we need a valid transport for
+	 * the lookup below.  Then clear the generic flags so that transport
+	 * drivers don't have to care about them.
+	 */
+	ret = nvmf_check_required_opts(opts, NVMF_REQUIRED_OPTS);
+	if (ret)
+		goto out_free_opts;
+	opts->mask &= ~NVMF_REQUIRED_OPTS;
+
+	down_read(&nvmf_transports_rwsem);
+	ops = nvmf_lookup_transport(opts);
+	if (!ops) {
+		pr_info("no handler found for transport %s.\n",
+			opts->transport);
+		ret = -EINVAL;
+		goto out_unlock;
+	}
+
+	if (!try_module_get(ops->module)) {
+		ret = -EBUSY;
+		goto out_unlock;
+	}
+	up_read(&nvmf_transports_rwsem);
+
+	ret = nvmf_check_required_opts(opts, ops->required_opts);
+	if (ret)
+		goto out_module_put;
+	ret = nvmf_check_allowed_opts(opts, NVMF_ALLOWED_OPTS |
+				ops->allowed_opts | ops->required_opts);
+	if (ret)
+		goto out_module_put;
+
+	ctrl = ops->create_ctrl(dev, opts);
+	if (IS_ERR(ctrl)) {
+		ret = PTR_ERR(ctrl);
+		goto out_module_put;
+	}
+
+	module_put(ops->module);
+	return ctrl;
+
+out_module_put:
+	module_put(ops->module);
+	goto out_free_opts;
+out_unlock:
+	up_read(&nvmf_transports_rwsem);
+out_free_opts:
+	nvmf_free_options(opts);
+	return ERR_PTR(ret);
+}
+
+static struct class *nvmf_class;
+static struct device *nvmf_device;
+static DEFINE_MUTEX(nvmf_dev_mutex);
+
+static ssize_t nvmf_dev_write(struct file *file, const char __user *ubuf,
+		size_t count, loff_t *pos)
+{
+	struct seq_file *seq_file = file->private_data;
+	struct nvme_ctrl *ctrl;
+	const char *buf;
+	int ret = 0;
+
+	if (count > PAGE_SIZE)
+		return -ENOMEM;
+
+	buf = memdup_user_nul(ubuf, count);
+	if (IS_ERR(buf))
+		return PTR_ERR(buf);
+
+	mutex_lock(&nvmf_dev_mutex);
+	if (seq_file->private) {
+		ret = -EINVAL;
+		goto out_unlock;
+	}
+
+	ctrl = nvmf_create_ctrl(nvmf_device, buf, count);
+	if (IS_ERR(ctrl)) {
+		ret = PTR_ERR(ctrl);
+		goto out_unlock;
+	}
+
+	seq_file->private = ctrl;
+
+out_unlock:
+	mutex_unlock(&nvmf_dev_mutex);
+	kfree(buf);
+	return ret ? ret : count;
+}
+
+static int nvmf_dev_show(struct seq_file *seq_file, void *private)
+{
+	struct nvme_ctrl *ctrl;
+	int ret = 0;
+
+	mutex_lock(&nvmf_dev_mutex);
+	ctrl = seq_file->private;
+	if (!ctrl) {
+		ret = -EINVAL;
+		goto out_unlock;
+	}
+
+	seq_printf(seq_file, "instance=%d,cntlid=%d\n",
+			ctrl->instance, ctrl->cntlid);
+
+out_unlock:
+	mutex_unlock(&nvmf_dev_mutex);
+	return ret;
+}
+
+static int nvmf_dev_open(struct inode *inode, struct file *file)
+{
+	/*
+	 * The miscdevice code initializes file->private_data, but doesn't
+	 * make use of it later.
+	 */
+	file->private_data = NULL;
+	return single_open(file, nvmf_dev_show, NULL);
+}
+
+static int nvmf_dev_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *seq_file = file->private_data;
+	struct nvme_ctrl *ctrl = seq_file->private;
+
+	if (ctrl)
+		nvme_put_ctrl(ctrl);
+	return single_release(inode, file);
+}
+
+static const struct file_operations nvmf_dev_fops = {
+	.owner		= THIS_MODULE,
+	.write		= nvmf_dev_write,
+	.read		= seq_read,
+	.open		= nvmf_dev_open,
+	.release	= nvmf_dev_release,
+};
+
+static struct miscdevice nvmf_misc = {
+	.minor		= MISC_DYNAMIC_MINOR,
+	.name           = "nvme-fabrics",
+	.fops		= &nvmf_dev_fops,
+};
+
+static int __init nvmf_init(void)
+{
+	int ret;
+
+	nvmf_default_host = nvmf_host_default();
+	if (!nvmf_default_host)
+		return -ENOMEM;
+
+	nvmf_class = class_create(THIS_MODULE, "nvme-fabrics");
+	if (IS_ERR(nvmf_class)) {
+		pr_err("couldn't register class nvme-fabrics\n");
+		ret = PTR_ERR(nvmf_class);
+		goto out_free_host;
+	}
+
+	nvmf_device =
+		device_create(nvmf_class, NULL, MKDEV(0, 0), NULL, "ctl");
+	if (IS_ERR(nvmf_device)) {
+		pr_err("couldn't create nvme-fabris device!\n");
+		ret = PTR_ERR(nvmf_device);
+		goto out_destroy_class;
+	}
+
+	ret = misc_register(&nvmf_misc);
+	if (ret) {
+		pr_err("couldn't register misc device: %d\n", ret);
+		goto out_destroy_device;
+	}
+
+	return 0;
+
+out_destroy_device:
+	device_destroy(nvmf_class, MKDEV(0, 0));
+out_destroy_class:
+	class_destroy(nvmf_class);
+out_free_host:
+	nvmf_host_put(nvmf_default_host);
+	return ret;
+}
+
+static void __exit nvmf_exit(void)
+{
+	misc_deregister(&nvmf_misc);
+	device_destroy(nvmf_class, MKDEV(0, 0));
+	class_destroy(nvmf_class);
+	nvmf_host_put(nvmf_default_host);
+
+	BUILD_BUG_ON(sizeof(struct nvmf_connect_command) != 64);
+	BUILD_BUG_ON(sizeof(struct nvmf_property_get_command) != 64);
+	BUILD_BUG_ON(sizeof(struct nvmf_property_set_command) != 64);
+	BUILD_BUG_ON(sizeof(struct nvmf_connect_data) != 1024);
+}
+
+MODULE_LICENSE("GPL v2");
+
+module_init(nvmf_init);
+module_exit(nvmf_exit);
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/fabrics.h b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/fabrics.h
new file mode 100644
index 0000000..3044d8b
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/fabrics.h
@@ -0,0 +1,188 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * NVMe over Fabrics common host code.
+ * Copyright (c) 2015-2016 HGST, a Western Digital Company.
+ */
+#ifndef _NVME_FABRICS_H
+#define _NVME_FABRICS_H 1
+
+#include <linux/in.h>
+#include <linux/inet.h>
+
+#define NVMF_MIN_QUEUE_SIZE	16
+#define NVMF_MAX_QUEUE_SIZE	1024
+#define NVMF_DEF_QUEUE_SIZE	128
+#define NVMF_DEF_RECONNECT_DELAY	10
+/* default to 600 seconds of reconnect attempts before giving up */
+#define NVMF_DEF_CTRL_LOSS_TMO		600
+
+/*
+ * Define a host as seen by the target.  We allocate one at boot, but also
+ * allow the override it when creating controllers.  This is both to provide
+ * persistence of the Host NQN over multiple boots, and to allow using
+ * multiple ones, for example in a container scenario.  Because we must not
+ * use different Host NQNs with the same Host ID we generate a Host ID and
+ * use this structure to keep track of the relation between the two.
+ */
+struct nvmf_host {
+	struct kref		ref;
+	struct list_head	list;
+	char			nqn[NVMF_NQN_SIZE];
+	uuid_t			id;
+};
+
+/**
+ * enum nvmf_parsing_opts - used to define the sysfs parsing options used.
+ */
+enum {
+	NVMF_OPT_ERR		= 0,
+	NVMF_OPT_TRANSPORT	= 1 << 0,
+	NVMF_OPT_NQN		= 1 << 1,
+	NVMF_OPT_TRADDR		= 1 << 2,
+	NVMF_OPT_TRSVCID	= 1 << 3,
+	NVMF_OPT_QUEUE_SIZE	= 1 << 4,
+	NVMF_OPT_NR_IO_QUEUES	= 1 << 5,
+	NVMF_OPT_TL_RETRY_COUNT	= 1 << 6,
+	NVMF_OPT_KATO		= 1 << 7,
+	NVMF_OPT_HOSTNQN	= 1 << 8,
+	NVMF_OPT_RECONNECT_DELAY = 1 << 9,
+	NVMF_OPT_HOST_TRADDR	= 1 << 10,
+	NVMF_OPT_CTRL_LOSS_TMO	= 1 << 11,
+	NVMF_OPT_HOST_ID	= 1 << 12,
+	NVMF_OPT_DUP_CONNECT	= 1 << 13,
+	NVMF_OPT_DISABLE_SQFLOW = 1 << 14,
+	NVMF_OPT_HDR_DIGEST	= 1 << 15,
+	NVMF_OPT_DATA_DIGEST	= 1 << 16,
+	NVMF_OPT_NR_WRITE_QUEUES = 1 << 17,
+	NVMF_OPT_NR_POLL_QUEUES = 1 << 18,
+};
+
+/**
+ * struct nvmf_ctrl_options - Used to hold the options specified
+ *			      with the parsing opts enum.
+ * @mask:	Used by the fabrics library to parse through sysfs options
+ *		on adding a NVMe controller.
+ * @transport:	Holds the fabric transport "technology name" (for a lack of
+ *		better description) that will be used by an NVMe controller
+ *		being added.
+ * @subsysnqn:	Hold the fully qualified NQN subystem name (format defined
+ *		in the NVMe specification, "NVMe Qualified Names").
+ * @traddr:	The transport-specific TRADDR field for a port on the
+ *              subsystem which is adding a controller.
+ * @trsvcid:	The transport-specific TRSVCID field for a port on the
+ *              subsystem which is adding a controller.
+ * @host_traddr: A transport-specific field identifying the NVME host port
+ *              to use for the connection to the controller.
+ * @queue_size: Number of IO queue elements.
+ * @nr_io_queues: Number of controller IO queues that will be established.
+ * @reconnect_delay: Time between two consecutive reconnect attempts.
+ * @discovery_nqn: indicates if the subsysnqn is the well-known discovery NQN.
+ * @kato:	Keep-alive timeout.
+ * @host:	Virtual NVMe host, contains the NQN and Host ID.
+ * @max_reconnects: maximum number of allowed reconnect attempts before removing
+ *              the controller, (-1) means reconnect forever, zero means remove
+ *              immediately;
+ * @disable_sqflow: disable controller sq flow control
+ * @hdr_digest: generate/verify header digest (TCP)
+ * @data_digest: generate/verify data digest (TCP)
+ * @nr_write_queues: number of queues for write I/O
+ * @nr_poll_queues: number of queues for polling I/O
+ */
+struct nvmf_ctrl_options {
+	unsigned		mask;
+	char			*transport;
+	char			*subsysnqn;
+	char			*traddr;
+	char			*trsvcid;
+	char			*host_traddr;
+	size_t			queue_size;
+	unsigned int		nr_io_queues;
+	unsigned int		reconnect_delay;
+	bool			discovery_nqn;
+	bool			duplicate_connect;
+	unsigned int		kato;
+	struct nvmf_host	*host;
+	int			max_reconnects;
+	bool			disable_sqflow;
+	bool			hdr_digest;
+	bool			data_digest;
+	unsigned int		nr_write_queues;
+	unsigned int		nr_poll_queues;
+};
+
+/*
+ * struct nvmf_transport_ops - used to register a specific
+ *			       fabric implementation of NVMe fabrics.
+ * @entry:		Used by the fabrics library to add the new
+ *			registration entry to its linked-list internal tree.
+ * @module:             Transport module reference
+ * @name:		Name of the NVMe fabric driver implementation.
+ * @required_opts:	sysfs command-line options that must be specified
+ *			when adding a new NVMe controller.
+ * @allowed_opts:	sysfs command-line options that can be specified
+ *			when adding a new NVMe controller.
+ * @create_ctrl():	function pointer that points to a non-NVMe
+ *			implementation-specific fabric technology
+ *			that would go into starting up that fabric
+ *			for the purpose of conneciton to an NVMe controller
+ *			using that fabric technology.
+ *
+ * Notes:
+ *	1. At minimum, 'required_opts' and 'allowed_opts' should
+ *	   be set to the same enum parsing options defined earlier.
+ *	2. create_ctrl() must be defined (even if it does nothing)
+ *	3. struct nvmf_transport_ops must be statically allocated in the
+ *	   modules .bss section so that a pure module_get on @module
+ *	   prevents the memory from beeing freed.
+ */
+struct nvmf_transport_ops {
+	struct list_head	entry;
+	struct module		*module;
+	const char		*name;
+	int			required_opts;
+	int			allowed_opts;
+	struct nvme_ctrl	*(*create_ctrl)(struct device *dev,
+					struct nvmf_ctrl_options *opts);
+};
+
+static inline bool
+nvmf_ctlr_matches_baseopts(struct nvme_ctrl *ctrl,
+			struct nvmf_ctrl_options *opts)
+{
+	if (ctrl->state == NVME_CTRL_DELETING ||
+	    ctrl->state == NVME_CTRL_DEAD ||
+	    strcmp(opts->subsysnqn, ctrl->opts->subsysnqn) ||
+	    strcmp(opts->host->nqn, ctrl->opts->host->nqn) ||
+	    memcmp(&opts->host->id, &ctrl->opts->host->id, sizeof(uuid_t)))
+		return false;
+
+	return true;
+}
+
+int nvmf_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val);
+int nvmf_reg_read64(struct nvme_ctrl *ctrl, u32 off, u64 *val);
+int nvmf_reg_write32(struct nvme_ctrl *ctrl, u32 off, u32 val);
+int nvmf_connect_admin_queue(struct nvme_ctrl *ctrl);
+int nvmf_connect_io_queue(struct nvme_ctrl *ctrl, u16 qid, bool poll);
+int nvmf_register_transport(struct nvmf_transport_ops *ops);
+void nvmf_unregister_transport(struct nvmf_transport_ops *ops);
+void nvmf_free_options(struct nvmf_ctrl_options *opts);
+int nvmf_get_address(struct nvme_ctrl *ctrl, char *buf, int size);
+bool nvmf_should_reconnect(struct nvme_ctrl *ctrl);
+blk_status_t nvmf_fail_nonready_command(struct nvme_ctrl *ctrl,
+		struct request *rq);
+bool __nvmf_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
+		bool queue_live);
+bool nvmf_ip_options_match(struct nvme_ctrl *ctrl,
+		struct nvmf_ctrl_options *opts);
+
+static inline bool nvmf_check_ready(struct nvme_ctrl *ctrl, struct request *rq,
+		bool queue_live)
+{
+	if (likely(ctrl->state == NVME_CTRL_LIVE ||
+		   ctrl->state == NVME_CTRL_ADMIN_ONLY))
+		return true;
+	return __nvmf_check_ready(ctrl, rq, queue_live);
+}
+
+#endif /* _NVME_FABRICS_H */
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/fault_inject.c b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/fault_inject.c
new file mode 100644
index 0000000..4cfd2c9
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/fault_inject.c
@@ -0,0 +1,79 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * fault injection support for nvme.
+ *
+ * Copyright (c) 2018, Oracle and/or its affiliates
+ */
+
+#include <linux/moduleparam.h>
+#include "nvme.h"
+
+static DECLARE_FAULT_ATTR(fail_default_attr);
+/* optional fault injection attributes boot time option:
+ * nvme_core.fail_request=<interval>,<probability>,<space>,<times>
+ */
+static char *fail_request;
+module_param(fail_request, charp, 0000);
+
+void nvme_fault_inject_init(struct nvme_ns *ns)
+{
+	struct dentry *dir, *parent;
+	char *name = ns->disk->disk_name;
+	struct nvme_fault_inject *fault_inj = &ns->fault_inject;
+	struct fault_attr *attr = &fault_inj->attr;
+
+	/* set default fault injection attribute */
+	if (fail_request)
+		setup_fault_attr(&fail_default_attr, fail_request);
+
+	/* create debugfs directory and attribute */
+	parent = debugfs_create_dir(name, NULL);
+	if (!parent) {
+		pr_warn("%s: failed to create debugfs directory\n", name);
+		return;
+	}
+
+	*attr = fail_default_attr;
+	dir = fault_create_debugfs_attr("fault_inject", parent, attr);
+	if (IS_ERR(dir)) {
+		pr_warn("%s: failed to create debugfs attr\n", name);
+		debugfs_remove_recursive(parent);
+		return;
+	}
+	ns->fault_inject.parent = parent;
+
+	/* create debugfs for status code and dont_retry */
+	fault_inj->status = NVME_SC_INVALID_OPCODE;
+	fault_inj->dont_retry = true;
+	debugfs_create_x16("status", 0600, dir,	&fault_inj->status);
+	debugfs_create_bool("dont_retry", 0600, dir, &fault_inj->dont_retry);
+}
+
+void nvme_fault_inject_fini(struct nvme_ns *ns)
+{
+	/* remove debugfs directories */
+	debugfs_remove_recursive(ns->fault_inject.parent);
+}
+
+void nvme_should_fail(struct request *req)
+{
+	struct gendisk *disk = req->rq_disk;
+	struct nvme_ns *ns = NULL;
+	u16 status;
+
+	/*
+	 * make sure this request is coming from a valid namespace
+	 */
+	if (!disk)
+		return;
+
+	ns = disk->private_data;
+	if (ns && should_fail(&ns->fault_inject.attr, 1)) {
+		/* inject status code and DNR bit */
+		status = ns->fault_inject.status;
+		if (ns->fault_inject.dont_retry)
+			status |= NVME_SC_DNR;
+		nvme_req(req)->status =	status;
+	}
+}
+EXPORT_SYMBOL_GPL(nvme_should_fail);
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/fc.c b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/fc.c
new file mode 100644
index 0000000..6d84513
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/fc.c
@@ -0,0 +1,3464 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2016 Avago Technologies.  All rights reserved.
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+#include <linux/module.h>
+#include <linux/parser.h>
+#include <uapi/scsi/fc/fc_fs.h>
+#include <uapi/scsi/fc/fc_els.h>
+#include <linux/delay.h>
+#include <linux/overflow.h>
+
+#include "nvme.h"
+#include "fabrics.h"
+#include <linux/nvme-fc-driver.h>
+#include <linux/nvme-fc.h>
+
+
+/* *************************** Data Structures/Defines ****************** */
+
+
+enum nvme_fc_queue_flags {
+	NVME_FC_Q_CONNECTED = 0,
+	NVME_FC_Q_LIVE,
+};
+
+#define NVME_FC_DEFAULT_DEV_LOSS_TMO	60	/* seconds */
+
+struct nvme_fc_queue {
+	struct nvme_fc_ctrl	*ctrl;
+	struct device		*dev;
+	struct blk_mq_hw_ctx	*hctx;
+	void			*lldd_handle;
+	size_t			cmnd_capsule_len;
+	u32			qnum;
+	u32			rqcnt;
+	u32			seqno;
+
+	u64			connection_id;
+	atomic_t		csn;
+
+	unsigned long		flags;
+} __aligned(sizeof(u64));	/* alignment for other things alloc'd with */
+
+enum nvme_fcop_flags {
+	FCOP_FLAGS_TERMIO	= (1 << 0),
+	FCOP_FLAGS_AEN		= (1 << 1),
+};
+
+struct nvmefc_ls_req_op {
+	struct nvmefc_ls_req	ls_req;
+
+	struct nvme_fc_rport	*rport;
+	struct nvme_fc_queue	*queue;
+	struct request		*rq;
+	u32			flags;
+
+	int			ls_error;
+	struct completion	ls_done;
+	struct list_head	lsreq_list;	/* rport->ls_req_list */
+	bool			req_queued;
+};
+
+enum nvme_fcpop_state {
+	FCPOP_STATE_UNINIT	= 0,
+	FCPOP_STATE_IDLE	= 1,
+	FCPOP_STATE_ACTIVE	= 2,
+	FCPOP_STATE_ABORTED	= 3,
+	FCPOP_STATE_COMPLETE	= 4,
+};
+
+struct nvme_fc_fcp_op {
+	struct nvme_request	nreq;		/*
+						 * nvme/host/core.c
+						 * requires this to be
+						 * the 1st element in the
+						 * private structure
+						 * associated with the
+						 * request.
+						 */
+	struct nvmefc_fcp_req	fcp_req;
+
+	struct nvme_fc_ctrl	*ctrl;
+	struct nvme_fc_queue	*queue;
+	struct request		*rq;
+
+	atomic_t		state;
+	u32			flags;
+	u32			rqno;
+	u32			nents;
+
+	struct nvme_fc_cmd_iu	cmd_iu;
+	struct nvme_fc_ersp_iu	rsp_iu;
+};
+
+struct nvme_fcp_op_w_sgl {
+	struct nvme_fc_fcp_op	op;
+	struct scatterlist	sgl[SG_CHUNK_SIZE];
+	uint8_t			priv[0];
+};
+
+struct nvme_fc_lport {
+	struct nvme_fc_local_port	localport;
+
+	struct ida			endp_cnt;
+	struct list_head		port_list;	/* nvme_fc_port_list */
+	struct list_head		endp_list;
+	struct device			*dev;	/* physical device for dma */
+	struct nvme_fc_port_template	*ops;
+	struct kref			ref;
+	atomic_t                        act_rport_cnt;
+} __aligned(sizeof(u64));	/* alignment for other things alloc'd with */
+
+struct nvme_fc_rport {
+	struct nvme_fc_remote_port	remoteport;
+
+	struct list_head		endp_list; /* for lport->endp_list */
+	struct list_head		ctrl_list;
+	struct list_head		ls_req_list;
+	struct list_head		disc_list;
+	struct device			*dev;	/* physical device for dma */
+	struct nvme_fc_lport		*lport;
+	spinlock_t			lock;
+	struct kref			ref;
+	atomic_t                        act_ctrl_cnt;
+	unsigned long			dev_loss_end;
+} __aligned(sizeof(u64));	/* alignment for other things alloc'd with */
+
+enum nvme_fcctrl_flags {
+	FCCTRL_TERMIO		= (1 << 0),
+};
+
+struct nvme_fc_ctrl {
+	spinlock_t		lock;
+	struct nvme_fc_queue	*queues;
+	struct device		*dev;
+	struct nvme_fc_lport	*lport;
+	struct nvme_fc_rport	*rport;
+	u32			cnum;
+
+	bool			ioq_live;
+	bool			assoc_active;
+	atomic_t		err_work_active;
+	u64			association_id;
+
+	struct list_head	ctrl_list;	/* rport->ctrl_list */
+
+	struct blk_mq_tag_set	admin_tag_set;
+	struct blk_mq_tag_set	tag_set;
+
+	struct delayed_work	connect_work;
+	struct work_struct	err_work;
+
+	struct kref		ref;
+	u32			flags;
+	u32			iocnt;
+	wait_queue_head_t	ioabort_wait;
+
+	struct nvme_fc_fcp_op	aen_ops[NVME_NR_AEN_COMMANDS];
+
+	struct nvme_ctrl	ctrl;
+};
+
+static inline struct nvme_fc_ctrl *
+to_fc_ctrl(struct nvme_ctrl *ctrl)
+{
+	return container_of(ctrl, struct nvme_fc_ctrl, ctrl);
+}
+
+static inline struct nvme_fc_lport *
+localport_to_lport(struct nvme_fc_local_port *portptr)
+{
+	return container_of(portptr, struct nvme_fc_lport, localport);
+}
+
+static inline struct nvme_fc_rport *
+remoteport_to_rport(struct nvme_fc_remote_port *portptr)
+{
+	return container_of(portptr, struct nvme_fc_rport, remoteport);
+}
+
+static inline struct nvmefc_ls_req_op *
+ls_req_to_lsop(struct nvmefc_ls_req *lsreq)
+{
+	return container_of(lsreq, struct nvmefc_ls_req_op, ls_req);
+}
+
+static inline struct nvme_fc_fcp_op *
+fcp_req_to_fcp_op(struct nvmefc_fcp_req *fcpreq)
+{
+	return container_of(fcpreq, struct nvme_fc_fcp_op, fcp_req);
+}
+
+
+
+/* *************************** Globals **************************** */
+
+
+static DEFINE_SPINLOCK(nvme_fc_lock);
+
+static LIST_HEAD(nvme_fc_lport_list);
+static DEFINE_IDA(nvme_fc_local_port_cnt);
+static DEFINE_IDA(nvme_fc_ctrl_cnt);
+
+
+
+/*
+ * These items are short-term. They will eventually be moved into
+ * a generic FC class. See comments in module init.
+ */
+static struct device *fc_udev_device;
+
+
+/* *********************** FC-NVME Port Management ************************ */
+
+static void __nvme_fc_delete_hw_queue(struct nvme_fc_ctrl *,
+			struct nvme_fc_queue *, unsigned int);
+
+static void
+nvme_fc_free_lport(struct kref *ref)
+{
+	struct nvme_fc_lport *lport =
+		container_of(ref, struct nvme_fc_lport, ref);
+	unsigned long flags;
+
+	WARN_ON(lport->localport.port_state != FC_OBJSTATE_DELETED);
+	WARN_ON(!list_empty(&lport->endp_list));
+
+	/* remove from transport list */
+	spin_lock_irqsave(&nvme_fc_lock, flags);
+	list_del(&lport->port_list);
+	spin_unlock_irqrestore(&nvme_fc_lock, flags);
+
+	ida_simple_remove(&nvme_fc_local_port_cnt, lport->localport.port_num);
+	ida_destroy(&lport->endp_cnt);
+
+	put_device(lport->dev);
+
+	kfree(lport);
+}
+
+static void
+nvme_fc_lport_put(struct nvme_fc_lport *lport)
+{
+	kref_put(&lport->ref, nvme_fc_free_lport);
+}
+
+static int
+nvme_fc_lport_get(struct nvme_fc_lport *lport)
+{
+	return kref_get_unless_zero(&lport->ref);
+}
+
+
+static struct nvme_fc_lport *
+nvme_fc_attach_to_unreg_lport(struct nvme_fc_port_info *pinfo,
+			struct nvme_fc_port_template *ops,
+			struct device *dev)
+{
+	struct nvme_fc_lport *lport;
+	unsigned long flags;
+
+	spin_lock_irqsave(&nvme_fc_lock, flags);
+
+	list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+		if (lport->localport.node_name != pinfo->node_name ||
+		    lport->localport.port_name != pinfo->port_name)
+			continue;
+
+		if (lport->dev != dev) {
+			lport = ERR_PTR(-EXDEV);
+			goto out_done;
+		}
+
+		if (lport->localport.port_state != FC_OBJSTATE_DELETED) {
+			lport = ERR_PTR(-EEXIST);
+			goto out_done;
+		}
+
+		if (!nvme_fc_lport_get(lport)) {
+			/*
+			 * fails if ref cnt already 0. If so,
+			 * act as if lport already deleted
+			 */
+			lport = NULL;
+			goto out_done;
+		}
+
+		/* resume the lport */
+
+		lport->ops = ops;
+		lport->localport.port_role = pinfo->port_role;
+		lport->localport.port_id = pinfo->port_id;
+		lport->localport.port_state = FC_OBJSTATE_ONLINE;
+
+		spin_unlock_irqrestore(&nvme_fc_lock, flags);
+
+		return lport;
+	}
+
+	lport = NULL;
+
+out_done:
+	spin_unlock_irqrestore(&nvme_fc_lock, flags);
+
+	return lport;
+}
+
+/**
+ * nvme_fc_register_localport - transport entry point called by an
+ *                              LLDD to register the existence of a NVME
+ *                              host FC port.
+ * @pinfo:     pointer to information about the port to be registered
+ * @template:  LLDD entrypoints and operational parameters for the port
+ * @dev:       physical hardware device node port corresponds to. Will be
+ *             used for DMA mappings
+ * @portptr:   pointer to a local port pointer. Upon success, the routine
+ *             will allocate a nvme_fc_local_port structure and place its
+ *             address in the local port pointer. Upon failure, local port
+ *             pointer will be set to 0.
+ *
+ * Returns:
+ * a completion status. Must be 0 upon success; a negative errno
+ * (ex: -ENXIO) upon failure.
+ */
+int
+nvme_fc_register_localport(struct nvme_fc_port_info *pinfo,
+			struct nvme_fc_port_template *template,
+			struct device *dev,
+			struct nvme_fc_local_port **portptr)
+{
+	struct nvme_fc_lport *newrec;
+	unsigned long flags;
+	int ret, idx;
+
+	if (!template->localport_delete || !template->remoteport_delete ||
+	    !template->ls_req || !template->fcp_io ||
+	    !template->ls_abort || !template->fcp_abort ||
+	    !template->max_hw_queues || !template->max_sgl_segments ||
+	    !template->max_dif_sgl_segments || !template->dma_boundary) {
+		ret = -EINVAL;
+		goto out_reghost_failed;
+	}
+
+	/*
+	 * look to see if there is already a localport that had been
+	 * deregistered and in the process of waiting for all the
+	 * references to fully be removed.  If the references haven't
+	 * expired, we can simply re-enable the localport. Remoteports
+	 * and controller reconnections should resume naturally.
+	 */
+	newrec = nvme_fc_attach_to_unreg_lport(pinfo, template, dev);
+
+	/* found an lport, but something about its state is bad */
+	if (IS_ERR(newrec)) {
+		ret = PTR_ERR(newrec);
+		goto out_reghost_failed;
+
+	/* found existing lport, which was resumed */
+	} else if (newrec) {
+		*portptr = &newrec->localport;
+		return 0;
+	}
+
+	/* nothing found - allocate a new localport struct */
+
+	newrec = kmalloc((sizeof(*newrec) + template->local_priv_sz),
+			 GFP_KERNEL);
+	if (!newrec) {
+		ret = -ENOMEM;
+		goto out_reghost_failed;
+	}
+
+	idx = ida_simple_get(&nvme_fc_local_port_cnt, 0, 0, GFP_KERNEL);
+	if (idx < 0) {
+		ret = -ENOSPC;
+		goto out_fail_kfree;
+	}
+
+	if (!get_device(dev) && dev) {
+		ret = -ENODEV;
+		goto out_ida_put;
+	}
+
+	INIT_LIST_HEAD(&newrec->port_list);
+	INIT_LIST_HEAD(&newrec->endp_list);
+	kref_init(&newrec->ref);
+	atomic_set(&newrec->act_rport_cnt, 0);
+	newrec->ops = template;
+	newrec->dev = dev;
+	ida_init(&newrec->endp_cnt);
+	newrec->localport.private = &newrec[1];
+	newrec->localport.node_name = pinfo->node_name;
+	newrec->localport.port_name = pinfo->port_name;
+	newrec->localport.port_role = pinfo->port_role;
+	newrec->localport.port_id = pinfo->port_id;
+	newrec->localport.port_state = FC_OBJSTATE_ONLINE;
+	newrec->localport.port_num = idx;
+
+	spin_lock_irqsave(&nvme_fc_lock, flags);
+	list_add_tail(&newrec->port_list, &nvme_fc_lport_list);
+	spin_unlock_irqrestore(&nvme_fc_lock, flags);
+
+	if (dev)
+		dma_set_seg_boundary(dev, template->dma_boundary);
+
+	*portptr = &newrec->localport;
+	return 0;
+
+out_ida_put:
+	ida_simple_remove(&nvme_fc_local_port_cnt, idx);
+out_fail_kfree:
+	kfree(newrec);
+out_reghost_failed:
+	*portptr = NULL;
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvme_fc_register_localport);
+
+/**
+ * nvme_fc_unregister_localport - transport entry point called by an
+ *                              LLDD to deregister/remove a previously
+ *                              registered a NVME host FC port.
+ * @portptr: pointer to the (registered) local port that is to be deregistered.
+ *
+ * Returns:
+ * a completion status. Must be 0 upon success; a negative errno
+ * (ex: -ENXIO) upon failure.
+ */
+int
+nvme_fc_unregister_localport(struct nvme_fc_local_port *portptr)
+{
+	struct nvme_fc_lport *lport = localport_to_lport(portptr);
+	unsigned long flags;
+
+	if (!portptr)
+		return -EINVAL;
+
+	spin_lock_irqsave(&nvme_fc_lock, flags);
+
+	if (portptr->port_state != FC_OBJSTATE_ONLINE) {
+		spin_unlock_irqrestore(&nvme_fc_lock, flags);
+		return -EINVAL;
+	}
+	portptr->port_state = FC_OBJSTATE_DELETED;
+
+	spin_unlock_irqrestore(&nvme_fc_lock, flags);
+
+	if (atomic_read(&lport->act_rport_cnt) == 0)
+		lport->ops->localport_delete(&lport->localport);
+
+	nvme_fc_lport_put(lport);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(nvme_fc_unregister_localport);
+
+/*
+ * TRADDR strings, per FC-NVME are fixed format:
+ *   "nn-0x<16hexdigits>:pn-0x<16hexdigits>" - 43 characters
+ * udev event will only differ by prefix of what field is
+ * being specified:
+ *    "NVMEFC_HOST_TRADDR=" or "NVMEFC_TRADDR=" - 19 max characters
+ *  19 + 43 + null_fudge = 64 characters
+ */
+#define FCNVME_TRADDR_LENGTH		64
+
+static void
+nvme_fc_signal_discovery_scan(struct nvme_fc_lport *lport,
+		struct nvme_fc_rport *rport)
+{
+	char hostaddr[FCNVME_TRADDR_LENGTH];	/* NVMEFC_HOST_TRADDR=...*/
+	char tgtaddr[FCNVME_TRADDR_LENGTH];	/* NVMEFC_TRADDR=...*/
+	char *envp[4] = { "FC_EVENT=nvmediscovery", hostaddr, tgtaddr, NULL };
+
+	if (!(rport->remoteport.port_role & FC_PORT_ROLE_NVME_DISCOVERY))
+		return;
+
+	snprintf(hostaddr, sizeof(hostaddr),
+		"NVMEFC_HOST_TRADDR=nn-0x%016llx:pn-0x%016llx",
+		lport->localport.node_name, lport->localport.port_name);
+	snprintf(tgtaddr, sizeof(tgtaddr),
+		"NVMEFC_TRADDR=nn-0x%016llx:pn-0x%016llx",
+		rport->remoteport.node_name, rport->remoteport.port_name);
+	kobject_uevent_env(&fc_udev_device->kobj, KOBJ_CHANGE, envp);
+}
+
+static void
+nvme_fc_free_rport(struct kref *ref)
+{
+	struct nvme_fc_rport *rport =
+		container_of(ref, struct nvme_fc_rport, ref);
+	struct nvme_fc_lport *lport =
+			localport_to_lport(rport->remoteport.localport);
+	unsigned long flags;
+
+	WARN_ON(rport->remoteport.port_state != FC_OBJSTATE_DELETED);
+	WARN_ON(!list_empty(&rport->ctrl_list));
+
+	/* remove from lport list */
+	spin_lock_irqsave(&nvme_fc_lock, flags);
+	list_del(&rport->endp_list);
+	spin_unlock_irqrestore(&nvme_fc_lock, flags);
+
+	WARN_ON(!list_empty(&rport->disc_list));
+	ida_simple_remove(&lport->endp_cnt, rport->remoteport.port_num);
+
+	kfree(rport);
+
+	nvme_fc_lport_put(lport);
+}
+
+static void
+nvme_fc_rport_put(struct nvme_fc_rport *rport)
+{
+	kref_put(&rport->ref, nvme_fc_free_rport);
+}
+
+static int
+nvme_fc_rport_get(struct nvme_fc_rport *rport)
+{
+	return kref_get_unless_zero(&rport->ref);
+}
+
+static void
+nvme_fc_resume_controller(struct nvme_fc_ctrl *ctrl)
+{
+	switch (ctrl->ctrl.state) {
+	case NVME_CTRL_NEW:
+	case NVME_CTRL_CONNECTING:
+		/*
+		 * As all reconnects were suppressed, schedule a
+		 * connect.
+		 */
+		dev_info(ctrl->ctrl.device,
+			"NVME-FC{%d}: connectivity re-established. "
+			"Attempting reconnect\n", ctrl->cnum);
+
+		queue_delayed_work(nvme_wq, &ctrl->connect_work, 0);
+		break;
+
+	case NVME_CTRL_RESETTING:
+		/*
+		 * Controller is already in the process of terminating the
+		 * association. No need to do anything further. The reconnect
+		 * step will naturally occur after the reset completes.
+		 */
+		break;
+
+	default:
+		/* no action to take - let it delete */
+		break;
+	}
+}
+
+static struct nvme_fc_rport *
+nvme_fc_attach_to_suspended_rport(struct nvme_fc_lport *lport,
+				struct nvme_fc_port_info *pinfo)
+{
+	struct nvme_fc_rport *rport;
+	struct nvme_fc_ctrl *ctrl;
+	unsigned long flags;
+
+	spin_lock_irqsave(&nvme_fc_lock, flags);
+
+	list_for_each_entry(rport, &lport->endp_list, endp_list) {
+		if (rport->remoteport.node_name != pinfo->node_name ||
+		    rport->remoteport.port_name != pinfo->port_name)
+			continue;
+
+		if (!nvme_fc_rport_get(rport)) {
+			rport = ERR_PTR(-ENOLCK);
+			goto out_done;
+		}
+
+		spin_unlock_irqrestore(&nvme_fc_lock, flags);
+
+		spin_lock_irqsave(&rport->lock, flags);
+
+		/* has it been unregistered */
+		if (rport->remoteport.port_state != FC_OBJSTATE_DELETED) {
+			/* means lldd called us twice */
+			spin_unlock_irqrestore(&rport->lock, flags);
+			nvme_fc_rport_put(rport);
+			return ERR_PTR(-ESTALE);
+		}
+
+		rport->remoteport.port_role = pinfo->port_role;
+		rport->remoteport.port_id = pinfo->port_id;
+		rport->remoteport.port_state = FC_OBJSTATE_ONLINE;
+		rport->dev_loss_end = 0;
+
+		/*
+		 * kick off a reconnect attempt on all associations to the
+		 * remote port. A successful reconnects will resume i/o.
+		 */
+		list_for_each_entry(ctrl, &rport->ctrl_list, ctrl_list)
+			nvme_fc_resume_controller(ctrl);
+
+		spin_unlock_irqrestore(&rport->lock, flags);
+
+		return rport;
+	}
+
+	rport = NULL;
+
+out_done:
+	spin_unlock_irqrestore(&nvme_fc_lock, flags);
+
+	return rport;
+}
+
+static inline void
+__nvme_fc_set_dev_loss_tmo(struct nvme_fc_rport *rport,
+			struct nvme_fc_port_info *pinfo)
+{
+	if (pinfo->dev_loss_tmo)
+		rport->remoteport.dev_loss_tmo = pinfo->dev_loss_tmo;
+	else
+		rport->remoteport.dev_loss_tmo = NVME_FC_DEFAULT_DEV_LOSS_TMO;
+}
+
+/**
+ * nvme_fc_register_remoteport - transport entry point called by an
+ *                              LLDD to register the existence of a NVME
+ *                              subsystem FC port on its fabric.
+ * @localport: pointer to the (registered) local port that the remote
+ *             subsystem port is connected to.
+ * @pinfo:     pointer to information about the port to be registered
+ * @portptr:   pointer to a remote port pointer. Upon success, the routine
+ *             will allocate a nvme_fc_remote_port structure and place its
+ *             address in the remote port pointer. Upon failure, remote port
+ *             pointer will be set to 0.
+ *
+ * Returns:
+ * a completion status. Must be 0 upon success; a negative errno
+ * (ex: -ENXIO) upon failure.
+ */
+int
+nvme_fc_register_remoteport(struct nvme_fc_local_port *localport,
+				struct nvme_fc_port_info *pinfo,
+				struct nvme_fc_remote_port **portptr)
+{
+	struct nvme_fc_lport *lport = localport_to_lport(localport);
+	struct nvme_fc_rport *newrec;
+	unsigned long flags;
+	int ret, idx;
+
+	if (!nvme_fc_lport_get(lport)) {
+		ret = -ESHUTDOWN;
+		goto out_reghost_failed;
+	}
+
+	/*
+	 * look to see if there is already a remoteport that is waiting
+	 * for a reconnect (within dev_loss_tmo) with the same WWN's.
+	 * If so, transition to it and reconnect.
+	 */
+	newrec = nvme_fc_attach_to_suspended_rport(lport, pinfo);
+
+	/* found an rport, but something about its state is bad */
+	if (IS_ERR(newrec)) {
+		ret = PTR_ERR(newrec);
+		goto out_lport_put;
+
+	/* found existing rport, which was resumed */
+	} else if (newrec) {
+		nvme_fc_lport_put(lport);
+		__nvme_fc_set_dev_loss_tmo(newrec, pinfo);
+		nvme_fc_signal_discovery_scan(lport, newrec);
+		*portptr = &newrec->remoteport;
+		return 0;
+	}
+
+	/* nothing found - allocate a new remoteport struct */
+
+	newrec = kmalloc((sizeof(*newrec) + lport->ops->remote_priv_sz),
+			 GFP_KERNEL);
+	if (!newrec) {
+		ret = -ENOMEM;
+		goto out_lport_put;
+	}
+
+	idx = ida_simple_get(&lport->endp_cnt, 0, 0, GFP_KERNEL);
+	if (idx < 0) {
+		ret = -ENOSPC;
+		goto out_kfree_rport;
+	}
+
+	INIT_LIST_HEAD(&newrec->endp_list);
+	INIT_LIST_HEAD(&newrec->ctrl_list);
+	INIT_LIST_HEAD(&newrec->ls_req_list);
+	INIT_LIST_HEAD(&newrec->disc_list);
+	kref_init(&newrec->ref);
+	atomic_set(&newrec->act_ctrl_cnt, 0);
+	spin_lock_init(&newrec->lock);
+	newrec->remoteport.localport = &lport->localport;
+	newrec->dev = lport->dev;
+	newrec->lport = lport;
+	newrec->remoteport.private = &newrec[1];
+	newrec->remoteport.port_role = pinfo->port_role;
+	newrec->remoteport.node_name = pinfo->node_name;
+	newrec->remoteport.port_name = pinfo->port_name;
+	newrec->remoteport.port_id = pinfo->port_id;
+	newrec->remoteport.port_state = FC_OBJSTATE_ONLINE;
+	newrec->remoteport.port_num = idx;
+	__nvme_fc_set_dev_loss_tmo(newrec, pinfo);
+
+	spin_lock_irqsave(&nvme_fc_lock, flags);
+	list_add_tail(&newrec->endp_list, &lport->endp_list);
+	spin_unlock_irqrestore(&nvme_fc_lock, flags);
+
+	nvme_fc_signal_discovery_scan(lport, newrec);
+
+	*portptr = &newrec->remoteport;
+	return 0;
+
+out_kfree_rport:
+	kfree(newrec);
+out_lport_put:
+	nvme_fc_lport_put(lport);
+out_reghost_failed:
+	*portptr = NULL;
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvme_fc_register_remoteport);
+
+static int
+nvme_fc_abort_lsops(struct nvme_fc_rport *rport)
+{
+	struct nvmefc_ls_req_op *lsop;
+	unsigned long flags;
+
+restart:
+	spin_lock_irqsave(&rport->lock, flags);
+
+	list_for_each_entry(lsop, &rport->ls_req_list, lsreq_list) {
+		if (!(lsop->flags & FCOP_FLAGS_TERMIO)) {
+			lsop->flags |= FCOP_FLAGS_TERMIO;
+			spin_unlock_irqrestore(&rport->lock, flags);
+			rport->lport->ops->ls_abort(&rport->lport->localport,
+						&rport->remoteport,
+						&lsop->ls_req);
+			goto restart;
+		}
+	}
+	spin_unlock_irqrestore(&rport->lock, flags);
+
+	return 0;
+}
+
+static void
+nvme_fc_ctrl_connectivity_loss(struct nvme_fc_ctrl *ctrl)
+{
+	dev_info(ctrl->ctrl.device,
+		"NVME-FC{%d}: controller connectivity lost. Awaiting "
+		"Reconnect", ctrl->cnum);
+
+	switch (ctrl->ctrl.state) {
+	case NVME_CTRL_NEW:
+	case NVME_CTRL_LIVE:
+		/*
+		 * Schedule a controller reset. The reset will terminate the
+		 * association and schedule the reconnect timer.  Reconnects
+		 * will be attempted until either the ctlr_loss_tmo
+		 * (max_retries * connect_delay) expires or the remoteport's
+		 * dev_loss_tmo expires.
+		 */
+		if (nvme_reset_ctrl(&ctrl->ctrl)) {
+			dev_warn(ctrl->ctrl.device,
+				"NVME-FC{%d}: Couldn't schedule reset.\n",
+				ctrl->cnum);
+			nvme_delete_ctrl(&ctrl->ctrl);
+		}
+		break;
+
+	case NVME_CTRL_CONNECTING:
+		/*
+		 * The association has already been terminated and the
+		 * controller is attempting reconnects.  No need to do anything
+		 * futher.  Reconnects will be attempted until either the
+		 * ctlr_loss_tmo (max_retries * connect_delay) expires or the
+		 * remoteport's dev_loss_tmo expires.
+		 */
+		break;
+
+	case NVME_CTRL_RESETTING:
+		/*
+		 * Controller is already in the process of terminating the
+		 * association.  No need to do anything further. The reconnect
+		 * step will kick in naturally after the association is
+		 * terminated.
+		 */
+		break;
+
+	case NVME_CTRL_DELETING:
+	default:
+		/* no action to take - let it delete */
+		break;
+	}
+}
+
+/**
+ * nvme_fc_unregister_remoteport - transport entry point called by an
+ *                              LLDD to deregister/remove a previously
+ *                              registered a NVME subsystem FC port.
+ * @portptr: pointer to the (registered) remote port that is to be
+ *           deregistered.
+ *
+ * Returns:
+ * a completion status. Must be 0 upon success; a negative errno
+ * (ex: -ENXIO) upon failure.
+ */
+int
+nvme_fc_unregister_remoteport(struct nvme_fc_remote_port *portptr)
+{
+	struct nvme_fc_rport *rport = remoteport_to_rport(portptr);
+	struct nvme_fc_ctrl *ctrl;
+	unsigned long flags;
+
+	if (!portptr)
+		return -EINVAL;
+
+	spin_lock_irqsave(&rport->lock, flags);
+
+	if (portptr->port_state != FC_OBJSTATE_ONLINE) {
+		spin_unlock_irqrestore(&rport->lock, flags);
+		return -EINVAL;
+	}
+	portptr->port_state = FC_OBJSTATE_DELETED;
+
+	rport->dev_loss_end = jiffies + (portptr->dev_loss_tmo * HZ);
+
+	list_for_each_entry(ctrl, &rport->ctrl_list, ctrl_list) {
+		/* if dev_loss_tmo==0, dev loss is immediate */
+		if (!portptr->dev_loss_tmo) {
+			dev_warn(ctrl->ctrl.device,
+				"NVME-FC{%d}: controller connectivity lost.\n",
+				ctrl->cnum);
+			nvme_delete_ctrl(&ctrl->ctrl);
+		} else
+			nvme_fc_ctrl_connectivity_loss(ctrl);
+	}
+
+	spin_unlock_irqrestore(&rport->lock, flags);
+
+	nvme_fc_abort_lsops(rport);
+
+	if (atomic_read(&rport->act_ctrl_cnt) == 0)
+		rport->lport->ops->remoteport_delete(portptr);
+
+	/*
+	 * release the reference, which will allow, if all controllers
+	 * go away, which should only occur after dev_loss_tmo occurs,
+	 * for the rport to be torn down.
+	 */
+	nvme_fc_rport_put(rport);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(nvme_fc_unregister_remoteport);
+
+/**
+ * nvme_fc_rescan_remoteport - transport entry point called by an
+ *                              LLDD to request a nvme device rescan.
+ * @remoteport: pointer to the (registered) remote port that is to be
+ *              rescanned.
+ *
+ * Returns: N/A
+ */
+void
+nvme_fc_rescan_remoteport(struct nvme_fc_remote_port *remoteport)
+{
+	struct nvme_fc_rport *rport = remoteport_to_rport(remoteport);
+
+	nvme_fc_signal_discovery_scan(rport->lport, rport);
+}
+EXPORT_SYMBOL_GPL(nvme_fc_rescan_remoteport);
+
+int
+nvme_fc_set_remoteport_devloss(struct nvme_fc_remote_port *portptr,
+			u32 dev_loss_tmo)
+{
+	struct nvme_fc_rport *rport = remoteport_to_rport(portptr);
+	unsigned long flags;
+
+	spin_lock_irqsave(&rport->lock, flags);
+
+	if (portptr->port_state != FC_OBJSTATE_ONLINE) {
+		spin_unlock_irqrestore(&rport->lock, flags);
+		return -EINVAL;
+	}
+
+	/* a dev_loss_tmo of 0 (immediate) is allowed to be set */
+	rport->remoteport.dev_loss_tmo = dev_loss_tmo;
+
+	spin_unlock_irqrestore(&rport->lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(nvme_fc_set_remoteport_devloss);
+
+
+/* *********************** FC-NVME DMA Handling **************************** */
+
+/*
+ * The fcloop device passes in a NULL device pointer. Real LLD's will
+ * pass in a valid device pointer. If NULL is passed to the dma mapping
+ * routines, depending on the platform, it may or may not succeed, and
+ * may crash.
+ *
+ * As such:
+ * Wrapper all the dma routines and check the dev pointer.
+ *
+ * If simple mappings (return just a dma address, we'll noop them,
+ * returning a dma address of 0.
+ *
+ * On more complex mappings (dma_map_sg), a pseudo routine fills
+ * in the scatter list, setting all dma addresses to 0.
+ */
+
+static inline dma_addr_t
+fc_dma_map_single(struct device *dev, void *ptr, size_t size,
+		enum dma_data_direction dir)
+{
+	return dev ? dma_map_single(dev, ptr, size, dir) : (dma_addr_t)0L;
+}
+
+static inline int
+fc_dma_mapping_error(struct device *dev, dma_addr_t dma_addr)
+{
+	return dev ? dma_mapping_error(dev, dma_addr) : 0;
+}
+
+static inline void
+fc_dma_unmap_single(struct device *dev, dma_addr_t addr, size_t size,
+	enum dma_data_direction dir)
+{
+	if (dev)
+		dma_unmap_single(dev, addr, size, dir);
+}
+
+static inline void
+fc_dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr, size_t size,
+		enum dma_data_direction dir)
+{
+	if (dev)
+		dma_sync_single_for_cpu(dev, addr, size, dir);
+}
+
+static inline void
+fc_dma_sync_single_for_device(struct device *dev, dma_addr_t addr, size_t size,
+		enum dma_data_direction dir)
+{
+	if (dev)
+		dma_sync_single_for_device(dev, addr, size, dir);
+}
+
+/* pseudo dma_map_sg call */
+static int
+fc_map_sg(struct scatterlist *sg, int nents)
+{
+	struct scatterlist *s;
+	int i;
+
+	WARN_ON(nents == 0 || sg[0].length == 0);
+
+	for_each_sg(sg, s, nents, i) {
+		s->dma_address = 0L;
+#ifdef CONFIG_NEED_SG_DMA_LENGTH
+		s->dma_length = s->length;
+#endif
+	}
+	return nents;
+}
+
+static inline int
+fc_dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
+		enum dma_data_direction dir)
+{
+	return dev ? dma_map_sg(dev, sg, nents, dir) : fc_map_sg(sg, nents);
+}
+
+static inline void
+fc_dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,
+		enum dma_data_direction dir)
+{
+	if (dev)
+		dma_unmap_sg(dev, sg, nents, dir);
+}
+
+/* *********************** FC-NVME LS Handling **************************** */
+
+static void nvme_fc_ctrl_put(struct nvme_fc_ctrl *);
+static int nvme_fc_ctrl_get(struct nvme_fc_ctrl *);
+
+
+static void
+__nvme_fc_finish_ls_req(struct nvmefc_ls_req_op *lsop)
+{
+	struct nvme_fc_rport *rport = lsop->rport;
+	struct nvmefc_ls_req *lsreq = &lsop->ls_req;
+	unsigned long flags;
+
+	spin_lock_irqsave(&rport->lock, flags);
+
+	if (!lsop->req_queued) {
+		spin_unlock_irqrestore(&rport->lock, flags);
+		return;
+	}
+
+	list_del(&lsop->lsreq_list);
+
+	lsop->req_queued = false;
+
+	spin_unlock_irqrestore(&rport->lock, flags);
+
+	fc_dma_unmap_single(rport->dev, lsreq->rqstdma,
+				  (lsreq->rqstlen + lsreq->rsplen),
+				  DMA_BIDIRECTIONAL);
+
+	nvme_fc_rport_put(rport);
+}
+
+static int
+__nvme_fc_send_ls_req(struct nvme_fc_rport *rport,
+		struct nvmefc_ls_req_op *lsop,
+		void (*done)(struct nvmefc_ls_req *req, int status))
+{
+	struct nvmefc_ls_req *lsreq = &lsop->ls_req;
+	unsigned long flags;
+	int ret = 0;
+
+	if (rport->remoteport.port_state != FC_OBJSTATE_ONLINE)
+		return -ECONNREFUSED;
+
+	if (!nvme_fc_rport_get(rport))
+		return -ESHUTDOWN;
+
+	lsreq->done = done;
+	lsop->rport = rport;
+	lsop->req_queued = false;
+	INIT_LIST_HEAD(&lsop->lsreq_list);
+	init_completion(&lsop->ls_done);
+
+	lsreq->rqstdma = fc_dma_map_single(rport->dev, lsreq->rqstaddr,
+				  lsreq->rqstlen + lsreq->rsplen,
+				  DMA_BIDIRECTIONAL);
+	if (fc_dma_mapping_error(rport->dev, lsreq->rqstdma)) {
+		ret = -EFAULT;
+		goto out_putrport;
+	}
+	lsreq->rspdma = lsreq->rqstdma + lsreq->rqstlen;
+
+	spin_lock_irqsave(&rport->lock, flags);
+
+	list_add_tail(&lsop->lsreq_list, &rport->ls_req_list);
+
+	lsop->req_queued = true;
+
+	spin_unlock_irqrestore(&rport->lock, flags);
+
+	ret = rport->lport->ops->ls_req(&rport->lport->localport,
+					&rport->remoteport, lsreq);
+	if (ret)
+		goto out_unlink;
+
+	return 0;
+
+out_unlink:
+	lsop->ls_error = ret;
+	spin_lock_irqsave(&rport->lock, flags);
+	lsop->req_queued = false;
+	list_del(&lsop->lsreq_list);
+	spin_unlock_irqrestore(&rport->lock, flags);
+	fc_dma_unmap_single(rport->dev, lsreq->rqstdma,
+				  (lsreq->rqstlen + lsreq->rsplen),
+				  DMA_BIDIRECTIONAL);
+out_putrport:
+	nvme_fc_rport_put(rport);
+
+	return ret;
+}
+
+static void
+nvme_fc_send_ls_req_done(struct nvmefc_ls_req *lsreq, int status)
+{
+	struct nvmefc_ls_req_op *lsop = ls_req_to_lsop(lsreq);
+
+	lsop->ls_error = status;
+	complete(&lsop->ls_done);
+}
+
+static int
+nvme_fc_send_ls_req(struct nvme_fc_rport *rport, struct nvmefc_ls_req_op *lsop)
+{
+	struct nvmefc_ls_req *lsreq = &lsop->ls_req;
+	struct fcnvme_ls_rjt *rjt = lsreq->rspaddr;
+	int ret;
+
+	ret = __nvme_fc_send_ls_req(rport, lsop, nvme_fc_send_ls_req_done);
+
+	if (!ret) {
+		/*
+		 * No timeout/not interruptible as we need the struct
+		 * to exist until the lldd calls us back. Thus mandate
+		 * wait until driver calls back. lldd responsible for
+		 * the timeout action
+		 */
+		wait_for_completion(&lsop->ls_done);
+
+		__nvme_fc_finish_ls_req(lsop);
+
+		ret = lsop->ls_error;
+	}
+
+	if (ret)
+		return ret;
+
+	/* ACC or RJT payload ? */
+	if (rjt->w0.ls_cmd == FCNVME_LS_RJT)
+		return -ENXIO;
+
+	return 0;
+}
+
+static int
+nvme_fc_send_ls_req_async(struct nvme_fc_rport *rport,
+		struct nvmefc_ls_req_op *lsop,
+		void (*done)(struct nvmefc_ls_req *req, int status))
+{
+	/* don't wait for completion */
+
+	return __nvme_fc_send_ls_req(rport, lsop, done);
+}
+
+/* Validation Error indexes into the string table below */
+enum {
+	VERR_NO_ERROR		= 0,
+	VERR_LSACC		= 1,
+	VERR_LSDESC_RQST	= 2,
+	VERR_LSDESC_RQST_LEN	= 3,
+	VERR_ASSOC_ID		= 4,
+	VERR_ASSOC_ID_LEN	= 5,
+	VERR_CONN_ID		= 6,
+	VERR_CONN_ID_LEN	= 7,
+	VERR_CR_ASSOC		= 8,
+	VERR_CR_ASSOC_ACC_LEN	= 9,
+	VERR_CR_CONN		= 10,
+	VERR_CR_CONN_ACC_LEN	= 11,
+	VERR_DISCONN		= 12,
+	VERR_DISCONN_ACC_LEN	= 13,
+};
+
+static char *validation_errors[] = {
+	"OK",
+	"Not LS_ACC",
+	"Not LSDESC_RQST",
+	"Bad LSDESC_RQST Length",
+	"Not Association ID",
+	"Bad Association ID Length",
+	"Not Connection ID",
+	"Bad Connection ID Length",
+	"Not CR_ASSOC Rqst",
+	"Bad CR_ASSOC ACC Length",
+	"Not CR_CONN Rqst",
+	"Bad CR_CONN ACC Length",
+	"Not Disconnect Rqst",
+	"Bad Disconnect ACC Length",
+};
+
+static int
+nvme_fc_connect_admin_queue(struct nvme_fc_ctrl *ctrl,
+	struct nvme_fc_queue *queue, u16 qsize, u16 ersp_ratio)
+{
+	struct nvmefc_ls_req_op *lsop;
+	struct nvmefc_ls_req *lsreq;
+	struct fcnvme_ls_cr_assoc_rqst *assoc_rqst;
+	struct fcnvme_ls_cr_assoc_acc *assoc_acc;
+	int ret, fcret = 0;
+
+	lsop = kzalloc((sizeof(*lsop) +
+			 ctrl->lport->ops->lsrqst_priv_sz +
+			 sizeof(*assoc_rqst) + sizeof(*assoc_acc)), GFP_KERNEL);
+	if (!lsop) {
+		ret = -ENOMEM;
+		goto out_no_memory;
+	}
+	lsreq = &lsop->ls_req;
+
+	lsreq->private = (void *)&lsop[1];
+	assoc_rqst = (struct fcnvme_ls_cr_assoc_rqst *)
+			(lsreq->private + ctrl->lport->ops->lsrqst_priv_sz);
+	assoc_acc = (struct fcnvme_ls_cr_assoc_acc *)&assoc_rqst[1];
+
+	assoc_rqst->w0.ls_cmd = FCNVME_LS_CREATE_ASSOCIATION;
+	assoc_rqst->desc_list_len =
+			cpu_to_be32(sizeof(struct fcnvme_lsdesc_cr_assoc_cmd));
+
+	assoc_rqst->assoc_cmd.desc_tag =
+			cpu_to_be32(FCNVME_LSDESC_CREATE_ASSOC_CMD);
+	assoc_rqst->assoc_cmd.desc_len =
+			fcnvme_lsdesc_len(
+				sizeof(struct fcnvme_lsdesc_cr_assoc_cmd));
+
+	assoc_rqst->assoc_cmd.ersp_ratio = cpu_to_be16(ersp_ratio);
+	assoc_rqst->assoc_cmd.sqsize = cpu_to_be16(qsize - 1);
+	/* Linux supports only Dynamic controllers */
+	assoc_rqst->assoc_cmd.cntlid = cpu_to_be16(0xffff);
+	uuid_copy(&assoc_rqst->assoc_cmd.hostid, &ctrl->ctrl.opts->host->id);
+	strncpy(assoc_rqst->assoc_cmd.hostnqn, ctrl->ctrl.opts->host->nqn,
+		min(FCNVME_ASSOC_HOSTNQN_LEN, NVMF_NQN_SIZE));
+	strncpy(assoc_rqst->assoc_cmd.subnqn, ctrl->ctrl.opts->subsysnqn,
+		min(FCNVME_ASSOC_SUBNQN_LEN, NVMF_NQN_SIZE));
+
+	lsop->queue = queue;
+	lsreq->rqstaddr = assoc_rqst;
+	lsreq->rqstlen = sizeof(*assoc_rqst);
+	lsreq->rspaddr = assoc_acc;
+	lsreq->rsplen = sizeof(*assoc_acc);
+	lsreq->timeout = NVME_FC_CONNECT_TIMEOUT_SEC;
+
+	ret = nvme_fc_send_ls_req(ctrl->rport, lsop);
+	if (ret)
+		goto out_free_buffer;
+
+	/* process connect LS completion */
+
+	/* validate the ACC response */
+	if (assoc_acc->hdr.w0.ls_cmd != FCNVME_LS_ACC)
+		fcret = VERR_LSACC;
+	else if (assoc_acc->hdr.desc_list_len !=
+			fcnvme_lsdesc_len(
+				sizeof(struct fcnvme_ls_cr_assoc_acc)))
+		fcret = VERR_CR_ASSOC_ACC_LEN;
+	else if (assoc_acc->hdr.rqst.desc_tag !=
+			cpu_to_be32(FCNVME_LSDESC_RQST))
+		fcret = VERR_LSDESC_RQST;
+	else if (assoc_acc->hdr.rqst.desc_len !=
+			fcnvme_lsdesc_len(sizeof(struct fcnvme_lsdesc_rqst)))
+		fcret = VERR_LSDESC_RQST_LEN;
+	else if (assoc_acc->hdr.rqst.w0.ls_cmd != FCNVME_LS_CREATE_ASSOCIATION)
+		fcret = VERR_CR_ASSOC;
+	else if (assoc_acc->associd.desc_tag !=
+			cpu_to_be32(FCNVME_LSDESC_ASSOC_ID))
+		fcret = VERR_ASSOC_ID;
+	else if (assoc_acc->associd.desc_len !=
+			fcnvme_lsdesc_len(
+				sizeof(struct fcnvme_lsdesc_assoc_id)))
+		fcret = VERR_ASSOC_ID_LEN;
+	else if (assoc_acc->connectid.desc_tag !=
+			cpu_to_be32(FCNVME_LSDESC_CONN_ID))
+		fcret = VERR_CONN_ID;
+	else if (assoc_acc->connectid.desc_len !=
+			fcnvme_lsdesc_len(sizeof(struct fcnvme_lsdesc_conn_id)))
+		fcret = VERR_CONN_ID_LEN;
+
+	if (fcret) {
+		ret = -EBADF;
+		dev_err(ctrl->dev,
+			"q %d connect failed: %s\n",
+			queue->qnum, validation_errors[fcret]);
+	} else {
+		ctrl->association_id =
+			be64_to_cpu(assoc_acc->associd.association_id);
+		queue->connection_id =
+			be64_to_cpu(assoc_acc->connectid.connection_id);
+		set_bit(NVME_FC_Q_CONNECTED, &queue->flags);
+	}
+
+out_free_buffer:
+	kfree(lsop);
+out_no_memory:
+	if (ret)
+		dev_err(ctrl->dev,
+			"queue %d connect admin queue failed (%d).\n",
+			queue->qnum, ret);
+	return ret;
+}
+
+static int
+nvme_fc_connect_queue(struct nvme_fc_ctrl *ctrl, struct nvme_fc_queue *queue,
+			u16 qsize, u16 ersp_ratio)
+{
+	struct nvmefc_ls_req_op *lsop;
+	struct nvmefc_ls_req *lsreq;
+	struct fcnvme_ls_cr_conn_rqst *conn_rqst;
+	struct fcnvme_ls_cr_conn_acc *conn_acc;
+	int ret, fcret = 0;
+
+	lsop = kzalloc((sizeof(*lsop) +
+			 ctrl->lport->ops->lsrqst_priv_sz +
+			 sizeof(*conn_rqst) + sizeof(*conn_acc)), GFP_KERNEL);
+	if (!lsop) {
+		ret = -ENOMEM;
+		goto out_no_memory;
+	}
+	lsreq = &lsop->ls_req;
+
+	lsreq->private = (void *)&lsop[1];
+	conn_rqst = (struct fcnvme_ls_cr_conn_rqst *)
+			(lsreq->private + ctrl->lport->ops->lsrqst_priv_sz);
+	conn_acc = (struct fcnvme_ls_cr_conn_acc *)&conn_rqst[1];
+
+	conn_rqst->w0.ls_cmd = FCNVME_LS_CREATE_CONNECTION;
+	conn_rqst->desc_list_len = cpu_to_be32(
+				sizeof(struct fcnvme_lsdesc_assoc_id) +
+				sizeof(struct fcnvme_lsdesc_cr_conn_cmd));
+
+	conn_rqst->associd.desc_tag = cpu_to_be32(FCNVME_LSDESC_ASSOC_ID);
+	conn_rqst->associd.desc_len =
+			fcnvme_lsdesc_len(
+				sizeof(struct fcnvme_lsdesc_assoc_id));
+	conn_rqst->associd.association_id = cpu_to_be64(ctrl->association_id);
+	conn_rqst->connect_cmd.desc_tag =
+			cpu_to_be32(FCNVME_LSDESC_CREATE_CONN_CMD);
+	conn_rqst->connect_cmd.desc_len =
+			fcnvme_lsdesc_len(
+				sizeof(struct fcnvme_lsdesc_cr_conn_cmd));
+	conn_rqst->connect_cmd.ersp_ratio = cpu_to_be16(ersp_ratio);
+	conn_rqst->connect_cmd.qid  = cpu_to_be16(queue->qnum);
+	conn_rqst->connect_cmd.sqsize = cpu_to_be16(qsize - 1);
+
+	lsop->queue = queue;
+	lsreq->rqstaddr = conn_rqst;
+	lsreq->rqstlen = sizeof(*conn_rqst);
+	lsreq->rspaddr = conn_acc;
+	lsreq->rsplen = sizeof(*conn_acc);
+	lsreq->timeout = NVME_FC_CONNECT_TIMEOUT_SEC;
+
+	ret = nvme_fc_send_ls_req(ctrl->rport, lsop);
+	if (ret)
+		goto out_free_buffer;
+
+	/* process connect LS completion */
+
+	/* validate the ACC response */
+	if (conn_acc->hdr.w0.ls_cmd != FCNVME_LS_ACC)
+		fcret = VERR_LSACC;
+	else if (conn_acc->hdr.desc_list_len !=
+			fcnvme_lsdesc_len(sizeof(struct fcnvme_ls_cr_conn_acc)))
+		fcret = VERR_CR_CONN_ACC_LEN;
+	else if (conn_acc->hdr.rqst.desc_tag != cpu_to_be32(FCNVME_LSDESC_RQST))
+		fcret = VERR_LSDESC_RQST;
+	else if (conn_acc->hdr.rqst.desc_len !=
+			fcnvme_lsdesc_len(sizeof(struct fcnvme_lsdesc_rqst)))
+		fcret = VERR_LSDESC_RQST_LEN;
+	else if (conn_acc->hdr.rqst.w0.ls_cmd != FCNVME_LS_CREATE_CONNECTION)
+		fcret = VERR_CR_CONN;
+	else if (conn_acc->connectid.desc_tag !=
+			cpu_to_be32(FCNVME_LSDESC_CONN_ID))
+		fcret = VERR_CONN_ID;
+	else if (conn_acc->connectid.desc_len !=
+			fcnvme_lsdesc_len(sizeof(struct fcnvme_lsdesc_conn_id)))
+		fcret = VERR_CONN_ID_LEN;
+
+	if (fcret) {
+		ret = -EBADF;
+		dev_err(ctrl->dev,
+			"q %d connect failed: %s\n",
+			queue->qnum, validation_errors[fcret]);
+	} else {
+		queue->connection_id =
+			be64_to_cpu(conn_acc->connectid.connection_id);
+		set_bit(NVME_FC_Q_CONNECTED, &queue->flags);
+	}
+
+out_free_buffer:
+	kfree(lsop);
+out_no_memory:
+	if (ret)
+		dev_err(ctrl->dev,
+			"queue %d connect command failed (%d).\n",
+			queue->qnum, ret);
+	return ret;
+}
+
+static void
+nvme_fc_disconnect_assoc_done(struct nvmefc_ls_req *lsreq, int status)
+{
+	struct nvmefc_ls_req_op *lsop = ls_req_to_lsop(lsreq);
+
+	__nvme_fc_finish_ls_req(lsop);
+
+	/* fc-nvme initiator doesn't care about success or failure of cmd */
+
+	kfree(lsop);
+}
+
+/*
+ * This routine sends a FC-NVME LS to disconnect (aka terminate)
+ * the FC-NVME Association.  Terminating the association also
+ * terminates the FC-NVME connections (per queue, both admin and io
+ * queues) that are part of the association. E.g. things are torn
+ * down, and the related FC-NVME Association ID and Connection IDs
+ * become invalid.
+ *
+ * The behavior of the fc-nvme initiator is such that it's
+ * understanding of the association and connections will implicitly
+ * be torn down. The action is implicit as it may be due to a loss of
+ * connectivity with the fc-nvme target, so you may never get a
+ * response even if you tried.  As such, the action of this routine
+ * is to asynchronously send the LS, ignore any results of the LS, and
+ * continue on with terminating the association. If the fc-nvme target
+ * is present and receives the LS, it too can tear down.
+ */
+static void
+nvme_fc_xmt_disconnect_assoc(struct nvme_fc_ctrl *ctrl)
+{
+	struct fcnvme_ls_disconnect_rqst *discon_rqst;
+	struct fcnvme_ls_disconnect_acc *discon_acc;
+	struct nvmefc_ls_req_op *lsop;
+	struct nvmefc_ls_req *lsreq;
+	int ret;
+
+	lsop = kzalloc((sizeof(*lsop) +
+			 ctrl->lport->ops->lsrqst_priv_sz +
+			 sizeof(*discon_rqst) + sizeof(*discon_acc)),
+			GFP_KERNEL);
+	if (!lsop)
+		/* couldn't sent it... too bad */
+		return;
+
+	lsreq = &lsop->ls_req;
+
+	lsreq->private = (void *)&lsop[1];
+	discon_rqst = (struct fcnvme_ls_disconnect_rqst *)
+			(lsreq->private + ctrl->lport->ops->lsrqst_priv_sz);
+	discon_acc = (struct fcnvme_ls_disconnect_acc *)&discon_rqst[1];
+
+	discon_rqst->w0.ls_cmd = FCNVME_LS_DISCONNECT;
+	discon_rqst->desc_list_len = cpu_to_be32(
+				sizeof(struct fcnvme_lsdesc_assoc_id) +
+				sizeof(struct fcnvme_lsdesc_disconn_cmd));
+
+	discon_rqst->associd.desc_tag = cpu_to_be32(FCNVME_LSDESC_ASSOC_ID);
+	discon_rqst->associd.desc_len =
+			fcnvme_lsdesc_len(
+				sizeof(struct fcnvme_lsdesc_assoc_id));
+
+	discon_rqst->associd.association_id = cpu_to_be64(ctrl->association_id);
+
+	discon_rqst->discon_cmd.desc_tag = cpu_to_be32(
+						FCNVME_LSDESC_DISCONN_CMD);
+	discon_rqst->discon_cmd.desc_len =
+			fcnvme_lsdesc_len(
+				sizeof(struct fcnvme_lsdesc_disconn_cmd));
+	discon_rqst->discon_cmd.scope = FCNVME_DISCONN_ASSOCIATION;
+	discon_rqst->discon_cmd.id = cpu_to_be64(ctrl->association_id);
+
+	lsreq->rqstaddr = discon_rqst;
+	lsreq->rqstlen = sizeof(*discon_rqst);
+	lsreq->rspaddr = discon_acc;
+	lsreq->rsplen = sizeof(*discon_acc);
+	lsreq->timeout = NVME_FC_CONNECT_TIMEOUT_SEC;
+
+	ret = nvme_fc_send_ls_req_async(ctrl->rport, lsop,
+				nvme_fc_disconnect_assoc_done);
+	if (ret)
+		kfree(lsop);
+
+	/* only meaningful part to terminating the association */
+	ctrl->association_id = 0;
+}
+
+
+/* *********************** NVME Ctrl Routines **************************** */
+
+static void nvme_fc_error_recovery(struct nvme_fc_ctrl *ctrl, char *errmsg);
+
+static void
+__nvme_fc_exit_request(struct nvme_fc_ctrl *ctrl,
+		struct nvme_fc_fcp_op *op)
+{
+	fc_dma_unmap_single(ctrl->lport->dev, op->fcp_req.rspdma,
+				sizeof(op->rsp_iu), DMA_FROM_DEVICE);
+	fc_dma_unmap_single(ctrl->lport->dev, op->fcp_req.cmddma,
+				sizeof(op->cmd_iu), DMA_TO_DEVICE);
+
+	atomic_set(&op->state, FCPOP_STATE_UNINIT);
+}
+
+static void
+nvme_fc_exit_request(struct blk_mq_tag_set *set, struct request *rq,
+		unsigned int hctx_idx)
+{
+	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
+
+	return __nvme_fc_exit_request(set->driver_data, op);
+}
+
+static int
+__nvme_fc_abort_op(struct nvme_fc_ctrl *ctrl, struct nvme_fc_fcp_op *op)
+{
+	unsigned long flags;
+	int opstate;
+
+	spin_lock_irqsave(&ctrl->lock, flags);
+	opstate = atomic_xchg(&op->state, FCPOP_STATE_ABORTED);
+	if (opstate != FCPOP_STATE_ACTIVE)
+		atomic_set(&op->state, opstate);
+	else if (ctrl->flags & FCCTRL_TERMIO)
+		ctrl->iocnt++;
+	spin_unlock_irqrestore(&ctrl->lock, flags);
+
+	if (opstate != FCPOP_STATE_ACTIVE)
+		return -ECANCELED;
+
+	ctrl->lport->ops->fcp_abort(&ctrl->lport->localport,
+					&ctrl->rport->remoteport,
+					op->queue->lldd_handle,
+					&op->fcp_req);
+
+	return 0;
+}
+
+static void
+nvme_fc_abort_aen_ops(struct nvme_fc_ctrl *ctrl)
+{
+	struct nvme_fc_fcp_op *aen_op = ctrl->aen_ops;
+	int i;
+
+	/* ensure we've initialized the ops once */
+	if (!(aen_op->flags & FCOP_FLAGS_AEN))
+		return;
+
+	for (i = 0; i < NVME_NR_AEN_COMMANDS; i++, aen_op++)
+		__nvme_fc_abort_op(ctrl, aen_op);
+}
+
+static inline void
+__nvme_fc_fcpop_chk_teardowns(struct nvme_fc_ctrl *ctrl,
+		struct nvme_fc_fcp_op *op, int opstate)
+{
+	unsigned long flags;
+
+	if (opstate == FCPOP_STATE_ABORTED) {
+		spin_lock_irqsave(&ctrl->lock, flags);
+		if (ctrl->flags & FCCTRL_TERMIO) {
+			if (!--ctrl->iocnt)
+				wake_up(&ctrl->ioabort_wait);
+		}
+		spin_unlock_irqrestore(&ctrl->lock, flags);
+	}
+}
+
+static void
+nvme_fc_fcpio_done(struct nvmefc_fcp_req *req)
+{
+	struct nvme_fc_fcp_op *op = fcp_req_to_fcp_op(req);
+	struct request *rq = op->rq;
+	struct nvmefc_fcp_req *freq = &op->fcp_req;
+	struct nvme_fc_ctrl *ctrl = op->ctrl;
+	struct nvme_fc_queue *queue = op->queue;
+	struct nvme_completion *cqe = &op->rsp_iu.cqe;
+	struct nvme_command *sqe = &op->cmd_iu.sqe;
+	__le16 status = cpu_to_le16(NVME_SC_SUCCESS << 1);
+	union nvme_result result;
+	bool terminate_assoc = true;
+	int opstate;
+
+	/*
+	 * WARNING:
+	 * The current linux implementation of a nvme controller
+	 * allocates a single tag set for all io queues and sizes
+	 * the io queues to fully hold all possible tags. Thus, the
+	 * implementation does not reference or care about the sqhd
+	 * value as it never needs to use the sqhd/sqtail pointers
+	 * for submission pacing.
+	 *
+	 * This affects the FC-NVME implementation in two ways:
+	 * 1) As the value doesn't matter, we don't need to waste
+	 *    cycles extracting it from ERSPs and stamping it in the
+	 *    cases where the transport fabricates CQEs on successful
+	 *    completions.
+	 * 2) The FC-NVME implementation requires that delivery of
+	 *    ERSP completions are to go back to the nvme layer in order
+	 *    relative to the rsn, such that the sqhd value will always
+	 *    be "in order" for the nvme layer. As the nvme layer in
+	 *    linux doesn't care about sqhd, there's no need to return
+	 *    them in order.
+	 *
+	 * Additionally:
+	 * As the core nvme layer in linux currently does not look at
+	 * every field in the cqe - in cases where the FC transport must
+	 * fabricate a CQE, the following fields will not be set as they
+	 * are not referenced:
+	 *      cqe.sqid,  cqe.sqhd,  cqe.command_id
+	 *
+	 * Failure or error of an individual i/o, in a transport
+	 * detected fashion unrelated to the nvme completion status,
+	 * potentially cause the initiator and target sides to get out
+	 * of sync on SQ head/tail (aka outstanding io count allowed).
+	 * Per FC-NVME spec, failure of an individual command requires
+	 * the connection to be terminated, which in turn requires the
+	 * association to be terminated.
+	 */
+
+	opstate = atomic_xchg(&op->state, FCPOP_STATE_COMPLETE);
+
+	fc_dma_sync_single_for_cpu(ctrl->lport->dev, op->fcp_req.rspdma,
+				sizeof(op->rsp_iu), DMA_FROM_DEVICE);
+
+	if (opstate == FCPOP_STATE_ABORTED)
+		status = cpu_to_le16(NVME_SC_ABORT_REQ << 1);
+	else if (freq->status)
+		status = cpu_to_le16(NVME_SC_INTERNAL << 1);
+
+	/*
+	 * For the linux implementation, if we have an unsuccesful
+	 * status, they blk-mq layer can typically be called with the
+	 * non-zero status and the content of the cqe isn't important.
+	 */
+	if (status)
+		goto done;
+
+	/*
+	 * command completed successfully relative to the wire
+	 * protocol. However, validate anything received and
+	 * extract the status and result from the cqe (create it
+	 * where necessary).
+	 */
+
+	switch (freq->rcv_rsplen) {
+
+	case 0:
+	case NVME_FC_SIZEOF_ZEROS_RSP:
+		/*
+		 * No response payload or 12 bytes of payload (which
+		 * should all be zeros) are considered successful and
+		 * no payload in the CQE by the transport.
+		 */
+		if (freq->transferred_length !=
+			be32_to_cpu(op->cmd_iu.data_len)) {
+			status = cpu_to_le16(NVME_SC_INTERNAL << 1);
+			goto done;
+		}
+		result.u64 = 0;
+		break;
+
+	case sizeof(struct nvme_fc_ersp_iu):
+		/*
+		 * The ERSP IU contains a full completion with CQE.
+		 * Validate ERSP IU and look at cqe.
+		 */
+		if (unlikely(be16_to_cpu(op->rsp_iu.iu_len) !=
+					(freq->rcv_rsplen / 4) ||
+			     be32_to_cpu(op->rsp_iu.xfrd_len) !=
+					freq->transferred_length ||
+			     op->rsp_iu.status_code ||
+			     sqe->common.command_id != cqe->command_id)) {
+			status = cpu_to_le16(NVME_SC_INTERNAL << 1);
+			goto done;
+		}
+		result = cqe->result;
+		status = cqe->status;
+		break;
+
+	default:
+		status = cpu_to_le16(NVME_SC_INTERNAL << 1);
+		goto done;
+	}
+
+	terminate_assoc = false;
+
+done:
+	if (op->flags & FCOP_FLAGS_AEN) {
+		nvme_complete_async_event(&queue->ctrl->ctrl, status, &result);
+		__nvme_fc_fcpop_chk_teardowns(ctrl, op, opstate);
+		atomic_set(&op->state, FCPOP_STATE_IDLE);
+		op->flags = FCOP_FLAGS_AEN;	/* clear other flags */
+		nvme_fc_ctrl_put(ctrl);
+		goto check_error;
+	}
+
+	__nvme_fc_fcpop_chk_teardowns(ctrl, op, opstate);
+	nvme_end_request(rq, status, result);
+
+check_error:
+	if (terminate_assoc)
+		nvme_fc_error_recovery(ctrl, "transport detected io error");
+}
+
+static int
+__nvme_fc_init_request(struct nvme_fc_ctrl *ctrl,
+		struct nvme_fc_queue *queue, struct nvme_fc_fcp_op *op,
+		struct request *rq, u32 rqno)
+{
+	struct nvme_fcp_op_w_sgl *op_w_sgl =
+		container_of(op, typeof(*op_w_sgl), op);
+	struct nvme_fc_cmd_iu *cmdiu = &op->cmd_iu;
+	int ret = 0;
+
+	memset(op, 0, sizeof(*op));
+	op->fcp_req.cmdaddr = &op->cmd_iu;
+	op->fcp_req.cmdlen = sizeof(op->cmd_iu);
+	op->fcp_req.rspaddr = &op->rsp_iu;
+	op->fcp_req.rsplen = sizeof(op->rsp_iu);
+	op->fcp_req.done = nvme_fc_fcpio_done;
+	op->ctrl = ctrl;
+	op->queue = queue;
+	op->rq = rq;
+	op->rqno = rqno;
+
+	cmdiu->scsi_id = NVME_CMD_SCSI_ID;
+	cmdiu->fc_id = NVME_CMD_FC_ID;
+	cmdiu->iu_len = cpu_to_be16(sizeof(*cmdiu) / sizeof(u32));
+
+	op->fcp_req.cmddma = fc_dma_map_single(ctrl->lport->dev,
+				&op->cmd_iu, sizeof(op->cmd_iu), DMA_TO_DEVICE);
+	if (fc_dma_mapping_error(ctrl->lport->dev, op->fcp_req.cmddma)) {
+		dev_err(ctrl->dev,
+			"FCP Op failed - cmdiu dma mapping failed.\n");
+		ret = EFAULT;
+		goto out_on_error;
+	}
+
+	op->fcp_req.rspdma = fc_dma_map_single(ctrl->lport->dev,
+				&op->rsp_iu, sizeof(op->rsp_iu),
+				DMA_FROM_DEVICE);
+	if (fc_dma_mapping_error(ctrl->lport->dev, op->fcp_req.rspdma)) {
+		dev_err(ctrl->dev,
+			"FCP Op failed - rspiu dma mapping failed.\n");
+		ret = EFAULT;
+	}
+
+	atomic_set(&op->state, FCPOP_STATE_IDLE);
+out_on_error:
+	return ret;
+}
+
+static int
+nvme_fc_init_request(struct blk_mq_tag_set *set, struct request *rq,
+		unsigned int hctx_idx, unsigned int numa_node)
+{
+	struct nvme_fc_ctrl *ctrl = set->driver_data;
+	struct nvme_fcp_op_w_sgl *op = blk_mq_rq_to_pdu(rq);
+	int queue_idx = (set == &ctrl->tag_set) ? hctx_idx + 1 : 0;
+	struct nvme_fc_queue *queue = &ctrl->queues[queue_idx];
+	int res;
+
+	res = __nvme_fc_init_request(ctrl, queue, &op->op, rq, queue->rqcnt++);
+	if (res)
+		return res;
+	op->op.fcp_req.first_sgl = &op->sgl[0];
+	op->op.fcp_req.private = &op->priv[0];
+	nvme_req(rq)->ctrl = &ctrl->ctrl;
+	return res;
+}
+
+static int
+nvme_fc_init_aen_ops(struct nvme_fc_ctrl *ctrl)
+{
+	struct nvme_fc_fcp_op *aen_op;
+	struct nvme_fc_cmd_iu *cmdiu;
+	struct nvme_command *sqe;
+	void *private;
+	int i, ret;
+
+	aen_op = ctrl->aen_ops;
+	for (i = 0; i < NVME_NR_AEN_COMMANDS; i++, aen_op++) {
+		private = kzalloc(ctrl->lport->ops->fcprqst_priv_sz,
+						GFP_KERNEL);
+		if (!private)
+			return -ENOMEM;
+
+		cmdiu = &aen_op->cmd_iu;
+		sqe = &cmdiu->sqe;
+		ret = __nvme_fc_init_request(ctrl, &ctrl->queues[0],
+				aen_op, (struct request *)NULL,
+				(NVME_AQ_BLK_MQ_DEPTH + i));
+		if (ret) {
+			kfree(private);
+			return ret;
+		}
+
+		aen_op->flags = FCOP_FLAGS_AEN;
+		aen_op->fcp_req.private = private;
+
+		memset(sqe, 0, sizeof(*sqe));
+		sqe->common.opcode = nvme_admin_async_event;
+		/* Note: core layer may overwrite the sqe.command_id value */
+		sqe->common.command_id = NVME_AQ_BLK_MQ_DEPTH + i;
+	}
+	return 0;
+}
+
+static void
+nvme_fc_term_aen_ops(struct nvme_fc_ctrl *ctrl)
+{
+	struct nvme_fc_fcp_op *aen_op;
+	int i;
+
+	aen_op = ctrl->aen_ops;
+	for (i = 0; i < NVME_NR_AEN_COMMANDS; i++, aen_op++) {
+		if (!aen_op->fcp_req.private)
+			continue;
+
+		__nvme_fc_exit_request(ctrl, aen_op);
+
+		kfree(aen_op->fcp_req.private);
+		aen_op->fcp_req.private = NULL;
+	}
+}
+
+static inline void
+__nvme_fc_init_hctx(struct blk_mq_hw_ctx *hctx, struct nvme_fc_ctrl *ctrl,
+		unsigned int qidx)
+{
+	struct nvme_fc_queue *queue = &ctrl->queues[qidx];
+
+	hctx->driver_data = queue;
+	queue->hctx = hctx;
+}
+
+static int
+nvme_fc_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
+		unsigned int hctx_idx)
+{
+	struct nvme_fc_ctrl *ctrl = data;
+
+	__nvme_fc_init_hctx(hctx, ctrl, hctx_idx + 1);
+
+	return 0;
+}
+
+static int
+nvme_fc_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
+		unsigned int hctx_idx)
+{
+	struct nvme_fc_ctrl *ctrl = data;
+
+	__nvme_fc_init_hctx(hctx, ctrl, hctx_idx);
+
+	return 0;
+}
+
+static void
+nvme_fc_init_queue(struct nvme_fc_ctrl *ctrl, int idx)
+{
+	struct nvme_fc_queue *queue;
+
+	queue = &ctrl->queues[idx];
+	memset(queue, 0, sizeof(*queue));
+	queue->ctrl = ctrl;
+	queue->qnum = idx;
+	atomic_set(&queue->csn, 0);
+	queue->dev = ctrl->dev;
+
+	if (idx > 0)
+		queue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;
+	else
+		queue->cmnd_capsule_len = sizeof(struct nvme_command);
+
+	/*
+	 * Considered whether we should allocate buffers for all SQEs
+	 * and CQEs and dma map them - mapping their respective entries
+	 * into the request structures (kernel vm addr and dma address)
+	 * thus the driver could use the buffers/mappings directly.
+	 * It only makes sense if the LLDD would use them for its
+	 * messaging api. It's very unlikely most adapter api's would use
+	 * a native NVME sqe/cqe. More reasonable if FC-NVME IU payload
+	 * structures were used instead.
+	 */
+}
+
+/*
+ * This routine terminates a queue at the transport level.
+ * The transport has already ensured that all outstanding ios on
+ * the queue have been terminated.
+ * The transport will send a Disconnect LS request to terminate
+ * the queue's connection. Termination of the admin queue will also
+ * terminate the association at the target.
+ */
+static void
+nvme_fc_free_queue(struct nvme_fc_queue *queue)
+{
+	if (!test_and_clear_bit(NVME_FC_Q_CONNECTED, &queue->flags))
+		return;
+
+	clear_bit(NVME_FC_Q_LIVE, &queue->flags);
+	/*
+	 * Current implementation never disconnects a single queue.
+	 * It always terminates a whole association. So there is never
+	 * a disconnect(queue) LS sent to the target.
+	 */
+
+	queue->connection_id = 0;
+	atomic_set(&queue->csn, 0);
+}
+
+static void
+__nvme_fc_delete_hw_queue(struct nvme_fc_ctrl *ctrl,
+	struct nvme_fc_queue *queue, unsigned int qidx)
+{
+	if (ctrl->lport->ops->delete_queue)
+		ctrl->lport->ops->delete_queue(&ctrl->lport->localport, qidx,
+				queue->lldd_handle);
+	queue->lldd_handle = NULL;
+}
+
+static void
+nvme_fc_free_io_queues(struct nvme_fc_ctrl *ctrl)
+{
+	int i;
+
+	for (i = 1; i < ctrl->ctrl.queue_count; i++)
+		nvme_fc_free_queue(&ctrl->queues[i]);
+}
+
+static int
+__nvme_fc_create_hw_queue(struct nvme_fc_ctrl *ctrl,
+	struct nvme_fc_queue *queue, unsigned int qidx, u16 qsize)
+{
+	int ret = 0;
+
+	queue->lldd_handle = NULL;
+	if (ctrl->lport->ops->create_queue)
+		ret = ctrl->lport->ops->create_queue(&ctrl->lport->localport,
+				qidx, qsize, &queue->lldd_handle);
+
+	return ret;
+}
+
+static void
+nvme_fc_delete_hw_io_queues(struct nvme_fc_ctrl *ctrl)
+{
+	struct nvme_fc_queue *queue = &ctrl->queues[ctrl->ctrl.queue_count - 1];
+	int i;
+
+	for (i = ctrl->ctrl.queue_count - 1; i >= 1; i--, queue--)
+		__nvme_fc_delete_hw_queue(ctrl, queue, i);
+}
+
+static int
+nvme_fc_create_hw_io_queues(struct nvme_fc_ctrl *ctrl, u16 qsize)
+{
+	struct nvme_fc_queue *queue = &ctrl->queues[1];
+	int i, ret;
+
+	for (i = 1; i < ctrl->ctrl.queue_count; i++, queue++) {
+		ret = __nvme_fc_create_hw_queue(ctrl, queue, i, qsize);
+		if (ret)
+			goto delete_queues;
+	}
+
+	return 0;
+
+delete_queues:
+	for (; i >= 0; i--)
+		__nvme_fc_delete_hw_queue(ctrl, &ctrl->queues[i], i);
+	return ret;
+}
+
+static int
+nvme_fc_connect_io_queues(struct nvme_fc_ctrl *ctrl, u16 qsize)
+{
+	int i, ret = 0;
+
+	for (i = 1; i < ctrl->ctrl.queue_count; i++) {
+		ret = nvme_fc_connect_queue(ctrl, &ctrl->queues[i], qsize,
+					(qsize / 5));
+		if (ret)
+			break;
+		ret = nvmf_connect_io_queue(&ctrl->ctrl, i, false);
+		if (ret)
+			break;
+
+		set_bit(NVME_FC_Q_LIVE, &ctrl->queues[i].flags);
+	}
+
+	return ret;
+}
+
+static void
+nvme_fc_init_io_queues(struct nvme_fc_ctrl *ctrl)
+{
+	int i;
+
+	for (i = 1; i < ctrl->ctrl.queue_count; i++)
+		nvme_fc_init_queue(ctrl, i);
+}
+
+static void
+nvme_fc_ctrl_free(struct kref *ref)
+{
+	struct nvme_fc_ctrl *ctrl =
+		container_of(ref, struct nvme_fc_ctrl, ref);
+	unsigned long flags;
+
+	if (ctrl->ctrl.tagset) {
+		blk_cleanup_queue(ctrl->ctrl.connect_q);
+		blk_mq_free_tag_set(&ctrl->tag_set);
+	}
+
+	/* remove from rport list */
+	spin_lock_irqsave(&ctrl->rport->lock, flags);
+	list_del(&ctrl->ctrl_list);
+	spin_unlock_irqrestore(&ctrl->rport->lock, flags);
+
+	blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+	blk_cleanup_queue(ctrl->ctrl.admin_q);
+	blk_mq_free_tag_set(&ctrl->admin_tag_set);
+
+	kfree(ctrl->queues);
+
+	put_device(ctrl->dev);
+	nvme_fc_rport_put(ctrl->rport);
+
+	ida_simple_remove(&nvme_fc_ctrl_cnt, ctrl->cnum);
+	if (ctrl->ctrl.opts)
+		nvmf_free_options(ctrl->ctrl.opts);
+	kfree(ctrl);
+}
+
+static void
+nvme_fc_ctrl_put(struct nvme_fc_ctrl *ctrl)
+{
+	kref_put(&ctrl->ref, nvme_fc_ctrl_free);
+}
+
+static int
+nvme_fc_ctrl_get(struct nvme_fc_ctrl *ctrl)
+{
+	return kref_get_unless_zero(&ctrl->ref);
+}
+
+/*
+ * All accesses from nvme core layer done - can now free the
+ * controller. Called after last nvme_put_ctrl() call
+ */
+static void
+nvme_fc_nvme_ctrl_freed(struct nvme_ctrl *nctrl)
+{
+	struct nvme_fc_ctrl *ctrl = to_fc_ctrl(nctrl);
+
+	WARN_ON(nctrl != &ctrl->ctrl);
+
+	nvme_fc_ctrl_put(ctrl);
+}
+
+static void
+nvme_fc_error_recovery(struct nvme_fc_ctrl *ctrl, char *errmsg)
+{
+	int active;
+
+	/*
+	 * if an error (io timeout, etc) while (re)connecting,
+	 * it's an error on creating the new association.
+	 * Start the error recovery thread if it hasn't already
+	 * been started. It is expected there could be multiple
+	 * ios hitting this path before things are cleaned up.
+	 */
+	if (ctrl->ctrl.state == NVME_CTRL_CONNECTING) {
+		active = atomic_xchg(&ctrl->err_work_active, 1);
+		if (!active && !schedule_work(&ctrl->err_work)) {
+			atomic_set(&ctrl->err_work_active, 0);
+			WARN_ON(1);
+		}
+		return;
+	}
+
+	/* Otherwise, only proceed if in LIVE state - e.g. on first error */
+	if (ctrl->ctrl.state != NVME_CTRL_LIVE)
+		return;
+
+	dev_warn(ctrl->ctrl.device,
+		"NVME-FC{%d}: transport association error detected: %s\n",
+		ctrl->cnum, errmsg);
+	dev_warn(ctrl->ctrl.device,
+		"NVME-FC{%d}: resetting controller\n", ctrl->cnum);
+
+	nvme_reset_ctrl(&ctrl->ctrl);
+}
+
+static enum blk_eh_timer_return
+nvme_fc_timeout(struct request *rq, bool reserved)
+{
+	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
+	struct nvme_fc_ctrl *ctrl = op->ctrl;
+
+	/*
+	 * we can't individually ABTS an io without affecting the queue,
+	 * thus killing the queue, and thus the association.
+	 * So resolve by performing a controller reset, which will stop
+	 * the host/io stack, terminate the association on the link,
+	 * and recreate an association on the link.
+	 */
+	nvme_fc_error_recovery(ctrl, "io timeout error");
+
+	/*
+	 * the io abort has been initiated. Have the reset timer
+	 * restarted and the abort completion will complete the io
+	 * shortly. Avoids a synchronous wait while the abort finishes.
+	 */
+	return BLK_EH_RESET_TIMER;
+}
+
+static int
+nvme_fc_map_data(struct nvme_fc_ctrl *ctrl, struct request *rq,
+		struct nvme_fc_fcp_op *op)
+{
+	struct nvmefc_fcp_req *freq = &op->fcp_req;
+	enum dma_data_direction dir;
+	int ret;
+
+	freq->sg_cnt = 0;
+
+	if (!blk_rq_nr_phys_segments(rq))
+		return 0;
+
+	freq->sg_table.sgl = freq->first_sgl;
+	ret = sg_alloc_table_chained(&freq->sg_table,
+			blk_rq_nr_phys_segments(rq), freq->sg_table.sgl);
+	if (ret)
+		return -ENOMEM;
+
+	op->nents = blk_rq_map_sg(rq->q, rq, freq->sg_table.sgl);
+	WARN_ON(op->nents > blk_rq_nr_phys_segments(rq));
+	dir = (rq_data_dir(rq) == WRITE) ? DMA_TO_DEVICE : DMA_FROM_DEVICE;
+	freq->sg_cnt = fc_dma_map_sg(ctrl->lport->dev, freq->sg_table.sgl,
+				op->nents, dir);
+	if (unlikely(freq->sg_cnt <= 0)) {
+		sg_free_table_chained(&freq->sg_table, true);
+		freq->sg_cnt = 0;
+		return -EFAULT;
+	}
+
+	/*
+	 * TODO: blk_integrity_rq(rq)  for DIF
+	 */
+	return 0;
+}
+
+static void
+nvme_fc_unmap_data(struct nvme_fc_ctrl *ctrl, struct request *rq,
+		struct nvme_fc_fcp_op *op)
+{
+	struct nvmefc_fcp_req *freq = &op->fcp_req;
+
+	if (!freq->sg_cnt)
+		return;
+
+	fc_dma_unmap_sg(ctrl->lport->dev, freq->sg_table.sgl, op->nents,
+				((rq_data_dir(rq) == WRITE) ?
+					DMA_TO_DEVICE : DMA_FROM_DEVICE));
+
+	nvme_cleanup_cmd(rq);
+
+	sg_free_table_chained(&freq->sg_table, true);
+
+	freq->sg_cnt = 0;
+}
+
+/*
+ * In FC, the queue is a logical thing. At transport connect, the target
+ * creates its "queue" and returns a handle that is to be given to the
+ * target whenever it posts something to the corresponding SQ.  When an
+ * SQE is sent on a SQ, FC effectively considers the SQE, or rather the
+ * command contained within the SQE, an io, and assigns a FC exchange
+ * to it. The SQE and the associated SQ handle are sent in the initial
+ * CMD IU sents on the exchange. All transfers relative to the io occur
+ * as part of the exchange.  The CQE is the last thing for the io,
+ * which is transferred (explicitly or implicitly) with the RSP IU
+ * sent on the exchange. After the CQE is received, the FC exchange is
+ * terminaed and the Exchange may be used on a different io.
+ *
+ * The transport to LLDD api has the transport making a request for a
+ * new fcp io request to the LLDD. The LLDD then allocates a FC exchange
+ * resource and transfers the command. The LLDD will then process all
+ * steps to complete the io. Upon completion, the transport done routine
+ * is called.
+ *
+ * So - while the operation is outstanding to the LLDD, there is a link
+ * level FC exchange resource that is also outstanding. This must be
+ * considered in all cleanup operations.
+ */
+static blk_status_t
+nvme_fc_start_fcp_op(struct nvme_fc_ctrl *ctrl, struct nvme_fc_queue *queue,
+	struct nvme_fc_fcp_op *op, u32 data_len,
+	enum nvmefc_fcp_datadir	io_dir)
+{
+	struct nvme_fc_cmd_iu *cmdiu = &op->cmd_iu;
+	struct nvme_command *sqe = &cmdiu->sqe;
+	int ret, opstate;
+
+	/*
+	 * before attempting to send the io, check to see if we believe
+	 * the target device is present
+	 */
+	if (ctrl->rport->remoteport.port_state != FC_OBJSTATE_ONLINE)
+		return BLK_STS_RESOURCE;
+
+	if (!nvme_fc_ctrl_get(ctrl))
+		return BLK_STS_IOERR;
+
+	/* format the FC-NVME CMD IU and fcp_req */
+	cmdiu->connection_id = cpu_to_be64(queue->connection_id);
+	cmdiu->data_len = cpu_to_be32(data_len);
+	switch (io_dir) {
+	case NVMEFC_FCP_WRITE:
+		cmdiu->flags = FCNVME_CMD_FLAGS_WRITE;
+		break;
+	case NVMEFC_FCP_READ:
+		cmdiu->flags = FCNVME_CMD_FLAGS_READ;
+		break;
+	case NVMEFC_FCP_NODATA:
+		cmdiu->flags = 0;
+		break;
+	}
+	op->fcp_req.payload_length = data_len;
+	op->fcp_req.io_dir = io_dir;
+	op->fcp_req.transferred_length = 0;
+	op->fcp_req.rcv_rsplen = 0;
+	op->fcp_req.status = NVME_SC_SUCCESS;
+	op->fcp_req.sqid = cpu_to_le16(queue->qnum);
+
+	/*
+	 * validate per fabric rules, set fields mandated by fabric spec
+	 * as well as those by FC-NVME spec.
+	 */
+	WARN_ON_ONCE(sqe->common.metadata);
+	sqe->common.flags |= NVME_CMD_SGL_METABUF;
+
+	/*
+	 * format SQE DPTR field per FC-NVME rules:
+	 *    type=0x5     Transport SGL Data Block Descriptor
+	 *    subtype=0xA  Transport-specific value
+	 *    address=0
+	 *    length=length of the data series
+	 */
+	sqe->rw.dptr.sgl.type = (NVME_TRANSPORT_SGL_DATA_DESC << 4) |
+					NVME_SGL_FMT_TRANSPORT_A;
+	sqe->rw.dptr.sgl.length = cpu_to_le32(data_len);
+	sqe->rw.dptr.sgl.addr = 0;
+
+	if (!(op->flags & FCOP_FLAGS_AEN)) {
+		ret = nvme_fc_map_data(ctrl, op->rq, op);
+		if (ret < 0) {
+			nvme_cleanup_cmd(op->rq);
+			nvme_fc_ctrl_put(ctrl);
+			if (ret == -ENOMEM || ret == -EAGAIN)
+				return BLK_STS_RESOURCE;
+			return BLK_STS_IOERR;
+		}
+	}
+
+	fc_dma_sync_single_for_device(ctrl->lport->dev, op->fcp_req.cmddma,
+				  sizeof(op->cmd_iu), DMA_TO_DEVICE);
+
+	atomic_set(&op->state, FCPOP_STATE_ACTIVE);
+
+	if (!(op->flags & FCOP_FLAGS_AEN))
+		blk_mq_start_request(op->rq);
+
+	cmdiu->csn = cpu_to_be32(atomic_inc_return(&queue->csn));
+	ret = ctrl->lport->ops->fcp_io(&ctrl->lport->localport,
+					&ctrl->rport->remoteport,
+					queue->lldd_handle, &op->fcp_req);
+
+	if (ret) {
+		/*
+		 * If the lld fails to send the command is there an issue with
+		 * the csn value?  If the command that fails is the Connect,
+		 * no - as the connection won't be live.  If it is a command
+		 * post-connect, it's possible a gap in csn may be created.
+		 * Does this matter?  As Linux initiators don't send fused
+		 * commands, no.  The gap would exist, but as there's nothing
+		 * that depends on csn order to be delivered on the target
+		 * side, it shouldn't hurt.  It would be difficult for a
+		 * target to even detect the csn gap as it has no idea when the
+		 * cmd with the csn was supposed to arrive.
+		 */
+		opstate = atomic_xchg(&op->state, FCPOP_STATE_COMPLETE);
+		__nvme_fc_fcpop_chk_teardowns(ctrl, op, opstate);
+
+		if (!(op->flags & FCOP_FLAGS_AEN))
+			nvme_fc_unmap_data(ctrl, op->rq, op);
+
+		nvme_fc_ctrl_put(ctrl);
+
+		if (ctrl->rport->remoteport.port_state == FC_OBJSTATE_ONLINE &&
+				ret != -EBUSY)
+			return BLK_STS_IOERR;
+
+		return BLK_STS_RESOURCE;
+	}
+
+	return BLK_STS_OK;
+}
+
+static blk_status_t
+nvme_fc_queue_rq(struct blk_mq_hw_ctx *hctx,
+			const struct blk_mq_queue_data *bd)
+{
+	struct nvme_ns *ns = hctx->queue->queuedata;
+	struct nvme_fc_queue *queue = hctx->driver_data;
+	struct nvme_fc_ctrl *ctrl = queue->ctrl;
+	struct request *rq = bd->rq;
+	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
+	struct nvme_fc_cmd_iu *cmdiu = &op->cmd_iu;
+	struct nvme_command *sqe = &cmdiu->sqe;
+	enum nvmefc_fcp_datadir	io_dir;
+	bool queue_ready = test_bit(NVME_FC_Q_LIVE, &queue->flags);
+	u32 data_len;
+	blk_status_t ret;
+
+	if (ctrl->rport->remoteport.port_state != FC_OBJSTATE_ONLINE ||
+	    !nvmf_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
+		return nvmf_fail_nonready_command(&queue->ctrl->ctrl, rq);
+
+	ret = nvme_setup_cmd(ns, rq, sqe);
+	if (ret)
+		return ret;
+
+	/*
+	 * nvme core doesn't quite treat the rq opaquely. Commands such
+	 * as WRITE ZEROES will return a non-zero rq payload_bytes yet
+	 * there is no actual payload to be transferred.
+	 * To get it right, key data transmission on there being 1 or
+	 * more physical segments in the sg list. If there is no
+	 * physical segments, there is no payload.
+	 */
+	if (blk_rq_nr_phys_segments(rq)) {
+		data_len = blk_rq_payload_bytes(rq);
+		io_dir = ((rq_data_dir(rq) == WRITE) ?
+					NVMEFC_FCP_WRITE : NVMEFC_FCP_READ);
+	} else {
+		data_len = 0;
+		io_dir = NVMEFC_FCP_NODATA;
+	}
+
+
+	return nvme_fc_start_fcp_op(ctrl, queue, op, data_len, io_dir);
+}
+
+static void
+nvme_fc_submit_async_event(struct nvme_ctrl *arg)
+{
+	struct nvme_fc_ctrl *ctrl = to_fc_ctrl(arg);
+	struct nvme_fc_fcp_op *aen_op;
+	unsigned long flags;
+	bool terminating = false;
+	blk_status_t ret;
+
+	spin_lock_irqsave(&ctrl->lock, flags);
+	if (ctrl->flags & FCCTRL_TERMIO)
+		terminating = true;
+	spin_unlock_irqrestore(&ctrl->lock, flags);
+
+	if (terminating)
+		return;
+
+	aen_op = &ctrl->aen_ops[0];
+
+	ret = nvme_fc_start_fcp_op(ctrl, aen_op->queue, aen_op, 0,
+					NVMEFC_FCP_NODATA);
+	if (ret)
+		dev_err(ctrl->ctrl.device,
+			"failed async event work\n");
+}
+
+static void
+nvme_fc_complete_rq(struct request *rq)
+{
+	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(rq);
+	struct nvme_fc_ctrl *ctrl = op->ctrl;
+
+	atomic_set(&op->state, FCPOP_STATE_IDLE);
+
+	nvme_fc_unmap_data(ctrl, rq, op);
+	nvme_complete_rq(rq);
+	nvme_fc_ctrl_put(ctrl);
+}
+
+/*
+ * This routine is used by the transport when it needs to find active
+ * io on a queue that is to be terminated. The transport uses
+ * blk_mq_tagset_busy_itr() to find the busy requests, which then invoke
+ * this routine to kill them on a 1 by 1 basis.
+ *
+ * As FC allocates FC exchange for each io, the transport must contact
+ * the LLDD to terminate the exchange, thus releasing the FC exchange.
+ * After terminating the exchange the LLDD will call the transport's
+ * normal io done path for the request, but it will have an aborted
+ * status. The done path will return the io request back to the block
+ * layer with an error status.
+ */
+static bool
+nvme_fc_terminate_exchange(struct request *req, void *data, bool reserved)
+{
+	struct nvme_ctrl *nctrl = data;
+	struct nvme_fc_ctrl *ctrl = to_fc_ctrl(nctrl);
+	struct nvme_fc_fcp_op *op = blk_mq_rq_to_pdu(req);
+
+	__nvme_fc_abort_op(ctrl, op);
+	return true;
+}
+
+
+static const struct blk_mq_ops nvme_fc_mq_ops = {
+	.queue_rq	= nvme_fc_queue_rq,
+	.complete	= nvme_fc_complete_rq,
+	.init_request	= nvme_fc_init_request,
+	.exit_request	= nvme_fc_exit_request,
+	.init_hctx	= nvme_fc_init_hctx,
+	.timeout	= nvme_fc_timeout,
+};
+
+static int
+nvme_fc_create_io_queues(struct nvme_fc_ctrl *ctrl)
+{
+	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
+	unsigned int nr_io_queues;
+	int ret;
+
+	nr_io_queues = min(min(opts->nr_io_queues, num_online_cpus()),
+				ctrl->lport->ops->max_hw_queues);
+	ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+	if (ret) {
+		dev_info(ctrl->ctrl.device,
+			"set_queue_count failed: %d\n", ret);
+		return ret;
+	}
+
+	ctrl->ctrl.queue_count = nr_io_queues + 1;
+	if (!nr_io_queues)
+		return 0;
+
+	nvme_fc_init_io_queues(ctrl);
+
+	memset(&ctrl->tag_set, 0, sizeof(ctrl->tag_set));
+	ctrl->tag_set.ops = &nvme_fc_mq_ops;
+	ctrl->tag_set.queue_depth = ctrl->ctrl.opts->queue_size;
+	ctrl->tag_set.reserved_tags = 1; /* fabric connect */
+	ctrl->tag_set.numa_node = ctrl->ctrl.numa_node;
+	ctrl->tag_set.flags = BLK_MQ_F_SHOULD_MERGE;
+	ctrl->tag_set.cmd_size =
+		struct_size((struct nvme_fcp_op_w_sgl *)NULL, priv,
+			    ctrl->lport->ops->fcprqst_priv_sz);
+	ctrl->tag_set.driver_data = ctrl;
+	ctrl->tag_set.nr_hw_queues = ctrl->ctrl.queue_count - 1;
+	ctrl->tag_set.timeout = NVME_IO_TIMEOUT;
+
+	ret = blk_mq_alloc_tag_set(&ctrl->tag_set);
+	if (ret)
+		return ret;
+
+	ctrl->ctrl.tagset = &ctrl->tag_set;
+
+	ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+	if (IS_ERR(ctrl->ctrl.connect_q)) {
+		ret = PTR_ERR(ctrl->ctrl.connect_q);
+		goto out_free_tag_set;
+	}
+
+	ret = nvme_fc_create_hw_io_queues(ctrl, ctrl->ctrl.sqsize + 1);
+	if (ret)
+		goto out_cleanup_blk_queue;
+
+	ret = nvme_fc_connect_io_queues(ctrl, ctrl->ctrl.sqsize + 1);
+	if (ret)
+		goto out_delete_hw_queues;
+
+	ctrl->ioq_live = true;
+
+	return 0;
+
+out_delete_hw_queues:
+	nvme_fc_delete_hw_io_queues(ctrl);
+out_cleanup_blk_queue:
+	blk_cleanup_queue(ctrl->ctrl.connect_q);
+out_free_tag_set:
+	blk_mq_free_tag_set(&ctrl->tag_set);
+	nvme_fc_free_io_queues(ctrl);
+
+	/* force put free routine to ignore io queues */
+	ctrl->ctrl.tagset = NULL;
+
+	return ret;
+}
+
+static int
+nvme_fc_recreate_io_queues(struct nvme_fc_ctrl *ctrl)
+{
+	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
+	u32 prior_ioq_cnt = ctrl->ctrl.queue_count - 1;
+	unsigned int nr_io_queues;
+	int ret;
+
+	nr_io_queues = min(min(opts->nr_io_queues, num_online_cpus()),
+				ctrl->lport->ops->max_hw_queues);
+	ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+	if (ret) {
+		dev_info(ctrl->ctrl.device,
+			"set_queue_count failed: %d\n", ret);
+		return ret;
+	}
+
+	if (!nr_io_queues && prior_ioq_cnt) {
+		dev_info(ctrl->ctrl.device,
+			"Fail Reconnect: At least 1 io queue "
+			"required (was %d)\n", prior_ioq_cnt);
+		return -ENOSPC;
+	}
+
+	ctrl->ctrl.queue_count = nr_io_queues + 1;
+	/* check for io queues existing */
+	if (ctrl->ctrl.queue_count == 1)
+		return 0;
+
+	ret = nvme_fc_create_hw_io_queues(ctrl, ctrl->ctrl.sqsize + 1);
+	if (ret)
+		goto out_free_io_queues;
+
+	ret = nvme_fc_connect_io_queues(ctrl, ctrl->ctrl.sqsize + 1);
+	if (ret)
+		goto out_delete_hw_queues;
+
+	if (prior_ioq_cnt != nr_io_queues)
+		dev_info(ctrl->ctrl.device,
+			"reconnect: revising io queue count from %d to %d\n",
+			prior_ioq_cnt, nr_io_queues);
+	blk_mq_update_nr_hw_queues(&ctrl->tag_set, nr_io_queues);
+
+	return 0;
+
+out_delete_hw_queues:
+	nvme_fc_delete_hw_io_queues(ctrl);
+out_free_io_queues:
+	nvme_fc_free_io_queues(ctrl);
+	return ret;
+}
+
+static void
+nvme_fc_rport_active_on_lport(struct nvme_fc_rport *rport)
+{
+	struct nvme_fc_lport *lport = rport->lport;
+
+	atomic_inc(&lport->act_rport_cnt);
+}
+
+static void
+nvme_fc_rport_inactive_on_lport(struct nvme_fc_rport *rport)
+{
+	struct nvme_fc_lport *lport = rport->lport;
+	u32 cnt;
+
+	cnt = atomic_dec_return(&lport->act_rport_cnt);
+	if (cnt == 0 && lport->localport.port_state == FC_OBJSTATE_DELETED)
+		lport->ops->localport_delete(&lport->localport);
+}
+
+static int
+nvme_fc_ctlr_active_on_rport(struct nvme_fc_ctrl *ctrl)
+{
+	struct nvme_fc_rport *rport = ctrl->rport;
+	u32 cnt;
+
+	if (ctrl->assoc_active)
+		return 1;
+
+	ctrl->assoc_active = true;
+	cnt = atomic_inc_return(&rport->act_ctrl_cnt);
+	if (cnt == 1)
+		nvme_fc_rport_active_on_lport(rport);
+
+	return 0;
+}
+
+static int
+nvme_fc_ctlr_inactive_on_rport(struct nvme_fc_ctrl *ctrl)
+{
+	struct nvme_fc_rport *rport = ctrl->rport;
+	struct nvme_fc_lport *lport = rport->lport;
+	u32 cnt;
+
+	/* ctrl->assoc_active=false will be set independently */
+
+	cnt = atomic_dec_return(&rport->act_ctrl_cnt);
+	if (cnt == 0) {
+		if (rport->remoteport.port_state == FC_OBJSTATE_DELETED)
+			lport->ops->remoteport_delete(&rport->remoteport);
+		nvme_fc_rport_inactive_on_lport(rport);
+	}
+
+	return 0;
+}
+
+/*
+ * This routine restarts the controller on the host side, and
+ * on the link side, recreates the controller association.
+ */
+static int
+nvme_fc_create_association(struct nvme_fc_ctrl *ctrl)
+{
+	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
+	int ret;
+	bool changed;
+
+	++ctrl->ctrl.nr_reconnects;
+
+	if (ctrl->rport->remoteport.port_state != FC_OBJSTATE_ONLINE)
+		return -ENODEV;
+
+	if (nvme_fc_ctlr_active_on_rport(ctrl))
+		return -ENOTUNIQ;
+
+	/*
+	 * Create the admin queue
+	 */
+
+	ret = __nvme_fc_create_hw_queue(ctrl, &ctrl->queues[0], 0,
+				NVME_AQ_DEPTH);
+	if (ret)
+		goto out_free_queue;
+
+	ret = nvme_fc_connect_admin_queue(ctrl, &ctrl->queues[0],
+				NVME_AQ_DEPTH, (NVME_AQ_DEPTH / 4));
+	if (ret)
+		goto out_delete_hw_queue;
+
+	blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+
+	ret = nvmf_connect_admin_queue(&ctrl->ctrl);
+	if (ret)
+		goto out_disconnect_admin_queue;
+
+	set_bit(NVME_FC_Q_LIVE, &ctrl->queues[0].flags);
+
+	/*
+	 * Check controller capabilities
+	 *
+	 * todo:- add code to check if ctrl attributes changed from
+	 * prior connection values
+	 */
+
+	ret = nvmf_reg_read64(&ctrl->ctrl, NVME_REG_CAP, &ctrl->ctrl.cap);
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"prop_get NVME_REG_CAP failed\n");
+		goto out_disconnect_admin_queue;
+	}
+
+	ctrl->ctrl.sqsize =
+		min_t(int, NVME_CAP_MQES(ctrl->ctrl.cap), ctrl->ctrl.sqsize);
+
+	ret = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
+	if (ret)
+		goto out_disconnect_admin_queue;
+
+	ctrl->ctrl.max_hw_sectors =
+		(ctrl->lport->ops->max_sgl_segments - 1) << (PAGE_SHIFT - 9);
+
+	ret = nvme_init_identify(&ctrl->ctrl);
+	if (ret)
+		goto out_disconnect_admin_queue;
+
+	/* sanity checks */
+
+	/* FC-NVME does not have other data in the capsule */
+	if (ctrl->ctrl.icdoff) {
+		dev_err(ctrl->ctrl.device, "icdoff %d is not supported!\n",
+				ctrl->ctrl.icdoff);
+		goto out_disconnect_admin_queue;
+	}
+
+	/* FC-NVME supports normal SGL Data Block Descriptors */
+
+	if (opts->queue_size > ctrl->ctrl.maxcmd) {
+		/* warn if maxcmd is lower than queue_size */
+		dev_warn(ctrl->ctrl.device,
+			"queue_size %zu > ctrl maxcmd %u, reducing "
+			"to queue_size\n",
+			opts->queue_size, ctrl->ctrl.maxcmd);
+		opts->queue_size = ctrl->ctrl.maxcmd;
+	}
+
+	if (opts->queue_size > ctrl->ctrl.sqsize + 1) {
+		/* warn if sqsize is lower than queue_size */
+		dev_warn(ctrl->ctrl.device,
+			"queue_size %zu > ctrl sqsize %u, clamping down\n",
+			opts->queue_size, ctrl->ctrl.sqsize + 1);
+		opts->queue_size = ctrl->ctrl.sqsize + 1;
+	}
+
+	ret = nvme_fc_init_aen_ops(ctrl);
+	if (ret)
+		goto out_term_aen_ops;
+
+	/*
+	 * Create the io queues
+	 */
+
+	if (ctrl->ctrl.queue_count > 1) {
+		if (!ctrl->ioq_live)
+			ret = nvme_fc_create_io_queues(ctrl);
+		else
+			ret = nvme_fc_recreate_io_queues(ctrl);
+		if (ret)
+			goto out_term_aen_ops;
+	}
+
+	changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
+
+	ctrl->ctrl.nr_reconnects = 0;
+
+	if (changed)
+		nvme_start_ctrl(&ctrl->ctrl);
+
+	return 0;	/* Success */
+
+out_term_aen_ops:
+	nvme_fc_term_aen_ops(ctrl);
+out_disconnect_admin_queue:
+	/* send a Disconnect(association) LS to fc-nvme target */
+	nvme_fc_xmt_disconnect_assoc(ctrl);
+out_delete_hw_queue:
+	__nvme_fc_delete_hw_queue(ctrl, &ctrl->queues[0], 0);
+out_free_queue:
+	nvme_fc_free_queue(&ctrl->queues[0]);
+	ctrl->assoc_active = false;
+	nvme_fc_ctlr_inactive_on_rport(ctrl);
+
+	return ret;
+}
+
+/*
+ * This routine stops operation of the controller on the host side.
+ * On the host os stack side: Admin and IO queues are stopped,
+ *   outstanding ios on them terminated via FC ABTS.
+ * On the link side: the association is terminated.
+ */
+static void
+nvme_fc_delete_association(struct nvme_fc_ctrl *ctrl)
+{
+	unsigned long flags;
+
+	if (!ctrl->assoc_active)
+		return;
+	ctrl->assoc_active = false;
+
+	spin_lock_irqsave(&ctrl->lock, flags);
+	ctrl->flags |= FCCTRL_TERMIO;
+	ctrl->iocnt = 0;
+	spin_unlock_irqrestore(&ctrl->lock, flags);
+
+	/*
+	 * If io queues are present, stop them and terminate all outstanding
+	 * ios on them. As FC allocates FC exchange for each io, the
+	 * transport must contact the LLDD to terminate the exchange,
+	 * thus releasing the FC exchange. We use blk_mq_tagset_busy_itr()
+	 * to tell us what io's are busy and invoke a transport routine
+	 * to kill them with the LLDD.  After terminating the exchange
+	 * the LLDD will call the transport's normal io done path, but it
+	 * will have an aborted status. The done path will return the
+	 * io requests back to the block layer as part of normal completions
+	 * (but with error status).
+	 */
+	if (ctrl->ctrl.queue_count > 1) {
+		nvme_stop_queues(&ctrl->ctrl);
+		blk_mq_tagset_busy_iter(&ctrl->tag_set,
+				nvme_fc_terminate_exchange, &ctrl->ctrl);
+	}
+
+	/*
+	 * Other transports, which don't have link-level contexts bound
+	 * to sqe's, would try to gracefully shutdown the controller by
+	 * writing the registers for shutdown and polling (call
+	 * nvme_shutdown_ctrl()). Given a bunch of i/o was potentially
+	 * just aborted and we will wait on those contexts, and given
+	 * there was no indication of how live the controlelr is on the
+	 * link, don't send more io to create more contexts for the
+	 * shutdown. Let the controller fail via keepalive failure if
+	 * its still present.
+	 */
+
+	/*
+	 * clean up the admin queue. Same thing as above.
+	 * use blk_mq_tagset_busy_itr() and the transport routine to
+	 * terminate the exchanges.
+	 */
+	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set,
+				nvme_fc_terminate_exchange, &ctrl->ctrl);
+
+	/* kill the aens as they are a separate path */
+	nvme_fc_abort_aen_ops(ctrl);
+
+	/* wait for all io that had to be aborted */
+	spin_lock_irq(&ctrl->lock);
+	wait_event_lock_irq(ctrl->ioabort_wait, ctrl->iocnt == 0, ctrl->lock);
+	ctrl->flags &= ~FCCTRL_TERMIO;
+	spin_unlock_irq(&ctrl->lock);
+
+	nvme_fc_term_aen_ops(ctrl);
+
+	/*
+	 * send a Disconnect(association) LS to fc-nvme target
+	 * Note: could have been sent at top of process, but
+	 * cleaner on link traffic if after the aborts complete.
+	 * Note: if association doesn't exist, association_id will be 0
+	 */
+	if (ctrl->association_id)
+		nvme_fc_xmt_disconnect_assoc(ctrl);
+
+	if (ctrl->ctrl.tagset) {
+		nvme_fc_delete_hw_io_queues(ctrl);
+		nvme_fc_free_io_queues(ctrl);
+	}
+
+	__nvme_fc_delete_hw_queue(ctrl, &ctrl->queues[0], 0);
+	nvme_fc_free_queue(&ctrl->queues[0]);
+
+	/* re-enable the admin_q so anything new can fast fail */
+	blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+
+	/* resume the io queues so that things will fast fail */
+	nvme_start_queues(&ctrl->ctrl);
+
+	nvme_fc_ctlr_inactive_on_rport(ctrl);
+}
+
+static void
+nvme_fc_delete_ctrl(struct nvme_ctrl *nctrl)
+{
+	struct nvme_fc_ctrl *ctrl = to_fc_ctrl(nctrl);
+
+	cancel_work_sync(&ctrl->err_work);
+	cancel_delayed_work_sync(&ctrl->connect_work);
+	/*
+	 * kill the association on the link side.  this will block
+	 * waiting for io to terminate
+	 */
+	nvme_fc_delete_association(ctrl);
+}
+
+static void
+nvme_fc_reconnect_or_delete(struct nvme_fc_ctrl *ctrl, int status)
+{
+	struct nvme_fc_rport *rport = ctrl->rport;
+	struct nvme_fc_remote_port *portptr = &rport->remoteport;
+	unsigned long recon_delay = ctrl->ctrl.opts->reconnect_delay * HZ;
+	bool recon = true;
+
+	if (ctrl->ctrl.state != NVME_CTRL_CONNECTING)
+		return;
+
+	if (portptr->port_state == FC_OBJSTATE_ONLINE)
+		dev_info(ctrl->ctrl.device,
+			"NVME-FC{%d}: reset: Reconnect attempt failed (%d)\n",
+			ctrl->cnum, status);
+	else if (time_after_eq(jiffies, rport->dev_loss_end))
+		recon = false;
+
+	if (recon && nvmf_should_reconnect(&ctrl->ctrl)) {
+		if (portptr->port_state == FC_OBJSTATE_ONLINE)
+			dev_info(ctrl->ctrl.device,
+				"NVME-FC{%d}: Reconnect attempt in %ld "
+				"seconds\n",
+				ctrl->cnum, recon_delay / HZ);
+		else if (time_after(jiffies + recon_delay, rport->dev_loss_end))
+			recon_delay = rport->dev_loss_end - jiffies;
+
+		queue_delayed_work(nvme_wq, &ctrl->connect_work, recon_delay);
+	} else {
+		if (portptr->port_state == FC_OBJSTATE_ONLINE)
+			dev_warn(ctrl->ctrl.device,
+				"NVME-FC{%d}: Max reconnect attempts (%d) "
+				"reached.\n",
+				ctrl->cnum, ctrl->ctrl.nr_reconnects);
+		else
+			dev_warn(ctrl->ctrl.device,
+				"NVME-FC{%d}: dev_loss_tmo (%d) expired "
+				"while waiting for remoteport connectivity.\n",
+				ctrl->cnum, portptr->dev_loss_tmo);
+		WARN_ON(nvme_delete_ctrl(&ctrl->ctrl));
+	}
+}
+
+static void
+__nvme_fc_terminate_io(struct nvme_fc_ctrl *ctrl)
+{
+	nvme_stop_keep_alive(&ctrl->ctrl);
+
+	/* will block will waiting for io to terminate */
+	nvme_fc_delete_association(ctrl);
+
+	if (ctrl->ctrl.state != NVME_CTRL_CONNECTING &&
+	    !nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING))
+		dev_err(ctrl->ctrl.device,
+			"NVME-FC{%d}: error_recovery: Couldn't change state "
+			"to CONNECTING\n", ctrl->cnum);
+}
+
+static void
+nvme_fc_reset_ctrl_work(struct work_struct *work)
+{
+	struct nvme_fc_ctrl *ctrl =
+		container_of(work, struct nvme_fc_ctrl, ctrl.reset_work);
+	int ret;
+
+	__nvme_fc_terminate_io(ctrl);
+
+	nvme_stop_ctrl(&ctrl->ctrl);
+
+	if (ctrl->rport->remoteport.port_state == FC_OBJSTATE_ONLINE)
+		ret = nvme_fc_create_association(ctrl);
+	else
+		ret = -ENOTCONN;
+
+	if (ret)
+		nvme_fc_reconnect_or_delete(ctrl, ret);
+	else
+		dev_info(ctrl->ctrl.device,
+			"NVME-FC{%d}: controller reset complete\n",
+			ctrl->cnum);
+}
+
+static void
+nvme_fc_connect_err_work(struct work_struct *work)
+{
+	struct nvme_fc_ctrl *ctrl =
+			container_of(work, struct nvme_fc_ctrl, err_work);
+
+	__nvme_fc_terminate_io(ctrl);
+
+	atomic_set(&ctrl->err_work_active, 0);
+
+	/*
+	 * Rescheduling the connection after recovering
+	 * from the io error is left to the reconnect work
+	 * item, which is what should have stalled waiting on
+	 * the io that had the error that scheduled this work.
+	 */
+}
+
+static const struct nvme_ctrl_ops nvme_fc_ctrl_ops = {
+	.name			= "fc",
+	.module			= THIS_MODULE,
+	.flags			= NVME_F_FABRICS,
+	.reg_read32		= nvmf_reg_read32,
+	.reg_read64		= nvmf_reg_read64,
+	.reg_write32		= nvmf_reg_write32,
+	.free_ctrl		= nvme_fc_nvme_ctrl_freed,
+	.submit_async_event	= nvme_fc_submit_async_event,
+	.delete_ctrl		= nvme_fc_delete_ctrl,
+	.get_address		= nvmf_get_address,
+};
+
+static void
+nvme_fc_connect_ctrl_work(struct work_struct *work)
+{
+	int ret;
+
+	struct nvme_fc_ctrl *ctrl =
+			container_of(to_delayed_work(work),
+				struct nvme_fc_ctrl, connect_work);
+
+	ret = nvme_fc_create_association(ctrl);
+	if (ret)
+		nvme_fc_reconnect_or_delete(ctrl, ret);
+	else
+		dev_info(ctrl->ctrl.device,
+			"NVME-FC{%d}: controller connect complete\n",
+			ctrl->cnum);
+}
+
+
+static const struct blk_mq_ops nvme_fc_admin_mq_ops = {
+	.queue_rq	= nvme_fc_queue_rq,
+	.complete	= nvme_fc_complete_rq,
+	.init_request	= nvme_fc_init_request,
+	.exit_request	= nvme_fc_exit_request,
+	.init_hctx	= nvme_fc_init_admin_hctx,
+	.timeout	= nvme_fc_timeout,
+};
+
+
+/*
+ * Fails a controller request if it matches an existing controller
+ * (association) with the same tuple:
+ * <Host NQN, Host ID, local FC port, remote FC port, SUBSYS NQN>
+ *
+ * The ports don't need to be compared as they are intrinsically
+ * already matched by the port pointers supplied.
+ */
+static bool
+nvme_fc_existing_controller(struct nvme_fc_rport *rport,
+		struct nvmf_ctrl_options *opts)
+{
+	struct nvme_fc_ctrl *ctrl;
+	unsigned long flags;
+	bool found = false;
+
+	spin_lock_irqsave(&rport->lock, flags);
+	list_for_each_entry(ctrl, &rport->ctrl_list, ctrl_list) {
+		found = nvmf_ctlr_matches_baseopts(&ctrl->ctrl, opts);
+		if (found)
+			break;
+	}
+	spin_unlock_irqrestore(&rport->lock, flags);
+
+	return found;
+}
+
+static struct nvme_ctrl *
+nvme_fc_init_ctrl(struct device *dev, struct nvmf_ctrl_options *opts,
+	struct nvme_fc_lport *lport, struct nvme_fc_rport *rport)
+{
+	struct nvme_fc_ctrl *ctrl;
+	unsigned long flags;
+	int ret, idx;
+
+	if (!(rport->remoteport.port_role &
+	    (FC_PORT_ROLE_NVME_DISCOVERY | FC_PORT_ROLE_NVME_TARGET))) {
+		ret = -EBADR;
+		goto out_fail;
+	}
+
+	if (!opts->duplicate_connect &&
+	    nvme_fc_existing_controller(rport, opts)) {
+		ret = -EALREADY;
+		goto out_fail;
+	}
+
+	ctrl = kzalloc(sizeof(*ctrl), GFP_KERNEL);
+	if (!ctrl) {
+		ret = -ENOMEM;
+		goto out_fail;
+	}
+
+	idx = ida_simple_get(&nvme_fc_ctrl_cnt, 0, 0, GFP_KERNEL);
+	if (idx < 0) {
+		ret = -ENOSPC;
+		goto out_free_ctrl;
+	}
+
+	ctrl->ctrl.opts = opts;
+	ctrl->ctrl.nr_reconnects = 0;
+	if (lport->dev)
+		ctrl->ctrl.numa_node = dev_to_node(lport->dev);
+	else
+		ctrl->ctrl.numa_node = NUMA_NO_NODE;
+	INIT_LIST_HEAD(&ctrl->ctrl_list);
+	ctrl->lport = lport;
+	ctrl->rport = rport;
+	ctrl->dev = lport->dev;
+	ctrl->cnum = idx;
+	ctrl->ioq_live = false;
+	ctrl->assoc_active = false;
+	atomic_set(&ctrl->err_work_active, 0);
+	init_waitqueue_head(&ctrl->ioabort_wait);
+
+	get_device(ctrl->dev);
+	kref_init(&ctrl->ref);
+
+	INIT_WORK(&ctrl->ctrl.reset_work, nvme_fc_reset_ctrl_work);
+	INIT_DELAYED_WORK(&ctrl->connect_work, nvme_fc_connect_ctrl_work);
+	INIT_WORK(&ctrl->err_work, nvme_fc_connect_err_work);
+	spin_lock_init(&ctrl->lock);
+
+	/* io queue count */
+	ctrl->ctrl.queue_count = min_t(unsigned int,
+				opts->nr_io_queues,
+				lport->ops->max_hw_queues);
+	ctrl->ctrl.queue_count++;	/* +1 for admin queue */
+
+	ctrl->ctrl.sqsize = opts->queue_size - 1;
+	ctrl->ctrl.kato = opts->kato;
+	ctrl->ctrl.cntlid = 0xffff;
+
+	ret = -ENOMEM;
+	ctrl->queues = kcalloc(ctrl->ctrl.queue_count,
+				sizeof(struct nvme_fc_queue), GFP_KERNEL);
+	if (!ctrl->queues)
+		goto out_free_ida;
+
+	nvme_fc_init_queue(ctrl, 0);
+
+	memset(&ctrl->admin_tag_set, 0, sizeof(ctrl->admin_tag_set));
+	ctrl->admin_tag_set.ops = &nvme_fc_admin_mq_ops;
+	ctrl->admin_tag_set.queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+	ctrl->admin_tag_set.reserved_tags = 2; /* fabric connect + Keep-Alive */
+	ctrl->admin_tag_set.numa_node = ctrl->ctrl.numa_node;
+	ctrl->admin_tag_set.cmd_size =
+		struct_size((struct nvme_fcp_op_w_sgl *)NULL, priv,
+			    ctrl->lport->ops->fcprqst_priv_sz);
+	ctrl->admin_tag_set.driver_data = ctrl;
+	ctrl->admin_tag_set.nr_hw_queues = 1;
+	ctrl->admin_tag_set.timeout = ADMIN_TIMEOUT;
+	ctrl->admin_tag_set.flags = BLK_MQ_F_NO_SCHED;
+
+	ret = blk_mq_alloc_tag_set(&ctrl->admin_tag_set);
+	if (ret)
+		goto out_free_queues;
+	ctrl->ctrl.admin_tagset = &ctrl->admin_tag_set;
+
+	ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+	if (IS_ERR(ctrl->ctrl.admin_q)) {
+		ret = PTR_ERR(ctrl->ctrl.admin_q);
+		goto out_free_admin_tag_set;
+	}
+
+	/*
+	 * Would have been nice to init io queues tag set as well.
+	 * However, we require interaction from the controller
+	 * for max io queue count before we can do so.
+	 * Defer this to the connect path.
+	 */
+
+	ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_fc_ctrl_ops, 0);
+	if (ret)
+		goto out_cleanup_admin_q;
+
+	/* at this point, teardown path changes to ref counting on nvme ctrl */
+
+	spin_lock_irqsave(&rport->lock, flags);
+	list_add_tail(&ctrl->ctrl_list, &rport->ctrl_list);
+	spin_unlock_irqrestore(&rport->lock, flags);
+
+	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RESETTING) ||
+	    !nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {
+		dev_err(ctrl->ctrl.device,
+			"NVME-FC{%d}: failed to init ctrl state\n", ctrl->cnum);
+		goto fail_ctrl;
+	}
+
+	nvme_get_ctrl(&ctrl->ctrl);
+
+	if (!queue_delayed_work(nvme_wq, &ctrl->connect_work, 0)) {
+		nvme_put_ctrl(&ctrl->ctrl);
+		dev_err(ctrl->ctrl.device,
+			"NVME-FC{%d}: failed to schedule initial connect\n",
+			ctrl->cnum);
+		goto fail_ctrl;
+	}
+
+	flush_delayed_work(&ctrl->connect_work);
+
+	dev_info(ctrl->ctrl.device,
+		"NVME-FC{%d}: new ctrl: NQN \"%s\"\n",
+		ctrl->cnum, ctrl->ctrl.opts->subsysnqn);
+
+	return &ctrl->ctrl;
+
+fail_ctrl:
+	nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_DELETING);
+	cancel_work_sync(&ctrl->ctrl.reset_work);
+	cancel_work_sync(&ctrl->err_work);
+	cancel_delayed_work_sync(&ctrl->connect_work);
+
+	ctrl->ctrl.opts = NULL;
+
+	/* initiate nvme ctrl ref counting teardown */
+	nvme_uninit_ctrl(&ctrl->ctrl);
+
+	/* Remove core ctrl ref. */
+	nvme_put_ctrl(&ctrl->ctrl);
+
+	/* as we're past the point where we transition to the ref
+	 * counting teardown path, if we return a bad pointer here,
+	 * the calling routine, thinking it's prior to the
+	 * transition, will do an rport put. Since the teardown
+	 * path also does a rport put, we do an extra get here to
+	 * so proper order/teardown happens.
+	 */
+	nvme_fc_rport_get(rport);
+
+	return ERR_PTR(-EIO);
+
+out_cleanup_admin_q:
+	blk_cleanup_queue(ctrl->ctrl.admin_q);
+out_free_admin_tag_set:
+	blk_mq_free_tag_set(&ctrl->admin_tag_set);
+out_free_queues:
+	kfree(ctrl->queues);
+out_free_ida:
+	put_device(ctrl->dev);
+	ida_simple_remove(&nvme_fc_ctrl_cnt, ctrl->cnum);
+out_free_ctrl:
+	kfree(ctrl);
+out_fail:
+	/* exit via here doesn't follow ctlr ref points */
+	return ERR_PTR(ret);
+}
+
+
+struct nvmet_fc_traddr {
+	u64	nn;
+	u64	pn;
+};
+
+static int
+__nvme_fc_parse_u64(substring_t *sstr, u64 *val)
+{
+	u64 token64;
+
+	if (match_u64(sstr, &token64))
+		return -EINVAL;
+	*val = token64;
+
+	return 0;
+}
+
+/*
+ * This routine validates and extracts the WWN's from the TRADDR string.
+ * As kernel parsers need the 0x to determine number base, universally
+ * build string to parse with 0x prefix before parsing name strings.
+ */
+static int
+nvme_fc_parse_traddr(struct nvmet_fc_traddr *traddr, char *buf, size_t blen)
+{
+	char name[2 + NVME_FC_TRADDR_HEXNAMELEN + 1];
+	substring_t wwn = { name, &name[sizeof(name)-1] };
+	int nnoffset, pnoffset;
+
+	/* validate if string is one of the 2 allowed formats */
+	if (strnlen(buf, blen) == NVME_FC_TRADDR_MAXLENGTH &&
+			!strncmp(buf, "nn-0x", NVME_FC_TRADDR_OXNNLEN) &&
+			!strncmp(&buf[NVME_FC_TRADDR_MAX_PN_OFFSET],
+				"pn-0x", NVME_FC_TRADDR_OXNNLEN)) {
+		nnoffset = NVME_FC_TRADDR_OXNNLEN;
+		pnoffset = NVME_FC_TRADDR_MAX_PN_OFFSET +
+						NVME_FC_TRADDR_OXNNLEN;
+	} else if ((strnlen(buf, blen) == NVME_FC_TRADDR_MINLENGTH &&
+			!strncmp(buf, "nn-", NVME_FC_TRADDR_NNLEN) &&
+			!strncmp(&buf[NVME_FC_TRADDR_MIN_PN_OFFSET],
+				"pn-", NVME_FC_TRADDR_NNLEN))) {
+		nnoffset = NVME_FC_TRADDR_NNLEN;
+		pnoffset = NVME_FC_TRADDR_MIN_PN_OFFSET + NVME_FC_TRADDR_NNLEN;
+	} else
+		goto out_einval;
+
+	name[0] = '0';
+	name[1] = 'x';
+	name[2 + NVME_FC_TRADDR_HEXNAMELEN] = 0;
+
+	memcpy(&name[2], &buf[nnoffset], NVME_FC_TRADDR_HEXNAMELEN);
+	if (__nvme_fc_parse_u64(&wwn, &traddr->nn))
+		goto out_einval;
+
+	memcpy(&name[2], &buf[pnoffset], NVME_FC_TRADDR_HEXNAMELEN);
+	if (__nvme_fc_parse_u64(&wwn, &traddr->pn))
+		goto out_einval;
+
+	return 0;
+
+out_einval:
+	pr_warn("%s: bad traddr string\n", __func__);
+	return -EINVAL;
+}
+
+static struct nvme_ctrl *
+nvme_fc_create_ctrl(struct device *dev, struct nvmf_ctrl_options *opts)
+{
+	struct nvme_fc_lport *lport;
+	struct nvme_fc_rport *rport;
+	struct nvme_ctrl *ctrl;
+	struct nvmet_fc_traddr laddr = { 0L, 0L };
+	struct nvmet_fc_traddr raddr = { 0L, 0L };
+	unsigned long flags;
+	int ret;
+
+	ret = nvme_fc_parse_traddr(&raddr, opts->traddr, NVMF_TRADDR_SIZE);
+	if (ret || !raddr.nn || !raddr.pn)
+		return ERR_PTR(-EINVAL);
+
+	ret = nvme_fc_parse_traddr(&laddr, opts->host_traddr, NVMF_TRADDR_SIZE);
+	if (ret || !laddr.nn || !laddr.pn)
+		return ERR_PTR(-EINVAL);
+
+	/* find the host and remote ports to connect together */
+	spin_lock_irqsave(&nvme_fc_lock, flags);
+	list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+		if (lport->localport.node_name != laddr.nn ||
+		    lport->localport.port_name != laddr.pn)
+			continue;
+
+		list_for_each_entry(rport, &lport->endp_list, endp_list) {
+			if (rport->remoteport.node_name != raddr.nn ||
+			    rport->remoteport.port_name != raddr.pn)
+				continue;
+
+			/* if fail to get reference fall through. Will error */
+			if (!nvme_fc_rport_get(rport))
+				break;
+
+			spin_unlock_irqrestore(&nvme_fc_lock, flags);
+
+			ctrl = nvme_fc_init_ctrl(dev, opts, lport, rport);
+			if (IS_ERR(ctrl))
+				nvme_fc_rport_put(rport);
+			return ctrl;
+		}
+	}
+	spin_unlock_irqrestore(&nvme_fc_lock, flags);
+
+	pr_warn("%s: %s - %s combination not found\n",
+		__func__, opts->traddr, opts->host_traddr);
+	return ERR_PTR(-ENOENT);
+}
+
+
+static struct nvmf_transport_ops nvme_fc_transport = {
+	.name		= "fc",
+	.module		= THIS_MODULE,
+	.required_opts	= NVMF_OPT_TRADDR | NVMF_OPT_HOST_TRADDR,
+	.allowed_opts	= NVMF_OPT_RECONNECT_DELAY | NVMF_OPT_CTRL_LOSS_TMO,
+	.create_ctrl	= nvme_fc_create_ctrl,
+};
+
+/* Arbitrary successive failures max. With lots of subsystems could be high */
+#define DISCOVERY_MAX_FAIL	20
+
+static ssize_t nvme_fc_nvme_discovery_store(struct device *dev,
+		struct device_attribute *attr, const char *buf, size_t count)
+{
+	unsigned long flags;
+	LIST_HEAD(local_disc_list);
+	struct nvme_fc_lport *lport;
+	struct nvme_fc_rport *rport;
+	int failcnt = 0;
+
+	spin_lock_irqsave(&nvme_fc_lock, flags);
+restart:
+	list_for_each_entry(lport, &nvme_fc_lport_list, port_list) {
+		list_for_each_entry(rport, &lport->endp_list, endp_list) {
+			if (!nvme_fc_lport_get(lport))
+				continue;
+			if (!nvme_fc_rport_get(rport)) {
+				/*
+				 * This is a temporary condition. Upon restart
+				 * this rport will be gone from the list.
+				 *
+				 * Revert the lport put and retry.  Anything
+				 * added to the list already will be skipped (as
+				 * they are no longer list_empty).  Loops should
+				 * resume at rports that were not yet seen.
+				 */
+				nvme_fc_lport_put(lport);
+
+				if (failcnt++ < DISCOVERY_MAX_FAIL)
+					goto restart;
+
+				pr_err("nvme_discovery: too many reference "
+				       "failures\n");
+				goto process_local_list;
+			}
+			if (list_empty(&rport->disc_list))
+				list_add_tail(&rport->disc_list,
+					      &local_disc_list);
+		}
+	}
+
+process_local_list:
+	while (!list_empty(&local_disc_list)) {
+		rport = list_first_entry(&local_disc_list,
+					 struct nvme_fc_rport, disc_list);
+		list_del_init(&rport->disc_list);
+		spin_unlock_irqrestore(&nvme_fc_lock, flags);
+
+		lport = rport->lport;
+		/* signal discovery. Won't hurt if it repeats */
+		nvme_fc_signal_discovery_scan(lport, rport);
+		nvme_fc_rport_put(rport);
+		nvme_fc_lport_put(lport);
+
+		spin_lock_irqsave(&nvme_fc_lock, flags);
+	}
+	spin_unlock_irqrestore(&nvme_fc_lock, flags);
+
+	return count;
+}
+static DEVICE_ATTR(nvme_discovery, 0200, NULL, nvme_fc_nvme_discovery_store);
+
+static struct attribute *nvme_fc_attrs[] = {
+	&dev_attr_nvme_discovery.attr,
+	NULL
+};
+
+static struct attribute_group nvme_fc_attr_group = {
+	.attrs = nvme_fc_attrs,
+};
+
+static const struct attribute_group *nvme_fc_attr_groups[] = {
+	&nvme_fc_attr_group,
+	NULL
+};
+
+static struct class fc_class = {
+	.name = "fc",
+	.dev_groups = nvme_fc_attr_groups,
+	.owner = THIS_MODULE,
+};
+
+static int __init nvme_fc_init_module(void)
+{
+	int ret;
+
+	/*
+	 * NOTE:
+	 * It is expected that in the future the kernel will combine
+	 * the FC-isms that are currently under scsi and now being
+	 * added to by NVME into a new standalone FC class. The SCSI
+	 * and NVME protocols and their devices would be under this
+	 * new FC class.
+	 *
+	 * As we need something to post FC-specific udev events to,
+	 * specifically for nvme probe events, start by creating the
+	 * new device class.  When the new standalone FC class is
+	 * put in place, this code will move to a more generic
+	 * location for the class.
+	 */
+	ret = class_register(&fc_class);
+	if (ret) {
+		pr_err("couldn't register class fc\n");
+		return ret;
+	}
+
+	/*
+	 * Create a device for the FC-centric udev events
+	 */
+	fc_udev_device = device_create(&fc_class, NULL, MKDEV(0, 0), NULL,
+				"fc_udev_device");
+	if (IS_ERR(fc_udev_device)) {
+		pr_err("couldn't create fc_udev device!\n");
+		ret = PTR_ERR(fc_udev_device);
+		goto out_destroy_class;
+	}
+
+	ret = nvmf_register_transport(&nvme_fc_transport);
+	if (ret)
+		goto out_destroy_device;
+
+	return 0;
+
+out_destroy_device:
+	device_destroy(&fc_class, MKDEV(0, 0));
+out_destroy_class:
+	class_unregister(&fc_class);
+	return ret;
+}
+
+static void __exit nvme_fc_exit_module(void)
+{
+	/* sanity check - all lports should be removed */
+	if (!list_empty(&nvme_fc_lport_list))
+		pr_warn("%s: localport list not empty\n", __func__);
+
+	nvmf_unregister_transport(&nvme_fc_transport);
+
+	ida_destroy(&nvme_fc_local_port_cnt);
+	ida_destroy(&nvme_fc_ctrl_cnt);
+
+	device_destroy(&fc_class, MKDEV(0, 0));
+	class_unregister(&fc_class);
+}
+
+module_init(nvme_fc_init_module);
+module_exit(nvme_fc_exit_module);
+
+MODULE_LICENSE("GPL v2");
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/lightnvm.c b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/lightnvm.c
new file mode 100644
index 0000000..949e29e
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/lightnvm.c
@@ -0,0 +1,1286 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * nvme-lightnvm.c - LightNVM NVMe device
+ *
+ * Copyright (C) 2014-2015 IT University of Copenhagen
+ * Initial release: Matias Bjorling <mb@lightnvm.io>
+ */
+
+#include "nvme.h"
+
+#include <linux/nvme.h>
+#include <linux/bitops.h>
+#include <linux/lightnvm.h>
+#include <linux/vmalloc.h>
+#include <linux/sched/sysctl.h>
+#include <uapi/linux/lightnvm.h>
+
+enum nvme_nvm_admin_opcode {
+	nvme_nvm_admin_identity		= 0xe2,
+	nvme_nvm_admin_get_bb_tbl	= 0xf2,
+	nvme_nvm_admin_set_bb_tbl	= 0xf1,
+};
+
+enum nvme_nvm_log_page {
+	NVME_NVM_LOG_REPORT_CHUNK	= 0xca,
+};
+
+struct nvme_nvm_ph_rw {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd2;
+	__le64			metadata;
+	__le64			prp1;
+	__le64			prp2;
+	__le64			spba;
+	__le16			length;
+	__le16			control;
+	__le32			dsmgmt;
+	__le64			resv;
+};
+
+struct nvme_nvm_erase_blk {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd[2];
+	__le64			prp1;
+	__le64			prp2;
+	__le64			spba;
+	__le16			length;
+	__le16			control;
+	__le32			dsmgmt;
+	__le64			resv;
+};
+
+struct nvme_nvm_identity {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd[2];
+	__le64			prp1;
+	__le64			prp2;
+	__u32			rsvd11[6];
+};
+
+struct nvme_nvm_getbbtbl {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd[2];
+	__le64			prp1;
+	__le64			prp2;
+	__le64			spba;
+	__u32			rsvd4[4];
+};
+
+struct nvme_nvm_setbbtbl {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__le64			rsvd[2];
+	__le64			prp1;
+	__le64			prp2;
+	__le64			spba;
+	__le16			nlb;
+	__u8			value;
+	__u8			rsvd3;
+	__u32			rsvd4[3];
+};
+
+struct nvme_nvm_command {
+	union {
+		struct nvme_common_command common;
+		struct nvme_nvm_ph_rw ph_rw;
+		struct nvme_nvm_erase_blk erase;
+		struct nvme_nvm_identity identity;
+		struct nvme_nvm_getbbtbl get_bb;
+		struct nvme_nvm_setbbtbl set_bb;
+	};
+};
+
+struct nvme_nvm_id12_grp {
+	__u8			mtype;
+	__u8			fmtype;
+	__le16			res16;
+	__u8			num_ch;
+	__u8			num_lun;
+	__u8			num_pln;
+	__u8			rsvd1;
+	__le16			num_chk;
+	__le16			num_pg;
+	__le16			fpg_sz;
+	__le16			csecs;
+	__le16			sos;
+	__le16			rsvd2;
+	__le32			trdt;
+	__le32			trdm;
+	__le32			tprt;
+	__le32			tprm;
+	__le32			tbet;
+	__le32			tbem;
+	__le32			mpos;
+	__le32			mccap;
+	__le16			cpar;
+	__u8			reserved[906];
+} __packed;
+
+struct nvme_nvm_id12_addrf {
+	__u8			ch_offset;
+	__u8			ch_len;
+	__u8			lun_offset;
+	__u8			lun_len;
+	__u8			pln_offset;
+	__u8			pln_len;
+	__u8			blk_offset;
+	__u8			blk_len;
+	__u8			pg_offset;
+	__u8			pg_len;
+	__u8			sec_offset;
+	__u8			sec_len;
+	__u8			res[4];
+} __packed;
+
+struct nvme_nvm_id12 {
+	__u8			ver_id;
+	__u8			vmnt;
+	__u8			cgrps;
+	__u8			res;
+	__le32			cap;
+	__le32			dom;
+	struct nvme_nvm_id12_addrf ppaf;
+	__u8			resv[228];
+	struct nvme_nvm_id12_grp grp;
+	__u8			resv2[2880];
+} __packed;
+
+struct nvme_nvm_bb_tbl {
+	__u8	tblid[4];
+	__le16	verid;
+	__le16	revid;
+	__le32	rvsd1;
+	__le32	tblks;
+	__le32	tfact;
+	__le32	tgrown;
+	__le32	tdresv;
+	__le32	thresv;
+	__le32	rsvd2[8];
+	__u8	blk[0];
+};
+
+struct nvme_nvm_id20_addrf {
+	__u8			grp_len;
+	__u8			pu_len;
+	__u8			chk_len;
+	__u8			lba_len;
+	__u8			resv[4];
+};
+
+struct nvme_nvm_id20 {
+	__u8			mjr;
+	__u8			mnr;
+	__u8			resv[6];
+
+	struct nvme_nvm_id20_addrf lbaf;
+
+	__le32			mccap;
+	__u8			resv2[12];
+
+	__u8			wit;
+	__u8			resv3[31];
+
+	/* Geometry */
+	__le16			num_grp;
+	__le16			num_pu;
+	__le32			num_chk;
+	__le32			clba;
+	__u8			resv4[52];
+
+	/* Write data requirements */
+	__le32			ws_min;
+	__le32			ws_opt;
+	__le32			mw_cunits;
+	__le32			maxoc;
+	__le32			maxocpu;
+	__u8			resv5[44];
+
+	/* Performance related metrics */
+	__le32			trdt;
+	__le32			trdm;
+	__le32			twrt;
+	__le32			twrm;
+	__le32			tcrst;
+	__le32			tcrsm;
+	__u8			resv6[40];
+
+	/* Reserved area */
+	__u8			resv7[2816];
+
+	/* Vendor specific */
+	__u8			vs[1024];
+};
+
+struct nvme_nvm_chk_meta {
+	__u8	state;
+	__u8	type;
+	__u8	wi;
+	__u8	rsvd[5];
+	__le64	slba;
+	__le64	cnlb;
+	__le64	wp;
+};
+
+/*
+ * Check we didn't inadvertently grow the command struct
+ */
+static inline void _nvme_nvm_check_size(void)
+{
+	BUILD_BUG_ON(sizeof(struct nvme_nvm_identity) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_nvm_ph_rw) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_nvm_erase_blk) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_nvm_getbbtbl) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_nvm_setbbtbl) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_nvm_id12_grp) != 960);
+	BUILD_BUG_ON(sizeof(struct nvme_nvm_id12_addrf) != 16);
+	BUILD_BUG_ON(sizeof(struct nvme_nvm_id12) != NVME_IDENTIFY_DATA_SIZE);
+	BUILD_BUG_ON(sizeof(struct nvme_nvm_bb_tbl) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_nvm_id20_addrf) != 8);
+	BUILD_BUG_ON(sizeof(struct nvme_nvm_id20) != NVME_IDENTIFY_DATA_SIZE);
+	BUILD_BUG_ON(sizeof(struct nvme_nvm_chk_meta) != 32);
+	BUILD_BUG_ON(sizeof(struct nvme_nvm_chk_meta) !=
+						sizeof(struct nvm_chk_meta));
+}
+
+static void nvme_nvm_set_addr_12(struct nvm_addrf_12 *dst,
+				 struct nvme_nvm_id12_addrf *src)
+{
+	dst->ch_len = src->ch_len;
+	dst->lun_len = src->lun_len;
+	dst->blk_len = src->blk_len;
+	dst->pg_len = src->pg_len;
+	dst->pln_len = src->pln_len;
+	dst->sec_len = src->sec_len;
+
+	dst->ch_offset = src->ch_offset;
+	dst->lun_offset = src->lun_offset;
+	dst->blk_offset = src->blk_offset;
+	dst->pg_offset = src->pg_offset;
+	dst->pln_offset = src->pln_offset;
+	dst->sec_offset = src->sec_offset;
+
+	dst->ch_mask = ((1ULL << dst->ch_len) - 1) << dst->ch_offset;
+	dst->lun_mask = ((1ULL << dst->lun_len) - 1) << dst->lun_offset;
+	dst->blk_mask = ((1ULL << dst->blk_len) - 1) << dst->blk_offset;
+	dst->pg_mask = ((1ULL << dst->pg_len) - 1) << dst->pg_offset;
+	dst->pln_mask = ((1ULL << dst->pln_len) - 1) << dst->pln_offset;
+	dst->sec_mask = ((1ULL << dst->sec_len) - 1) << dst->sec_offset;
+}
+
+static int nvme_nvm_setup_12(struct nvme_nvm_id12 *id,
+			     struct nvm_geo *geo)
+{
+	struct nvme_nvm_id12_grp *src;
+	int sec_per_pg, sec_per_pl, pg_per_blk;
+
+	if (id->cgrps != 1)
+		return -EINVAL;
+
+	src = &id->grp;
+
+	if (src->mtype != 0) {
+		pr_err("nvm: memory type not supported\n");
+		return -EINVAL;
+	}
+
+	/* 1.2 spec. only reports a single version id - unfold */
+	geo->major_ver_id = id->ver_id;
+	geo->minor_ver_id = 2;
+
+	/* Set compacted version for upper layers */
+	geo->version = NVM_OCSSD_SPEC_12;
+
+	geo->num_ch = src->num_ch;
+	geo->num_lun = src->num_lun;
+	geo->all_luns = geo->num_ch * geo->num_lun;
+
+	geo->num_chk = le16_to_cpu(src->num_chk);
+
+	geo->csecs = le16_to_cpu(src->csecs);
+	geo->sos = le16_to_cpu(src->sos);
+
+	pg_per_blk = le16_to_cpu(src->num_pg);
+	sec_per_pg = le16_to_cpu(src->fpg_sz) / geo->csecs;
+	sec_per_pl = sec_per_pg * src->num_pln;
+	geo->clba = sec_per_pl * pg_per_blk;
+
+	geo->all_chunks = geo->all_luns * geo->num_chk;
+	geo->total_secs = geo->clba * geo->all_chunks;
+
+	geo->ws_min = sec_per_pg;
+	geo->ws_opt = sec_per_pg;
+	geo->mw_cunits = geo->ws_opt << 3;	/* default to MLC safe values */
+
+	/* Do not impose values for maximum number of open blocks as it is
+	 * unspecified in 1.2. Users of 1.2 must be aware of this and eventually
+	 * specify these values through a quirk if restrictions apply.
+	 */
+	geo->maxoc = geo->all_luns * geo->num_chk;
+	geo->maxocpu = geo->num_chk;
+
+	geo->mccap = le32_to_cpu(src->mccap);
+
+	geo->trdt = le32_to_cpu(src->trdt);
+	geo->trdm = le32_to_cpu(src->trdm);
+	geo->tprt = le32_to_cpu(src->tprt);
+	geo->tprm = le32_to_cpu(src->tprm);
+	geo->tbet = le32_to_cpu(src->tbet);
+	geo->tbem = le32_to_cpu(src->tbem);
+
+	/* 1.2 compatibility */
+	geo->vmnt = id->vmnt;
+	geo->cap = le32_to_cpu(id->cap);
+	geo->dom = le32_to_cpu(id->dom);
+
+	geo->mtype = src->mtype;
+	geo->fmtype = src->fmtype;
+
+	geo->cpar = le16_to_cpu(src->cpar);
+	geo->mpos = le32_to_cpu(src->mpos);
+
+	geo->pln_mode = NVM_PLANE_SINGLE;
+
+	if (geo->mpos & 0x020202) {
+		geo->pln_mode = NVM_PLANE_DOUBLE;
+		geo->ws_opt <<= 1;
+	} else if (geo->mpos & 0x040404) {
+		geo->pln_mode = NVM_PLANE_QUAD;
+		geo->ws_opt <<= 2;
+	}
+
+	geo->num_pln = src->num_pln;
+	geo->num_pg = le16_to_cpu(src->num_pg);
+	geo->fpg_sz = le16_to_cpu(src->fpg_sz);
+
+	nvme_nvm_set_addr_12((struct nvm_addrf_12 *)&geo->addrf, &id->ppaf);
+
+	return 0;
+}
+
+static void nvme_nvm_set_addr_20(struct nvm_addrf *dst,
+				 struct nvme_nvm_id20_addrf *src)
+{
+	dst->ch_len = src->grp_len;
+	dst->lun_len = src->pu_len;
+	dst->chk_len = src->chk_len;
+	dst->sec_len = src->lba_len;
+
+	dst->sec_offset = 0;
+	dst->chk_offset = dst->sec_len;
+	dst->lun_offset = dst->chk_offset + dst->chk_len;
+	dst->ch_offset = dst->lun_offset + dst->lun_len;
+
+	dst->ch_mask = ((1ULL << dst->ch_len) - 1) << dst->ch_offset;
+	dst->lun_mask = ((1ULL << dst->lun_len) - 1) << dst->lun_offset;
+	dst->chk_mask = ((1ULL << dst->chk_len) - 1) << dst->chk_offset;
+	dst->sec_mask = ((1ULL << dst->sec_len) - 1) << dst->sec_offset;
+}
+
+static int nvme_nvm_setup_20(struct nvme_nvm_id20 *id,
+			     struct nvm_geo *geo)
+{
+	geo->major_ver_id = id->mjr;
+	geo->minor_ver_id = id->mnr;
+
+	/* Set compacted version for upper layers */
+	geo->version = NVM_OCSSD_SPEC_20;
+
+	geo->num_ch = le16_to_cpu(id->num_grp);
+	geo->num_lun = le16_to_cpu(id->num_pu);
+	geo->all_luns = geo->num_ch * geo->num_lun;
+
+	geo->num_chk = le32_to_cpu(id->num_chk);
+	geo->clba = le32_to_cpu(id->clba);
+
+	geo->all_chunks = geo->all_luns * geo->num_chk;
+	geo->total_secs = geo->clba * geo->all_chunks;
+
+	geo->ws_min = le32_to_cpu(id->ws_min);
+	geo->ws_opt = le32_to_cpu(id->ws_opt);
+	geo->mw_cunits = le32_to_cpu(id->mw_cunits);
+	geo->maxoc = le32_to_cpu(id->maxoc);
+	geo->maxocpu = le32_to_cpu(id->maxocpu);
+
+	geo->trdt = le32_to_cpu(id->trdt);
+	geo->trdm = le32_to_cpu(id->trdm);
+	geo->tprt = le32_to_cpu(id->twrt);
+	geo->tprm = le32_to_cpu(id->twrm);
+	geo->tbet = le32_to_cpu(id->tcrst);
+	geo->tbem = le32_to_cpu(id->tcrsm);
+
+	nvme_nvm_set_addr_20(&geo->addrf, &id->lbaf);
+
+	return 0;
+}
+
+static int nvme_nvm_identity(struct nvm_dev *nvmdev)
+{
+	struct nvme_ns *ns = nvmdev->q->queuedata;
+	struct nvme_nvm_id12 *id;
+	struct nvme_nvm_command c = {};
+	int ret;
+
+	c.identity.opcode = nvme_nvm_admin_identity;
+	c.identity.nsid = cpu_to_le32(ns->head->ns_id);
+
+	id = kmalloc(sizeof(struct nvme_nvm_id12), GFP_KERNEL);
+	if (!id)
+		return -ENOMEM;
+
+	ret = nvme_submit_sync_cmd(ns->ctrl->admin_q, (struct nvme_command *)&c,
+				id, sizeof(struct nvme_nvm_id12));
+	if (ret) {
+		ret = -EIO;
+		goto out;
+	}
+
+	/*
+	 * The 1.2 and 2.0 specifications share the first byte in their geometry
+	 * command to make it possible to know what version a device implements.
+	 */
+	switch (id->ver_id) {
+	case 1:
+		ret = nvme_nvm_setup_12(id, &nvmdev->geo);
+		break;
+	case 2:
+		ret = nvme_nvm_setup_20((struct nvme_nvm_id20 *)id,
+							&nvmdev->geo);
+		break;
+	default:
+		dev_err(ns->ctrl->device, "OCSSD revision not supported (%d)\n",
+							id->ver_id);
+		ret = -EINVAL;
+	}
+
+out:
+	kfree(id);
+	return ret;
+}
+
+static int nvme_nvm_get_bb_tbl(struct nvm_dev *nvmdev, struct ppa_addr ppa,
+								u8 *blks)
+{
+	struct request_queue *q = nvmdev->q;
+	struct nvm_geo *geo = &nvmdev->geo;
+	struct nvme_ns *ns = q->queuedata;
+	struct nvme_ctrl *ctrl = ns->ctrl;
+	struct nvme_nvm_command c = {};
+	struct nvme_nvm_bb_tbl *bb_tbl;
+	int nr_blks = geo->num_chk * geo->num_pln;
+	int tblsz = sizeof(struct nvme_nvm_bb_tbl) + nr_blks;
+	int ret = 0;
+
+	c.get_bb.opcode = nvme_nvm_admin_get_bb_tbl;
+	c.get_bb.nsid = cpu_to_le32(ns->head->ns_id);
+	c.get_bb.spba = cpu_to_le64(ppa.ppa);
+
+	bb_tbl = kzalloc(tblsz, GFP_KERNEL);
+	if (!bb_tbl)
+		return -ENOMEM;
+
+	ret = nvme_submit_sync_cmd(ctrl->admin_q, (struct nvme_command *)&c,
+								bb_tbl, tblsz);
+	if (ret) {
+		dev_err(ctrl->device, "get bad block table failed (%d)\n", ret);
+		ret = -EIO;
+		goto out;
+	}
+
+	if (bb_tbl->tblid[0] != 'B' || bb_tbl->tblid[1] != 'B' ||
+		bb_tbl->tblid[2] != 'L' || bb_tbl->tblid[3] != 'T') {
+		dev_err(ctrl->device, "bbt format mismatch\n");
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (le16_to_cpu(bb_tbl->verid) != 1) {
+		ret = -EINVAL;
+		dev_err(ctrl->device, "bbt version not supported\n");
+		goto out;
+	}
+
+	if (le32_to_cpu(bb_tbl->tblks) != nr_blks) {
+		ret = -EINVAL;
+		dev_err(ctrl->device,
+				"bbt unsuspected blocks returned (%u!=%u)",
+				le32_to_cpu(bb_tbl->tblks), nr_blks);
+		goto out;
+	}
+
+	memcpy(blks, bb_tbl->blk, geo->num_chk * geo->num_pln);
+out:
+	kfree(bb_tbl);
+	return ret;
+}
+
+static int nvme_nvm_set_bb_tbl(struct nvm_dev *nvmdev, struct ppa_addr *ppas,
+							int nr_ppas, int type)
+{
+	struct nvme_ns *ns = nvmdev->q->queuedata;
+	struct nvme_nvm_command c = {};
+	int ret = 0;
+
+	c.set_bb.opcode = nvme_nvm_admin_set_bb_tbl;
+	c.set_bb.nsid = cpu_to_le32(ns->head->ns_id);
+	c.set_bb.spba = cpu_to_le64(ppas->ppa);
+	c.set_bb.nlb = cpu_to_le16(nr_ppas - 1);
+	c.set_bb.value = type;
+
+	ret = nvme_submit_sync_cmd(ns->ctrl->admin_q, (struct nvme_command *)&c,
+								NULL, 0);
+	if (ret)
+		dev_err(ns->ctrl->device, "set bad block table failed (%d)\n",
+									ret);
+	return ret;
+}
+
+/*
+ * Expect the lba in device format
+ */
+static int nvme_nvm_get_chk_meta(struct nvm_dev *ndev,
+				 sector_t slba, int nchks,
+				 struct nvm_chk_meta *meta)
+{
+	struct nvm_geo *geo = &ndev->geo;
+	struct nvme_ns *ns = ndev->q->queuedata;
+	struct nvme_ctrl *ctrl = ns->ctrl;
+	struct nvme_nvm_chk_meta *dev_meta, *dev_meta_off;
+	struct ppa_addr ppa;
+	size_t left = nchks * sizeof(struct nvme_nvm_chk_meta);
+	size_t log_pos, offset, len;
+	int i, max_len;
+	int ret = 0;
+
+	/*
+	 * limit requests to maximum 256K to avoid issuing arbitrary large
+	 * requests when the device does not specific a maximum transfer size.
+	 */
+	max_len = min_t(unsigned int, ctrl->max_hw_sectors << 9, 256 * 1024);
+
+	dev_meta = kmalloc(max_len, GFP_KERNEL);
+	if (!dev_meta)
+		return -ENOMEM;
+
+	/* Normalize lba address space to obtain log offset */
+	ppa.ppa = slba;
+	ppa = dev_to_generic_addr(ndev, ppa);
+
+	log_pos = ppa.m.chk;
+	log_pos += ppa.m.pu * geo->num_chk;
+	log_pos += ppa.m.grp * geo->num_lun * geo->num_chk;
+
+	offset = log_pos * sizeof(struct nvme_nvm_chk_meta);
+
+	while (left) {
+		len = min_t(unsigned int, left, max_len);
+
+		memset(dev_meta, 0, max_len);
+		dev_meta_off = dev_meta;
+
+		ret = nvme_get_log(ctrl, ns->head->ns_id,
+				NVME_NVM_LOG_REPORT_CHUNK, 0, dev_meta, len,
+				offset);
+		if (ret) {
+			dev_err(ctrl->device, "Get REPORT CHUNK log error\n");
+			break;
+		}
+
+		for (i = 0; i < len; i += sizeof(struct nvme_nvm_chk_meta)) {
+			meta->state = dev_meta_off->state;
+			meta->type = dev_meta_off->type;
+			meta->wi = dev_meta_off->wi;
+			meta->slba = le64_to_cpu(dev_meta_off->slba);
+			meta->cnlb = le64_to_cpu(dev_meta_off->cnlb);
+			meta->wp = le64_to_cpu(dev_meta_off->wp);
+
+			meta++;
+			dev_meta_off++;
+		}
+
+		offset += len;
+		left -= len;
+	}
+
+	kfree(dev_meta);
+
+	return ret;
+}
+
+static inline void nvme_nvm_rqtocmd(struct nvm_rq *rqd, struct nvme_ns *ns,
+				    struct nvme_nvm_command *c)
+{
+	c->ph_rw.opcode = rqd->opcode;
+	c->ph_rw.nsid = cpu_to_le32(ns->head->ns_id);
+	c->ph_rw.spba = cpu_to_le64(rqd->ppa_addr.ppa);
+	c->ph_rw.metadata = cpu_to_le64(rqd->dma_meta_list);
+	c->ph_rw.control = cpu_to_le16(rqd->flags);
+	c->ph_rw.length = cpu_to_le16(rqd->nr_ppas - 1);
+}
+
+static void nvme_nvm_end_io(struct request *rq, blk_status_t status)
+{
+	struct nvm_rq *rqd = rq->end_io_data;
+
+	rqd->ppa_status = le64_to_cpu(nvme_req(rq)->result.u64);
+	rqd->error = nvme_req(rq)->status;
+	nvm_end_io(rqd);
+
+	kfree(nvme_req(rq)->cmd);
+	blk_mq_free_request(rq);
+}
+
+static struct request *nvme_nvm_alloc_request(struct request_queue *q,
+					      struct nvm_rq *rqd,
+					      struct nvme_nvm_command *cmd)
+{
+	struct nvme_ns *ns = q->queuedata;
+	struct request *rq;
+
+	nvme_nvm_rqtocmd(rqd, ns, cmd);
+
+	rq = nvme_alloc_request(q, (struct nvme_command *)cmd, 0, NVME_QID_ANY);
+	if (IS_ERR(rq))
+		return rq;
+
+	rq->cmd_flags &= ~REQ_FAILFAST_DRIVER;
+
+	if (rqd->bio)
+		blk_init_request_from_bio(rq, rqd->bio);
+	else
+		rq->ioprio = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_BE, IOPRIO_NORM);
+
+	return rq;
+}
+
+static int nvme_nvm_submit_io(struct nvm_dev *dev, struct nvm_rq *rqd)
+{
+	struct request_queue *q = dev->q;
+	struct nvme_nvm_command *cmd;
+	struct request *rq;
+
+	cmd = kzalloc(sizeof(struct nvme_nvm_command), GFP_KERNEL);
+	if (!cmd)
+		return -ENOMEM;
+
+	rq = nvme_nvm_alloc_request(q, rqd, cmd);
+	if (IS_ERR(rq)) {
+		kfree(cmd);
+		return PTR_ERR(rq);
+	}
+
+	rq->end_io_data = rqd;
+
+	blk_execute_rq_nowait(q, NULL, rq, 0, nvme_nvm_end_io);
+
+	return 0;
+}
+
+static int nvme_nvm_submit_io_sync(struct nvm_dev *dev, struct nvm_rq *rqd)
+{
+	struct request_queue *q = dev->q;
+	struct request *rq;
+	struct nvme_nvm_command cmd;
+	int ret = 0;
+
+	memset(&cmd, 0, sizeof(struct nvme_nvm_command));
+
+	rq = nvme_nvm_alloc_request(q, rqd, &cmd);
+	if (IS_ERR(rq))
+		return PTR_ERR(rq);
+
+	/* I/Os can fail and the error is signaled through rqd. Callers must
+	 * handle the error accordingly.
+	 */
+	blk_execute_rq(q, NULL, rq, 0);
+	if (nvme_req(rq)->flags & NVME_REQ_CANCELLED)
+		ret = -EINTR;
+
+	rqd->ppa_status = le64_to_cpu(nvme_req(rq)->result.u64);
+	rqd->error = nvme_req(rq)->status;
+
+	blk_mq_free_request(rq);
+
+	return ret;
+}
+
+static void *nvme_nvm_create_dma_pool(struct nvm_dev *nvmdev, char *name,
+					int size)
+{
+	struct nvme_ns *ns = nvmdev->q->queuedata;
+
+	return dma_pool_create(name, ns->ctrl->dev, size, PAGE_SIZE, 0);
+}
+
+static void nvme_nvm_destroy_dma_pool(void *pool)
+{
+	struct dma_pool *dma_pool = pool;
+
+	dma_pool_destroy(dma_pool);
+}
+
+static void *nvme_nvm_dev_dma_alloc(struct nvm_dev *dev, void *pool,
+				    gfp_t mem_flags, dma_addr_t *dma_handler)
+{
+	return dma_pool_alloc(pool, mem_flags, dma_handler);
+}
+
+static void nvme_nvm_dev_dma_free(void *pool, void *addr,
+							dma_addr_t dma_handler)
+{
+	dma_pool_free(pool, addr, dma_handler);
+}
+
+static struct nvm_dev_ops nvme_nvm_dev_ops = {
+	.identity		= nvme_nvm_identity,
+
+	.get_bb_tbl		= nvme_nvm_get_bb_tbl,
+	.set_bb_tbl		= nvme_nvm_set_bb_tbl,
+
+	.get_chk_meta		= nvme_nvm_get_chk_meta,
+
+	.submit_io		= nvme_nvm_submit_io,
+	.submit_io_sync		= nvme_nvm_submit_io_sync,
+
+	.create_dma_pool	= nvme_nvm_create_dma_pool,
+	.destroy_dma_pool	= nvme_nvm_destroy_dma_pool,
+	.dev_dma_alloc		= nvme_nvm_dev_dma_alloc,
+	.dev_dma_free		= nvme_nvm_dev_dma_free,
+};
+
+static int nvme_nvm_submit_user_cmd(struct request_queue *q,
+				struct nvme_ns *ns,
+				struct nvme_nvm_command *vcmd,
+				void __user *ubuf, unsigned int bufflen,
+				void __user *meta_buf, unsigned int meta_len,
+				void __user *ppa_buf, unsigned int ppa_len,
+				u32 *result, u64 *status, unsigned int timeout)
+{
+	bool write = nvme_is_write((struct nvme_command *)vcmd);
+	struct nvm_dev *dev = ns->ndev;
+	struct gendisk *disk = ns->disk;
+	struct request *rq;
+	struct bio *bio = NULL;
+	__le64 *ppa_list = NULL;
+	dma_addr_t ppa_dma;
+	__le64 *metadata = NULL;
+	dma_addr_t metadata_dma;
+	DECLARE_COMPLETION_ONSTACK(wait);
+	int ret = 0;
+
+	rq = nvme_alloc_request(q, (struct nvme_command *)vcmd, 0,
+			NVME_QID_ANY);
+	if (IS_ERR(rq)) {
+		ret = -ENOMEM;
+		goto err_cmd;
+	}
+
+	rq->timeout = timeout ? timeout : ADMIN_TIMEOUT;
+
+	if (ppa_buf && ppa_len) {
+		ppa_list = dma_pool_alloc(dev->dma_pool, GFP_KERNEL, &ppa_dma);
+		if (!ppa_list) {
+			ret = -ENOMEM;
+			goto err_rq;
+		}
+		if (copy_from_user(ppa_list, (void __user *)ppa_buf,
+						sizeof(u64) * (ppa_len + 1))) {
+			ret = -EFAULT;
+			goto err_ppa;
+		}
+		vcmd->ph_rw.spba = cpu_to_le64(ppa_dma);
+	} else {
+		vcmd->ph_rw.spba = cpu_to_le64((uintptr_t)ppa_buf);
+	}
+
+	if (ubuf && bufflen) {
+		ret = blk_rq_map_user(q, rq, NULL, ubuf, bufflen, GFP_KERNEL);
+		if (ret)
+			goto err_ppa;
+		bio = rq->bio;
+
+		if (meta_buf && meta_len) {
+			metadata = dma_pool_alloc(dev->dma_pool, GFP_KERNEL,
+								&metadata_dma);
+			if (!metadata) {
+				ret = -ENOMEM;
+				goto err_map;
+			}
+
+			if (write) {
+				if (copy_from_user(metadata,
+						(void __user *)meta_buf,
+						meta_len)) {
+					ret = -EFAULT;
+					goto err_meta;
+				}
+			}
+			vcmd->ph_rw.metadata = cpu_to_le64(metadata_dma);
+		}
+
+		bio->bi_disk = disk;
+	}
+
+	blk_execute_rq(q, NULL, rq, 0);
+
+	if (nvme_req(rq)->flags & NVME_REQ_CANCELLED)
+		ret = -EINTR;
+	else if (nvme_req(rq)->status & 0x7ff)
+		ret = -EIO;
+	if (result)
+		*result = nvme_req(rq)->status & 0x7ff;
+	if (status)
+		*status = le64_to_cpu(nvme_req(rq)->result.u64);
+
+	if (metadata && !ret && !write) {
+		if (copy_to_user(meta_buf, (void *)metadata, meta_len))
+			ret = -EFAULT;
+	}
+err_meta:
+	if (meta_buf && meta_len)
+		dma_pool_free(dev->dma_pool, metadata, metadata_dma);
+err_map:
+	if (bio)
+		blk_rq_unmap_user(bio);
+err_ppa:
+	if (ppa_buf && ppa_len)
+		dma_pool_free(dev->dma_pool, ppa_list, ppa_dma);
+err_rq:
+	blk_mq_free_request(rq);
+err_cmd:
+	return ret;
+}
+
+static int nvme_nvm_submit_vio(struct nvme_ns *ns,
+					struct nvm_user_vio __user *uvio)
+{
+	struct nvm_user_vio vio;
+	struct nvme_nvm_command c;
+	unsigned int length;
+	int ret;
+
+	if (copy_from_user(&vio, uvio, sizeof(vio)))
+		return -EFAULT;
+	if (vio.flags)
+		return -EINVAL;
+
+	memset(&c, 0, sizeof(c));
+	c.ph_rw.opcode = vio.opcode;
+	c.ph_rw.nsid = cpu_to_le32(ns->head->ns_id);
+	c.ph_rw.control = cpu_to_le16(vio.control);
+	c.ph_rw.length = cpu_to_le16(vio.nppas);
+
+	length = (vio.nppas + 1) << ns->lba_shift;
+
+	ret = nvme_nvm_submit_user_cmd(ns->queue, ns, &c,
+			(void __user *)(uintptr_t)vio.addr, length,
+			(void __user *)(uintptr_t)vio.metadata,
+							vio.metadata_len,
+			(void __user *)(uintptr_t)vio.ppa_list, vio.nppas,
+			&vio.result, &vio.status, 0);
+
+	if (ret && copy_to_user(uvio, &vio, sizeof(vio)))
+		return -EFAULT;
+
+	return ret;
+}
+
+static int nvme_nvm_user_vcmd(struct nvme_ns *ns, int admin,
+					struct nvm_passthru_vio __user *uvcmd)
+{
+	struct nvm_passthru_vio vcmd;
+	struct nvme_nvm_command c;
+	struct request_queue *q;
+	unsigned int timeout = 0;
+	int ret;
+
+	if (copy_from_user(&vcmd, uvcmd, sizeof(vcmd)))
+		return -EFAULT;
+	if ((vcmd.opcode != 0xF2) && (!capable(CAP_SYS_ADMIN)))
+		return -EACCES;
+	if (vcmd.flags)
+		return -EINVAL;
+
+	memset(&c, 0, sizeof(c));
+	c.common.opcode = vcmd.opcode;
+	c.common.nsid = cpu_to_le32(ns->head->ns_id);
+	c.common.cdw2[0] = cpu_to_le32(vcmd.cdw2);
+	c.common.cdw2[1] = cpu_to_le32(vcmd.cdw3);
+	/* cdw11-12 */
+	c.ph_rw.length = cpu_to_le16(vcmd.nppas);
+	c.ph_rw.control  = cpu_to_le16(vcmd.control);
+	c.common.cdw13 = cpu_to_le32(vcmd.cdw13);
+	c.common.cdw14 = cpu_to_le32(vcmd.cdw14);
+	c.common.cdw15 = cpu_to_le32(vcmd.cdw15);
+
+	if (vcmd.timeout_ms)
+		timeout = msecs_to_jiffies(vcmd.timeout_ms);
+
+	q = admin ? ns->ctrl->admin_q : ns->queue;
+
+	ret = nvme_nvm_submit_user_cmd(q, ns,
+			(struct nvme_nvm_command *)&c,
+			(void __user *)(uintptr_t)vcmd.addr, vcmd.data_len,
+			(void __user *)(uintptr_t)vcmd.metadata,
+							vcmd.metadata_len,
+			(void __user *)(uintptr_t)vcmd.ppa_list, vcmd.nppas,
+			&vcmd.result, &vcmd.status, timeout);
+
+	if (ret && copy_to_user(uvcmd, &vcmd, sizeof(vcmd)))
+		return -EFAULT;
+
+	return ret;
+}
+
+int nvme_nvm_ioctl(struct nvme_ns *ns, unsigned int cmd, unsigned long arg)
+{
+	switch (cmd) {
+	case NVME_NVM_IOCTL_ADMIN_VIO:
+		return nvme_nvm_user_vcmd(ns, 1, (void __user *)arg);
+	case NVME_NVM_IOCTL_IO_VIO:
+		return nvme_nvm_user_vcmd(ns, 0, (void __user *)arg);
+	case NVME_NVM_IOCTL_SUBMIT_VIO:
+		return nvme_nvm_submit_vio(ns, (void __user *)arg);
+	default:
+		return -ENOTTY;
+	}
+}
+
+int nvme_nvm_register(struct nvme_ns *ns, char *disk_name, int node)
+{
+	struct request_queue *q = ns->queue;
+	struct nvm_dev *dev;
+	struct nvm_geo *geo;
+
+	_nvme_nvm_check_size();
+
+	dev = nvm_alloc_dev(node);
+	if (!dev)
+		return -ENOMEM;
+
+	/* Note that csecs and sos will be overridden if it is a 1.2 drive. */
+	geo = &dev->geo;
+	geo->csecs = 1 << ns->lba_shift;
+	geo->sos = ns->ms;
+	geo->ext = ns->ext;
+
+	dev->q = q;
+	memcpy(dev->name, disk_name, DISK_NAME_LEN);
+	dev->ops = &nvme_nvm_dev_ops;
+	dev->private_data = ns;
+	ns->ndev = dev;
+
+	return nvm_register(dev);
+}
+
+void nvme_nvm_unregister(struct nvme_ns *ns)
+{
+	nvm_unregister(ns->ndev);
+}
+
+static ssize_t nvm_dev_attr_show(struct device *dev,
+		struct device_attribute *dattr, char *page)
+{
+	struct nvme_ns *ns = nvme_get_ns_from_dev(dev);
+	struct nvm_dev *ndev = ns->ndev;
+	struct nvm_geo *geo = &ndev->geo;
+	struct attribute *attr;
+
+	if (!ndev)
+		return 0;
+
+	attr = &dattr->attr;
+
+	if (strcmp(attr->name, "version") == 0) {
+		if (geo->major_ver_id == 1)
+			return scnprintf(page, PAGE_SIZE, "%u\n",
+						geo->major_ver_id);
+		else
+			return scnprintf(page, PAGE_SIZE, "%u.%u\n",
+						geo->major_ver_id,
+						geo->minor_ver_id);
+	} else if (strcmp(attr->name, "capabilities") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->cap);
+	} else if (strcmp(attr->name, "read_typ") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->trdt);
+	} else if (strcmp(attr->name, "read_max") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->trdm);
+	} else {
+		return scnprintf(page,
+				 PAGE_SIZE,
+				 "Unhandled attr(%s) in `%s`\n",
+				 attr->name, __func__);
+	}
+}
+
+static ssize_t nvm_dev_attr_show_ppaf(struct nvm_addrf_12 *ppaf, char *page)
+{
+	return scnprintf(page, PAGE_SIZE,
+		"0x%02x%02x%02x%02x%02x%02x%02x%02x%02x%02x%02x%02x\n",
+				ppaf->ch_offset, ppaf->ch_len,
+				ppaf->lun_offset, ppaf->lun_len,
+				ppaf->pln_offset, ppaf->pln_len,
+				ppaf->blk_offset, ppaf->blk_len,
+				ppaf->pg_offset, ppaf->pg_len,
+				ppaf->sec_offset, ppaf->sec_len);
+}
+
+static ssize_t nvm_dev_attr_show_12(struct device *dev,
+		struct device_attribute *dattr, char *page)
+{
+	struct nvme_ns *ns = nvme_get_ns_from_dev(dev);
+	struct nvm_dev *ndev = ns->ndev;
+	struct nvm_geo *geo = &ndev->geo;
+	struct attribute *attr;
+
+	if (!ndev)
+		return 0;
+
+	attr = &dattr->attr;
+
+	if (strcmp(attr->name, "vendor_opcode") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->vmnt);
+	} else if (strcmp(attr->name, "device_mode") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->dom);
+	/* kept for compatibility */
+	} else if (strcmp(attr->name, "media_manager") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%s\n", "gennvm");
+	} else if (strcmp(attr->name, "ppa_format") == 0) {
+		return nvm_dev_attr_show_ppaf((void *)&geo->addrf, page);
+	} else if (strcmp(attr->name, "media_type") == 0) {	/* u8 */
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->mtype);
+	} else if (strcmp(attr->name, "flash_media_type") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->fmtype);
+	} else if (strcmp(attr->name, "num_channels") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->num_ch);
+	} else if (strcmp(attr->name, "num_luns") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->num_lun);
+	} else if (strcmp(attr->name, "num_planes") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->num_pln);
+	} else if (strcmp(attr->name, "num_blocks") == 0) {	/* u16 */
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->num_chk);
+	} else if (strcmp(attr->name, "num_pages") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->num_pg);
+	} else if (strcmp(attr->name, "page_size") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->fpg_sz);
+	} else if (strcmp(attr->name, "hw_sector_size") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->csecs);
+	} else if (strcmp(attr->name, "oob_sector_size") == 0) {/* u32 */
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->sos);
+	} else if (strcmp(attr->name, "prog_typ") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->tprt);
+	} else if (strcmp(attr->name, "prog_max") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->tprm);
+	} else if (strcmp(attr->name, "erase_typ") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->tbet);
+	} else if (strcmp(attr->name, "erase_max") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->tbem);
+	} else if (strcmp(attr->name, "multiplane_modes") == 0) {
+		return scnprintf(page, PAGE_SIZE, "0x%08x\n", geo->mpos);
+	} else if (strcmp(attr->name, "media_capabilities") == 0) {
+		return scnprintf(page, PAGE_SIZE, "0x%08x\n", geo->mccap);
+	} else if (strcmp(attr->name, "max_phys_secs") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", NVM_MAX_VLBA);
+	} else {
+		return scnprintf(page, PAGE_SIZE,
+			"Unhandled attr(%s) in `%s`\n",
+			attr->name, __func__);
+	}
+}
+
+static ssize_t nvm_dev_attr_show_20(struct device *dev,
+		struct device_attribute *dattr, char *page)
+{
+	struct nvme_ns *ns = nvme_get_ns_from_dev(dev);
+	struct nvm_dev *ndev = ns->ndev;
+	struct nvm_geo *geo = &ndev->geo;
+	struct attribute *attr;
+
+	if (!ndev)
+		return 0;
+
+	attr = &dattr->attr;
+
+	if (strcmp(attr->name, "groups") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->num_ch);
+	} else if (strcmp(attr->name, "punits") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->num_lun);
+	} else if (strcmp(attr->name, "chunks") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->num_chk);
+	} else if (strcmp(attr->name, "clba") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->clba);
+	} else if (strcmp(attr->name, "ws_min") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->ws_min);
+	} else if (strcmp(attr->name, "ws_opt") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->ws_opt);
+	} else if (strcmp(attr->name, "maxoc") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->maxoc);
+	} else if (strcmp(attr->name, "maxocpu") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->maxocpu);
+	} else if (strcmp(attr->name, "mw_cunits") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->mw_cunits);
+	} else if (strcmp(attr->name, "write_typ") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->tprt);
+	} else if (strcmp(attr->name, "write_max") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->tprm);
+	} else if (strcmp(attr->name, "reset_typ") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->tbet);
+	} else if (strcmp(attr->name, "reset_max") == 0) {
+		return scnprintf(page, PAGE_SIZE, "%u\n", geo->tbem);
+	} else {
+		return scnprintf(page, PAGE_SIZE,
+			"Unhandled attr(%s) in `%s`\n",
+			attr->name, __func__);
+	}
+}
+
+#define NVM_DEV_ATTR_RO(_name)					\
+	DEVICE_ATTR(_name, S_IRUGO, nvm_dev_attr_show, NULL)
+#define NVM_DEV_ATTR_12_RO(_name)					\
+	DEVICE_ATTR(_name, S_IRUGO, nvm_dev_attr_show_12, NULL)
+#define NVM_DEV_ATTR_20_RO(_name)					\
+	DEVICE_ATTR(_name, S_IRUGO, nvm_dev_attr_show_20, NULL)
+
+/* general attributes */
+static NVM_DEV_ATTR_RO(version);
+static NVM_DEV_ATTR_RO(capabilities);
+
+static NVM_DEV_ATTR_RO(read_typ);
+static NVM_DEV_ATTR_RO(read_max);
+
+/* 1.2 values */
+static NVM_DEV_ATTR_12_RO(vendor_opcode);
+static NVM_DEV_ATTR_12_RO(device_mode);
+static NVM_DEV_ATTR_12_RO(ppa_format);
+static NVM_DEV_ATTR_12_RO(media_manager);
+static NVM_DEV_ATTR_12_RO(media_type);
+static NVM_DEV_ATTR_12_RO(flash_media_type);
+static NVM_DEV_ATTR_12_RO(num_channels);
+static NVM_DEV_ATTR_12_RO(num_luns);
+static NVM_DEV_ATTR_12_RO(num_planes);
+static NVM_DEV_ATTR_12_RO(num_blocks);
+static NVM_DEV_ATTR_12_RO(num_pages);
+static NVM_DEV_ATTR_12_RO(page_size);
+static NVM_DEV_ATTR_12_RO(hw_sector_size);
+static NVM_DEV_ATTR_12_RO(oob_sector_size);
+static NVM_DEV_ATTR_12_RO(prog_typ);
+static NVM_DEV_ATTR_12_RO(prog_max);
+static NVM_DEV_ATTR_12_RO(erase_typ);
+static NVM_DEV_ATTR_12_RO(erase_max);
+static NVM_DEV_ATTR_12_RO(multiplane_modes);
+static NVM_DEV_ATTR_12_RO(media_capabilities);
+static NVM_DEV_ATTR_12_RO(max_phys_secs);
+
+/* 2.0 values */
+static NVM_DEV_ATTR_20_RO(groups);
+static NVM_DEV_ATTR_20_RO(punits);
+static NVM_DEV_ATTR_20_RO(chunks);
+static NVM_DEV_ATTR_20_RO(clba);
+static NVM_DEV_ATTR_20_RO(ws_min);
+static NVM_DEV_ATTR_20_RO(ws_opt);
+static NVM_DEV_ATTR_20_RO(maxoc);
+static NVM_DEV_ATTR_20_RO(maxocpu);
+static NVM_DEV_ATTR_20_RO(mw_cunits);
+static NVM_DEV_ATTR_20_RO(write_typ);
+static NVM_DEV_ATTR_20_RO(write_max);
+static NVM_DEV_ATTR_20_RO(reset_typ);
+static NVM_DEV_ATTR_20_RO(reset_max);
+
+static struct attribute *nvm_dev_attrs[] = {
+	/* version agnostic attrs */
+	&dev_attr_version.attr,
+	&dev_attr_capabilities.attr,
+	&dev_attr_read_typ.attr,
+	&dev_attr_read_max.attr,
+
+	/* 1.2 attrs */
+	&dev_attr_vendor_opcode.attr,
+	&dev_attr_device_mode.attr,
+	&dev_attr_media_manager.attr,
+	&dev_attr_ppa_format.attr,
+	&dev_attr_media_type.attr,
+	&dev_attr_flash_media_type.attr,
+	&dev_attr_num_channels.attr,
+	&dev_attr_num_luns.attr,
+	&dev_attr_num_planes.attr,
+	&dev_attr_num_blocks.attr,
+	&dev_attr_num_pages.attr,
+	&dev_attr_page_size.attr,
+	&dev_attr_hw_sector_size.attr,
+	&dev_attr_oob_sector_size.attr,
+	&dev_attr_prog_typ.attr,
+	&dev_attr_prog_max.attr,
+	&dev_attr_erase_typ.attr,
+	&dev_attr_erase_max.attr,
+	&dev_attr_multiplane_modes.attr,
+	&dev_attr_media_capabilities.attr,
+	&dev_attr_max_phys_secs.attr,
+
+	/* 2.0 attrs */
+	&dev_attr_groups.attr,
+	&dev_attr_punits.attr,
+	&dev_attr_chunks.attr,
+	&dev_attr_clba.attr,
+	&dev_attr_ws_min.attr,
+	&dev_attr_ws_opt.attr,
+	&dev_attr_maxoc.attr,
+	&dev_attr_maxocpu.attr,
+	&dev_attr_mw_cunits.attr,
+
+	&dev_attr_write_typ.attr,
+	&dev_attr_write_max.attr,
+	&dev_attr_reset_typ.attr,
+	&dev_attr_reset_max.attr,
+
+	NULL,
+};
+
+static umode_t nvm_dev_attrs_visible(struct kobject *kobj,
+				     struct attribute *attr, int index)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct gendisk *disk = dev_to_disk(dev);
+	struct nvme_ns *ns = disk->private_data;
+	struct nvm_dev *ndev = ns->ndev;
+	struct device_attribute *dev_attr =
+		container_of(attr, typeof(*dev_attr), attr);
+
+	if (!ndev)
+		return 0;
+
+	if (dev_attr->show == nvm_dev_attr_show)
+		return attr->mode;
+
+	switch (ndev->geo.major_ver_id) {
+	case 1:
+		if (dev_attr->show == nvm_dev_attr_show_12)
+			return attr->mode;
+		break;
+	case 2:
+		if (dev_attr->show == nvm_dev_attr_show_20)
+			return attr->mode;
+		break;
+	}
+
+	return 0;
+}
+
+const struct attribute_group nvme_nvm_attr_group = {
+	.name		= "lightnvm",
+	.attrs		= nvm_dev_attrs,
+	.is_visible	= nvm_dev_attrs_visible,
+};
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/linux_nvme.h b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/linux_nvme.h
new file mode 100644
index 0000000..ebd7804
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/linux_nvme.h
@@ -0,0 +1,1611 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Definitions for the NVM Express interface
+ * Copyright (c) 2011-2014, Intel Corporation.
+ */
+
+#ifndef _LINUX_NVME_H
+#define _LINUX_NVME_H
+
+#include <linux/types.h>
+#include <linux/uuid.h>
+
+/* NQN names in commands fields specified one size */
+#define NVMF_NQN_FIELD_LEN	256
+
+/* However the max length of a qualified name is another size */
+#define NVMF_NQN_SIZE		223
+
+#define NVMF_TRSVCID_SIZE	32
+#define NVMF_TRADDR_SIZE	256
+#define NVMF_TSAS_SIZE		256
+
+#define NVME_DISC_SUBSYS_NAME	"nqn.2014-08.org.nvmexpress.discovery"
+
+#define NVME_RDMA_IP_PORT	4420
+
+#define NVME_NSID_ALL		0xffffffff
+
+enum nvme_subsys_type {
+	NVME_NQN_DISC	= 1,		/* Discovery type target subsystem */
+	NVME_NQN_NVME	= 2,		/* NVME type target subsystem */
+};
+
+/* Address Family codes for Discovery Log Page entry ADRFAM field */
+enum {
+	NVMF_ADDR_FAMILY_PCI	= 0,	/* PCIe */
+	NVMF_ADDR_FAMILY_IP4	= 1,	/* IP4 */
+	NVMF_ADDR_FAMILY_IP6	= 2,	/* IP6 */
+	NVMF_ADDR_FAMILY_IB	= 3,	/* InfiniBand */
+	NVMF_ADDR_FAMILY_FC	= 4,	/* Fibre Channel */
+};
+
+/* Transport Type codes for Discovery Log Page entry TRTYPE field */
+enum {
+	NVMF_TRTYPE_RDMA	= 1,	/* RDMA */
+	NVMF_TRTYPE_FC		= 2,	/* Fibre Channel */
+	NVMF_TRTYPE_TCP		= 3,	/* TCP/IP */
+	NVMF_TRTYPE_LOOP	= 254,	/* Reserved for host usage */
+	NVMF_TRTYPE_MAX,
+};
+
+/* Transport Requirements codes for Discovery Log Page entry TREQ field */
+enum {
+	NVMF_TREQ_NOT_SPECIFIED	= 0,		/* Not specified */
+	NVMF_TREQ_REQUIRED	= 1,		/* Required */
+	NVMF_TREQ_NOT_REQUIRED	= 2,		/* Not Required */
+#define NVME_TREQ_SECURE_CHANNEL_MASK \
+	(NVMF_TREQ_REQUIRED | NVMF_TREQ_NOT_REQUIRED)
+
+	NVMF_TREQ_DISABLE_SQFLOW = (1 << 2),	/* Supports SQ flow control disable */
+};
+
+/* RDMA QP Service Type codes for Discovery Log Page entry TSAS
+ * RDMA_QPTYPE field
+ */
+enum {
+	NVMF_RDMA_QPTYPE_CONNECTED	= 1, /* Reliable Connected */
+	NVMF_RDMA_QPTYPE_DATAGRAM	= 2, /* Reliable Datagram */
+};
+
+/* RDMA QP Service Type codes for Discovery Log Page entry TSAS
+ * RDMA_QPTYPE field
+ */
+enum {
+	NVMF_RDMA_PRTYPE_NOT_SPECIFIED	= 1, /* No Provider Specified */
+	NVMF_RDMA_PRTYPE_IB		= 2, /* InfiniBand */
+	NVMF_RDMA_PRTYPE_ROCE		= 3, /* InfiniBand RoCE */
+	NVMF_RDMA_PRTYPE_ROCEV2		= 4, /* InfiniBand RoCEV2 */
+	NVMF_RDMA_PRTYPE_IWARP		= 5, /* IWARP */
+};
+
+/* RDMA Connection Management Service Type codes for Discovery Log Page
+ * entry TSAS RDMA_CMS field
+ */
+enum {
+	NVMF_RDMA_CMS_RDMA_CM	= 1, /* Sockets based endpoint addressing */
+};
+
+#define NVME_AQ_DEPTH		32
+#define NVME_NR_AEN_COMMANDS	1
+#define NVME_AQ_BLK_MQ_DEPTH	(NVME_AQ_DEPTH - NVME_NR_AEN_COMMANDS)
+
+/*
+ * Subtract one to leave an empty queue entry for 'Full Queue' condition. See
+ * NVM-Express 1.2 specification, section 4.1.2.
+ */
+#define NVME_AQ_MQ_TAG_DEPTH	(NVME_AQ_BLK_MQ_DEPTH - 1)
+
+enum {
+	NVME_REG_CAP	= 0x0000,	/* Controller Capabilities */
+	NVME_REG_VS	= 0x0008,	/* Version */
+	NVME_REG_INTMS	= 0x000c,	/* Interrupt Mask Set */
+	NVME_REG_INTMC	= 0x0010,	/* Interrupt Mask Clear */
+	NVME_REG_CC	= 0x0014,	/* Controller Configuration */
+	NVME_REG_CSTS	= 0x001c,	/* Controller Status */
+	NVME_REG_NSSR	= 0x0020,	/* NVM Subsystem Reset */
+	NVME_REG_AQA	= 0x0024,	/* Admin Queue Attributes */
+	NVME_REG_ASQ	= 0x0028,	/* Admin SQ Base Address */
+	NVME_REG_ACQ	= 0x0030,	/* Admin CQ Base Address */
+	NVME_REG_CMBLOC = 0x0038,	/* Controller Memory Buffer Location */
+	NVME_REG_CMBSZ	= 0x003c,	/* Controller Memory Buffer Size */
+	NVME_REG_DBS	= 0x1000,	/* SQ 0 Tail Doorbell */
+};
+
+#define NVME_CAP_MQES(cap)	((cap) & 0xffff)
+#define NVME_CAP_TIMEOUT(cap)	(((cap) >> 24) & 0xff)
+#define NVME_CAP_STRIDE(cap)	(((cap) >> 32) & 0xf)
+#define NVME_CAP_NSSRC(cap)	(((cap) >> 36) & 0x1)
+#define NVME_CAP_MPSMIN(cap)	(((cap) >> 48) & 0xf)
+#define NVME_CAP_MPSMAX(cap)	(((cap) >> 52) & 0xf)
+
+#define NVME_CMB_BIR(cmbloc)	((cmbloc) & 0x7)
+#define NVME_CMB_OFST(cmbloc)	(((cmbloc) >> 12) & 0xfffff)
+
+enum {
+	NVME_CMBSZ_SQS		= 1 << 0,
+	NVME_CMBSZ_CQS		= 1 << 1,
+	NVME_CMBSZ_LISTS	= 1 << 2,
+	NVME_CMBSZ_RDS		= 1 << 3,
+	NVME_CMBSZ_WDS		= 1 << 4,
+
+	NVME_CMBSZ_SZ_SHIFT	= 12,
+	NVME_CMBSZ_SZ_MASK	= 0xfffff,
+
+	NVME_CMBSZ_SZU_SHIFT	= 8,
+	NVME_CMBSZ_SZU_MASK	= 0xf,
+};
+
+/*
+ * Submission and Completion Queue Entry Sizes for the NVM command set.
+ * (In bytes and specified as a power of two (2^n)).
+ */
+#define NVME_NVM_IOSQES		6
+#define NVME_NVM_IOCQES		4
+
+enum {
+	NVME_CC_ENABLE		= 1 << 0,
+	NVME_CC_CSS_NVM		= 0 << 4,
+	NVME_CC_EN_SHIFT	= 0,
+	NVME_CC_CSS_SHIFT	= 4,
+	NVME_CC_MPS_SHIFT	= 7,
+	NVME_CC_AMS_SHIFT	= 11,
+	NVME_CC_SHN_SHIFT	= 14,
+	NVME_CC_IOSQES_SHIFT	= 16,
+	NVME_CC_IOCQES_SHIFT	= 20,
+	NVME_CC_AMS_RR		= 0 << NVME_CC_AMS_SHIFT,
+	NVME_CC_AMS_WRRU	= 1 << NVME_CC_AMS_SHIFT,
+	NVME_CC_AMS_VS		= 7 << NVME_CC_AMS_SHIFT,
+	NVME_CC_SHN_NONE	= 0 << NVME_CC_SHN_SHIFT,
+	NVME_CC_SHN_NORMAL	= 1 << NVME_CC_SHN_SHIFT,
+	NVME_CC_SHN_ABRUPT	= 2 << NVME_CC_SHN_SHIFT,
+	NVME_CC_SHN_MASK	= 3 << NVME_CC_SHN_SHIFT,
+	NVME_CC_IOSQES		= NVME_NVM_IOSQES << NVME_CC_IOSQES_SHIFT,
+	NVME_CC_IOCQES		= NVME_NVM_IOCQES << NVME_CC_IOCQES_SHIFT,
+	NVME_CSTS_RDY		= 1 << 0,
+	NVME_CSTS_CFS		= 1 << 1,
+	NVME_CSTS_NSSRO		= 1 << 4,
+	NVME_CSTS_PP		= 1 << 5,
+	NVME_CSTS_SHST_NORMAL	= 0 << 2,
+	NVME_CSTS_SHST_OCCUR	= 1 << 2,
+	NVME_CSTS_SHST_CMPLT	= 2 << 2,
+	NVME_CSTS_SHST_MASK	= 3 << 2,
+};
+
+struct nvme_id_power_state {
+	__le16			max_power;	/* centiwatts */
+	__u8			rsvd2;
+	__u8			flags;
+	__le32			entry_lat;	/* microseconds */
+	__le32			exit_lat;	/* microseconds */
+	__u8			read_tput;
+	__u8			read_lat;
+	__u8			write_tput;
+	__u8			write_lat;
+	__le16			idle_power;
+	__u8			idle_scale;
+	__u8			rsvd19;
+	__le16			active_power;
+	__u8			active_work_scale;
+	__u8			rsvd23[9];
+};
+
+enum {
+	NVME_PS_FLAGS_MAX_POWER_SCALE	= 1 << 0,
+	NVME_PS_FLAGS_NON_OP_STATE	= 1 << 1,
+};
+
+enum nvme_ctrl_attr {
+	NVME_CTRL_ATTR_HID_128_BIT	= (1 << 0),
+	NVME_CTRL_ATTR_TBKAS		= (1 << 6),
+};
+
+struct nvme_id_ctrl {
+	__le16			vid;
+	__le16			ssvid;
+	char			sn[20];
+	char			mn[40];
+	char			fr[8];
+	__u8			rab;
+	__u8			ieee[3];
+	__u8			cmic;
+	__u8			mdts;
+	__le16			cntlid;
+	__le32			ver;
+	__le32			rtd3r;
+	__le32			rtd3e;
+	__le32			oaes;
+	__le32			ctratt;
+	__u8			rsvd100[28];
+	__le16			crdt1;
+	__le16			crdt2;
+	__le16			crdt3;
+	__u8			rsvd134[122];
+	__le16			oacs;
+	__u8			acl;
+	__u8			aerl;
+	__u8			frmw;
+	__u8			lpa;
+	__u8			elpe;
+	__u8			npss;
+	__u8			avscc;
+	__u8			apsta;
+	__le16			wctemp;
+	__le16			cctemp;
+	__le16			mtfa;
+	__le32			hmpre;
+	__le32			hmmin;
+	__u8			tnvmcap[16];
+	__u8			unvmcap[16];
+	__le32			rpmbs;
+	__le16			edstt;
+	__u8			dsto;
+	__u8			fwug;
+	__le16			kas;
+	__le16			hctma;
+	__le16			mntmt;
+	__le16			mxtmt;
+	__le32			sanicap;
+	__le32			hmminds;
+	__le16			hmmaxd;
+	__u8			rsvd338[4];
+	__u8			anatt;
+	__u8			anacap;
+	__le32			anagrpmax;
+	__le32			nanagrpid;
+	__u8			rsvd352[160];
+	__u8			sqes;
+	__u8			cqes;
+	__le16			maxcmd;
+	__le32			nn;
+	__le16			oncs;
+	__le16			fuses;
+	__u8			fna;
+	__u8			vwc;
+	__le16			awun;
+	__le16			awupf;
+	__u8			nvscc;
+	__u8			nwpc;
+	__le16			acwu;
+	__u8			rsvd534[2];
+	__le32			sgls;
+	__le32			mnan;
+	__u8			rsvd544[224];
+	char			subnqn[256];
+	__u8			rsvd1024[768];
+	__le32			ioccsz;
+	__le32			iorcsz;
+	__le16			icdoff;
+	__u8			ctrattr;
+	__u8			msdbd;
+	__u8			rsvd1804[244];
+	struct nvme_id_power_state	psd[32];
+	__u8			vs[1024];
+};
+
+enum {
+	NVME_CTRL_ONCS_COMPARE			= 1 << 0,
+	NVME_CTRL_ONCS_WRITE_UNCORRECTABLE	= 1 << 1,
+	NVME_CTRL_ONCS_DSM			= 1 << 2,
+	NVME_CTRL_ONCS_WRITE_ZEROES		= 1 << 3,
+	NVME_CTRL_ONCS_TIMESTAMP		= 1 << 6,
+	NVME_CTRL_VWC_PRESENT			= 1 << 0,
+	NVME_CTRL_OACS_SEC_SUPP                 = 1 << 0,
+	NVME_CTRL_OACS_DIRECTIVES		= 1 << 5,
+	NVME_CTRL_OACS_DBBUF_SUPP		= 1 << 8,
+	NVME_CTRL_LPA_CMD_EFFECTS_LOG		= 1 << 1,
+};
+
+struct nvme_lbaf {
+	__le16			ms;
+	__u8			ds;
+	__u8			rp;
+};
+
+struct nvme_id_ns {
+	__le64			nsze;
+	__le64			ncap;
+	__le64			nuse;
+	__u8			nsfeat;
+	__u8			nlbaf;
+	__u8			flbas;
+	__u8			mc;
+	__u8			dpc;
+	__u8			dps;
+	__u8			nmic;
+	__u8			rescap;
+	__u8			fpi;
+	__u8			rsvd33;
+	__le16			nawun;
+	__le16			nawupf;
+	__le16			nacwu;
+	__le16			nabsn;
+	__le16			nabo;
+	__le16			nabspf;
+	__le16			noiob;
+	__u8			nvmcap[16];
+	__u8			rsvd64[28];
+	__le32			anagrpid;
+	__u8			rsvd96[3];
+	__u8			nsattr;
+	__u8			rsvd100[4];
+	__u8			nguid[16];
+	__u8			eui64[8];
+	struct nvme_lbaf	lbaf[16];
+	__u8			rsvd192[192];
+	__u8			vs[3712];
+};
+
+enum {
+	NVME_ID_CNS_NS			= 0x00,
+	NVME_ID_CNS_CTRL		= 0x01,
+	NVME_ID_CNS_NS_ACTIVE_LIST	= 0x02,
+	NVME_ID_CNS_NS_DESC_LIST	= 0x03,
+	NVME_ID_CNS_NS_PRESENT_LIST	= 0x10,
+	NVME_ID_CNS_NS_PRESENT		= 0x11,
+	NVME_ID_CNS_CTRL_NS_LIST	= 0x12,
+	NVME_ID_CNS_CTRL_LIST		= 0x13,
+};
+
+enum {
+	NVME_DIR_IDENTIFY		= 0x00,
+	NVME_DIR_STREAMS		= 0x01,
+	NVME_DIR_SND_ID_OP_ENABLE	= 0x01,
+	NVME_DIR_SND_ST_OP_REL_ID	= 0x01,
+	NVME_DIR_SND_ST_OP_REL_RSC	= 0x02,
+	NVME_DIR_RCV_ID_OP_PARAM	= 0x01,
+	NVME_DIR_RCV_ST_OP_PARAM	= 0x01,
+	NVME_DIR_RCV_ST_OP_STATUS	= 0x02,
+	NVME_DIR_RCV_ST_OP_RESOURCE	= 0x03,
+	NVME_DIR_ENDIR			= 0x01,
+};
+
+enum {
+	NVME_NS_FEAT_THIN	= 1 << 0,
+	NVME_NS_FLBAS_LBA_MASK	= 0xf,
+	NVME_NS_FLBAS_META_EXT	= 0x10,
+	NVME_LBAF_RP_BEST	= 0,
+	NVME_LBAF_RP_BETTER	= 1,
+	NVME_LBAF_RP_GOOD	= 2,
+	NVME_LBAF_RP_DEGRADED	= 3,
+	NVME_NS_DPC_PI_LAST	= 1 << 4,
+	NVME_NS_DPC_PI_FIRST	= 1 << 3,
+	NVME_NS_DPC_PI_TYPE3	= 1 << 2,
+	NVME_NS_DPC_PI_TYPE2	= 1 << 1,
+	NVME_NS_DPC_PI_TYPE1	= 1 << 0,
+	NVME_NS_DPS_PI_FIRST	= 1 << 3,
+	NVME_NS_DPS_PI_MASK	= 0x7,
+	NVME_NS_DPS_PI_TYPE1	= 1,
+	NVME_NS_DPS_PI_TYPE2	= 2,
+	NVME_NS_DPS_PI_TYPE3	= 3,
+};
+
+struct nvme_ns_id_desc {
+	__u8 nidt;
+	__u8 nidl;
+	__le16 reserved;
+};
+
+#define NVME_NIDT_EUI64_LEN	8
+#define NVME_NIDT_NGUID_LEN	16
+#define NVME_NIDT_UUID_LEN	16
+
+enum {
+	NVME_NIDT_EUI64		= 0x01,
+	NVME_NIDT_NGUID		= 0x02,
+	NVME_NIDT_UUID		= 0x03,
+};
+
+struct nvme_smart_log {
+	__u8			critical_warning;
+	__u8			temperature[2];
+	__u8			avail_spare;
+	__u8			spare_thresh;
+	__u8			percent_used;
+	__u8			rsvd6[26];
+	__u8			data_units_read[16];
+	__u8			data_units_written[16];
+	__u8			host_reads[16];
+	__u8			host_writes[16];
+	__u8			ctrl_busy_time[16];
+	__u8			power_cycles[16];
+	__u8			power_on_hours[16];
+	__u8			unsafe_shutdowns[16];
+	__u8			media_errors[16];
+	__u8			num_err_log_entries[16];
+	__le32			warning_temp_time;
+	__le32			critical_comp_time;
+	__le16			temp_sensor[8];
+	__u8			rsvd216[296];
+};
+
+struct nvme_fw_slot_info_log {
+	__u8			afi;
+	__u8			rsvd1[7];
+	__le64			frs[7];
+	__u8			rsvd64[448];
+};
+
+enum {
+	NVME_CMD_EFFECTS_CSUPP		= 1 << 0,
+	NVME_CMD_EFFECTS_LBCC		= 1 << 1,
+	NVME_CMD_EFFECTS_NCC		= 1 << 2,
+	NVME_CMD_EFFECTS_NIC		= 1 << 3,
+	NVME_CMD_EFFECTS_CCC		= 1 << 4,
+	NVME_CMD_EFFECTS_CSE_MASK	= 3 << 16,
+};
+
+struct nvme_effects_log {
+	__le32 acs[256];
+	__le32 iocs[256];
+	__u8   resv[2048];
+};
+
+enum nvme_ana_state {
+	NVME_ANA_OPTIMIZED		= 0x01,
+	NVME_ANA_NONOPTIMIZED		= 0x02,
+	NVME_ANA_INACCESSIBLE		= 0x03,
+	NVME_ANA_PERSISTENT_LOSS	= 0x04,
+	NVME_ANA_CHANGE			= 0x0f,
+};
+
+struct nvme_ana_group_desc {
+	__le32	grpid;
+	__le32	nnsids;
+	__le64	chgcnt;
+	__u8	state;
+	__u8	rsvd17[15];
+	__le32	nsids[];
+};
+
+/* flag for the log specific field of the ANA log */
+#define NVME_ANA_LOG_RGO	(1 << 0)
+
+struct nvme_ana_rsp_hdr {
+	__le64	chgcnt;
+	__le16	ngrps;
+	__le16	rsvd10[3];
+};
+
+enum {
+	NVME_SMART_CRIT_SPARE		= 1 << 0,
+	NVME_SMART_CRIT_TEMPERATURE	= 1 << 1,
+	NVME_SMART_CRIT_RELIABILITY	= 1 << 2,
+	NVME_SMART_CRIT_MEDIA		= 1 << 3,
+	NVME_SMART_CRIT_VOLATILE_MEMORY	= 1 << 4,
+};
+
+enum {
+	NVME_AER_ERROR			= 0,
+	NVME_AER_SMART			= 1,
+	NVME_AER_NOTICE			= 2,
+	NVME_AER_CSS			= 6,
+	NVME_AER_VS			= 7,
+};
+
+enum {
+	NVME_AER_NOTICE_NS_CHANGED	= 0x00,
+	NVME_AER_NOTICE_FW_ACT_STARTING = 0x01,
+	NVME_AER_NOTICE_ANA		= 0x03,
+	NVME_AER_NOTICE_DISC_CHANGED	= 0xf0,
+};
+
+enum {
+	NVME_AEN_BIT_NS_ATTR		= 8,
+	NVME_AEN_BIT_FW_ACT		= 9,
+	NVME_AEN_BIT_ANA_CHANGE		= 11,
+	NVME_AEN_BIT_DISC_CHANGE	= 31,
+};
+
+enum {
+	NVME_AEN_CFG_NS_ATTR		= 1 << NVME_AEN_BIT_NS_ATTR,
+	NVME_AEN_CFG_FW_ACT		= 1 << NVME_AEN_BIT_FW_ACT,
+	NVME_AEN_CFG_ANA_CHANGE		= 1 << NVME_AEN_BIT_ANA_CHANGE,
+	NVME_AEN_CFG_DISC_CHANGE	= 1 << NVME_AEN_BIT_DISC_CHANGE,
+};
+
+struct nvme_lba_range_type {
+	__u8			type;
+	__u8			attributes;
+	__u8			rsvd2[14];
+	__u64			slba;
+	__u64			nlb;
+	__u8			guid[16];
+	__u8			rsvd48[16];
+};
+
+enum {
+	NVME_LBART_TYPE_FS	= 0x01,
+	NVME_LBART_TYPE_RAID	= 0x02,
+	NVME_LBART_TYPE_CACHE	= 0x03,
+	NVME_LBART_TYPE_SWAP	= 0x04,
+
+	NVME_LBART_ATTRIB_TEMP	= 1 << 0,
+	NVME_LBART_ATTRIB_HIDE	= 1 << 1,
+};
+
+struct nvme_reservation_status {
+	__le32	gen;
+	__u8	rtype;
+	__u8	regctl[2];
+	__u8	resv5[2];
+	__u8	ptpls;
+	__u8	resv10[13];
+	struct {
+		__le16	cntlid;
+		__u8	rcsts;
+		__u8	resv3[5];
+		__le64	hostid;
+		__le64	rkey;
+	} regctl_ds[];
+};
+
+enum nvme_async_event_type {
+	NVME_AER_TYPE_ERROR	= 0,
+	NVME_AER_TYPE_SMART	= 1,
+	NVME_AER_TYPE_NOTICE	= 2,
+};
+
+/* I/O commands */
+
+enum nvme_opcode {
+	nvme_cmd_flush		= 0x00,
+	nvme_cmd_write		= 0x01,
+	nvme_cmd_read		= 0x02,
+	nvme_cmd_write_uncor	= 0x04,
+	nvme_cmd_compare	= 0x05,
+	nvme_cmd_write_zeroes	= 0x08,
+	nvme_cmd_dsm		= 0x09,
+	nvme_cmd_resv_register	= 0x0d,
+	nvme_cmd_resv_report	= 0x0e,
+	nvme_cmd_resv_acquire	= 0x11,
+	nvme_cmd_resv_release	= 0x15,
+
+    /* KV command */
+	nvme_cmd_kv_store	= 0x81,
+	nvme_cmd_kv_append	= 0x83,
+	nvme_cmd_kv_retrieve	= 0x90,
+	nvme_cmd_kv_delete	= 0xA1,
+	nvme_cmd_kv_iter_req	= 0xB1,
+	nvme_cmd_kv_iter_read	= 0xB2,
+	nvme_cmd_kv_exist	= 0xB3,
+	nvme_cmd_kv_list        = 0xD2,
+	nvme_cmd_kv_lock        = 0xA0,
+	nvme_cmd_kv_unlock      = 0xA4
+};
+
+#define KVCMD_INLINE_KEY_MAX	(16)
+//#define KVCMD_MAX_KEY_SIZE	(255)
+#define KVCMD_MAX_KEY_SIZE	(1024)
+#define KVCMD_MIN_KEY_SIZE	(4)
+
+#define KVCMD_MAX_VALUE_SIZE	(SZ_1M)
+#define KVCMD_MAX_RDD_VALUE_SIZE	(SZ_64M)
+
+/*
+ * Descriptor subtype - lower 4 bits of nvme_(keyed_)sgl_desc identifier
+ *
+ * @NVME_SGL_FMT_ADDRESS:     absolute address of the data block
+ * @NVME_SGL_FMT_OFFSET:      relative offset of the in-capsule data block
+ * @NVME_SGL_FMT_TRANSPORT_A: transport defined format, value 0xA
+ * @NVME_SGL_FMT_INVALIDATE:  RDMA transport specific remote invalidation
+ *                            request subtype
+ */
+enum {
+	NVME_SGL_FMT_ADDRESS		= 0x00,
+	NVME_SGL_FMT_OFFSET		= 0x01,
+	NVME_SGL_FMT_TRANSPORT_A	= 0x0A,
+	NVME_SGL_FMT_INVALIDATE		= 0x0f,
+};
+
+/*
+ * Descriptor type - upper 4 bits of nvme_(keyed_)sgl_desc identifier
+ *
+ * For struct nvme_sgl_desc:
+ *   @NVME_SGL_FMT_DATA_DESC:		data block descriptor
+ *   @NVME_SGL_FMT_SEG_DESC:		sgl segment descriptor
+ *   @NVME_SGL_FMT_LAST_SEG_DESC:	last sgl segment descriptor
+ *
+ * For struct nvme_keyed_sgl_desc:
+ *   @NVME_KEY_SGL_FMT_DATA_DESC:	keyed data block descriptor
+ *
+ * Transport-specific SGL types:
+ *   @NVME_TRANSPORT_SGL_DATA_DESC:	Transport SGL data dlock descriptor
+ */
+enum {
+	NVME_SGL_FMT_DATA_DESC		= 0x00,
+	NVME_SGL_FMT_SEG_DESC		= 0x02,
+	NVME_SGL_FMT_LAST_SEG_DESC	= 0x03,
+	NVME_KEY_SGL_FMT_DATA_DESC	= 0x04,
+	NVME_TRANSPORT_SGL_DATA_DESC	= 0x05,
+	/* 0x6 - 0xE reserved */
+	NVME_SGL_TYPE_VENDOR_SPECIFIC   = 0x0F
+};
+
+struct nvme_sgl_desc {
+	__le64	addr;
+	__le32	length;
+	__u8	rsvd[3];
+	__u8	type;
+};
+
+struct nvme_keyed_sgl_desc {
+	__le64	addr;
+	__u8	length[3];
+	__u8	key[4];
+	__u8	type;
+};
+
+union nvme_data_ptr {
+	struct {
+		__le64	prp1;
+		__le64	prp2;
+	};
+	struct nvme_sgl_desc	sgl;
+	struct nvme_keyed_sgl_desc ksgl;
+};
+
+/*
+ * Lowest two bits of our flags field (FUSE field in the spec):
+ *
+ * @NVME_CMD_FUSE_FIRST:   Fused Operation, first command
+ * @NVME_CMD_FUSE_SECOND:  Fused Operation, second command
+ *
+ * Highest two bits in our flags field (PSDT field in the spec):
+ *
+ * @NVME_CMD_PSDT_SGL_METABUF:	Use SGLS for this transfer,
+ *	If used, MPTR contains addr of single physical buffer (byte aligned).
+ * @NVME_CMD_PSDT_SGL_METASEG:	Use SGLS for this transfer,
+ *	If used, MPTR contains an address of an SGL segment containing
+ *	exactly 1 SGL descriptor (qword aligned).
+ */
+enum {
+	NVME_CMD_FUSE_FIRST	= (1 << 0),
+	NVME_CMD_FUSE_SECOND	= (1 << 1),
+
+	NVME_CMD_SGL_METABUF	= (1 << 6),
+	NVME_CMD_SGL_METASEG	= (1 << 7),
+	NVME_CMD_SGL_ALL	= NVME_CMD_SGL_METABUF | NVME_CMD_SGL_METASEG,
+};
+
+struct nvme_common_command {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__le32			cdw2[2];
+	__le64			metadata;
+	union nvme_data_ptr	dptr;
+	__le32			cdw10;
+	__le32			cdw11;
+	__le32			cdw12;
+	__le32			cdw13;
+	__le32			cdw14;
+	__le32			cdw15;
+};
+
+struct nvme_kv_tcp_common {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__le32			cdw2[2];
+	__le64			metadata;
+	union nvme_data_ptr	dptr;
+	__le32			cdw10;
+	__le32			cdw11;
+	union nvme_data_ptr	kptr;
+};
+
+
+struct nvme_rw_command {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd2;
+	__le64			metadata;
+	union nvme_data_ptr	dptr;
+	__le64			slba;
+	__le16			length;
+	__le16			control;
+	__le32			dsmgmt;
+	__le32			reftag;
+	__le16			apptag;
+	__le16			appmask;
+};
+
+enum {
+	NVME_RW_LR			= 1 << 15,
+	NVME_RW_FUA			= 1 << 14,
+	NVME_RW_DSM_FREQ_UNSPEC		= 0,
+	NVME_RW_DSM_FREQ_TYPICAL	= 1,
+	NVME_RW_DSM_FREQ_RARE		= 2,
+	NVME_RW_DSM_FREQ_READS		= 3,
+	NVME_RW_DSM_FREQ_WRITES		= 4,
+	NVME_RW_DSM_FREQ_RW		= 5,
+	NVME_RW_DSM_FREQ_ONCE		= 6,
+	NVME_RW_DSM_FREQ_PREFETCH	= 7,
+	NVME_RW_DSM_FREQ_TEMP		= 8,
+	NVME_RW_DSM_LATENCY_NONE	= 0 << 4,
+	NVME_RW_DSM_LATENCY_IDLE	= 1 << 4,
+	NVME_RW_DSM_LATENCY_NORM	= 2 << 4,
+	NVME_RW_DSM_LATENCY_LOW		= 3 << 4,
+	NVME_RW_DSM_SEQ_REQ		= 1 << 6,
+	NVME_RW_DSM_COMPRESSED		= 1 << 7,
+	NVME_RW_PRINFO_PRCHK_REF	= 1 << 10,
+	NVME_RW_PRINFO_PRCHK_APP	= 1 << 11,
+	NVME_RW_PRINFO_PRCHK_GUARD	= 1 << 12,
+	NVME_RW_PRINFO_PRACT		= 1 << 13,
+	NVME_RW_DTYPE_STREAMS		= 1 << 4,
+};
+
+struct nvme_dsm_cmd {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd2[2];
+	union nvme_data_ptr	dptr;
+	__le32			nr;
+	__le32			attributes;
+	__u32			rsvd12[4];
+};
+
+enum {
+	NVME_DSMGMT_IDR		= 1 << 0,
+	NVME_DSMGMT_IDW		= 1 << 1,
+	NVME_DSMGMT_AD		= 1 << 2,
+};
+
+#define NVME_DSM_MAX_RANGES	256
+
+struct nvme_dsm_range {
+	__le32			cattr;
+	__le32			nlb;
+	__le64			slba;
+};
+
+struct nvme_write_zeroes_cmd {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd2;
+	__le64			metadata;
+	union nvme_data_ptr	dptr;
+	__le64			slba;
+	__le16			length;
+	__le16			control;
+	__le32			dsmgmt;
+	__le32			reftag;
+	__le16			apptag;
+	__le16			appmask;
+};
+
+/* Features */
+
+struct nvme_feat_auto_pst {
+	__le64 entries[32];
+};
+
+enum {
+	NVME_HOST_MEM_ENABLE	= (1 << 0),
+	NVME_HOST_MEM_RETURN	= (1 << 1),
+};
+
+
+/* KV SSD command */
+struct nvme_kv_store_command {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+    union {
+	    __u64			rsvd;
+        struct {
+            __u32       rsvd2_1;
+            __u16       rdd_chandle;
+            __u16       rsvd3_1;
+        };
+    };
+	__le32			offset;
+    __u32			rsvd2;
+	union nvme_data_ptr	dptr; /* value dptr prp1,2 */
+	__le32			value_len; /* size in word */
+	__u8			key_len; /* 0 ~ 255 (key len -1) */
+	__u8			option;
+	__u8			invalid_byte:2;
+	__u8			rsvd3:6;
+	__u8			rsvd4;
+	union {
+		struct {
+			char	key[KVCMD_INLINE_KEY_MAX];
+		};
+		struct {
+			__le64	key_prp;
+			__le64	key_prp2;
+		};
+	};
+};
+
+struct nvme_kv_list_command {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd; /* 0:1 0->â€˜root/â€™ and start_key is beginning,
+                                               1-> prefix is â€˜root/â€™, key is start_key,
+                                               2-> keys is prefix, start_from beginning,
+                                               3-> key has prefix and start_key */
+	__le32			offset;
+	__u32			rsvd2;
+	union nvme_data_ptr	dptr; /* value dptr prp1,2 */
+	__le32			value_len; /* size in word */
+    __u8            key_len; /* 0 ~ 255 (key len -1) */
+    __u8            key_offset;
+    __u16           max_keys;
+	union {
+		struct {
+			char	key[16];
+		};
+		struct {
+			__le64	key_prp;
+			__le64	key_prp2;
+		};
+	};
+};
+
+struct nvme_kv_lock_command {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			uuid; /* Lock owner identifier */
+	__le32			rsvd1;
+	__u32			rsvd2;
+	union nvme_data_ptr	dptr; /* value dptr prp1,2 */
+	__le32			duration; /* lock valid duration*/
+    __u8            key_len; /* 0 ~ 255 (key len -1) */
+    __u8            priority:2;
+    __u8            writer:1;
+	__u8			blocking:1;
+    __u8			cdw11_rsvd1:4;
+    __u8			cdw11_rsvd2;
+	union {
+		struct {
+			char	key[16];
+		};
+		struct {
+			__le64	key_prp;
+			__le64	key_prp2;
+		};
+	};
+};
+
+struct nvme_kv_append_command {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd;
+	__le32			offset;
+	__u32			rsvd2;
+	union nvme_data_ptr	dptr; /* value dptr prp1,2 */
+	__le32			value_len; /* size in word */
+    __u8            key_len; /* 0 ~ 255 (key len -1) */
+    __u8            option;
+    __u8            invalid_byte:2;
+    __u8            rsvd3:6;
+    __u8            rsvd4;
+	union {
+		struct {
+			char	key[16];
+		};
+		struct {
+			__le64	key_prp;
+			__le64	key_prp2;
+		};
+	};
+};
+
+struct nvme_kv_retrieve_command {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+    union {
+	    __u64			rsvd;
+        struct {
+            __u32       rsvd2_1;
+            __u16       rdd_chandle;
+            __u16       rsvd3_1;
+        };
+    };
+	__le32			offset;
+	__u32			rsvd2;
+	union nvme_data_ptr	dptr; /* value dptr prp1,2 */
+	__le32			value_len; /* size in word */
+    __u8            key_len; /* 0 ~ 255 (key len -1) */
+    __u8            option;
+    __u16           rsvd3;
+	union {
+		struct {
+			char	key[16];
+		};
+		struct {
+			__le64	key_prp;
+			__le64	key_prp2;
+		};
+	};
+};
+
+struct nvme_kv_delete_command {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd;
+	__le32			offset;
+	__u32			rsvd2;
+	__u64			rsvd3[2];
+	__le32			value_len; /* should be zero*/
+    __u8            key_len; /* 0 ~ 255 (key len -1) */
+    __u8            option;
+    __u16           rsvd4;
+	union {
+		struct {
+			char	key[16];
+		};
+		struct {
+			__le64	key_prp;
+			__le64	key_prp2;
+		};
+	};
+};
+
+struct nvme_kv_iter_req_command {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd[4];
+	__le32			zero; /* should be zero*/
+    __u8            iter_handle;
+    __u8            option;
+    __u16           rsvd2;
+    __le32          iter_val;
+    __le32          iter_bitmask;
+    __u64           rsvd3;
+};
+
+
+struct nvme_kv_iter_read_command {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd[2];
+	union nvme_data_ptr	dptr; /* value dptr prp1,2 */
+	__le32			value_len; /* size in word */
+    __u8            iter_handle; /* 0 ~ 255 (key len -1) */
+    __u8            option;
+    __u16           rsvd2;
+    __u64           rsvd3[2];
+};
+struct nvme_kv_exist_command {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd;
+	__le32			offset;
+	__u32			rsvd2;
+	__u64			rsvd3[2];
+	__le32			value_len; /* should be zero*/
+    __u8            key_len; /* 0 ~ 255 (key len -1) */
+    __u8            option;
+    __u16           rsvd4;
+	union {
+		struct {
+			char	key[16];
+		};
+		struct {
+			__le64	key_prp;
+			__le64	key_prp2;
+		};
+	};
+};
+
+
+struct nvme_feat_host_behavior {
+	__u8 acre;
+	__u8 resv1[511];
+};
+
+enum {
+	NVME_ENABLE_ACRE	= 1,
+};
+
+/* Admin commands */
+
+enum nvme_admin_opcode {
+	nvme_admin_delete_sq		= 0x00,
+	nvme_admin_create_sq		= 0x01,
+	nvme_admin_get_log_page		= 0x02,
+	nvme_admin_delete_cq		= 0x04,
+	nvme_admin_create_cq		= 0x05,
+	nvme_admin_identify		= 0x06,
+	nvme_admin_abort_cmd		= 0x08,
+	nvme_admin_set_features		= 0x09,
+	nvme_admin_get_features		= 0x0a,
+	nvme_admin_async_event		= 0x0c,
+	nvme_admin_ns_mgmt		= 0x0d,
+	nvme_admin_activate_fw		= 0x10,
+	nvme_admin_download_fw		= 0x11,
+	nvme_admin_ns_attach		= 0x15,
+	nvme_admin_keep_alive		= 0x18,
+	nvme_admin_directive_send	= 0x19,
+	nvme_admin_directive_recv	= 0x1a,
+	nvme_admin_dbbuf		= 0x7C,
+	nvme_admin_format_nvm		= 0x80,
+	nvme_admin_security_send	= 0x81,
+	nvme_admin_security_recv	= 0x82,
+	nvme_admin_sanitize_nvm		= 0x84,
+};
+
+enum {
+	NVME_QUEUE_PHYS_CONTIG	= (1 << 0),
+	NVME_CQ_IRQ_ENABLED	= (1 << 1),
+	NVME_SQ_PRIO_URGENT	= (0 << 1),
+	NVME_SQ_PRIO_HIGH	= (1 << 1),
+	NVME_SQ_PRIO_MEDIUM	= (2 << 1),
+	NVME_SQ_PRIO_LOW	= (3 << 1),
+	NVME_FEAT_ARBITRATION	= 0x01,
+	NVME_FEAT_POWER_MGMT	= 0x02,
+	NVME_FEAT_LBA_RANGE	= 0x03,
+	NVME_FEAT_TEMP_THRESH	= 0x04,
+	NVME_FEAT_ERR_RECOVERY	= 0x05,
+	NVME_FEAT_VOLATILE_WC	= 0x06,
+	NVME_FEAT_NUM_QUEUES	= 0x07,
+	NVME_FEAT_IRQ_COALESCE	= 0x08,
+	NVME_FEAT_IRQ_CONFIG	= 0x09,
+	NVME_FEAT_WRITE_ATOMIC	= 0x0a,
+	NVME_FEAT_ASYNC_EVENT	= 0x0b,
+	NVME_FEAT_AUTO_PST	= 0x0c,
+	NVME_FEAT_HOST_MEM_BUF	= 0x0d,
+	NVME_FEAT_TIMESTAMP	= 0x0e,
+	NVME_FEAT_KATO		= 0x0f,
+	NVME_FEAT_HCTM		= 0x10,
+	NVME_FEAT_NOPSC		= 0x11,
+	NVME_FEAT_RRL		= 0x12,
+	NVME_FEAT_PLM_CONFIG	= 0x13,
+	NVME_FEAT_PLM_WINDOW	= 0x14,
+	NVME_FEAT_HOST_BEHAVIOR	= 0x16,
+	NVME_FEAT_SW_PROGRESS	= 0x80,
+	NVME_FEAT_HOST_ID	= 0x81,
+	NVME_FEAT_RESV_MASK	= 0x82,
+	NVME_FEAT_RESV_PERSIST	= 0x83,
+	NVME_FEAT_WRITE_PROTECT	= 0x84,
+	NVME_LOG_ERROR		= 0x01,
+	NVME_LOG_SMART		= 0x02,
+	NVME_LOG_FW_SLOT	= 0x03,
+	NVME_LOG_CHANGED_NS	= 0x04,
+	NVME_LOG_CMD_EFFECTS	= 0x05,
+	NVME_LOG_ANA		= 0x0c,
+	NVME_LOG_DISC		= 0x70,
+	NVME_LOG_RESERVATION	= 0x80,
+	NVME_FWACT_REPL		= (0 << 3),
+	NVME_FWACT_REPL_ACTV	= (1 << 3),
+	NVME_FWACT_ACTV		= (2 << 3),
+};
+
+/* NVMe Namespace Write Protect State */
+enum {
+	NVME_NS_NO_WRITE_PROTECT = 0,
+	NVME_NS_WRITE_PROTECT,
+	NVME_NS_WRITE_PROTECT_POWER_CYCLE,
+	NVME_NS_WRITE_PROTECT_PERMANENT,
+};
+
+#define NVME_MAX_CHANGED_NAMESPACES	1024
+
+struct nvme_identify {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd2[2];
+	union nvme_data_ptr	dptr;
+	__u8			cns;
+	__u8			rsvd3;
+	__le16			ctrlid;
+	__u32			rsvd11[5];
+};
+
+#define NVME_IDENTIFY_DATA_SIZE 4096
+
+struct nvme_features {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd2[2];
+	union nvme_data_ptr	dptr;
+	__le32			fid;
+	__le32			dword11;
+	__le32                  dword12;
+	__le32                  dword13;
+	__le32                  dword14;
+	__le32                  dword15;
+};
+
+struct nvme_host_mem_buf_desc {
+	__le64			addr;
+	__le32			size;
+	__u32			rsvd;
+};
+
+struct nvme_create_cq {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__u32			rsvd1[5];
+	__le64			prp1;
+	__u64			rsvd8;
+	__le16			cqid;
+	__le16			qsize;
+	__le16			cq_flags;
+	__le16			irq_vector;
+	__u32			rsvd12[4];
+};
+
+struct nvme_create_sq {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__u32			rsvd1[5];
+	__le64			prp1;
+	__u64			rsvd8;
+	__le16			sqid;
+	__le16			qsize;
+	__le16			sq_flags;
+	__le16			cqid;
+	__u32			rsvd12[4];
+};
+
+struct nvme_delete_queue {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__u32			rsvd1[9];
+	__le16			qid;
+	__u16			rsvd10;
+	__u32			rsvd11[5];
+};
+
+struct nvme_abort_cmd {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__u32			rsvd1[9];
+	__le16			sqid;
+	__u16			cid;
+	__u32			rsvd11[5];
+};
+
+struct nvme_download_firmware {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__u32			rsvd1[5];
+	union nvme_data_ptr	dptr;
+	__le32			numd;
+	__le32			offset;
+	__u32			rsvd12[4];
+};
+
+struct nvme_format_cmd {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd2[4];
+	__le32			cdw10;
+	__u32			rsvd11[5];
+};
+
+struct nvme_get_log_page_command {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd2[2];
+	union nvme_data_ptr	dptr;
+	__u8			lid;
+	__u8			lsp; /* upper 4 bits reserved */
+	__le16			numdl;
+	__le16			numdu;
+	__u16			rsvd11;
+	union {
+		struct {
+			__le32 lpol;
+			__le32 lpou;
+		};
+		__le64 lpo;
+	};
+	__u32			rsvd14[2];
+};
+
+struct nvme_directive_cmd {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__le32			nsid;
+	__u64			rsvd2[2];
+	union nvme_data_ptr	dptr;
+	__le32			numd;
+	__u8			doper;
+	__u8			dtype;
+	__le16			dspec;
+	__u8			endir;
+	__u8			tdtype;
+	__u16			rsvd15;
+
+	__u32			rsvd16[3];
+};
+
+/*
+ * Fabrics subcommands.
+ */
+enum nvmf_fabrics_opcode {
+	nvme_fabrics_command		= 0x7f,
+};
+
+enum nvmf_capsule_command {
+	nvme_fabrics_type_property_set	= 0x00,
+	nvme_fabrics_type_connect	= 0x01,
+	nvme_fabrics_type_property_get	= 0x04,
+};
+
+struct nvmf_common_command {
+	__u8	opcode;
+	__u8	resv1;
+	__u16	command_id;
+	__u8	fctype;
+	__u8	resv2[35];
+	__u8	ts[24];
+};
+
+/*
+ * The legal cntlid range a NVMe Target will provide.
+ * Note that cntlid of value 0 is considered illegal in the fabrics world.
+ * Devices based on earlier specs did not have the subsystem concept;
+ * therefore, those devices had their cntlid value set to 0 as a result.
+ */
+#define NVME_CNTLID_MIN		1
+#define NVME_CNTLID_MAX		0xffef
+#define NVME_CNTLID_DYNAMIC	0xffff
+
+#define MAX_DISC_LOGS	255
+
+/* Discovery log page entry */
+struct nvmf_disc_rsp_page_entry {
+	__u8		trtype;
+	__u8		adrfam;
+	__u8		subtype;
+	__u8		treq;
+	__le16		portid;
+	__le16		cntlid;
+	__le16		asqsz;
+	__u8		resv8[22];
+	char		trsvcid[NVMF_TRSVCID_SIZE];
+	__u8		resv64[192];
+	char		subnqn[NVMF_NQN_FIELD_LEN];
+	char		traddr[NVMF_TRADDR_SIZE];
+	union tsas {
+		char		common[NVMF_TSAS_SIZE];
+		struct rdma {
+			__u8	qptype;
+			__u8	prtype;
+			__u8	cms;
+			__u8	resv3[5];
+			__u16	pkey;
+			__u8	resv10[246];
+		} rdma;
+	} tsas;
+};
+
+/* Discovery log page header */
+struct nvmf_disc_rsp_page_hdr {
+	__le64		genctr;
+	__le64		numrec;
+	__le16		recfmt;
+	__u8		resv14[1006];
+	struct nvmf_disc_rsp_page_entry entries[0];
+};
+
+enum {
+	NVME_CONNECT_DISABLE_SQFLOW	= (1 << 2),
+};
+
+struct nvmf_connect_command {
+	__u8		opcode;
+	__u8		resv1;
+	__u16		command_id;
+	__u8		fctype;
+	__u8		resv2[19];
+	union nvme_data_ptr dptr;
+	__le16		recfmt;
+	__le16		qid;
+	__le16		sqsize;
+	__u8		cattr;
+	__u8		resv3;
+	__le32		kato;
+	__u8		resv4[12];
+};
+
+struct nvmf_connect_data {
+	uuid_t		hostid;
+	__le16		cntlid;
+	char		resv4[238];
+	char		subsysnqn[NVMF_NQN_FIELD_LEN];
+	char		hostnqn[NVMF_NQN_FIELD_LEN];
+	char		resv5[256];
+};
+
+struct nvmf_property_set_command {
+	__u8		opcode;
+	__u8		resv1;
+	__u16		command_id;
+	__u8		fctype;
+	__u8		resv2[35];
+	__u8		attrib;
+	__u8		resv3[3];
+	__le32		offset;
+	__le64		value;
+	__u8		resv4[8];
+};
+
+struct nvmf_property_get_command {
+	__u8		opcode;
+	__u8		resv1;
+	__u16		command_id;
+	__u8		fctype;
+	__u8		resv2[35];
+	__u8		attrib;
+	__u8		resv3[3];
+	__le32		offset;
+	__u8		resv4[16];
+};
+
+struct nvme_dbbuf {
+	__u8			opcode;
+	__u8			flags;
+	__u16			command_id;
+	__u32			rsvd1[5];
+	__le64			prp1;
+	__le64			prp2;
+	__u32			rsvd12[6];
+};
+
+struct streams_directive_params {
+	__le16	msl;
+	__le16	nssa;
+	__le16	nsso;
+	__u8	rsvd[10];
+	__le32	sws;
+	__le16	sgs;
+	__le16	nsa;
+	__le16	nso;
+	__u8	rsvd2[6];
+};
+
+struct nvme_command {
+	union {
+		struct nvme_common_command common;
+		struct nvme_rw_command rw;
+		struct nvme_identify identify;
+		struct nvme_features features;
+		struct nvme_create_cq create_cq;
+		struct nvme_create_sq create_sq;
+		struct nvme_delete_queue delete_queue;
+		struct nvme_download_firmware dlfw;
+		struct nvme_format_cmd format;
+		struct nvme_dsm_cmd dsm;
+		struct nvme_write_zeroes_cmd write_zeroes;
+		struct nvme_abort_cmd abort;
+		struct nvme_get_log_page_command get_log_page;
+		struct nvmf_common_command fabrics;
+		struct nvmf_connect_command connect;
+		struct nvmf_property_set_command prop_set;
+		struct nvmf_property_get_command prop_get;
+		struct nvme_dbbuf dbbuf;
+		struct nvme_directive_cmd directive;
+
+		struct nvme_kv_store_command kv_store;
+		struct nvme_kv_list_command kv_list;
+		struct nvme_kv_retrieve_command kv_retrieve;
+		struct nvme_kv_delete_command kv_delete;
+		struct nvme_kv_append_command kv_append;
+		struct nvme_kv_iter_req_command kv_iter_req;
+		struct nvme_kv_iter_read_command kv_iter_read;
+		struct nvme_kv_exist_command kv_exist;
+		struct nvme_kv_lock_command kv_lock;
+		struct nvme_kv_lock_command kv_unlock;
+
+		struct nvme_kv_tcp_common kv_tcp;
+	};
+};
+
+struct nvme_error_slot {
+	__le64		error_count;
+	__le16		sqid;
+	__le16		cmdid;
+	__le16		status_field;
+	__le16		param_error_location;
+	__le64		lba;
+	__le32		nsid;
+	__u8		vs;
+	__u8		resv[3];
+	__le64		cs;
+	__u8		resv2[24];
+};
+
+static inline bool nvme_is_write(struct nvme_command *cmd)
+{
+	/*
+	 * What a mess...
+	 *
+	 * Why can't we simply have a Fabrics In and Fabrics out command?
+	 */
+
+	if (cmd->common.opcode == nvme_cmd_kv_store ||
+	    cmd->common.opcode == nvme_cmd_kv_append ||
+	    cmd->common.opcode == nvme_cmd_kv_delete )
+		return 1;
+	if (cmd->common.opcode == nvme_cmd_kv_retrieve ||
+		//cmd->common.opcode == nvme_cmd_kv_delete ||
+                cmd->common.opcode == nvme_cmd_kv_list ||
+                cmd->common.opcode == nvme_cmd_kv_iter_req ||
+		cmd->common.opcode == nvme_cmd_kv_iter_read ||
+		cmd->common.opcode == nvme_cmd_kv_exist )
+		return 0;
+
+	if (unlikely(cmd->common.opcode == nvme_fabrics_command))
+		return cmd->fabrics.fctype & 1;
+	return cmd->common.opcode & 1;
+}
+
+enum {
+	/*
+	 * Generic Command Status:
+	 */
+	NVME_SC_SUCCESS			= 0x0,
+	NVME_SC_INVALID_OPCODE		= 0x1,
+	NVME_SC_INVALID_FIELD		= 0x2,
+	NVME_SC_CMDID_CONFLICT		= 0x3,
+	NVME_SC_DATA_XFER_ERROR		= 0x4,
+	NVME_SC_POWER_LOSS		= 0x5,
+	NVME_SC_INTERNAL		= 0x6,
+	NVME_SC_ABORT_REQ		= 0x7,
+	NVME_SC_ABORT_QUEUE		= 0x8,
+	NVME_SC_FUSED_FAIL		= 0x9,
+	NVME_SC_FUSED_MISSING		= 0xa,
+	NVME_SC_INVALID_NS		= 0xb,
+	NVME_SC_CMD_SEQ_ERROR		= 0xc,
+	NVME_SC_SGL_INVALID_LAST	= 0xd,
+	NVME_SC_SGL_INVALID_COUNT	= 0xe,
+	NVME_SC_SGL_INVALID_DATA	= 0xf,
+	NVME_SC_SGL_INVALID_METADATA	= 0x10,
+	NVME_SC_SGL_INVALID_TYPE	= 0x11,
+
+	NVME_SC_SGL_INVALID_OFFSET	= 0x16,
+	NVME_SC_SGL_INVALID_SUBTYPE	= 0x17,
+
+	NVME_SC_NS_WRITE_PROTECTED	= 0x20,
+
+	NVME_SC_LBA_RANGE		= 0x80,
+	NVME_SC_CAP_EXCEEDED		= 0x81,
+	NVME_SC_NS_NOT_READY		= 0x82,
+	NVME_SC_RESERVATION_CONFLICT	= 0x83,
+
+	/*
+	 * Command Specific Status:
+	 */
+	NVME_SC_CQ_INVALID		= 0x100,
+	NVME_SC_QID_INVALID		= 0x101,
+	NVME_SC_QUEUE_SIZE		= 0x102,
+	NVME_SC_ABORT_LIMIT		= 0x103,
+	NVME_SC_ABORT_MISSING		= 0x104,
+	NVME_SC_ASYNC_LIMIT		= 0x105,
+	NVME_SC_FIRMWARE_SLOT		= 0x106,
+	NVME_SC_FIRMWARE_IMAGE		= 0x107,
+	NVME_SC_INVALID_VECTOR		= 0x108,
+	NVME_SC_INVALID_LOG_PAGE	= 0x109,
+	NVME_SC_INVALID_FORMAT		= 0x10a,
+	NVME_SC_FW_NEEDS_CONV_RESET	= 0x10b,
+	NVME_SC_INVALID_QUEUE		= 0x10c,
+	NVME_SC_FEATURE_NOT_SAVEABLE	= 0x10d,
+	NVME_SC_FEATURE_NOT_CHANGEABLE	= 0x10e,
+	NVME_SC_FEATURE_NOT_PER_NS	= 0x10f,
+	NVME_SC_FW_NEEDS_SUBSYS_RESET	= 0x110,
+	NVME_SC_FW_NEEDS_RESET		= 0x111,
+	NVME_SC_FW_NEEDS_MAX_TIME	= 0x112,
+	NVME_SC_FW_ACIVATE_PROHIBITED	= 0x113,
+	NVME_SC_OVERLAPPING_RANGE	= 0x114,
+	NVME_SC_NS_INSUFFICENT_CAP	= 0x115,
+	NVME_SC_NS_ID_UNAVAILABLE	= 0x116,
+	NVME_SC_NS_ALREADY_ATTACHED	= 0x118,
+	NVME_SC_NS_IS_PRIVATE		= 0x119,
+	NVME_SC_NS_NOT_ATTACHED		= 0x11a,
+	NVME_SC_THIN_PROV_NOT_SUPP	= 0x11b,
+	NVME_SC_CTRL_LIST_INVALID	= 0x11c,
+
+	/*
+	 * I/O Command Set Specific - NVM commands:
+	 */
+	NVME_SC_BAD_ATTRIBUTES		= 0x180,
+	NVME_SC_INVALID_PI		= 0x181,
+	NVME_SC_READ_ONLY		= 0x182,
+	NVME_SC_ONCS_NOT_SUPPORTED	= 0x183,
+
+	/*
+	 * I/O Command Set Specific - Fabrics commands:
+	 */
+	NVME_SC_CONNECT_FORMAT		= 0x180,
+	NVME_SC_CONNECT_CTRL_BUSY	= 0x181,
+	NVME_SC_CONNECT_INVALID_PARAM	= 0x182,
+	NVME_SC_CONNECT_RESTART_DISC	= 0x183,
+	NVME_SC_CONNECT_INVALID_HOST	= 0x184,
+
+	NVME_SC_DISCOVERY_RESTART	= 0x190,
+	NVME_SC_AUTH_REQUIRED		= 0x191,
+
+	/*
+	 * Media and Data Integrity Errors:
+	 */
+	NVME_SC_WRITE_FAULT		= 0x280,
+	NVME_SC_READ_ERROR		= 0x281,
+	NVME_SC_GUARD_CHECK		= 0x282,
+	NVME_SC_APPTAG_CHECK		= 0x283,
+	NVME_SC_REFTAG_CHECK		= 0x284,
+	NVME_SC_COMPARE_FAILED		= 0x285,
+	NVME_SC_ACCESS_DENIED		= 0x286,
+	NVME_SC_UNWRITTEN_BLOCK		= 0x287,
+
+	/*
+	 * Path-related Errors:
+	 */
+	NVME_SC_ANA_PERSISTENT_LOSS	= 0x301,
+	NVME_SC_ANA_INACCESSIBLE	= 0x302,
+	NVME_SC_ANA_TRANSITION		= 0x303,
+	NVME_SC_HOST_PATH_ERROR		= 0x370,
+
+	NVME_SC_CRD			= 0x1800,
+	NVME_SC_DNR			= 0x4000,
+};
+
+struct nvme_completion {
+	/*
+	 * Used by Admin and Fabrics commands to return data:
+	 */
+	union nvme_result {
+		__le16	u16;
+		__le32	u32;
+		__le64	u64;
+	} result;
+	__le16	sq_head;	/* how much of this queue may be reclaimed */
+	__le16	sq_id;		/* submission queue that generated this entry */
+	__u16	command_id;	/* of the command which completed */
+	__le16	status;		/* did the command fail, and if so, why? */
+};
+
+#define NVME_VS(major, minor, tertiary) \
+	(((major) << 16) | ((minor) << 8) | (tertiary))
+
+#define NVME_MAJOR(ver)		((ver) >> 16)
+#define NVME_MINOR(ver)		(((ver) >> 8) & 0xff)
+#define NVME_TERTIARY(ver)	((ver) & 0xff)
+
+#endif /* _LINUX_NVME_H */
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/linux_nvme_ioctl.h b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/linux_nvme_ioctl.h
new file mode 100644
index 0000000..b4809e0
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/linux_nvme_ioctl.h
@@ -0,0 +1,165 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ * Definitions for the NVM Express ioctl interface
+ * Copyright (c) 2011-2014, Intel Corporation.
+ */
+
+#ifndef _UAPI_LINUX_NVME_IOCTL_H
+#define _UAPI_LINUX_NVME_IOCTL_H
+
+#include <linux/types.h>
+
+struct nvme_user_io {
+	__u8	opcode;
+	__u8	flags;
+	__u16	control;
+	__u16	nblocks;
+	__u16	rsvd;
+	__u64	metadata;
+	__u64	addr;
+	__u64	slba;
+	__u32	dsmgmt;
+	__u32	reftag;
+	__u16	apptag;
+	__u16	appmask;
+};
+
+struct nvme_passthru_cmd {
+	__u8	opcode;
+	__u8	flags;
+	__u16	rsvd1;
+	__u32	nsid;
+	__u32	cdw2;
+	__u32	cdw3;
+	__u64	metadata;
+	__u64	addr;
+	__u32	metadata_len;
+	__u32	data_len;
+	__u32	cdw10;
+	__u32	cdw11;
+	__u32	cdw12;
+	__u32	cdw13;
+	__u32	cdw14;
+	__u32	cdw15;
+	__u32	timeout_ms;
+	__u32	result;
+};
+
+#define	KVS_SUCCESS		0
+#define KVS_ERR_ALIGNMENT	(-1)
+#define KVS_ERR_CAPAPCITY	(-2)
+#define KVS_ERR_CLOSE	(-3)
+#define KVS_ERR_CONT_EXIST	(-4)
+#define KVS_ERR_CONT_NAME	(-5)
+#define KVS_ERR_CONT_NOT_EXIST	(-6)
+#define KVS_ERR_DEVICE_NOT_EXIST (-7)
+#define KVS_ERR_GROUP	(-8)
+#define KVS_ERR_INDEX	(-9)
+#define KVS_ERR_IO	(-10)
+#define KVS_ERR_KEY	(-11)
+#define KVS_ERR_KEY_TYPE	(-12)
+#define KVS_ERR_MEMORY	(-13)
+#define KVS_ERR_NULL_INPUT	(-14)
+#define KVS_ERR_OFFSET	(-15)
+#define KVS_ERR_OPEN	(-16)
+#define KVS_ERR_OPTION_NOT_SUPPORTED	(-17)
+#define KVS_ERR_PERMISSION	(-18)
+#define KVS_ERR_SPACE	(-19)
+#define KVS_ERR_TIMEOUT	(-20)
+#define KVS_ERR_TUPLE_EXIST	(-21)
+#define KVS_ERR_TUPLE_NOT_EXIST	(-22)
+#define KVS_ERR_VALUE	(-23)
+
+
+struct nvme_passthru_kv_cmd {
+	__u8	opcode;
+	__u8	flags;
+	__u16	rsvd1;
+	__u32	nsid;
+	__u32	cdw2;
+	__u32	cdw3;
+	__u32	cdw4;
+	__u32	cdw5;
+	struct {
+		__u64	data_addr;
+		union {
+			struct {
+				__u32	data_length;
+				__u8	rsvd2;//Not used
+				__u8	list_key_offset;//List key offset
+				__u16	list_max_keys;//List max keys
+			};
+			struct {
+				__u8    length[3];//Length in 24 bit
+				__u8    rkey[4];//RDMA remote key
+				__u8    type;//SGL Type
+			};//Keyed SGL second half
+		};
+	};//dptr
+	__u32	cdw10;
+	union {
+		__u32	cdw11;
+		struct {
+			__u8 kv_klen;
+			__u8 kv_option;
+			__u16 rdd_chandle;//RDMA direct client handle
+		};
+	};
+	union {
+		struct {
+			__u64 key_addr;
+			__u32 key_length;
+			__u32 rsvd6;
+		};
+		__u8 key[16];
+		struct {
+			__u32 cdw12;
+			__u32 cdw13;
+			__u32 cdw14;
+			__u32 cdw15;
+		};
+	};
+	__u32	timeout_ms;
+	__u32	result;
+	__u32	status;
+	__u32	ctxid;
+	__u64	reqid;
+};
+
+struct nvme_aioctx {
+	__u32	ctxid;
+	__u32	eventfd;
+};
+
+
+struct nvme_aioevent {
+	__u64	reqid;
+	__u32	ctxid;
+	__u32	result;
+	__u16	status;
+};
+
+#define MAX_AIO_EVENTS	128
+struct nvme_aioevents {
+	__u16	nr;
+	__u32   ctxid;
+	struct nvme_aioevent events[MAX_AIO_EVENTS];
+};
+
+
+#define nvme_admin_cmd nvme_passthru_cmd
+
+#define NVME_IOCTL_ID		_IO('N', 0x40)
+#define NVME_IOCTL_ADMIN_CMD	_IOWR('N', 0x41, struct nvme_admin_cmd)
+#define NVME_IOCTL_SUBMIT_IO	_IOW('N', 0x42, struct nvme_user_io)
+#define NVME_IOCTL_IO_CMD	_IOWR('N', 0x43, struct nvme_passthru_cmd)
+#define NVME_IOCTL_RESET	_IO('N', 0x44)
+#define NVME_IOCTL_SUBSYS_RESET	_IO('N', 0x45)
+#define NVME_IOCTL_RESCAN	_IO('N', 0x46)
+
+#define NVME_IOCTL_AIO_CMD		_IOWR('N', 0x47, struct nvme_passthru_kv_cmd)
+#define NVME_IOCTL_SET_AIOCTX	_IOWR('N', 0x48, struct nvme_aioctx)
+#define NVME_IOCTL_DEL_AIOCTX	_IOWR('N', 0x49, struct nvme_aioctx)
+#define NVME_IOCTL_GET_AIOEVENT	_IOWR('N', 0x50, struct nvme_aioevents)
+#define NVME_IOCTL_IO_KV_CMD	_IOWR('N', 0x51, struct nvme_passthru_kv_cmd)
+#endif /* _UAPI_LINUX_NVME_IOCTL_H */
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/multipath.c b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/multipath.c
new file mode 100644
index 0000000..f0716f6
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/multipath.c
@@ -0,0 +1,655 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2017-2018 Christoph Hellwig.
+ */
+
+#include <linux/moduleparam.h>
+#include <trace/events/block.h>
+#include "nvme.h"
+
+static bool multipath = true;
+module_param(multipath, bool, 0444);
+MODULE_PARM_DESC(multipath,
+	"turn on native support for multiple controllers per subsystem");
+
+inline bool nvme_ctrl_use_ana(struct nvme_ctrl *ctrl)
+{
+	return multipath && ctrl->subsys && (ctrl->subsys->cmic & (1 << 3));
+}
+
+/*
+ * If multipathing is enabled we need to always use the subsystem instance
+ * number for numbering our devices to avoid conflicts between subsystems that
+ * have multiple controllers and thus use the multipath-aware subsystem node
+ * and those that have a single controller and use the controller node
+ * directly.
+ */
+void nvme_set_disk_name(char *disk_name, struct nvme_ns *ns,
+			struct nvme_ctrl *ctrl, int *flags)
+{
+	if (!multipath) {
+		sprintf(disk_name, "nvme%dn%d", ctrl->instance, ns->head->instance);
+	} else if (ns->head->disk) {
+		sprintf(disk_name, "nvme%dc%dn%d", ctrl->subsys->instance,
+				ctrl->cntlid, ns->head->instance);
+		*flags = GENHD_FL_HIDDEN;
+	} else {
+		sprintf(disk_name, "nvme%dn%d", ctrl->subsys->instance,
+				ns->head->instance);
+	}
+}
+
+void nvme_failover_req(struct request *req)
+{
+	struct nvme_ns *ns = req->q->queuedata;
+	u16 status = nvme_req(req)->status;
+	unsigned long flags;
+
+	spin_lock_irqsave(&ns->head->requeue_lock, flags);
+	blk_steal_bios(&ns->head->requeue_list, req);
+	spin_unlock_irqrestore(&ns->head->requeue_lock, flags);
+	blk_mq_end_request(req, 0);
+
+	switch (status & 0x7ff) {
+	case NVME_SC_ANA_TRANSITION:
+	case NVME_SC_ANA_INACCESSIBLE:
+	case NVME_SC_ANA_PERSISTENT_LOSS:
+		/*
+		 * If we got back an ANA error we know the controller is alive,
+		 * but not ready to serve this namespaces.  The spec suggests
+		 * we should update our general state here, but due to the fact
+		 * that the admin and I/O queues are not serialized that is
+		 * fundamentally racy.  So instead just clear the current path,
+		 * mark the the path as pending and kick of a re-read of the ANA
+		 * log page ASAP.
+		 */
+		nvme_mpath_clear_current_path(ns);
+		if (ns->ctrl->ana_log_buf) {
+			set_bit(NVME_NS_ANA_PENDING, &ns->flags);
+			queue_work(nvme_wq, &ns->ctrl->ana_work);
+		}
+		break;
+	case NVME_SC_HOST_PATH_ERROR:
+		/*
+		 * Temporary transport disruption in talking to the controller.
+		 * Try to send on a new path.
+		 */
+		nvme_mpath_clear_current_path(ns);
+		break;
+	default:
+		/*
+		 * Reset the controller for any non-ANA error as we don't know
+		 * what caused the error.
+		 */
+		nvme_reset_ctrl(ns->ctrl);
+		break;
+	}
+
+	kblockd_schedule_work(&ns->head->requeue_work);
+}
+
+void nvme_kick_requeue_lists(struct nvme_ctrl *ctrl)
+{
+	struct nvme_ns *ns;
+
+	down_read(&ctrl->namespaces_rwsem);
+	list_for_each_entry(ns, &ctrl->namespaces, list) {
+		if (ns->head->disk)
+			kblockd_schedule_work(&ns->head->requeue_work);
+	}
+	up_read(&ctrl->namespaces_rwsem);
+}
+
+static const char *nvme_ana_state_names[] = {
+	[0]				= "invalid state",
+	[NVME_ANA_OPTIMIZED]		= "optimized",
+	[NVME_ANA_NONOPTIMIZED]		= "non-optimized",
+	[NVME_ANA_INACCESSIBLE]		= "inaccessible",
+	[NVME_ANA_PERSISTENT_LOSS]	= "persistent-loss",
+	[NVME_ANA_CHANGE]		= "change",
+};
+
+void nvme_mpath_clear_current_path(struct nvme_ns *ns)
+{
+	struct nvme_ns_head *head = ns->head;
+	int node;
+
+	if (!head)
+		return;
+
+	for_each_node(node) {
+		if (ns == rcu_access_pointer(head->current_path[node]))
+			rcu_assign_pointer(head->current_path[node], NULL);
+	}
+}
+
+static struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head, int node)
+{
+	int found_distance = INT_MAX, fallback_distance = INT_MAX, distance;
+	struct nvme_ns *found = NULL, *fallback = NULL, *ns;
+
+	list_for_each_entry_rcu(ns, &head->list, siblings) {
+		if (ns->ctrl->state != NVME_CTRL_LIVE ||
+		    test_bit(NVME_NS_ANA_PENDING, &ns->flags))
+			continue;
+
+		if (READ_ONCE(head->subsys->iopolicy) == NVME_IOPOLICY_NUMA)
+			distance = node_distance(node, ns->ctrl->numa_node);
+		else
+			distance = LOCAL_DISTANCE;
+
+		switch (ns->ana_state) {
+		case NVME_ANA_OPTIMIZED:
+			if (distance < found_distance) {
+				found_distance = distance;
+				found = ns;
+			}
+			break;
+		case NVME_ANA_NONOPTIMIZED:
+			if (distance < fallback_distance) {
+				fallback_distance = distance;
+				fallback = ns;
+			}
+			break;
+		default:
+			break;
+		}
+	}
+
+	if (!found)
+		found = fallback;
+	if (found)
+		rcu_assign_pointer(head->current_path[node], found);
+	return found;
+}
+
+static struct nvme_ns *nvme_next_ns(struct nvme_ns_head *head,
+		struct nvme_ns *ns)
+{
+	ns = list_next_or_null_rcu(&head->list, &ns->siblings, struct nvme_ns,
+			siblings);
+	if (ns)
+		return ns;
+	return list_first_or_null_rcu(&head->list, struct nvme_ns, siblings);
+}
+
+static struct nvme_ns *nvme_round_robin_path(struct nvme_ns_head *head,
+		int node, struct nvme_ns *old)
+{
+	struct nvme_ns *ns, *found, *fallback = NULL;
+
+	if (list_is_singular(&head->list))
+		return old;
+
+	for (ns = nvme_next_ns(head, old);
+	     ns != old;
+	     ns = nvme_next_ns(head, ns)) {
+		if (ns->ctrl->state != NVME_CTRL_LIVE ||
+		    test_bit(NVME_NS_ANA_PENDING, &ns->flags))
+			continue;
+
+		if (ns->ana_state == NVME_ANA_OPTIMIZED) {
+			found = ns;
+			goto out;
+		}
+		if (ns->ana_state == NVME_ANA_NONOPTIMIZED)
+			fallback = ns;
+	}
+
+	if (!fallback)
+		return NULL;
+	found = fallback;
+out:
+	rcu_assign_pointer(head->current_path[node], found);
+	return found;
+}
+
+static inline bool nvme_path_is_optimized(struct nvme_ns *ns)
+{
+	return ns->ctrl->state == NVME_CTRL_LIVE &&
+		ns->ana_state == NVME_ANA_OPTIMIZED;
+}
+
+inline struct nvme_ns *nvme_find_path(struct nvme_ns_head *head)
+{
+	int node = numa_node_id();
+	struct nvme_ns *ns;
+
+	ns = srcu_dereference(head->current_path[node], &head->srcu);
+	if (READ_ONCE(head->subsys->iopolicy) == NVME_IOPOLICY_RR && ns)
+		ns = nvme_round_robin_path(head, node, ns);
+	if (unlikely(!ns || !nvme_path_is_optimized(ns)))
+		ns = __nvme_find_path(head, node);
+	return ns;
+}
+
+static blk_qc_t nvme_ns_head_make_request(struct request_queue *q,
+		struct bio *bio)
+{
+	struct nvme_ns_head *head = q->queuedata;
+	struct device *dev = disk_to_dev(head->disk);
+	struct nvme_ns *ns;
+	blk_qc_t ret = BLK_QC_T_NONE;
+	int srcu_idx;
+
+	srcu_idx = srcu_read_lock(&head->srcu);
+	ns = nvme_find_path(head);
+	if (likely(ns)) {
+		bio->bi_disk = ns->disk;
+		bio->bi_opf |= REQ_NVME_MPATH;
+		trace_block_bio_remap(bio->bi_disk->queue, bio,
+				      disk_devt(ns->head->disk),
+				      bio->bi_iter.bi_sector);
+		ret = direct_make_request(bio);
+	} else if (!list_empty_careful(&head->list)) {
+		dev_warn_ratelimited(dev, "no path available - requeuing I/O\n");
+
+		spin_lock_irq(&head->requeue_lock);
+		bio_list_add(&head->requeue_list, bio);
+		spin_unlock_irq(&head->requeue_lock);
+	} else {
+		dev_warn_ratelimited(dev, "no path - failing I/O\n");
+
+		bio->bi_status = BLK_STS_IOERR;
+		bio_endio(bio);
+	}
+
+	srcu_read_unlock(&head->srcu, srcu_idx);
+	return ret;
+}
+
+static void nvme_requeue_work(struct work_struct *work)
+{
+	struct nvme_ns_head *head =
+		container_of(work, struct nvme_ns_head, requeue_work);
+	struct bio *bio, *next;
+
+	spin_lock_irq(&head->requeue_lock);
+	next = bio_list_get(&head->requeue_list);
+	spin_unlock_irq(&head->requeue_lock);
+
+	while ((bio = next) != NULL) {
+		next = bio->bi_next;
+		bio->bi_next = NULL;
+
+		/*
+		 * Reset disk to the mpath node and resubmit to select a new
+		 * path.
+		 */
+		bio->bi_disk = head->disk;
+		generic_make_request(bio);
+	}
+}
+
+int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)
+{
+	struct request_queue *q;
+	bool vwc = false;
+
+	mutex_init(&head->lock);
+	bio_list_init(&head->requeue_list);
+	spin_lock_init(&head->requeue_lock);
+	INIT_WORK(&head->requeue_work, nvme_requeue_work);
+
+	/*
+	 * Add a multipath node if the subsystems supports multiple controllers.
+	 * We also do this for private namespaces as the namespace sharing data could
+	 * change after a rescan.
+	 */
+	if (!(ctrl->subsys->cmic & (1 << 1)) || !multipath)
+		return 0;
+
+	q = blk_alloc_queue_node(GFP_KERNEL, ctrl->numa_node);
+	if (!q)
+		goto out;
+	q->queuedata = head;
+	blk_queue_make_request(q, nvme_ns_head_make_request);
+	blk_queue_flag_set(QUEUE_FLAG_NONROT, q);
+	/* set to a default value for 512 until disk is validated */
+	blk_queue_logical_block_size(q, 512);
+	blk_set_stacking_limits(&q->limits);
+
+	/* we need to propagate up the VMC settings */
+	if (ctrl->vwc & NVME_CTRL_VWC_PRESENT)
+		vwc = true;
+	blk_queue_write_cache(q, vwc, vwc);
+
+	head->disk = alloc_disk(0);
+	if (!head->disk)
+		goto out_cleanup_queue;
+	head->disk->fops = &nvme_ns_head_ops;
+	head->disk->private_data = head;
+	head->disk->queue = q;
+	head->disk->flags = GENHD_FL_EXT_DEVT;
+	sprintf(head->disk->disk_name, "nvme%dn%d",
+			ctrl->subsys->instance, head->instance);
+	return 0;
+
+out_cleanup_queue:
+	blk_cleanup_queue(q);
+out:
+	return -ENOMEM;
+}
+
+static void nvme_mpath_set_live(struct nvme_ns *ns)
+{
+	struct nvme_ns_head *head = ns->head;
+
+	lockdep_assert_held(&ns->head->lock);
+
+	if (!head->disk)
+		return;
+
+	if (!(head->disk->flags & GENHD_FL_UP))
+		device_add_disk(&head->subsys->dev, head->disk,
+				nvme_ns_id_attr_groups);
+
+	if (nvme_path_is_optimized(ns)) {
+		int node, srcu_idx;
+
+		srcu_idx = srcu_read_lock(&head->srcu);
+		for_each_node(node)
+			__nvme_find_path(head, node);
+		srcu_read_unlock(&head->srcu, srcu_idx);
+	}
+
+	kblockd_schedule_work(&ns->head->requeue_work);
+}
+
+static int nvme_parse_ana_log(struct nvme_ctrl *ctrl, void *data,
+		int (*cb)(struct nvme_ctrl *ctrl, struct nvme_ana_group_desc *,
+			void *))
+{
+	void *base = ctrl->ana_log_buf;
+	size_t offset = sizeof(struct nvme_ana_rsp_hdr);
+	int error, i;
+
+	lockdep_assert_held(&ctrl->ana_lock);
+
+	for (i = 0; i < le16_to_cpu(ctrl->ana_log_buf->ngrps); i++) {
+		struct nvme_ana_group_desc *desc = base + offset;
+		u32 nr_nsids = le32_to_cpu(desc->nnsids);
+		size_t nsid_buf_size = nr_nsids * sizeof(__le32);
+
+		if (WARN_ON_ONCE(desc->grpid == 0))
+			return -EINVAL;
+		if (WARN_ON_ONCE(le32_to_cpu(desc->grpid) > ctrl->anagrpmax))
+			return -EINVAL;
+		if (WARN_ON_ONCE(desc->state == 0))
+			return -EINVAL;
+		if (WARN_ON_ONCE(desc->state > NVME_ANA_CHANGE))
+			return -EINVAL;
+
+		offset += sizeof(*desc);
+		if (WARN_ON_ONCE(offset > ctrl->ana_log_size - nsid_buf_size))
+			return -EINVAL;
+
+		error = cb(ctrl, desc, data);
+		if (error)
+			return error;
+
+		offset += nsid_buf_size;
+		if (WARN_ON_ONCE(offset > ctrl->ana_log_size - sizeof(*desc)))
+			return -EINVAL;
+	}
+
+	return 0;
+}
+
+static inline bool nvme_state_is_live(enum nvme_ana_state state)
+{
+	return state == NVME_ANA_OPTIMIZED || state == NVME_ANA_NONOPTIMIZED;
+}
+
+static void nvme_update_ns_ana_state(struct nvme_ana_group_desc *desc,
+		struct nvme_ns *ns)
+{
+	mutex_lock(&ns->head->lock);
+	ns->ana_grpid = le32_to_cpu(desc->grpid);
+	ns->ana_state = desc->state;
+	clear_bit(NVME_NS_ANA_PENDING, &ns->flags);
+
+	if (nvme_state_is_live(ns->ana_state))
+		nvme_mpath_set_live(ns);
+	mutex_unlock(&ns->head->lock);
+}
+
+static int nvme_update_ana_state(struct nvme_ctrl *ctrl,
+		struct nvme_ana_group_desc *desc, void *data)
+{
+	u32 nr_nsids = le32_to_cpu(desc->nnsids), n = 0;
+	unsigned *nr_change_groups = data;
+	struct nvme_ns *ns;
+
+	dev_info(ctrl->device, "ANA group %d: %s.\n",
+			le32_to_cpu(desc->grpid),
+			nvme_ana_state_names[desc->state]);
+
+	if (desc->state == NVME_ANA_CHANGE)
+		(*nr_change_groups)++;
+
+	if (!nr_nsids)
+		return 0;
+
+	down_write(&ctrl->namespaces_rwsem);
+	list_for_each_entry(ns, &ctrl->namespaces, list) {
+		if (ns->head->ns_id != le32_to_cpu(desc->nsids[n]))
+			continue;
+		nvme_update_ns_ana_state(desc, ns);
+		if (++n == nr_nsids)
+			break;
+	}
+	up_write(&ctrl->namespaces_rwsem);
+	WARN_ON_ONCE(n < nr_nsids);
+	return 0;
+}
+
+static int nvme_read_ana_log(struct nvme_ctrl *ctrl, bool groups_only)
+{
+	u32 nr_change_groups = 0;
+	int error;
+
+	mutex_lock(&ctrl->ana_lock);
+	error = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_ANA,
+			groups_only ? NVME_ANA_LOG_RGO : 0,
+			ctrl->ana_log_buf, ctrl->ana_log_size, 0);
+	if (error) {
+		dev_warn(ctrl->device, "Failed to get ANA log: %d\n", error);
+		goto out_unlock;
+	}
+
+	error = nvme_parse_ana_log(ctrl, &nr_change_groups,
+			nvme_update_ana_state);
+	if (error)
+		goto out_unlock;
+
+	/*
+	 * In theory we should have an ANATT timer per group as they might enter
+	 * the change state at different times.  But that is a lot of overhead
+	 * just to protect against a target that keeps entering new changes
+	 * states while never finishing previous ones.  But we'll still
+	 * eventually time out once all groups are in change state, so this
+	 * isn't a big deal.
+	 *
+	 * We also double the ANATT value to provide some slack for transports
+	 * or AEN processing overhead.
+	 */
+	if (nr_change_groups)
+		mod_timer(&ctrl->anatt_timer, ctrl->anatt * HZ * 2 + jiffies);
+	else
+		del_timer_sync(&ctrl->anatt_timer);
+out_unlock:
+	mutex_unlock(&ctrl->ana_lock);
+	return error;
+}
+
+static void nvme_ana_work(struct work_struct *work)
+{
+	struct nvme_ctrl *ctrl = container_of(work, struct nvme_ctrl, ana_work);
+
+	nvme_read_ana_log(ctrl, false);
+}
+
+static void nvme_anatt_timeout(struct timer_list *t)
+{
+	struct nvme_ctrl *ctrl = from_timer(ctrl, t, anatt_timer);
+
+	dev_info(ctrl->device, "ANATT timeout, resetting controller.\n");
+	nvme_reset_ctrl(ctrl);
+}
+
+void nvme_mpath_stop(struct nvme_ctrl *ctrl)
+{
+	if (!nvme_ctrl_use_ana(ctrl))
+		return;
+	del_timer_sync(&ctrl->anatt_timer);
+	cancel_work_sync(&ctrl->ana_work);
+}
+
+#define SUBSYS_ATTR_RW(_name, _mode, _show, _store)  \
+	struct device_attribute subsys_attr_##_name =	\
+		__ATTR(_name, _mode, _show, _store)
+
+static const char *nvme_iopolicy_names[] = {
+	[NVME_IOPOLICY_NUMA]	= "numa",
+	[NVME_IOPOLICY_RR]	= "round-robin",
+};
+
+static ssize_t nvme_subsys_iopolicy_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct nvme_subsystem *subsys =
+		container_of(dev, struct nvme_subsystem, dev);
+
+	return sprintf(buf, "%s\n",
+			nvme_iopolicy_names[READ_ONCE(subsys->iopolicy)]);
+}
+
+static ssize_t nvme_subsys_iopolicy_store(struct device *dev,
+		struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct nvme_subsystem *subsys =
+		container_of(dev, struct nvme_subsystem, dev);
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(nvme_iopolicy_names); i++) {
+		if (sysfs_streq(buf, nvme_iopolicy_names[i])) {
+			WRITE_ONCE(subsys->iopolicy, i);
+			return count;
+		}
+	}
+
+	return -EINVAL;
+}
+SUBSYS_ATTR_RW(iopolicy, S_IRUGO | S_IWUSR,
+		      nvme_subsys_iopolicy_show, nvme_subsys_iopolicy_store);
+
+static ssize_t ana_grpid_show(struct device *dev, struct device_attribute *attr,
+		char *buf)
+{
+	return sprintf(buf, "%d\n", nvme_get_ns_from_dev(dev)->ana_grpid);
+}
+DEVICE_ATTR_RO(ana_grpid);
+
+static ssize_t ana_state_show(struct device *dev, struct device_attribute *attr,
+		char *buf)
+{
+	struct nvme_ns *ns = nvme_get_ns_from_dev(dev);
+
+	return sprintf(buf, "%s\n", nvme_ana_state_names[ns->ana_state]);
+}
+DEVICE_ATTR_RO(ana_state);
+
+static int nvme_set_ns_ana_state(struct nvme_ctrl *ctrl,
+		struct nvme_ana_group_desc *desc, void *data)
+{
+	struct nvme_ns *ns = data;
+
+	if (ns->ana_grpid == le32_to_cpu(desc->grpid)) {
+		nvme_update_ns_ana_state(desc, ns);
+		return -ENXIO; /* just break out of the loop */
+	}
+
+	return 0;
+}
+
+void nvme_mpath_add_disk(struct nvme_ns *ns, struct nvme_id_ns *id)
+{
+	if (nvme_ctrl_use_ana(ns->ctrl)) {
+		mutex_lock(&ns->ctrl->ana_lock);
+		ns->ana_grpid = le32_to_cpu(id->anagrpid);
+		nvme_parse_ana_log(ns->ctrl, ns, nvme_set_ns_ana_state);
+		mutex_unlock(&ns->ctrl->ana_lock);
+	} else {
+		mutex_lock(&ns->head->lock);
+		ns->ana_state = NVME_ANA_OPTIMIZED; 
+		nvme_mpath_set_live(ns);
+		mutex_unlock(&ns->head->lock);
+	}
+}
+
+void nvme_mpath_remove_disk(struct nvme_ns_head *head)
+{
+	if (!head->disk)
+		return;
+	if (head->disk->flags & GENHD_FL_UP)
+		del_gendisk(head->disk);
+	blk_set_queue_dying(head->disk->queue);
+	/* make sure all pending bios are cleaned up */
+	kblockd_schedule_work(&head->requeue_work);
+	flush_work(&head->requeue_work);
+	blk_cleanup_queue(head->disk->queue);
+	put_disk(head->disk);
+}
+
+int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)
+{
+	int error;
+
+	if (!nvme_ctrl_use_ana(ctrl))
+		return 0;
+
+	ctrl->anacap = id->anacap;
+	ctrl->anatt = id->anatt;
+	ctrl->nanagrpid = le32_to_cpu(id->nanagrpid);
+	ctrl->anagrpmax = le32_to_cpu(id->anagrpmax);
+
+	mutex_init(&ctrl->ana_lock);
+	timer_setup(&ctrl->anatt_timer, nvme_anatt_timeout, 0);
+	ctrl->ana_log_size = sizeof(struct nvme_ana_rsp_hdr) +
+		ctrl->nanagrpid * sizeof(struct nvme_ana_group_desc);
+	ctrl->ana_log_size += ctrl->max_namespaces * sizeof(__le32);
+
+	if (ctrl->ana_log_size > ctrl->max_hw_sectors << SECTOR_SHIFT) {
+		dev_err(ctrl->device,
+			"ANA log page size (%zd) larger than MDTS (%d).\n",
+			ctrl->ana_log_size,
+			ctrl->max_hw_sectors << SECTOR_SHIFT);
+		dev_err(ctrl->device, "disabling ANA support.\n");
+		return 0;
+	}
+
+	INIT_WORK(&ctrl->ana_work, nvme_ana_work);
+	ctrl->ana_log_buf = kmalloc(ctrl->ana_log_size, GFP_KERNEL);
+	if (!ctrl->ana_log_buf) {
+		error = -ENOMEM;
+		goto out;
+	}
+
+	error = nvme_read_ana_log(ctrl, true);
+	if (error)
+		goto out_free_ana_log_buf;
+	return 0;
+out_free_ana_log_buf:
+	kfree(ctrl->ana_log_buf);
+	ctrl->ana_log_buf = NULL;
+out:
+	return error;
+}
+
+void nvme_mpath_uninit(struct nvme_ctrl *ctrl)
+{
+	kfree(ctrl->ana_log_buf);
+	ctrl->ana_log_buf = NULL;
+}
+
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/nvme.h b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/nvme.h
new file mode 100644
index 0000000..34d893f
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/nvme.h
@@ -0,0 +1,628 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2011-2014, Intel Corporation.
+ */
+
+#ifndef _NVME_H
+#define _NVME_H
+
+//#define KSID_SUPPORT
+
+#include <linux/nvme.h>
+#include <linux/cdev.h>
+#include <linux/pci.h>
+#include <linux/kref.h>
+#include <linux/blk-mq.h>
+#include <linux/lightnvm.h>
+#include <linux/sed-opal.h>
+#include <linux/fault-inject.h>
+#include <linux/rcupdate.h>
+
+#define KV_DRV_VERSION     "20210331"
+
+#define is_kv_append_cmd(opcode)	((opcode) == nvme_cmd_kv_append)
+#define is_kv_store_cmd(opcode)	   	((opcode) == nvme_cmd_kv_store)
+#define is_kv_retrieve_cmd(opcode)	((opcode) == nvme_cmd_kv_retrieve)
+#define is_kv_delete_cmd(opcode)	((opcode) == nvme_cmd_kv_delete)
+#define is_kv_iter_req_cmd(opcode)	((opcode) == nvme_cmd_kv_iter_req)
+#define is_kv_iter_read_cmd(opcode)	((opcode) == nvme_cmd_kv_iter_read)
+#define is_kv_exist_cmd(opcode)	    	((opcode) == nvme_cmd_kv_exist)
+#define is_kv_list_cmd(opcode)         ((opcode) == nvme_cmd_kv_list)
+#define is_kv_lock_cmd(opcode)         ((opcode) == nvme_cmd_kv_lock)
+#define is_kv_unlock_cmd(opcode)         ((opcode) == nvme_cmd_kv_unlock)
+#define is_kv_cmd(opcode)		(is_kv_store_cmd(opcode) || is_kv_append_cmd(opcode) ||\
+				         is_kv_retrieve_cmd(opcode) || is_kv_delete_cmd(opcode) ||\
+					 is_kv_iter_req_cmd(opcode) || is_kv_iter_read_cmd(opcode) ||\
+					 is_kv_exist_cmd(opcode) || is_kv_list_cmd(opcode) ||\
+					 is_kv_lock_cmd(opcode) || is_kv_unlock_cmd(opcode))
+
+extern unsigned int nvme_io_timeout;
+#define NVME_IO_TIMEOUT	(nvme_io_timeout * HZ)
+
+extern unsigned int admin_timeout;
+#define ADMIN_TIMEOUT	(admin_timeout * HZ)
+
+#define NVME_DEFAULT_KATO	5
+#define NVME_KATO_GRACE		10
+
+extern struct workqueue_struct *nvme_wq;
+extern struct workqueue_struct *nvme_reset_wq;
+extern struct workqueue_struct *nvme_delete_wq;
+
+enum {
+	NVME_NS_LBA		= 0,
+	NVME_NS_LIGHTNVM	= 1,
+};
+
+/*
+ * List of workarounds for devices that required behavior not specified in
+ * the standard.
+ */
+enum nvme_quirks {
+	/*
+	 * Prefers I/O aligned to a stripe size specified in a vendor
+	 * specific Identify field.
+	 */
+	NVME_QUIRK_STRIPE_SIZE			= (1 << 0),
+
+	/*
+	 * The controller doesn't handle Identify value others than 0 or 1
+	 * correctly.
+	 */
+	NVME_QUIRK_IDENTIFY_CNS			= (1 << 1),
+
+	/*
+	 * The controller deterministically returns O's on reads to
+	 * logical blocks that deallocate was called on.
+	 */
+	NVME_QUIRK_DEALLOCATE_ZEROES		= (1 << 2),
+
+	/*
+	 * The controller needs a delay before starts checking the device
+	 * readiness, which is done by reading the NVME_CSTS_RDY bit.
+	 */
+	NVME_QUIRK_DELAY_BEFORE_CHK_RDY		= (1 << 3),
+
+	/*
+	 * APST should not be used.
+	 */
+	NVME_QUIRK_NO_APST			= (1 << 4),
+
+	/*
+	 * The deepest sleep state should not be used.
+	 */
+	NVME_QUIRK_NO_DEEPEST_PS		= (1 << 5),
+
+	/*
+	 * Supports the LighNVM command set if indicated in vs[1].
+	 */
+	NVME_QUIRK_LIGHTNVM			= (1 << 6),
+
+	/*
+	 * Set MEDIUM priority on SQ creation
+	 */
+	NVME_QUIRK_MEDIUM_PRIO_SQ		= (1 << 7),
+
+	/*
+	 * Ignore device provided subnqn.
+	 */
+	NVME_QUIRK_IGNORE_DEV_SUBNQN		= (1 << 8),
+
+	/*
+	 * Broken Write Zeroes.
+	 */
+	NVME_QUIRK_DISABLE_WRITE_ZEROES		= (1 << 9),
+};
+
+/*
+ * Common request structure for NVMe passthrough.  All drivers must have
+ * this structure as the first member of their request-private data.
+ */
+struct nvme_request {
+	struct nvme_command	*cmd;
+	union nvme_result	result;
+	u8			retries;
+	u8			flags;
+	u16			status;
+	struct nvme_ctrl	*ctrl;
+};
+
+struct nvme_io_param {
+	struct nvme_request req;
+	struct scatterlist* kv_data_sg_ptr;
+	union {
+		struct scatterlist* kv_meta_sg_ptr;
+		struct bio *key_bio;
+		void *ptr;
+	};
+	int kv_data_nents;
+	int kv_data_len;
+	int kv_rddpt;//RDMA Data Direct Pass through
+};
+
+/* So I don't need to make changes in pci.c */
+typedef struct nvme_io_param kv_request;
+
+/*
+ * Mark a bio as coming in through the mpath node.
+ */
+#define REQ_NVME_MPATH		REQ_DRV
+
+enum {
+	NVME_REQ_CANCELLED		= (1 << 0),
+	NVME_REQ_USERCMD		= (1 << 1),
+};
+
+#define kv_req(req)	((kv_request *) ((req) + 1))
+
+#define key_page(req) 	kv_req(req)->key_bio->bi_io_vec->bv_page
+#define key_len(req) 	kv_req(req)->key_bio->bi_iter.bi_size
+#define key_off(req) 	kv_req(req)->key_bio->bi_io_vec->bv_offset
+
+
+static inline struct nvme_request *nvme_req(struct request *req)
+{
+	return blk_mq_rq_to_pdu(req);
+}
+
+static inline u16 nvme_req_qid(struct request *req)
+{
+	if (!req->rq_disk)
+		return 0;
+	return blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(req)) + 1;
+}
+
+/* The below value is the specific amount of delay needed before checking
+ * readiness in case of the PCI_DEVICE(0x1c58, 0x0003), which needs the
+ * NVME_QUIRK_DELAY_BEFORE_CHK_RDY quirk enabled. The value (in ms) was
+ * found empirically.
+ */
+#define NVME_QUIRK_DELAY_AMOUNT		2300
+
+enum nvme_ctrl_state {
+	NVME_CTRL_NEW,
+	NVME_CTRL_LIVE,
+	NVME_CTRL_ADMIN_ONLY,    /* Only admin queue live */
+	NVME_CTRL_RESETTING,
+	NVME_CTRL_CONNECTING,
+	NVME_CTRL_DELETING,
+	NVME_CTRL_DEAD,
+};
+
+struct nvme_ctrl {
+	bool comp_seen;
+	enum nvme_ctrl_state state;
+	bool identified;
+	spinlock_t lock;
+	struct mutex scan_lock;
+	const struct nvme_ctrl_ops *ops;
+	struct request_queue *admin_q;
+	struct request_queue *connect_q;
+	struct device *dev;
+	int instance;
+	int numa_node;
+	struct blk_mq_tag_set *tagset;
+	struct blk_mq_tag_set *admin_tagset;
+	struct list_head namespaces;
+	struct rw_semaphore namespaces_rwsem;
+	struct device ctrl_device;
+	struct device *device;	/* char device */
+	struct cdev cdev;
+	struct work_struct reset_work;
+	struct work_struct delete_work;
+
+	struct nvme_subsystem *subsys;
+	struct list_head subsys_entry;
+
+	struct opal_dev *opal_dev;
+
+	char name[12];
+	u16 cntlid;
+
+	u32 ctrl_config;
+	u16 mtfa;
+	u32 queue_count;
+
+	u64 cap;
+	u32 page_size;
+	u32 max_hw_sectors;
+	u32 max_segments;
+	u16 crdt[3];
+	u16 oncs;
+	u16 oacs;
+	u16 nssa;
+	u16 nr_streams;
+	u32 max_namespaces;
+	atomic_t abort_limit;
+	u8 vwc;
+	u32 vs;
+	u32 sgls;
+	u16 kas;
+	u8 npss;
+	u8 apsta;
+	u32 oaes;
+	u32 aen_result;
+	u32 ctratt;
+	unsigned int shutdown_timeout;
+	unsigned int kato;
+	bool subsystem;
+	unsigned long quirks;
+	struct nvme_id_power_state psd[32];
+	struct nvme_effects_log *effects;
+	struct work_struct scan_work;
+	struct work_struct async_event_work;
+	struct delayed_work ka_work;
+	struct nvme_command ka_cmd;
+	struct work_struct fw_act_work;
+	unsigned long events;
+
+#ifdef CONFIG_NVME_MULTIPATH
+	/* asymmetric namespace access: */
+	u8 anacap;
+	u8 anatt;
+	u32 anagrpmax;
+	u32 nanagrpid;
+	struct mutex ana_lock;
+	struct nvme_ana_rsp_hdr *ana_log_buf;
+	size_t ana_log_size;
+	struct timer_list anatt_timer;
+	struct work_struct ana_work;
+#endif
+
+	/* Power saving configuration */
+	u64 ps_max_latency_us;
+	bool apst_enabled;
+
+	/* PCIe only: */
+	u32 hmpre;
+	u32 hmmin;
+	u32 hmminds;
+	u16 hmmaxd;
+
+	/* Fabrics only */
+	u16 sqsize;
+	u32 ioccsz;
+	u32 iorcsz;
+	u16 icdoff;
+	u16 maxcmd;
+	int nr_reconnects;
+	struct nvmf_ctrl_options *opts;
+
+	struct page *discard_page;
+	unsigned long discard_page_busy;
+};
+
+enum nvme_iopolicy {
+	NVME_IOPOLICY_NUMA,
+	NVME_IOPOLICY_RR,
+};
+
+struct nvme_subsystem {
+	int			instance;
+	struct device		dev;
+	/*
+	 * Because we unregister the device on the last put we need
+	 * a separate refcount.
+	 */
+	struct kref		ref;
+	struct list_head	entry;
+	struct mutex		lock;
+	struct list_head	ctrls;
+	struct list_head	nsheads;
+	char			subnqn[NVMF_NQN_SIZE];
+	char			serial[20];
+	char			model[40];
+	char			firmware_rev[8];
+	u8			cmic;
+	u16			vendor_id;
+	struct ida		ns_ida;
+#ifdef CONFIG_NVME_MULTIPATH
+	enum nvme_iopolicy	iopolicy;
+#endif
+};
+
+/*
+ * Container structure for uniqueue namespace identifiers.
+ */
+struct nvme_ns_ids {
+	u8	eui64[8];
+	u8	nguid[16];
+	uuid_t	uuid;
+};
+
+/*
+ * Anchor structure for namespaces.  There is one for each namespace in a
+ * NVMe subsystem that any of our controllers can see, and the namespace
+ * structure for each controller is chained of it.  For private namespaces
+ * there is a 1:1 relation to our namespace structures, that is ->list
+ * only ever has a single entry for private namespaces.
+ */
+struct nvme_ns_head {
+	struct list_head	list;
+	struct srcu_struct      srcu;
+	struct nvme_subsystem	*subsys;
+	unsigned		ns_id;
+	struct nvme_ns_ids	ids;
+	struct list_head	entry;
+	struct kref		ref;
+	int			instance;
+#ifdef CONFIG_NVME_MULTIPATH
+	struct gendisk		*disk;
+	struct bio_list		requeue_list;
+	spinlock_t		requeue_lock;
+	struct work_struct	requeue_work;
+	struct mutex		lock;
+	struct nvme_ns __rcu	*current_path[];
+#endif
+};
+
+#ifdef CONFIG_FAULT_INJECTION_DEBUG_FS
+struct nvme_fault_inject {
+	struct fault_attr attr;
+	struct dentry *parent;
+	bool dont_retry;	/* DNR, do not retry */
+	u16 status;		/* status code */
+};
+#endif
+
+struct nvme_ns {
+	struct list_head list;
+
+	struct nvme_ctrl *ctrl;
+	struct request_queue *queue;
+	struct gendisk *disk;
+#ifdef CONFIG_NVME_MULTIPATH
+	enum nvme_ana_state ana_state;
+	u32 ana_grpid;
+#endif
+	struct list_head siblings;
+	struct nvm_dev *ndev;
+	struct kref kref;
+	struct nvme_ns_head *head;
+
+	int lba_shift;
+	u16 ms;
+	u16 sgs;
+	u32 sws;
+	bool ext;
+	u8 pi_type;
+	unsigned long flags;
+#define NVME_NS_REMOVING	0
+#define NVME_NS_DEAD     	1
+#define NVME_NS_ANA_PENDING	2
+	u16 noiob;
+
+#ifdef CONFIG_FAULT_INJECTION_DEBUG_FS
+	struct nvme_fault_inject fault_inject;
+#endif
+
+};
+
+struct nvme_ctrl_ops {
+	const char *name;
+	struct module *module;
+	unsigned int flags;
+#define NVME_F_FABRICS			(1 << 0)
+#define NVME_F_METADATA_SUPPORTED	(1 << 1)
+#define NVME_F_PCI_P2PDMA		(1 << 2)
+	int (*reg_read32)(struct nvme_ctrl *ctrl, u32 off, u32 *val);
+	int (*reg_write32)(struct nvme_ctrl *ctrl, u32 off, u32 val);
+	int (*reg_read64)(struct nvme_ctrl *ctrl, u32 off, u64 *val);
+	void (*free_ctrl)(struct nvme_ctrl *ctrl);
+	void (*submit_async_event)(struct nvme_ctrl *ctrl);
+	void (*delete_ctrl)(struct nvme_ctrl *ctrl);
+	int (*get_address)(struct nvme_ctrl *ctrl, char *buf, int size);
+};
+
+#ifdef CONFIG_FAULT_INJECTION_DEBUG_FS
+void nvme_fault_inject_init(struct nvme_ns *ns);
+void nvme_fault_inject_fini(struct nvme_ns *ns);
+void nvme_should_fail(struct request *req);
+#else
+static inline void nvme_fault_inject_init(struct nvme_ns *ns) {}
+static inline void nvme_fault_inject_fini(struct nvme_ns *ns) {}
+static inline void nvme_should_fail(struct request *req) {}
+#endif
+
+static inline int nvme_reset_subsystem(struct nvme_ctrl *ctrl)
+{
+	if (!ctrl->subsystem)
+		return -ENOTTY;
+	return ctrl->ops->reg_write32(ctrl, NVME_REG_NSSR, 0x4E564D65);
+}
+
+static inline u64 nvme_block_nr(struct nvme_ns *ns, sector_t sector)
+{
+	return (sector >> (ns->lba_shift - 9));
+}
+
+static inline void nvme_end_request(struct request *req, __le16 status,
+		union nvme_result result)
+{
+	struct nvme_request *rq = nvme_req(req);
+
+	rq->status = le16_to_cpu(status) >> 1;
+	rq->result = result;
+	/* inject error when permitted by fault injection framework */
+
+	pr_debug("%s: cid %d status %u result 0x%llx\n", __FUNCTION__, req->tag, rq->status, rq->result.u64);
+	nvme_should_fail(req);
+	blk_mq_complete_request(req);
+}
+
+static inline void nvme_get_ctrl(struct nvme_ctrl *ctrl)
+{
+	get_device(ctrl->device);
+}
+
+static inline void nvme_put_ctrl(struct nvme_ctrl *ctrl)
+{
+	put_device(ctrl->device);
+}
+
+void nvme_complete_rq(struct request *req);
+bool nvme_cancel_request(struct request *req, void *data, bool reserved);
+bool nvme_change_ctrl_state(struct nvme_ctrl *ctrl,
+		enum nvme_ctrl_state new_state);
+int nvme_disable_ctrl(struct nvme_ctrl *ctrl, u64 cap);
+int nvme_enable_ctrl(struct nvme_ctrl *ctrl, u64 cap);
+int nvme_shutdown_ctrl(struct nvme_ctrl *ctrl);
+int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
+		const struct nvme_ctrl_ops *ops, unsigned long quirks);
+void nvme_uninit_ctrl(struct nvme_ctrl *ctrl);
+void nvme_start_ctrl(struct nvme_ctrl *ctrl);
+void nvme_stop_ctrl(struct nvme_ctrl *ctrl);
+void nvme_put_ctrl(struct nvme_ctrl *ctrl);
+int nvme_init_identify(struct nvme_ctrl *ctrl);
+
+void nvme_remove_namespaces(struct nvme_ctrl *ctrl);
+
+int nvme_sec_submit(void *data, u16 spsp, u8 secp, void *buffer, size_t len,
+		bool send);
+
+void nvme_complete_async_event(struct nvme_ctrl *ctrl, __le16 status,
+		volatile union nvme_result *res);
+
+void nvme_stop_queues(struct nvme_ctrl *ctrl);
+void nvme_start_queues(struct nvme_ctrl *ctrl);
+void nvme_kill_queues(struct nvme_ctrl *ctrl);
+void nvme_unfreeze(struct nvme_ctrl *ctrl);
+void nvme_wait_freeze(struct nvme_ctrl *ctrl);
+void nvme_wait_freeze_timeout(struct nvme_ctrl *ctrl, long timeout);
+void nvme_start_freeze(struct nvme_ctrl *ctrl);
+
+#define NVME_QID_ANY -1
+struct request *nvme_alloc_request(struct request_queue *q,
+		struct nvme_command *cmd, blk_mq_req_flags_t flags, int qid);
+void nvme_cleanup_cmd(struct request *req);
+blk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req,
+		struct nvme_command *cmd);
+int nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
+		void *buf, unsigned bufflen);
+int __nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,
+		union nvme_result *result, void *buffer, unsigned bufflen,
+		unsigned timeout, int qid, int at_head,
+		blk_mq_req_flags_t flags, bool poll);
+int nvme_set_queue_count(struct nvme_ctrl *ctrl, int *count);
+void nvme_stop_keep_alive(struct nvme_ctrl *ctrl);
+int nvme_reset_ctrl(struct nvme_ctrl *ctrl);
+int nvme_reset_ctrl_sync(struct nvme_ctrl *ctrl);
+int nvme_delete_ctrl(struct nvme_ctrl *ctrl);
+
+int nvme_get_log(struct nvme_ctrl *ctrl, u32 nsid, u8 log_page, u8 lsp,
+		void *log, size_t size, u64 offset);
+
+extern const struct attribute_group *nvme_ns_id_attr_groups[];
+extern const struct block_device_operations nvme_ns_head_ops;
+
+#ifdef CONFIG_NVME_MULTIPATH
+bool nvme_ctrl_use_ana(struct nvme_ctrl *ctrl);
+void nvme_set_disk_name(char *disk_name, struct nvme_ns *ns,
+			struct nvme_ctrl *ctrl, int *flags);
+void nvme_failover_req(struct request *req);
+void nvme_kick_requeue_lists(struct nvme_ctrl *ctrl);
+int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl,struct nvme_ns_head *head);
+void nvme_mpath_add_disk(struct nvme_ns *ns, struct nvme_id_ns *id);
+void nvme_mpath_remove_disk(struct nvme_ns_head *head);
+int nvme_mpath_init(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id);
+void nvme_mpath_uninit(struct nvme_ctrl *ctrl);
+void nvme_mpath_stop(struct nvme_ctrl *ctrl);
+void nvme_mpath_clear_current_path(struct nvme_ns *ns);
+struct nvme_ns *nvme_find_path(struct nvme_ns_head *head);
+
+static inline void nvme_mpath_check_last_path(struct nvme_ns *ns)
+{
+	struct nvme_ns_head *head = ns->head;
+
+	if (head->disk && list_empty(&head->list))
+		kblockd_schedule_work(&head->requeue_work);
+}
+
+extern struct device_attribute dev_attr_ana_grpid;
+extern struct device_attribute dev_attr_ana_state;
+extern struct device_attribute subsys_attr_iopolicy;
+
+#else
+static inline bool nvme_ctrl_use_ana(struct nvme_ctrl *ctrl)
+{
+	return false;
+}
+/*
+ * Without the multipath code enabled, multiple controller per subsystems are
+ * visible as devices and thus we cannot use the subsystem instance.
+ */
+static inline void nvme_set_disk_name(char *disk_name, struct nvme_ns *ns,
+				      struct nvme_ctrl *ctrl, int *flags)
+{
+	sprintf(disk_name, "nvme%dn%d", ctrl->instance, ns->head->instance);
+}
+
+static inline void nvme_failover_req(struct request *req)
+{
+}
+static inline void nvme_kick_requeue_lists(struct nvme_ctrl *ctrl)
+{
+}
+static inline int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl,
+		struct nvme_ns_head *head)
+{
+	return 0;
+}
+static inline void nvme_mpath_add_disk(struct nvme_ns *ns,
+		struct nvme_id_ns *id)
+{
+}
+static inline void nvme_mpath_remove_disk(struct nvme_ns_head *head)
+{
+}
+static inline void nvme_mpath_clear_current_path(struct nvme_ns *ns)
+{
+}
+static inline void nvme_mpath_check_last_path(struct nvme_ns *ns)
+{
+}
+static inline int nvme_mpath_init(struct nvme_ctrl *ctrl,
+		struct nvme_id_ctrl *id)
+{
+	if (ctrl->subsys->cmic & (1 << 3))
+		dev_warn(ctrl->device,
+"Please enable CONFIG_NVME_MULTIPATH for full support of multi-port devices.\n");
+	return 0;
+}
+static inline void nvme_mpath_uninit(struct nvme_ctrl *ctrl)
+{
+}
+static inline void nvme_mpath_stop(struct nvme_ctrl *ctrl)
+{
+}
+#endif /* CONFIG_NVME_MULTIPATH */
+
+#ifdef CONFIG_NVM
+int nvme_nvm_register(struct nvme_ns *ns, char *disk_name, int node);
+void nvme_nvm_unregister(struct nvme_ns *ns);
+extern const struct attribute_group nvme_nvm_attr_group;
+int nvme_nvm_ioctl(struct nvme_ns *ns, unsigned int cmd, unsigned long arg);
+#else
+static inline int nvme_nvm_register(struct nvme_ns *ns, char *disk_name,
+				    int node)
+{
+	return 0;
+}
+
+static inline void nvme_nvm_unregister(struct nvme_ns *ns) {};
+static inline int nvme_nvm_ioctl(struct nvme_ns *ns, unsigned int cmd,
+							unsigned long arg)
+{
+	return -ENOTTY;
+}
+#endif /* CONFIG_NVM */
+
+static inline struct nvme_ns *nvme_get_ns_from_dev(struct device *dev)
+{
+	return dev_to_disk(dev)->private_data;
+}
+
+int __init nvme_core_init(void);
+void __exit nvme_core_exit(void);
+
+#endif /* _NVME_H */
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/pci.c b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/pci.c
new file mode 100644
index 0000000..1e4747f
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/pci.c
@@ -0,0 +1,3155 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * NVM Express device driver
+ * Copyright (c) 2011-2014, Intel Corporation.
+ */
+
+#include <linux/aer.h>
+#include <linux/async.h>
+#include <linux/blkdev.h>
+#include <linux/blk-mq.h>
+#include <linux/blk-mq-pci.h>
+#include <linux/dmi.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/once.h>
+#include <linux/pci.h>
+#include <linux/t10-pi.h>
+#include <linux/types.h>
+#include <linux/io-64-nonatomic-lo-hi.h>
+#include <linux/sed-opal.h>
+#include <linux/pci-p2pdma.h>
+
+#include "trace.h"
+#include "nvme.h"
+
+#define SQ_SIZE(depth)		(depth * sizeof(struct nvme_command))
+#define CQ_SIZE(depth)		(depth * sizeof(struct nvme_completion))
+
+#define SGES_PER_PAGE	(PAGE_SIZE / sizeof(struct nvme_sgl_desc))
+
+/*
+ * These can be higher, but we need to ensure that any command doesn't
+ * require an sg allocation that needs more than a page of data.
+ */
+#define NVME_MAX_KB_SZ	4096
+#define NVME_MAX_SEGS	127
+
+static int use_threaded_interrupts;
+module_param(use_threaded_interrupts, int, 0);
+
+static bool use_cmb_sqes = true;
+module_param(use_cmb_sqes, bool, 0444);
+MODULE_PARM_DESC(use_cmb_sqes, "use controller's memory buffer for I/O SQes");
+
+static unsigned int max_host_mem_size_mb = 128;
+module_param(max_host_mem_size_mb, uint, 0444);
+MODULE_PARM_DESC(max_host_mem_size_mb,
+	"Maximum Host Memory Buffer (HMB) size per controller (in MiB)");
+
+static unsigned int sgl_threshold = SZ_32K;
+module_param(sgl_threshold, uint, 0644);
+MODULE_PARM_DESC(sgl_threshold,
+		"Use SGLs when average request segment size is larger or equal to "
+		"this size. Use 0 to disable SGLs.");
+
+static int io_queue_depth_set(const char *val, const struct kernel_param *kp);
+static const struct kernel_param_ops io_queue_depth_ops = {
+	.set = io_queue_depth_set,
+	.get = param_get_int,
+};
+
+static int io_queue_depth = 1024;
+module_param_cb(io_queue_depth, &io_queue_depth_ops, &io_queue_depth, 0644);
+MODULE_PARM_DESC(io_queue_depth, "set io queue depth, should >= 2");
+
+static int queue_count_set(const char *val, const struct kernel_param *kp);
+static const struct kernel_param_ops queue_count_ops = {
+	.set = queue_count_set,
+	.get = param_get_int,
+};
+
+static int write_queues;
+module_param_cb(write_queues, &queue_count_ops, &write_queues, 0644);
+MODULE_PARM_DESC(write_queues,
+	"Number of queues to use for writes. If not set, reads and writes "
+	"will share a queue set.");
+
+static int poll_queues = 0;
+module_param_cb(poll_queues, &queue_count_ops, &poll_queues, 0644);
+MODULE_PARM_DESC(poll_queues, "Number of queues to use for polled IO.");
+
+struct nvme_dev;
+struct nvme_queue;
+
+static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown);
+static bool __nvme_disable_io_queues(struct nvme_dev *dev, u8 opcode);
+
+/*
+ * Represents an NVM Express device.  Each nvme_dev is a PCI function.
+ */
+struct nvme_dev {
+	struct nvme_queue *queues;
+	struct blk_mq_tag_set tagset;
+	struct blk_mq_tag_set admin_tagset;
+	u32 __iomem *dbs;
+	struct device *dev;
+	struct dma_pool *prp_page_pool;
+	struct dma_pool *prp_small_pool;
+	unsigned online_queues;
+	unsigned max_qid;
+	unsigned io_queues[HCTX_MAX_TYPES];
+	unsigned int num_vecs;
+	int q_depth;
+	u32 db_stride;
+	void __iomem *bar;
+	unsigned long bar_mapped_size;
+	struct work_struct remove_work;
+	struct mutex shutdown_lock;
+	bool subsystem;
+	u64 cmb_size;
+	bool cmb_use_sqes;
+	u32 cmbsz;
+	u32 cmbloc;
+	struct nvme_ctrl ctrl;
+
+	mempool_t *iod_mempool;
+
+	/* shadow doorbell buffer support: */
+	u32 *dbbuf_dbs;
+	dma_addr_t dbbuf_dbs_dma_addr;
+	u32 *dbbuf_eis;
+	dma_addr_t dbbuf_eis_dma_addr;
+
+	/* host memory buffer support: */
+	u64 host_mem_size;
+	u32 nr_host_mem_descs;
+	dma_addr_t host_mem_descs_dma;
+	struct nvme_host_mem_buf_desc *host_mem_descs;
+	void **host_mem_desc_bufs;
+};
+
+static int io_queue_depth_set(const char *val, const struct kernel_param *kp)
+{
+	int n = 0, ret;
+
+	ret = kstrtoint(val, 10, &n);
+	if (ret != 0 || n < 2)
+		return -EINVAL;
+
+	return param_set_int(val, kp);
+}
+
+static int queue_count_set(const char *val, const struct kernel_param *kp)
+{
+	int n = 0, ret;
+
+	ret = kstrtoint(val, 10, &n);
+	if (ret)
+		return ret;
+	if (n > num_possible_cpus())
+		n = num_possible_cpus();
+
+	return param_set_int(val, kp);
+}
+
+static inline unsigned int sq_idx(unsigned int qid, u32 stride)
+{
+	return qid * 2 * stride;
+}
+
+static inline unsigned int cq_idx(unsigned int qid, u32 stride)
+{
+	return (qid * 2 + 1) * stride;
+}
+
+static inline struct nvme_dev *to_nvme_dev(struct nvme_ctrl *ctrl)
+{
+	return container_of(ctrl, struct nvme_dev, ctrl);
+}
+
+/*
+ * An NVM Express queue.  Each device has at least two (one for admin
+ * commands and one for I/O commands).
+ */
+struct nvme_queue {
+	struct device *q_dmadev;
+	struct nvme_dev *dev;
+	spinlock_t sq_lock;
+	struct nvme_command *sq_cmds;
+	 /* only used for poll queues: */
+	spinlock_t cq_poll_lock ____cacheline_aligned_in_smp;
+	volatile struct nvme_completion *cqes;
+	struct blk_mq_tags **tags;
+	dma_addr_t sq_dma_addr;
+	dma_addr_t cq_dma_addr;
+	u32 __iomem *q_db;
+	u16 q_depth;
+	s16 cq_vector;
+	u16 sq_tail;
+	u16 last_sq_tail;
+	u16 cq_head;
+	u16 last_cq_head;
+	u16 qid;
+	u8 cq_phase;
+	unsigned long flags;
+#define NVMEQ_ENABLED		0
+#define NVMEQ_SQ_CMB		1
+#define NVMEQ_DELETE_ERROR	2
+	u32 *dbbuf_sq_db;
+	u32 *dbbuf_cq_db;
+	u32 *dbbuf_sq_ei;
+	u32 *dbbuf_cq_ei;
+	struct completion delete_done;
+};
+
+/*
+ * The nvme_iod describes the data in an I/O, including the list of PRP
+ * entries.  You can't see it in this data structure because C doesn't let
+ * me express that.  Use nvme_init_iod to ensure there's enough space
+ * allocated to store the PRP list.
+ */
+struct nvme_iod {
+    struct nvme_io_param param;
+	//struct nvme_request req;
+	struct nvme_queue *nvmeq;
+	int kv_cmd;
+	int rserv;
+	bool use_sgl;
+	int aborted;
+	int npages;		/* In the PRP list. 0 means small pool in use */
+	int nents;		/* Used in scatterlist */
+	int length;		/* Of data, in bytes */
+	dma_addr_t first_dma;
+	struct scatterlist meta_sg; /* metadata requires single contiguous buffer */
+	struct scatterlist *sg;
+	struct scatterlist inline_sg[0];
+};
+
+/*
+ * Check we didin't inadvertently grow the command struct
+ */
+static inline void _nvme_check_size(void)
+{
+	BUILD_BUG_ON(sizeof(struct nvme_rw_command) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_create_cq) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_create_sq) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_delete_queue) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_features) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_format_cmd) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_abort_cmd) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_command) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_id_ctrl) != NVME_IDENTIFY_DATA_SIZE);
+	BUILD_BUG_ON(sizeof(struct nvme_id_ns) != NVME_IDENTIFY_DATA_SIZE);
+	BUILD_BUG_ON(sizeof(struct nvme_lba_range_type) != 64);
+	BUILD_BUG_ON(sizeof(struct nvme_smart_log) != 512);
+	BUILD_BUG_ON(sizeof(struct nvme_dbbuf) != 64);
+}
+
+static unsigned int max_io_queues(void)
+{
+	return num_possible_cpus() + write_queues + poll_queues;
+}
+
+static unsigned int max_queue_count(void)
+{
+	/* IO queues + admin queue */
+	return 1 + max_io_queues();
+}
+
+static inline unsigned int nvme_dbbuf_size(u32 stride)
+{
+	return (max_queue_count() * 8 * stride);
+}
+
+static int nvme_dbbuf_dma_alloc(struct nvme_dev *dev)
+{
+	unsigned int mem_size = nvme_dbbuf_size(dev->db_stride);
+
+	if (dev->dbbuf_dbs)
+		return 0;
+
+	dev->dbbuf_dbs = dma_alloc_coherent(dev->dev, mem_size,
+					    &dev->dbbuf_dbs_dma_addr,
+					    GFP_KERNEL);
+	if (!dev->dbbuf_dbs)
+		return -ENOMEM;
+	dev->dbbuf_eis = dma_alloc_coherent(dev->dev, mem_size,
+					    &dev->dbbuf_eis_dma_addr,
+					    GFP_KERNEL);
+	if (!dev->dbbuf_eis) {
+		dma_free_coherent(dev->dev, mem_size,
+				  dev->dbbuf_dbs, dev->dbbuf_dbs_dma_addr);
+		dev->dbbuf_dbs = NULL;
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static void nvme_dbbuf_dma_free(struct nvme_dev *dev)
+{
+	unsigned int mem_size = nvme_dbbuf_size(dev->db_stride);
+
+	if (dev->dbbuf_dbs) {
+		dma_free_coherent(dev->dev, mem_size,
+				  dev->dbbuf_dbs, dev->dbbuf_dbs_dma_addr);
+		dev->dbbuf_dbs = NULL;
+	}
+	if (dev->dbbuf_eis) {
+		dma_free_coherent(dev->dev, mem_size,
+				  dev->dbbuf_eis, dev->dbbuf_eis_dma_addr);
+		dev->dbbuf_eis = NULL;
+	}
+}
+
+static void nvme_dbbuf_init(struct nvme_dev *dev,
+			    struct nvme_queue *nvmeq, int qid)
+{
+	if (!dev->dbbuf_dbs || !qid)
+		return;
+
+	nvmeq->dbbuf_sq_db = &dev->dbbuf_dbs[sq_idx(qid, dev->db_stride)];
+	nvmeq->dbbuf_cq_db = &dev->dbbuf_dbs[cq_idx(qid, dev->db_stride)];
+	nvmeq->dbbuf_sq_ei = &dev->dbbuf_eis[sq_idx(qid, dev->db_stride)];
+	nvmeq->dbbuf_cq_ei = &dev->dbbuf_eis[cq_idx(qid, dev->db_stride)];
+}
+
+static void nvme_dbbuf_set(struct nvme_dev *dev)
+{
+	struct nvme_command c;
+
+	if (!dev->dbbuf_dbs)
+		return;
+
+	memset(&c, 0, sizeof(c));
+	c.dbbuf.opcode = nvme_admin_dbbuf;
+	c.dbbuf.prp1 = cpu_to_le64(dev->dbbuf_dbs_dma_addr);
+	c.dbbuf.prp2 = cpu_to_le64(dev->dbbuf_eis_dma_addr);
+
+	if (nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0)) {
+		dev_warn(dev->ctrl.device, "unable to set dbbuf\n");
+		/* Free memory and continue on */
+		nvme_dbbuf_dma_free(dev);
+	}
+}
+
+static inline int nvme_dbbuf_need_event(u16 event_idx, u16 new_idx, u16 old)
+{
+	return (u16)(new_idx - event_idx - 1) < (u16)(new_idx - old);
+}
+
+/* Update dbbuf and return true if an MMIO is required */
+static bool nvme_dbbuf_update_and_check_event(u16 value, u32 *dbbuf_db,
+					      volatile u32 *dbbuf_ei)
+{
+	if (dbbuf_db) {
+		u16 old_value;
+
+		/*
+		 * Ensure that the queue is written before updating
+		 * the doorbell in memory
+		 */
+		wmb();
+
+		old_value = *dbbuf_db;
+		*dbbuf_db = value;
+
+		/*
+		 * Ensure that the doorbell is updated before reading the event
+		 * index from memory.  The controller needs to provide similar
+		 * ordering to ensure the envent index is updated before reading
+		 * the doorbell.
+		 */
+		mb();
+
+		if (!nvme_dbbuf_need_event(*dbbuf_ei, value, old_value))
+			return false;
+	}
+
+	return true;
+}
+
+/*
+ * Max size of iod being embedded in the request payload
+ */
+#define NVME_INT_PAGES		2
+#define NVME_INT_BYTES(dev)	(NVME_INT_PAGES * (dev)->ctrl.page_size)
+
+/*
+ * Will slightly overestimate the number of pages needed.  This is OK
+ * as it only leads to a small amount of wasted memory for the lifetime of
+ * the I/O.
+ */
+static int nvme_npages(unsigned size, struct nvme_dev *dev)
+{
+	unsigned nprps = DIV_ROUND_UP(size + dev->ctrl.page_size,
+				      dev->ctrl.page_size);
+	return DIV_ROUND_UP(8 * nprps, PAGE_SIZE - 8);
+}
+
+/*
+ * Calculates the number of pages needed for the SGL segments. For example a 4k
+ * page can accommodate 256 SGL descriptors.
+ */
+static int nvme_pci_npages_sgl(unsigned int num_seg)
+{
+	return DIV_ROUND_UP(num_seg * sizeof(struct nvme_sgl_desc), PAGE_SIZE);
+}
+
+static unsigned int nvme_pci_iod_alloc_size(struct nvme_dev *dev,
+		unsigned int size, unsigned int nseg, bool use_sgl)
+{
+	size_t alloc_size;
+
+	if (use_sgl)
+		alloc_size = sizeof(__le64 *) * nvme_pci_npages_sgl(nseg);
+	else
+		alloc_size = sizeof(__le64 *) * nvme_npages(size, dev);
+
+	return alloc_size + sizeof(struct scatterlist) * nseg;
+}
+
+static unsigned int nvme_pci_cmd_size(struct nvme_dev *dev, bool use_sgl)
+{
+	unsigned int alloc_size = nvme_pci_iod_alloc_size(dev,
+				    NVME_INT_BYTES(dev), NVME_INT_PAGES,
+				    use_sgl);
+
+	return sizeof(struct nvme_iod) + alloc_size;
+}
+
+static int nvme_admin_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
+				unsigned int hctx_idx)
+{
+	struct nvme_dev *dev = data;
+	struct nvme_queue *nvmeq = &dev->queues[0];
+
+	WARN_ON(hctx_idx != 0);
+	WARN_ON(dev->admin_tagset.tags[0] != hctx->tags);
+	WARN_ON(nvmeq->tags);
+
+	hctx->driver_data = nvmeq;
+	nvmeq->tags = &dev->admin_tagset.tags[0];
+	return 0;
+}
+
+static void nvme_admin_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
+{
+	struct nvme_queue *nvmeq = hctx->driver_data;
+
+	nvmeq->tags = NULL;
+}
+
+static int nvme_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
+			  unsigned int hctx_idx)
+{
+	struct nvme_dev *dev = data;
+	struct nvme_queue *nvmeq = &dev->queues[hctx_idx + 1];
+
+	if (!nvmeq->tags)
+		nvmeq->tags = &dev->tagset.tags[hctx_idx];
+
+	WARN_ON(dev->tagset.tags[hctx_idx] != hctx->tags);
+	hctx->driver_data = nvmeq;
+	return 0;
+}
+
+static int nvme_init_request(struct blk_mq_tag_set *set, struct request *req,
+		unsigned int hctx_idx, unsigned int numa_node)
+{
+	struct nvme_dev *dev = set->driver_data;
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	int queue_idx = (set == &dev->tagset) ? hctx_idx + 1 : 0;
+	struct nvme_queue *nvmeq = &dev->queues[queue_idx];
+
+	BUG_ON(!nvmeq);
+	iod->nvmeq = nvmeq;
+
+	nvme_req(req)->ctrl = &dev->ctrl;
+	return 0;
+}
+
+static int queue_irq_offset(struct nvme_dev *dev)
+{
+	/* if we have more than 1 vec, admin queue offsets us by 1 */
+	if (dev->num_vecs > 1)
+		return 1;
+
+	return 0;
+}
+
+static int nvme_pci_map_queues(struct blk_mq_tag_set *set)
+{
+	struct nvme_dev *dev = set->driver_data;
+	int i, qoff, offset;
+
+	offset = queue_irq_offset(dev);
+	for (i = 0, qoff = 0; i < set->nr_maps; i++) {
+		struct blk_mq_queue_map *map = &set->map[i];
+
+		map->nr_queues = dev->io_queues[i];
+		if (!map->nr_queues) {
+			BUG_ON(i == HCTX_TYPE_DEFAULT);
+			continue;
+		}
+
+		/*
+		 * The poll queue(s) doesn't have an IRQ (and hence IRQ
+		 * affinity), so use the regular blk-mq cpu mapping
+		 */
+		map->queue_offset = qoff;
+		if (i != HCTX_TYPE_POLL)
+			blk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);
+		else
+			blk_mq_map_queues(map);
+		qoff += map->nr_queues;
+		offset += map->nr_queues;
+	}
+
+	return 0;
+}
+
+/*
+ * Write sq tail if we are asked to, or if the next command would wrap.
+ */
+static inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)
+{
+	if (!write_sq) {
+		u16 next_tail = nvmeq->sq_tail + 1;
+
+		if (next_tail == nvmeq->q_depth)
+			next_tail = 0;
+		if (next_tail != nvmeq->last_sq_tail)
+			return;
+	}
+
+	if (nvme_dbbuf_update_and_check_event(nvmeq->sq_tail,
+			nvmeq->dbbuf_sq_db, nvmeq->dbbuf_sq_ei))
+		writel(nvmeq->sq_tail, nvmeq->q_db);
+	nvmeq->last_sq_tail = nvmeq->sq_tail;
+}
+
+/**
+ * nvme_submit_cmd() - Copy a command into a queue and ring the doorbell
+ * @nvmeq: The queue to use
+ * @cmd: The command to send
+ * @write_sq: whether to write to the SQ doorbell
+ */
+static void nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
+			    bool write_sq)
+{
+	spin_lock(&nvmeq->sq_lock);
+	memcpy(&nvmeq->sq_cmds[nvmeq->sq_tail], cmd, sizeof(*cmd));
+	if (++nvmeq->sq_tail == nvmeq->q_depth)
+		nvmeq->sq_tail = 0;
+	nvme_write_sq_db(nvmeq, write_sq);
+	spin_unlock(&nvmeq->sq_lock);
+}
+
+static void nvme_commit_rqs(struct blk_mq_hw_ctx *hctx)
+{
+	struct nvme_queue *nvmeq = hctx->driver_data;
+
+	spin_lock(&nvmeq->sq_lock);
+	if (nvmeq->sq_tail != nvmeq->last_sq_tail)
+		nvme_write_sq_db(nvmeq, true);
+	spin_unlock(&nvmeq->sq_lock);
+}
+
+static void **nvme_pci_iod_list(struct request *req)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	return (void **)(iod->sg + blk_rq_nr_phys_segments(req));
+}
+
+//static inline bool nvme_pci_use_sgls(struct nvme_dev *dev, struct request *req)
+static inline bool __nvme_pci_use_sgls(struct nvme_dev *dev,
+		                               struct request *req, bool kv_cmd)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	int nseg;// = blk_rq_nr_phys_segments(req);
+	unsigned int avg_seg_size;
+
+    if (kv_cmd) {
+        struct nvme_io_param *param = &iod->param;
+	    nseg = param->kv_data_nents;
+	    if (nseg == 0)
+		    return false;
+		//TODO: this divide macro is very suspicious
+	    avg_seg_size = DIV_ROUND_UP(param->kv_data_len, nseg);
+    } else {
+	    nseg = blk_rq_nr_phys_segments(req);
+	    if (nseg == 0)
+		    return false;
+	    avg_seg_size = DIV_ROUND_UP(blk_rq_payload_bytes(req), nseg);
+    }
+#if 0
+	if (nseg == 0)
+		return false;
+
+	avg_seg_size = DIV_ROUND_UP(blk_rq_payload_bytes(req), nseg);
+#endif
+	if (!(dev->ctrl.sgls & ((1 << 0) | (1 << 1))))
+		return false;
+	if (!iod->nvmeq->qid)
+		return false;
+	if (!sgl_threshold || avg_seg_size < sgl_threshold)
+		return false;
+	return true;
+}
+
+static inline bool nvme_pci_use_sgls(struct nvme_dev *dev, struct request *req)
+{
+    return __nvme_pci_use_sgls(dev, req, false);
+}
+
+static inline bool nvme_pci_use_sgls_for_kv(struct nvme_dev *dev, struct request *req)
+{
+    return __nvme_pci_use_sgls(dev, req, true);
+}
+
+//static blk_status_t nvme_init_iod(struct request *rq, struct nvme_dev *dev)
+static blk_status_t __nvme_init_iod(struct request *rq,
+                                    struct nvme_dev *dev, bool kv_cmd)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(rq);
+	int nseg;// = blk_rq_nr_phys_segments(rq);
+	unsigned int size;// = blk_rq_payload_bytes(rq);
+
+    if (kv_cmd) {
+        struct nvme_io_param* param = &iod->param;
+	    nseg = param->kv_data_nents;
+	    size = param->kv_data_len;
+	    iod->use_sgl = nvme_pci_use_sgls_for_kv(dev, rq);
+    } else {
+	    nseg = blk_rq_nr_phys_segments(rq);
+	    size = blk_rq_payload_bytes(rq);
+	    iod->use_sgl = nvme_pci_use_sgls(dev, rq);
+    }
+
+	//iod->use_sgl = nvme_pci_use_sgls(dev, rq);
+
+	if (nseg > NVME_INT_PAGES || size > NVME_INT_BYTES(dev)) {
+		iod->sg = mempool_alloc(dev->iod_mempool, GFP_ATOMIC);
+		if (!iod->sg)
+			return BLK_STS_RESOURCE;
+	} else {
+		iod->sg = iod->inline_sg;
+	}
+
+    iod->kv_cmd = 0;
+
+	iod->aborted = 0;
+	iod->npages = -1;
+	iod->nents = 0;
+	iod->length = size;
+
+	return BLK_STS_OK;
+}
+
+static blk_status_t nvme_init_iod(struct request *rq, struct nvme_dev *dev)
+{
+    return  __nvme_init_iod(rq, dev, false);
+}
+
+static blk_status_t nvme_init_iod_for_kv(struct request *rq, struct nvme_dev *dev)
+{
+    return __nvme_init_iod(rq, dev, true);
+}
+
+static void nvme_free_iod(struct nvme_dev *dev, struct request *req)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	const int last_prp = dev->ctrl.page_size / sizeof(__le64) - 1;
+	dma_addr_t dma_addr = iod->first_dma, next_dma_addr;
+
+	int i;
+
+	if (iod->npages == 0)
+		dma_pool_free(dev->prp_small_pool, nvme_pci_iod_list(req)[0],
+			dma_addr);
+
+	for (i = 0; i < iod->npages; i++) {
+		void *addr = nvme_pci_iod_list(req)[i];
+
+		if (iod->use_sgl) {
+			struct nvme_sgl_desc *sg_list = addr;
+
+			next_dma_addr =
+			    le64_to_cpu((sg_list[SGES_PER_PAGE - 1]).addr);
+		} else {
+			__le64 *prp_list = addr;
+
+			next_dma_addr = le64_to_cpu(prp_list[last_prp]);
+		}
+
+		dma_pool_free(dev->prp_page_pool, addr, dma_addr);
+		dma_addr = next_dma_addr;
+	}
+
+	if (iod->sg != iod->inline_sg)
+		mempool_free(iod->sg, dev->iod_mempool);
+}
+
+static void nvme_print_sgl(struct scatterlist *sgl, int nents)
+{
+	int i;
+	struct scatterlist *sg;
+
+	for_each_sg(sgl, sg, nents, i) {
+		dma_addr_t phys = sg_phys(sg);
+		pr_warn("sg[%d] phys_addr:%pad offset:%d length:%d "
+			"dma_address:%pad dma_length:%d\n",
+			i, &phys, sg->offset, sg->length, &sg_dma_address(sg),
+			sg_dma_len(sg));
+	}
+}
+
+static blk_status_t nvme_pci_setup_prps(struct nvme_dev *dev,
+		struct request *req, struct nvme_rw_command *cmnd)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	struct dma_pool *pool;
+	int length;// = blk_rq_payload_bytes(req);
+	struct scatterlist *sg = iod->sg;
+	int dma_len = sg_dma_len(sg);
+	u64 dma_addr = sg_dma_address(sg);
+	u32 page_size = dev->ctrl.page_size;
+	int offset = dma_addr & (page_size - 1);
+	__le64 *prp_list;
+	void **list = nvme_pci_iod_list(req);
+	dma_addr_t prp_dma;
+	int nprps, i;
+
+    if (iod->kv_cmd) {
+        struct nvme_io_param* param = &iod->param;
+        length = param->kv_data_len;
+    } else {
+	    length = blk_rq_payload_bytes(req);
+    }
+
+	length -= (page_size - offset);
+	if (length <= 0) {
+		iod->first_dma = 0;
+		goto done;
+	}
+
+	dma_len -= (page_size - offset);
+	if (dma_len) {
+		dma_addr += (page_size - offset);
+	} else {
+		sg = sg_next(sg);
+		dma_addr = sg_dma_address(sg);
+		dma_len = sg_dma_len(sg);
+	}
+
+	if (length <= page_size) {
+		iod->first_dma = dma_addr;
+		goto done;
+	}
+
+	nprps = DIV_ROUND_UP(length, page_size);
+	if (nprps <= (256 / 8)) {
+		pool = dev->prp_small_pool;
+		iod->npages = 0;
+	} else {
+		pool = dev->prp_page_pool;
+		iod->npages = 1;
+	}
+
+	prp_list = dma_pool_alloc(pool, GFP_ATOMIC, &prp_dma);
+	if (!prp_list) {
+		iod->first_dma = dma_addr;
+		iod->npages = -1;
+		return BLK_STS_RESOURCE;
+	}
+	list[0] = prp_list;
+	iod->first_dma = prp_dma;
+	i = 0;
+	for (;;) {
+		if (i == page_size >> 3) {
+			__le64 *old_prp_list = prp_list;
+			prp_list = dma_pool_alloc(pool, GFP_ATOMIC, &prp_dma);
+			if (!prp_list)
+				return BLK_STS_RESOURCE;
+			list[iod->npages++] = prp_list;
+			prp_list[0] = old_prp_list[i - 1];
+			old_prp_list[i - 1] = cpu_to_le64(prp_dma);
+			i = 1;
+		}
+		prp_list[i++] = cpu_to_le64(dma_addr);
+		dma_len -= page_size;
+		dma_addr += page_size;
+		length -= page_size;
+		if (length <= 0)
+			break;
+		if (dma_len > 0)
+			continue;
+		if (unlikely(dma_len < 0))
+			goto bad_sgl;
+		sg = sg_next(sg);
+		dma_addr = sg_dma_address(sg);
+		dma_len = sg_dma_len(sg);
+	}
+
+done:
+	cmnd->dptr.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
+	cmnd->dptr.prp2 = cpu_to_le64(iod->first_dma);
+
+	return BLK_STS_OK;
+
+ bad_sgl:
+	WARN(DO_ONCE(nvme_print_sgl, iod->sg, iod->nents),
+			"Invalid SGL for payload:%d nents:%d\n",
+			blk_rq_payload_bytes(req), iod->nents);
+	return BLK_STS_IOERR;
+}
+
+static void nvme_pci_sgl_set_data(struct nvme_sgl_desc *sge,
+		struct scatterlist *sg)
+{
+	sge->addr = cpu_to_le64(sg_dma_address(sg));
+	sge->length = cpu_to_le32(sg_dma_len(sg));
+	sge->type = NVME_SGL_FMT_DATA_DESC << 4;
+}
+
+static void nvme_pci_sgl_set_seg(struct nvme_sgl_desc *sge,
+		dma_addr_t dma_addr, int entries)
+{
+	sge->addr = cpu_to_le64(dma_addr);
+	if (entries < SGES_PER_PAGE) {
+		sge->length = cpu_to_le32(entries * sizeof(*sge));
+		sge->type = NVME_SGL_FMT_LAST_SEG_DESC << 4;
+	} else {
+		sge->length = cpu_to_le32(PAGE_SIZE);
+		sge->type = NVME_SGL_FMT_SEG_DESC << 4;
+	}
+}
+
+static blk_status_t nvme_pci_setup_sgls(struct nvme_dev *dev,
+		struct request *req, struct nvme_rw_command *cmd, int entries)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	struct dma_pool *pool;
+	struct nvme_sgl_desc *sg_list;
+	struct scatterlist *sg = iod->sg;
+	dma_addr_t sgl_dma;
+	int i = 0;
+
+	/* setting the transfer type as SGL */
+	cmd->flags = NVME_CMD_SGL_METABUF;
+
+	if (entries == 1) {
+		nvme_pci_sgl_set_data(&cmd->dptr.sgl, sg);
+		return BLK_STS_OK;
+	}
+
+	if (entries <= (256 / sizeof(struct nvme_sgl_desc))) {
+		pool = dev->prp_small_pool;
+		iod->npages = 0;
+	} else {
+		pool = dev->prp_page_pool;
+		iod->npages = 1;
+	}
+
+	sg_list = dma_pool_alloc(pool, GFP_ATOMIC, &sgl_dma);
+	if (!sg_list) {
+		iod->npages = -1;
+		return BLK_STS_RESOURCE;
+	}
+
+	nvme_pci_iod_list(req)[0] = sg_list;
+	iod->first_dma = sgl_dma;
+
+	nvme_pci_sgl_set_seg(&cmd->dptr.sgl, sgl_dma, entries);
+
+	do {
+		if (i == SGES_PER_PAGE) {
+			struct nvme_sgl_desc *old_sg_desc = sg_list;
+			struct nvme_sgl_desc *link = &old_sg_desc[i - 1];
+
+			sg_list = dma_pool_alloc(pool, GFP_ATOMIC, &sgl_dma);
+			if (!sg_list)
+				return BLK_STS_RESOURCE;
+
+			i = 0;
+			nvme_pci_iod_list(req)[iod->npages++] = sg_list;
+			sg_list[i++] = *link;
+			nvme_pci_sgl_set_seg(link, sgl_dma, entries);
+		}
+
+		nvme_pci_sgl_set_data(&sg_list[i++], sg);
+		sg = sg_next(sg);
+	} while (--entries > 0);
+
+	return BLK_STS_OK;
+}
+
+static blk_status_t nvme_kv_map_data(struct nvme_dev *dev, struct request *req,
+		                             struct nvme_command *cmnd)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+    struct nvme_io_param* param = &iod->param;
+	enum dma_data_direction dma_dir = DMA_BIDIRECTIONAL;
+	blk_status_t ret = BLK_STS_IOERR;
+    int nr_mapped;
+
+    if (param->kv_data_sg_ptr) {
+        struct scatterlist* sg;
+        int i;
+        /* copy user sg list to iod->sg */
+        for_each_sg(param->kv_data_sg_ptr, sg, param->kv_data_nents, i) {
+            iod->sg[i] = *sg;
+        }
+        iod->nents = param->kv_data_nents;
+    	ret = BLK_STS_RESOURCE;
+    	nr_mapped = dma_map_sg_attrs(dev->dev, iod->sg, iod->nents, dma_dir,
+			    DMA_ATTR_NO_WARN);
+	    if (!nr_mapped)
+		    goto out;
+
+        if (iod->use_sgl)
+		    ret = nvme_pci_setup_sgls(dev, req, &cmnd->rw, nr_mapped);
+        else
+		    ret = nvme_pci_setup_prps(dev, req, &cmnd->rw);
+        if (ret != BLK_STS_OK)
+	    	goto out_unmap;
+
+		//TODO: overwritten what nvme_pci_sgl_set_data() did previously
+		cmnd->kv_store.dptr.prp1 = cpu_to_le64(sg_dma_address(iod->sg));
+		cmnd->kv_store.dptr.prp2 = cpu_to_le64(iod->first_dma);
+    }
+	ret = BLK_STS_IOERR;
+
+	if (param->kv_meta_sg_ptr)
+	{
+		if (!dma_map_sg(dev->dev, param->kv_meta_sg_ptr, 1, DMA_BIDIRECTIONAL))
+			goto out_unmap;
+		cmnd->kv_store.key_prp = cpu_to_le64(sg_dma_address(param->kv_meta_sg_ptr));
+	}
+	return BLK_STS_OK;
+
+out_unmap:
+    if (param->kv_data_sg_ptr)
+	    dma_unmap_sg(dev->dev, iod->sg, iod->nents, dma_dir);
+out:
+	return ret;
+}
+
+
+static void nvme_kv_unmap_data(struct nvme_dev *dev, struct request *req)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+    struct nvme_io_param* param = &iod->param;
+	enum dma_data_direction dma_dir = DMA_BIDIRECTIONAL;
+
+	if (param->kv_meta_sg_ptr) {
+		dma_unmap_sg(dev->dev, iod->sg, iod->nents, dma_dir);
+    }
+	if (iod->param.kv_meta_sg_ptr)
+	{
+		dma_unmap_sg(dev->dev, iod->param.kv_meta_sg_ptr, 1, DMA_BIDIRECTIONAL);
+	}
+    nvme_cleanup_cmd(req);
+	nvme_free_iod(dev, req);
+}
+
+static blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,
+		struct nvme_command *cmnd)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	struct request_queue *q = req->q;
+	enum dma_data_direction dma_dir = rq_data_dir(req) ?
+			DMA_TO_DEVICE : DMA_FROM_DEVICE;
+	blk_status_t ret = BLK_STS_IOERR;
+	int nr_mapped;
+
+    if (is_kv_cmd(cmnd->common.opcode)) {
+        return nvme_kv_map_data(dev, req, cmnd);
+    }
+
+	sg_init_table(iod->sg, blk_rq_nr_phys_segments(req));
+	iod->nents = blk_rq_map_sg(q, req, iod->sg);
+	if (!iod->nents)
+		goto out;
+
+	ret = BLK_STS_RESOURCE;
+
+	if (is_pci_p2pdma_page(sg_page(iod->sg)))
+		nr_mapped = pci_p2pdma_map_sg(dev->dev, iod->sg, iod->nents,
+					  dma_dir);
+	else
+		nr_mapped = dma_map_sg_attrs(dev->dev, iod->sg, iod->nents,
+					     dma_dir,  DMA_ATTR_NO_WARN);
+	if (!nr_mapped)
+		goto out;
+
+	if (iod->use_sgl)
+		ret = nvme_pci_setup_sgls(dev, req, &cmnd->rw, nr_mapped);
+	else
+		ret = nvme_pci_setup_prps(dev, req, &cmnd->rw);
+
+	if (ret != BLK_STS_OK)
+		goto out_unmap;
+
+	ret = BLK_STS_IOERR;
+	if (blk_integrity_rq(req)) {
+		if (blk_rq_count_integrity_sg(q, req->bio) != 1)
+			goto out_unmap;
+
+		sg_init_table(&iod->meta_sg, 1);
+		if (blk_rq_map_integrity_sg(q, req->bio, &iod->meta_sg) != 1)
+			goto out_unmap;
+
+		if (!dma_map_sg(dev->dev, &iod->meta_sg, 1, dma_dir))
+			goto out_unmap;
+
+		cmnd->rw.metadata = cpu_to_le64(sg_dma_address(&iod->meta_sg));
+	}
+
+	return BLK_STS_OK;
+
+out_unmap:
+	dma_unmap_sg(dev->dev, iod->sg, iod->nents, dma_dir);
+out:
+	return ret;
+}
+
+static void nvme_unmap_data(struct nvme_dev *dev, struct request *req)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	enum dma_data_direction dma_dir = rq_data_dir(req) ?
+			DMA_TO_DEVICE : DMA_FROM_DEVICE;
+
+    if (iod->kv_cmd) {
+        return nvme_kv_unmap_data(dev, req);
+    }
+
+	if (iod->nents) {
+		/* P2PDMA requests do not need to be unmapped */
+		if (!is_pci_p2pdma_page(sg_page(iod->sg)))
+			dma_unmap_sg(dev->dev, iod->sg, iod->nents, dma_dir);
+
+		if (blk_integrity_rq(req))
+			dma_unmap_sg(dev->dev, &iod->meta_sg, 1, dma_dir);
+	}
+
+	nvme_cleanup_cmd(req);
+	nvme_free_iod(dev, req);
+}
+
+/*
+ * NOTE: ns is NULL when called on the admin queue.
+ */
+static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
+			 const struct blk_mq_queue_data *bd)
+{
+	struct nvme_ns *ns = hctx->queue->queuedata;
+	struct nvme_queue *nvmeq = hctx->driver_data;
+	struct nvme_dev *dev = nvmeq->dev;
+	struct request *req = bd->rq;
+	struct nvme_command cmnd;
+	blk_status_t ret;
+	bool b_kv_cmd = false;
+
+	/*
+	 * We should not need to do this, but we're still using this to
+	 * ensure we can drain requests on a dying queue.
+	 */
+	if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
+		return BLK_STS_IOERR;
+
+	ret = nvme_setup_cmd(ns, req, &cmnd);
+	if (ret)
+		return ret;
+
+    	b_kv_cmd = is_kv_cmd(cmnd.common.opcode);
+    	if (b_kv_cmd) {
+		ret = nvme_init_iod_for_kv(req, dev);
+		if (ret)
+			goto out_free_cmd;
+	} else {
+		ret = nvme_init_iod(req, dev);
+	    	if (ret)
+			goto out_free_cmd;
+    	}
+	/*
+	ret = nvme_init_iod(req, dev);
+	if (ret)
+		goto out_free_cmd;
+	*/
+	//TODO: isn't it perfectly fine to do this in nvme_init_iod()?
+    	if (b_kv_cmd) {
+        	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+        	iod->kv_cmd = 1;
+    	}
+
+    	if (blk_rq_nr_phys_segments(req) ||
+            is_kv_cmd(cmnd.common.opcode)) {
+		ret = nvme_map_data(dev, req, &cmnd);
+		if (ret)
+			goto out_cleanup_iod;
+	}
+
+	blk_mq_start_request(req);
+
+	nvme_submit_cmd(nvmeq, &cmnd, bd->last);
+	return BLK_STS_OK;
+out_cleanup_iod:
+	nvme_free_iod(dev, req);
+out_free_cmd:
+	nvme_cleanup_cmd(req);
+	return ret;
+}
+
+static void nvme_pci_complete_rq(struct request *req)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+
+	nvme_unmap_data(iod->nvmeq->dev, req);
+	nvme_complete_rq(req);
+}
+
+/* We read the CQE phase first to check if the rest of the entry is valid */
+static inline bool nvme_cqe_pending(struct nvme_queue *nvmeq)
+{
+	return (le16_to_cpu(nvmeq->cqes[nvmeq->cq_head].status) & 1) ==
+			nvmeq->cq_phase;
+}
+
+static inline void nvme_ring_cq_doorbell(struct nvme_queue *nvmeq)
+{
+	u16 head = nvmeq->cq_head;
+
+	if (nvme_dbbuf_update_and_check_event(head, nvmeq->dbbuf_cq_db,
+					      nvmeq->dbbuf_cq_ei))
+		writel(head, nvmeq->q_db + nvmeq->dev->db_stride);
+}
+
+static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
+{
+	volatile struct nvme_completion *cqe = &nvmeq->cqes[idx];
+	struct request *req;
+
+	if (unlikely(cqe->command_id >= nvmeq->q_depth)) {
+		dev_warn(nvmeq->dev->ctrl.device,
+			"invalid id %d completed on queue %d\n",
+			cqe->command_id, le16_to_cpu(cqe->sq_id));
+		return;
+	}
+
+	/*
+	 * AEN requests are special as they don't time out and can
+	 * survive any kind of queue freeze and often don't respond to
+	 * aborts.  We don't even bother to allocate a struct request
+	 * for them but rather special case them here.
+	 */
+	if (unlikely(nvmeq->qid == 0 &&
+			cqe->command_id >= NVME_AQ_BLK_MQ_DEPTH)) {
+		nvme_complete_async_event(&nvmeq->dev->ctrl,
+				cqe->status, &cqe->result);
+		return;
+	}
+
+	req = blk_mq_tag_to_rq(*nvmeq->tags, cqe->command_id);
+	trace_nvme_sq(req, cqe->sq_head, nvmeq->sq_tail);
+	nvme_end_request(req, cqe->status, cqe->result);
+}
+
+static void nvme_complete_cqes(struct nvme_queue *nvmeq, u16 start, u16 end)
+{
+	while (start != end) {
+		nvme_handle_cqe(nvmeq, start);
+		if (++start == nvmeq->q_depth)
+			start = 0;
+	}
+}
+
+static inline void nvme_update_cq_head(struct nvme_queue *nvmeq)
+{
+	if (nvmeq->cq_head == nvmeq->q_depth - 1) {
+		nvmeq->cq_head = 0;
+		nvmeq->cq_phase = !nvmeq->cq_phase;
+	} else {
+		nvmeq->cq_head++;
+	}
+}
+
+static inline int nvme_process_cq(struct nvme_queue *nvmeq, u16 *start,
+				  u16 *end, unsigned int tag)
+{
+	int found = 0;
+
+	*start = nvmeq->cq_head;
+	while (nvme_cqe_pending(nvmeq)) {
+		if (tag == -1U || nvmeq->cqes[nvmeq->cq_head].command_id == tag)
+			found++;
+		nvme_update_cq_head(nvmeq);
+	}
+	*end = nvmeq->cq_head;
+
+	if (*start != *end)
+		nvme_ring_cq_doorbell(nvmeq);
+	return found;
+}
+
+static irqreturn_t nvme_irq(int irq, void *data)
+{
+	struct nvme_queue *nvmeq = data;
+	irqreturn_t ret = IRQ_NONE;
+	u16 start, end;
+
+	/*
+	 * The rmb/wmb pair ensures we see all updates from a previous run of
+	 * the irq handler, even if that was on another CPU.
+	 */
+	rmb();
+	if (nvmeq->cq_head != nvmeq->last_cq_head)
+		ret = IRQ_HANDLED;
+	nvme_process_cq(nvmeq, &start, &end, -1);
+	nvmeq->last_cq_head = nvmeq->cq_head;
+	wmb();
+
+	if (start != end) {
+		nvme_complete_cqes(nvmeq, start, end);
+		return IRQ_HANDLED;
+	}
+
+	return ret;
+}
+
+static irqreturn_t nvme_irq_check(int irq, void *data)
+{
+	struct nvme_queue *nvmeq = data;
+	if (nvme_cqe_pending(nvmeq))
+		return IRQ_WAKE_THREAD;
+	return IRQ_NONE;
+}
+
+/*
+ * Poll for completions any queue, including those not dedicated to polling.
+ * Can be called from any context.
+ */
+static int nvme_poll_irqdisable(struct nvme_queue *nvmeq, unsigned int tag)
+{
+	struct pci_dev *pdev = to_pci_dev(nvmeq->dev->dev);
+	u16 start, end;
+	int found;
+
+	/*
+	 * For a poll queue we need to protect against the polling thread
+	 * using the CQ lock.  For normal interrupt driven threads we have
+	 * to disable the interrupt to avoid racing with it.
+	 */
+	if (nvmeq->cq_vector == -1) {
+		spin_lock(&nvmeq->cq_poll_lock);
+		found = nvme_process_cq(nvmeq, &start, &end, tag);
+		spin_unlock(&nvmeq->cq_poll_lock);
+	} else {
+		disable_irq(pci_irq_vector(pdev, nvmeq->cq_vector));
+		found = nvme_process_cq(nvmeq, &start, &end, tag);
+		enable_irq(pci_irq_vector(pdev, nvmeq->cq_vector));
+	}
+
+	nvme_complete_cqes(nvmeq, start, end);
+	return found;
+}
+
+static int nvme_poll(struct blk_mq_hw_ctx *hctx)
+{
+	struct nvme_queue *nvmeq = hctx->driver_data;
+	u16 start, end;
+	bool found;
+
+	if (!nvme_cqe_pending(nvmeq))
+		return 0;
+
+	spin_lock(&nvmeq->cq_poll_lock);
+	found = nvme_process_cq(nvmeq, &start, &end, -1);
+	spin_unlock(&nvmeq->cq_poll_lock);
+
+	nvme_complete_cqes(nvmeq, start, end);
+	return found;
+}
+
+static void nvme_pci_submit_async_event(struct nvme_ctrl *ctrl)
+{
+	struct nvme_dev *dev = to_nvme_dev(ctrl);
+	struct nvme_queue *nvmeq = &dev->queues[0];
+	struct nvme_command c;
+
+	memset(&c, 0, sizeof(c));
+	c.common.opcode = nvme_admin_async_event;
+	c.common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+	nvme_submit_cmd(nvmeq, &c, true);
+}
+
+static int adapter_delete_queue(struct nvme_dev *dev, u8 opcode, u16 id)
+{
+	struct nvme_command c;
+
+	memset(&c, 0, sizeof(c));
+	c.delete_queue.opcode = opcode;
+	c.delete_queue.qid = cpu_to_le16(id);
+
+	return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
+}
+
+static int adapter_alloc_cq(struct nvme_dev *dev, u16 qid,
+		struct nvme_queue *nvmeq, s16 vector)
+{
+	struct nvme_command c;
+	int flags = NVME_QUEUE_PHYS_CONTIG;
+
+	if (vector != -1)
+		flags |= NVME_CQ_IRQ_ENABLED;
+
+	/*
+	 * Note: we (ab)use the fact that the prp fields survive if no data
+	 * is attached to the request.
+	 */
+	memset(&c, 0, sizeof(c));
+	c.create_cq.opcode = nvme_admin_create_cq;
+	c.create_cq.prp1 = cpu_to_le64(nvmeq->cq_dma_addr);
+	c.create_cq.cqid = cpu_to_le16(qid);
+	c.create_cq.qsize = cpu_to_le16(nvmeq->q_depth - 1);
+	c.create_cq.cq_flags = cpu_to_le16(flags);
+	if (vector != -1)
+		c.create_cq.irq_vector = cpu_to_le16(vector);
+	else
+		c.create_cq.irq_vector = 0;
+
+	return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
+}
+
+static int adapter_alloc_sq(struct nvme_dev *dev, u16 qid,
+						struct nvme_queue *nvmeq)
+{
+	struct nvme_ctrl *ctrl = &dev->ctrl;
+	struct nvme_command c;
+	int flags = NVME_QUEUE_PHYS_CONTIG;
+
+	/*
+	 * Some drives have a bug that auto-enables WRRU if MEDIUM isn't
+	 * set. Since URGENT priority is zeroes, it makes all queues
+	 * URGENT.
+	 */
+	if (ctrl->quirks & NVME_QUIRK_MEDIUM_PRIO_SQ)
+		flags |= NVME_SQ_PRIO_MEDIUM;
+
+	/*
+	 * Note: we (ab)use the fact that the prp fields survive if no data
+	 * is attached to the request.
+	 */
+	memset(&c, 0, sizeof(c));
+	c.create_sq.opcode = nvme_admin_create_sq;
+	c.create_sq.prp1 = cpu_to_le64(nvmeq->sq_dma_addr);
+	c.create_sq.sqid = cpu_to_le16(qid);
+	c.create_sq.qsize = cpu_to_le16(nvmeq->q_depth - 1);
+	c.create_sq.sq_flags = cpu_to_le16(flags);
+	c.create_sq.cqid = cpu_to_le16(qid);
+
+	return nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
+}
+
+static int adapter_delete_cq(struct nvme_dev *dev, u16 cqid)
+{
+	return adapter_delete_queue(dev, nvme_admin_delete_cq, cqid);
+}
+
+static int adapter_delete_sq(struct nvme_dev *dev, u16 sqid)
+{
+	return adapter_delete_queue(dev, nvme_admin_delete_sq, sqid);
+}
+
+static void abort_endio(struct request *req, blk_status_t error)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	struct nvme_queue *nvmeq = iod->nvmeq;
+
+	dev_warn(nvmeq->dev->ctrl.device,
+		 "Abort status: 0x%x", nvme_req(req)->status);
+	atomic_inc(&nvmeq->dev->ctrl.abort_limit);
+	blk_mq_free_request(req);
+}
+
+static bool nvme_should_reset(struct nvme_dev *dev, u32 csts)
+{
+
+	/* If true, indicates loss of adapter communication, possibly by a
+	 * NVMe Subsystem reset.
+	 */
+	bool nssro = dev->subsystem && (csts & NVME_CSTS_NSSRO);
+
+	/* If there is a reset/reinit ongoing, we shouldn't reset again. */
+	switch (dev->ctrl.state) {
+	case NVME_CTRL_RESETTING:
+	case NVME_CTRL_CONNECTING:
+		return false;
+	default:
+		break;
+	}
+
+	/* We shouldn't reset unless the controller is on fatal error state
+	 * _or_ if we lost the communication with it.
+	 */
+	if (!(csts & NVME_CSTS_CFS) && !nssro)
+		return false;
+
+	return true;
+}
+
+static void nvme_warn_reset(struct nvme_dev *dev, u32 csts)
+{
+	/* Read a config register to help see what died. */
+	u16 pci_status;
+	int result;
+
+	result = pci_read_config_word(to_pci_dev(dev->dev), PCI_STATUS,
+				      &pci_status);
+	if (result == PCIBIOS_SUCCESSFUL)
+		dev_warn(dev->ctrl.device,
+			 "controller is down; will reset: CSTS=0x%x, PCI_STATUS=0x%hx\n",
+			 csts, pci_status);
+	else
+		dev_warn(dev->ctrl.device,
+			 "controller is down; will reset: CSTS=0x%x, PCI_STATUS read failed (%d)\n",
+			 csts, result);
+}
+
+static enum blk_eh_timer_return nvme_timeout(struct request *req, bool reserved)
+{
+	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
+	struct nvme_queue *nvmeq = iod->nvmeq;
+	struct nvme_dev *dev = nvmeq->dev;
+	struct request *abort_req;
+	struct nvme_command cmd;
+	u32 csts = readl(dev->bar + NVME_REG_CSTS);
+
+	/* If PCI error recovery process is happening, we cannot reset or
+	 * the recovery mechanism will surely fail.
+	 */
+	mb();
+	if (pci_channel_offline(to_pci_dev(dev->dev)))
+		return BLK_EH_RESET_TIMER;
+
+	/*
+	 * Reset immediately if the controller is failed
+	 */
+	if (nvme_should_reset(dev, csts)) {
+		nvme_warn_reset(dev, csts);
+		nvme_dev_disable(dev, false);
+		nvme_reset_ctrl(&dev->ctrl);
+		return BLK_EH_DONE;
+	}
+
+	/*
+	 * Did we miss an interrupt?
+	 */
+	if (nvme_poll_irqdisable(nvmeq, req->tag)) {
+		dev_warn(dev->ctrl.device,
+			 "I/O %d QID %d timeout, completion polled\n",
+			 req->tag, nvmeq->qid);
+		return BLK_EH_DONE;
+	}
+
+	/*
+	 * Shutdown immediately if controller times out while starting. The
+	 * reset work will see the pci device disabled when it gets the forced
+	 * cancellation error. All outstanding requests are completed on
+	 * shutdown, so we return BLK_EH_DONE.
+	 */
+	switch (dev->ctrl.state) {
+	case NVME_CTRL_CONNECTING:
+	case NVME_CTRL_RESETTING:
+		dev_warn_ratelimited(dev->ctrl.device,
+			 "I/O %d QID %d timeout, disable controller\n",
+			 req->tag, nvmeq->qid);
+		nvme_dev_disable(dev, false);
+		nvme_req(req)->flags |= NVME_REQ_CANCELLED;
+		return BLK_EH_DONE;
+	default:
+		break;
+	}
+
+	/*
+ 	 * Shutdown the controller immediately and schedule a reset if the
+ 	 * command was already aborted once before and still hasn't been
+ 	 * returned to the driver, or if this is the admin queue.
+	 */
+	if (!nvmeq->qid || iod->aborted) {
+		dev_warn(dev->ctrl.device,
+			 "I/O %d QID %d timeout, reset controller\n",
+			 req->tag, nvmeq->qid);
+		nvme_dev_disable(dev, false);
+		nvme_reset_ctrl(&dev->ctrl);
+
+		nvme_req(req)->flags |= NVME_REQ_CANCELLED;
+		return BLK_EH_DONE;
+	}
+
+	if (atomic_dec_return(&dev->ctrl.abort_limit) < 0) {
+		atomic_inc(&dev->ctrl.abort_limit);
+		return BLK_EH_RESET_TIMER;
+	}
+	iod->aborted = 1;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd.abort.opcode = nvme_admin_abort_cmd;
+	cmd.abort.cid = req->tag;
+	cmd.abort.sqid = cpu_to_le16(nvmeq->qid);
+
+	dev_warn(nvmeq->dev->ctrl.device,
+		"I/O %d QID %d timeout, aborting\n",
+		 req->tag, nvmeq->qid);
+
+	abort_req = nvme_alloc_request(dev->ctrl.admin_q, &cmd,
+			BLK_MQ_REQ_NOWAIT, NVME_QID_ANY);
+	if (IS_ERR(abort_req)) {
+		atomic_inc(&dev->ctrl.abort_limit);
+		return BLK_EH_RESET_TIMER;
+	}
+
+	abort_req->timeout = ADMIN_TIMEOUT;
+	abort_req->end_io_data = NULL;
+	blk_execute_rq_nowait(abort_req->q, NULL, abort_req, 0, abort_endio);
+
+	/*
+	 * The aborted req will be completed on receiving the abort req.
+	 * We enable the timer again. If hit twice, it'll cause a device reset,
+	 * as the device then is in a faulty state.
+	 */
+	return BLK_EH_RESET_TIMER;
+}
+
+static void nvme_free_queue(struct nvme_queue *nvmeq)
+{
+	dma_free_coherent(nvmeq->q_dmadev, CQ_SIZE(nvmeq->q_depth),
+				(void *)nvmeq->cqes, nvmeq->cq_dma_addr);
+	if (!nvmeq->sq_cmds)
+		return;
+
+	if (test_and_clear_bit(NVMEQ_SQ_CMB, &nvmeq->flags)) {
+		pci_free_p2pmem(to_pci_dev(nvmeq->q_dmadev),
+				nvmeq->sq_cmds, SQ_SIZE(nvmeq->q_depth));
+	} else {
+		dma_free_coherent(nvmeq->q_dmadev, SQ_SIZE(nvmeq->q_depth),
+				nvmeq->sq_cmds, nvmeq->sq_dma_addr);
+	}
+}
+
+static void nvme_free_queues(struct nvme_dev *dev, int lowest)
+{
+	int i;
+
+	for (i = dev->ctrl.queue_count - 1; i >= lowest; i--) {
+		dev->ctrl.queue_count--;
+		nvme_free_queue(&dev->queues[i]);
+	}
+}
+
+/**
+ * nvme_suspend_queue - put queue into suspended state
+ * @nvmeq: queue to suspend
+ */
+static int nvme_suspend_queue(struct nvme_queue *nvmeq)
+{
+	if (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))
+		return 1;
+
+	/* ensure that nvme_queue_rq() sees NVMEQ_ENABLED cleared */
+	mb();
+
+	nvmeq->dev->online_queues--;
+	if (!nvmeq->qid && nvmeq->dev->ctrl.admin_q)
+		blk_mq_quiesce_queue(nvmeq->dev->ctrl.admin_q);
+	if (nvmeq->cq_vector == -1)
+		return 0;
+	pci_free_irq(to_pci_dev(nvmeq->dev->dev), nvmeq->cq_vector, nvmeq);
+	nvmeq->cq_vector = -1;
+	return 0;
+}
+
+static void nvme_suspend_io_queues(struct nvme_dev *dev)
+{
+	int i;
+
+	for (i = dev->ctrl.queue_count - 1; i > 0; i--)
+		nvme_suspend_queue(&dev->queues[i]);
+}
+
+static void nvme_disable_admin_queue(struct nvme_dev *dev, bool shutdown)
+{
+	struct nvme_queue *nvmeq = &dev->queues[0];
+
+	if (shutdown)
+		nvme_shutdown_ctrl(&dev->ctrl);
+	else
+		nvme_disable_ctrl(&dev->ctrl, dev->ctrl.cap);
+
+	nvme_poll_irqdisable(nvmeq, -1);
+}
+
+static int nvme_cmb_qdepth(struct nvme_dev *dev, int nr_io_queues,
+				int entry_size)
+{
+	int q_depth = dev->q_depth;
+	unsigned q_size_aligned = roundup(q_depth * entry_size,
+					  dev->ctrl.page_size);
+
+	if (q_size_aligned * nr_io_queues > dev->cmb_size) {
+		u64 mem_per_q = div_u64(dev->cmb_size, nr_io_queues);
+		mem_per_q = round_down(mem_per_q, dev->ctrl.page_size);
+		q_depth = div_u64(mem_per_q, entry_size);
+
+		/*
+		 * Ensure the reduced q_depth is above some threshold where it
+		 * would be better to map queues in system memory with the
+		 * original depth
+		 */
+		if (q_depth < 64)
+			return -ENOMEM;
+	}
+
+	return q_depth;
+}
+
+static int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,
+				int qid, int depth)
+{
+	struct pci_dev *pdev = to_pci_dev(dev->dev);
+
+	if (qid && dev->cmb_use_sqes && (dev->cmbsz & NVME_CMBSZ_SQS)) {
+		nvmeq->sq_cmds = pci_alloc_p2pmem(pdev, SQ_SIZE(depth));
+		nvmeq->sq_dma_addr = pci_p2pmem_virt_to_bus(pdev,
+						nvmeq->sq_cmds);
+		if (nvmeq->sq_dma_addr) {
+			set_bit(NVMEQ_SQ_CMB, &nvmeq->flags);
+			return 0; 
+		}
+	}
+
+	nvmeq->sq_cmds = dma_alloc_coherent(dev->dev, SQ_SIZE(depth),
+				&nvmeq->sq_dma_addr, GFP_KERNEL);
+	if (!nvmeq->sq_cmds)
+		return -ENOMEM;
+	return 0;
+}
+
+static int nvme_alloc_queue(struct nvme_dev *dev, int qid, int depth)
+{
+	struct nvme_queue *nvmeq = &dev->queues[qid];
+
+	if (dev->ctrl.queue_count > qid)
+		return 0;
+
+	nvmeq->cqes = dma_alloc_coherent(dev->dev, CQ_SIZE(depth),
+					 &nvmeq->cq_dma_addr, GFP_KERNEL);
+	if (!nvmeq->cqes)
+		goto free_nvmeq;
+
+	if (nvme_alloc_sq_cmds(dev, nvmeq, qid, depth))
+		goto free_cqdma;
+
+	nvmeq->q_dmadev = dev->dev;
+	nvmeq->dev = dev;
+	spin_lock_init(&nvmeq->sq_lock);
+	spin_lock_init(&nvmeq->cq_poll_lock);
+	nvmeq->cq_head = 0;
+	nvmeq->cq_phase = 1;
+	nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
+	nvmeq->q_depth = depth;
+	nvmeq->qid = qid;
+	nvmeq->cq_vector = -1;
+	dev->ctrl.queue_count++;
+
+	return 0;
+
+ free_cqdma:
+	dma_free_coherent(dev->dev, CQ_SIZE(depth), (void *)nvmeq->cqes,
+							nvmeq->cq_dma_addr);
+ free_nvmeq:
+	return -ENOMEM;
+}
+
+static int queue_request_irq(struct nvme_queue *nvmeq)
+{
+	struct pci_dev *pdev = to_pci_dev(nvmeq->dev->dev);
+	int nr = nvmeq->dev->ctrl.instance;
+
+	if (use_threaded_interrupts) {
+		return pci_request_irq(pdev, nvmeq->cq_vector, nvme_irq_check,
+				nvme_irq, nvmeq, "nvme%dq%d", nr, nvmeq->qid);
+	} else {
+		return pci_request_irq(pdev, nvmeq->cq_vector, nvme_irq,
+				NULL, nvmeq, "nvme%dq%d", nr, nvmeq->qid);
+	}
+}
+
+static void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)
+{
+	struct nvme_dev *dev = nvmeq->dev;
+
+	nvmeq->sq_tail = 0;
+	nvmeq->last_sq_tail = 0;
+	nvmeq->cq_head = 0;
+	nvmeq->cq_phase = 1;
+	nvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];
+	memset((void *)nvmeq->cqes, 0, CQ_SIZE(nvmeq->q_depth));
+	nvme_dbbuf_init(dev, nvmeq, qid);
+	dev->online_queues++;
+	wmb(); /* ensure the first interrupt sees the initialization */
+}
+
+static int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)
+{
+	struct nvme_dev *dev = nvmeq->dev;
+	int result;
+	s16 vector;
+
+	clear_bit(NVMEQ_DELETE_ERROR, &nvmeq->flags);
+
+	/*
+	 * A queue's vector matches the queue identifier unless the controller
+	 * has only one vector available.
+	 */
+	if (!polled)
+		vector = dev->num_vecs == 1 ? 0 : qid;
+	else
+		vector = -1;
+
+	result = adapter_alloc_cq(dev, qid, nvmeq, vector);
+	if (result)
+		return result;
+
+	result = adapter_alloc_sq(dev, qid, nvmeq);
+	if (result < 0)
+		return result;
+	else if (result)
+		goto release_cq;
+
+	nvmeq->cq_vector = vector;
+	nvme_init_queue(nvmeq, qid);
+
+	if (vector != -1) {
+		result = queue_request_irq(nvmeq);
+		if (result < 0)
+			goto release_sq;
+	}
+
+	set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	return result;
+
+release_sq:
+	nvmeq->cq_vector = -1;
+	dev->online_queues--;
+	adapter_delete_sq(dev, qid);
+release_cq:
+	adapter_delete_cq(dev, qid);
+	return result;
+}
+
+static const struct blk_mq_ops nvme_mq_admin_ops = {
+	.queue_rq	= nvme_queue_rq,
+	.complete	= nvme_pci_complete_rq,
+	.init_hctx	= nvme_admin_init_hctx,
+	.exit_hctx      = nvme_admin_exit_hctx,
+	.init_request	= nvme_init_request,
+	.timeout	= nvme_timeout,
+};
+
+static const struct blk_mq_ops nvme_mq_ops = {
+	.queue_rq	= nvme_queue_rq,
+	.complete	= nvme_pci_complete_rq,
+	.commit_rqs	= nvme_commit_rqs,
+	.init_hctx	= nvme_init_hctx,
+	.init_request	= nvme_init_request,
+	.map_queues	= nvme_pci_map_queues,
+	.timeout	= nvme_timeout,
+	.poll		= nvme_poll,
+};
+
+static void nvme_dev_remove_admin(struct nvme_dev *dev)
+{
+	if (dev->ctrl.admin_q && !blk_queue_dying(dev->ctrl.admin_q)) {
+		/*
+		 * If the controller was reset during removal, it's possible
+		 * user requests may be waiting on a stopped queue. Start the
+		 * queue to flush these to completion.
+		 */
+		blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+		blk_cleanup_queue(dev->ctrl.admin_q);
+		blk_mq_free_tag_set(&dev->admin_tagset);
+	}
+}
+
+static int nvme_alloc_admin_tags(struct nvme_dev *dev)
+{
+	if (!dev->ctrl.admin_q) {
+		dev->admin_tagset.ops = &nvme_mq_admin_ops;
+		dev->admin_tagset.nr_hw_queues = 1;
+
+		dev->admin_tagset.queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+		dev->admin_tagset.timeout = ADMIN_TIMEOUT;
+		dev->admin_tagset.numa_node = dev_to_node(dev->dev);
+		dev->admin_tagset.cmd_size = nvme_pci_cmd_size(dev, false);
+		dev->admin_tagset.flags = BLK_MQ_F_NO_SCHED;
+		dev->admin_tagset.driver_data = dev;
+
+		if (blk_mq_alloc_tag_set(&dev->admin_tagset))
+			return -ENOMEM;
+		dev->ctrl.admin_tagset = &dev->admin_tagset;
+
+		dev->ctrl.admin_q = blk_mq_init_queue(&dev->admin_tagset);
+		if (IS_ERR(dev->ctrl.admin_q)) {
+			blk_mq_free_tag_set(&dev->admin_tagset);
+			return -ENOMEM;
+		}
+		if (!blk_get_queue(dev->ctrl.admin_q)) {
+			nvme_dev_remove_admin(dev);
+			dev->ctrl.admin_q = NULL;
+			return -ENODEV;
+		}
+	} else
+		blk_mq_unquiesce_queue(dev->ctrl.admin_q);
+
+	return 0;
+}
+
+static unsigned long db_bar_size(struct nvme_dev *dev, unsigned nr_io_queues)
+{
+	return NVME_REG_DBS + ((nr_io_queues + 1) * 8 * dev->db_stride);
+}
+
+static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
+{
+	struct pci_dev *pdev = to_pci_dev(dev->dev);
+
+	if (size <= dev->bar_mapped_size)
+		return 0;
+	if (size > pci_resource_len(pdev, 0))
+		return -ENOMEM;
+	if (dev->bar)
+		iounmap(dev->bar);
+	dev->bar = ioremap(pci_resource_start(pdev, 0), size);
+	if (!dev->bar) {
+		dev->bar_mapped_size = 0;
+		return -ENOMEM;
+	}
+	dev->bar_mapped_size = size;
+	dev->dbs = dev->bar + NVME_REG_DBS;
+
+	return 0;
+}
+
+static int nvme_pci_configure_admin_queue(struct nvme_dev *dev)
+{
+	int result;
+	u32 aqa;
+	struct nvme_queue *nvmeq;
+
+	result = nvme_remap_bar(dev, db_bar_size(dev, 0));
+	if (result < 0)
+		return result;
+
+	dev->subsystem = readl(dev->bar + NVME_REG_VS) >= NVME_VS(1, 1, 0) ?
+				NVME_CAP_NSSRC(dev->ctrl.cap) : 0;
+
+	if (dev->subsystem &&
+	    (readl(dev->bar + NVME_REG_CSTS) & NVME_CSTS_NSSRO))
+		writel(NVME_CSTS_NSSRO, dev->bar + NVME_REG_CSTS);
+
+	result = nvme_disable_ctrl(&dev->ctrl, dev->ctrl.cap);
+	if (result < 0)
+		return result;
+
+	result = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);
+	if (result)
+		return result;
+
+	nvmeq = &dev->queues[0];
+	aqa = nvmeq->q_depth - 1;
+	aqa |= aqa << 16;
+
+	writel(aqa, dev->bar + NVME_REG_AQA);
+	lo_hi_writeq(nvmeq->sq_dma_addr, dev->bar + NVME_REG_ASQ);
+	lo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);
+
+	result = nvme_enable_ctrl(&dev->ctrl, dev->ctrl.cap);
+	if (result)
+		return result;
+
+	nvmeq->cq_vector = 0;
+	nvme_init_queue(nvmeq, 0);
+	result = queue_request_irq(nvmeq);
+	if (result) {
+		nvmeq->cq_vector = -1;
+		return result;
+	}
+
+	set_bit(NVMEQ_ENABLED, &nvmeq->flags);
+	return result;
+}
+
+static int nvme_create_io_queues(struct nvme_dev *dev)
+{
+	unsigned i, max, rw_queues;
+	int ret = 0;
+
+	for (i = dev->ctrl.queue_count; i <= dev->max_qid; i++) {
+		if (nvme_alloc_queue(dev, i, dev->q_depth)) {
+			ret = -ENOMEM;
+			break;
+		}
+	}
+
+	max = min(dev->max_qid, dev->ctrl.queue_count - 1);
+	if (max != 1 && dev->io_queues[HCTX_TYPE_POLL]) {
+		rw_queues = dev->io_queues[HCTX_TYPE_DEFAULT] +
+				dev->io_queues[HCTX_TYPE_READ];
+	} else {
+		rw_queues = max;
+	}
+
+	for (i = dev->online_queues; i <= max; i++) {
+		bool polled = i > rw_queues;
+
+		ret = nvme_create_queue(&dev->queues[i], i, polled);
+		if (ret)
+			break;
+	}
+
+	/*
+	 * Ignore failing Create SQ/CQ commands, we can continue with less
+	 * than the desired amount of queues, and even a controller without
+	 * I/O queues can still be used to issue admin commands.  This might
+	 * be useful to upgrade a buggy firmware for example.
+	 */
+	return ret >= 0 ? 0 : ret;
+}
+
+static ssize_t nvme_cmb_show(struct device *dev,
+			     struct device_attribute *attr,
+			     char *buf)
+{
+	struct nvme_dev *ndev = to_nvme_dev(dev_get_drvdata(dev));
+
+	return scnprintf(buf, PAGE_SIZE, "cmbloc : x%08x\ncmbsz  : x%08x\n",
+		       ndev->cmbloc, ndev->cmbsz);
+}
+static DEVICE_ATTR(cmb, S_IRUGO, nvme_cmb_show, NULL);
+
+static u64 nvme_cmb_size_unit(struct nvme_dev *dev)
+{
+	u8 szu = (dev->cmbsz >> NVME_CMBSZ_SZU_SHIFT) & NVME_CMBSZ_SZU_MASK;
+
+	return 1ULL << (12 + 4 * szu);
+}
+
+static u32 nvme_cmb_size(struct nvme_dev *dev)
+{
+	return (dev->cmbsz >> NVME_CMBSZ_SZ_SHIFT) & NVME_CMBSZ_SZ_MASK;
+}
+
+static void nvme_map_cmb(struct nvme_dev *dev)
+{
+	u64 size, offset;
+	resource_size_t bar_size;
+	struct pci_dev *pdev = to_pci_dev(dev->dev);
+	int bar;
+
+	if (dev->cmb_size)
+		return;
+
+	dev->cmbsz = readl(dev->bar + NVME_REG_CMBSZ);
+	if (!dev->cmbsz)
+		return;
+	dev->cmbloc = readl(dev->bar + NVME_REG_CMBLOC);
+
+	size = nvme_cmb_size_unit(dev) * nvme_cmb_size(dev);
+	offset = nvme_cmb_size_unit(dev) * NVME_CMB_OFST(dev->cmbloc);
+	bar = NVME_CMB_BIR(dev->cmbloc);
+	bar_size = pci_resource_len(pdev, bar);
+
+	if (offset > bar_size)
+		return;
+
+	/*
+	 * Controllers may support a CMB size larger than their BAR,
+	 * for example, due to being behind a bridge. Reduce the CMB to
+	 * the reported size of the BAR
+	 */
+	if (size > bar_size - offset)
+		size = bar_size - offset;
+
+	if (pci_p2pdma_add_resource(pdev, bar, size, offset)) {
+		dev_warn(dev->ctrl.device,
+			 "failed to register the CMB\n");
+		return;
+	}
+
+	dev->cmb_size = size;
+	dev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);
+
+	if ((dev->cmbsz & (NVME_CMBSZ_WDS | NVME_CMBSZ_RDS)) ==
+			(NVME_CMBSZ_WDS | NVME_CMBSZ_RDS))
+		pci_p2pmem_publish(pdev, true);
+
+	if (sysfs_add_file_to_group(&dev->ctrl.device->kobj,
+				    &dev_attr_cmb.attr, NULL))
+		dev_warn(dev->ctrl.device,
+			 "failed to add sysfs attribute for CMB\n");
+}
+
+static inline void nvme_release_cmb(struct nvme_dev *dev)
+{
+	if (dev->cmb_size) {
+		sysfs_remove_file_from_group(&dev->ctrl.device->kobj,
+					     &dev_attr_cmb.attr, NULL);
+		dev->cmb_size = 0;
+	}
+}
+
+static int nvme_set_host_mem(struct nvme_dev *dev, u32 bits)
+{
+	u64 dma_addr = dev->host_mem_descs_dma;
+	struct nvme_command c;
+	int ret;
+
+	memset(&c, 0, sizeof(c));
+	c.features.opcode	= nvme_admin_set_features;
+	c.features.fid		= cpu_to_le32(NVME_FEAT_HOST_MEM_BUF);
+	c.features.dword11	= cpu_to_le32(bits);
+	c.features.dword12	= cpu_to_le32(dev->host_mem_size >>
+					      ilog2(dev->ctrl.page_size));
+	c.features.dword13	= cpu_to_le32(lower_32_bits(dma_addr));
+	c.features.dword14	= cpu_to_le32(upper_32_bits(dma_addr));
+	c.features.dword15	= cpu_to_le32(dev->nr_host_mem_descs);
+
+	ret = nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);
+	if (ret) {
+		dev_warn(dev->ctrl.device,
+			 "failed to set host mem (err %d, flags %#x).\n",
+			 ret, bits);
+	}
+	return ret;
+}
+
+static void nvme_free_host_mem(struct nvme_dev *dev)
+{
+	int i;
+
+	for (i = 0; i < dev->nr_host_mem_descs; i++) {
+		struct nvme_host_mem_buf_desc *desc = &dev->host_mem_descs[i];
+		size_t size = le32_to_cpu(desc->size) * dev->ctrl.page_size;
+
+		dma_free_attrs(dev->dev, size, dev->host_mem_desc_bufs[i],
+			       le64_to_cpu(desc->addr),
+			       DMA_ATTR_NO_KERNEL_MAPPING | DMA_ATTR_NO_WARN);
+	}
+
+	kfree(dev->host_mem_desc_bufs);
+	dev->host_mem_desc_bufs = NULL;
+	dma_free_coherent(dev->dev,
+			dev->nr_host_mem_descs * sizeof(*dev->host_mem_descs),
+			dev->host_mem_descs, dev->host_mem_descs_dma);
+	dev->host_mem_descs = NULL;
+	dev->nr_host_mem_descs = 0;
+}
+
+static int __nvme_alloc_host_mem(struct nvme_dev *dev, u64 preferred,
+		u32 chunk_size)
+{
+	struct nvme_host_mem_buf_desc *descs;
+	u32 max_entries, len;
+	dma_addr_t descs_dma;
+	int i = 0;
+	void **bufs;
+	u64 size, tmp;
+
+	tmp = (preferred + chunk_size - 1);
+	do_div(tmp, chunk_size);
+	max_entries = tmp;
+
+	if (dev->ctrl.hmmaxd && dev->ctrl.hmmaxd < max_entries)
+		max_entries = dev->ctrl.hmmaxd;
+
+	descs = dma_alloc_coherent(dev->dev, max_entries * sizeof(*descs),
+				   &descs_dma, GFP_KERNEL);
+	if (!descs)
+		goto out;
+
+	bufs = kcalloc(max_entries, sizeof(*bufs), GFP_KERNEL);
+	if (!bufs)
+		goto out_free_descs;
+
+	for (size = 0; size < preferred && i < max_entries; size += len) {
+		dma_addr_t dma_addr;
+
+		len = min_t(u64, chunk_size, preferred - size);
+		bufs[i] = dma_alloc_attrs(dev->dev, len, &dma_addr, GFP_KERNEL,
+				DMA_ATTR_NO_KERNEL_MAPPING | DMA_ATTR_NO_WARN);
+		if (!bufs[i])
+			break;
+
+		descs[i].addr = cpu_to_le64(dma_addr);
+		descs[i].size = cpu_to_le32(len / dev->ctrl.page_size);
+		i++;
+	}
+
+	if (!size)
+		goto out_free_bufs;
+
+	dev->nr_host_mem_descs = i;
+	dev->host_mem_size = size;
+	dev->host_mem_descs = descs;
+	dev->host_mem_descs_dma = descs_dma;
+	dev->host_mem_desc_bufs = bufs;
+	return 0;
+
+out_free_bufs:
+	while (--i >= 0) {
+		size_t size = le32_to_cpu(descs[i].size) * dev->ctrl.page_size;
+
+		dma_free_attrs(dev->dev, size, bufs[i],
+			       le64_to_cpu(descs[i].addr),
+			       DMA_ATTR_NO_KERNEL_MAPPING | DMA_ATTR_NO_WARN);
+	}
+
+	kfree(bufs);
+out_free_descs:
+	dma_free_coherent(dev->dev, max_entries * sizeof(*descs), descs,
+			descs_dma);
+out:
+	dev->host_mem_descs = NULL;
+	return -ENOMEM;
+}
+
+static int nvme_alloc_host_mem(struct nvme_dev *dev, u64 min, u64 preferred)
+{
+	u32 chunk_size;
+
+	/* start big and work our way down */
+	for (chunk_size = min_t(u64, preferred, PAGE_SIZE * MAX_ORDER_NR_PAGES);
+	     chunk_size >= max_t(u32, dev->ctrl.hmminds * 4096, PAGE_SIZE * 2);
+	     chunk_size /= 2) {
+		if (!__nvme_alloc_host_mem(dev, preferred, chunk_size)) {
+			if (!min || dev->host_mem_size >= min)
+				return 0;
+			nvme_free_host_mem(dev);
+		}
+	}
+
+	return -ENOMEM;
+}
+
+static int nvme_setup_host_mem(struct nvme_dev *dev)
+{
+	u64 max = (u64)max_host_mem_size_mb * SZ_1M;
+	u64 preferred = (u64)dev->ctrl.hmpre * 4096;
+	u64 min = (u64)dev->ctrl.hmmin * 4096;
+	u32 enable_bits = NVME_HOST_MEM_ENABLE;
+	int ret;
+
+	preferred = min(preferred, max);
+	if (min > max) {
+		dev_warn(dev->ctrl.device,
+			"min host memory (%lld MiB) above limit (%d MiB).\n",
+			min >> ilog2(SZ_1M), max_host_mem_size_mb);
+		nvme_free_host_mem(dev);
+		return 0;
+	}
+
+	/*
+	 * If we already have a buffer allocated check if we can reuse it.
+	 */
+	if (dev->host_mem_descs) {
+		if (dev->host_mem_size >= min)
+			enable_bits |= NVME_HOST_MEM_RETURN;
+		else
+			nvme_free_host_mem(dev);
+	}
+
+	if (!dev->host_mem_descs) {
+		if (nvme_alloc_host_mem(dev, min, preferred)) {
+			dev_warn(dev->ctrl.device,
+				"failed to allocate host memory buffer.\n");
+			return 0; /* controller must work without HMB */
+		}
+
+		dev_info(dev->ctrl.device,
+			"allocated %lld MiB host memory buffer.\n",
+			dev->host_mem_size >> ilog2(SZ_1M));
+	}
+
+	ret = nvme_set_host_mem(dev, enable_bits);
+	if (ret)
+		nvme_free_host_mem(dev);
+	return ret;
+}
+
+/*
+ * nirqs is the number of interrupts available for write and read
+ * queues. The core already reserved an interrupt for the admin queue.
+ */
+static void nvme_calc_irq_sets(struct irq_affinity *affd, unsigned int nrirqs)
+{
+	struct nvme_dev *dev = affd->priv;
+	unsigned int nr_read_queues;
+
+	/*
+	 * If there is no interupt available for queues, ensure that
+	 * the default queue is set to 1. The affinity set size is
+	 * also set to one, but the irq core ignores it for this case.
+	 *
+	 * If only one interrupt is available or 'write_queue' == 0, combine
+	 * write and read queues.
+	 *
+	 * If 'write_queues' > 0, ensure it leaves room for at least one read
+	 * queue.
+	 */
+	if (!nrirqs) {
+		nrirqs = 1;
+		nr_read_queues = 0;
+	} else if (nrirqs == 1 || !write_queues) {
+		nr_read_queues = 0;
+	} else if (write_queues >= nrirqs) {
+		nr_read_queues = 1;
+	} else {
+		nr_read_queues = nrirqs - write_queues;
+	}
+
+	dev->io_queues[HCTX_TYPE_DEFAULT] = nrirqs - nr_read_queues;
+	affd->set_size[HCTX_TYPE_DEFAULT] = nrirqs - nr_read_queues;
+	dev->io_queues[HCTX_TYPE_READ] = nr_read_queues;
+	affd->set_size[HCTX_TYPE_READ] = nr_read_queues;
+	affd->nr_sets = nr_read_queues ? 2 : 1;
+}
+
+static int nvme_setup_irqs(struct nvme_dev *dev, unsigned int nr_io_queues)
+{
+	struct pci_dev *pdev = to_pci_dev(dev->dev);
+	struct irq_affinity affd = {
+		.pre_vectors	= 1,
+		.calc_sets	= nvme_calc_irq_sets,
+		.priv		= dev,
+	};
+	unsigned int irq_queues, this_p_queues;
+
+	/*
+	 * Poll queues don't need interrupts, but we need at least one IO
+	 * queue left over for non-polled IO.
+	 */
+	this_p_queues = poll_queues;
+	if (this_p_queues >= nr_io_queues) {
+		this_p_queues = nr_io_queues - 1;
+		irq_queues = 1;
+	} else {
+		irq_queues = nr_io_queues - this_p_queues + 1;
+	}
+	dev->io_queues[HCTX_TYPE_POLL] = this_p_queues;
+
+	/* Initialize for the single interrupt case */
+	dev->io_queues[HCTX_TYPE_DEFAULT] = 1;
+	dev->io_queues[HCTX_TYPE_READ] = 0;
+
+	return pci_alloc_irq_vectors_affinity(pdev, 1, irq_queues,
+			      PCI_IRQ_ALL_TYPES | PCI_IRQ_AFFINITY, &affd);
+}
+
+static void nvme_disable_io_queues(struct nvme_dev *dev)
+{
+	if (__nvme_disable_io_queues(dev, nvme_admin_delete_sq))
+		__nvme_disable_io_queues(dev, nvme_admin_delete_cq);
+}
+
+static int nvme_setup_io_queues(struct nvme_dev *dev)
+{
+	struct nvme_queue *adminq = &dev->queues[0];
+	struct pci_dev *pdev = to_pci_dev(dev->dev);
+	int result, nr_io_queues;
+	unsigned long size;
+
+	nr_io_queues = max_io_queues();
+	result = nvme_set_queue_count(&dev->ctrl, &nr_io_queues);
+	if (result < 0)
+		return result;
+
+	if (nr_io_queues == 0)
+		return 0;
+	
+	clear_bit(NVMEQ_ENABLED, &adminq->flags);
+
+	if (dev->cmb_use_sqes) {
+		result = nvme_cmb_qdepth(dev, nr_io_queues,
+				sizeof(struct nvme_command));
+		if (result > 0)
+			dev->q_depth = result;
+		else
+			dev->cmb_use_sqes = false;
+	}
+
+	do {
+		size = db_bar_size(dev, nr_io_queues);
+		result = nvme_remap_bar(dev, size);
+		if (!result)
+			break;
+		if (!--nr_io_queues)
+			return -ENOMEM;
+	} while (1);
+	adminq->q_db = dev->dbs;
+
+ retry:
+	/* Deregister the admin queue's interrupt */
+	pci_free_irq(pdev, 0, adminq);
+
+	/*
+	 * If we enable msix early due to not intx, disable it again before
+	 * setting up the full range we need.
+	 */
+	pci_free_irq_vectors(pdev);
+
+	result = nvme_setup_irqs(dev, nr_io_queues);
+	if (result <= 0)
+		return -EIO;
+
+	dev->num_vecs = result;
+	result = max(result - 1, 1);
+	dev->max_qid = result + dev->io_queues[HCTX_TYPE_POLL];
+
+	/*
+	 * Should investigate if there's a performance win from allocating
+	 * more queues than interrupt vectors; it might allow the submission
+	 * path to scale better, even if the receive path is limited by the
+	 * number of interrupts.
+	 */
+	result = queue_request_irq(adminq);
+	if (result) {
+		adminq->cq_vector = -1;
+		return result;
+	}
+	set_bit(NVMEQ_ENABLED, &adminq->flags);
+
+	result = nvme_create_io_queues(dev);
+	if (result || dev->online_queues < 2)
+		return result;
+
+	if (dev->online_queues - 1 < dev->max_qid) {
+		nr_io_queues = dev->online_queues - 1;
+		nvme_disable_io_queues(dev);
+		nvme_suspend_io_queues(dev);
+		goto retry;
+	}
+	dev_info(dev->ctrl.device, "%d/%d/%d default/read/poll queues\n",
+					dev->io_queues[HCTX_TYPE_DEFAULT],
+					dev->io_queues[HCTX_TYPE_READ],
+					dev->io_queues[HCTX_TYPE_POLL]);
+	return 0;
+}
+
+static void nvme_del_queue_end(struct request *req, blk_status_t error)
+{
+	struct nvme_queue *nvmeq = req->end_io_data;
+
+	blk_mq_free_request(req);
+	complete(&nvmeq->delete_done);
+}
+
+static void nvme_del_cq_end(struct request *req, blk_status_t error)
+{
+	struct nvme_queue *nvmeq = req->end_io_data;
+
+	if (error)
+		set_bit(NVMEQ_DELETE_ERROR, &nvmeq->flags);
+
+	nvme_del_queue_end(req, error);
+}
+
+static int nvme_delete_queue(struct nvme_queue *nvmeq, u8 opcode)
+{
+	struct request_queue *q = nvmeq->dev->ctrl.admin_q;
+	struct request *req;
+	struct nvme_command cmd;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd.delete_queue.opcode = opcode;
+	cmd.delete_queue.qid = cpu_to_le16(nvmeq->qid);
+
+	req = nvme_alloc_request(q, &cmd, BLK_MQ_REQ_NOWAIT, NVME_QID_ANY);
+	if (IS_ERR(req))
+		return PTR_ERR(req);
+
+	req->timeout = ADMIN_TIMEOUT;
+	req->end_io_data = nvmeq;
+
+	init_completion(&nvmeq->delete_done);
+	blk_execute_rq_nowait(q, NULL, req, false,
+			opcode == nvme_admin_delete_cq ?
+				nvme_del_cq_end : nvme_del_queue_end);
+	return 0;
+}
+
+static bool __nvme_disable_io_queues(struct nvme_dev *dev, u8 opcode)
+{
+	int nr_queues = dev->online_queues - 1, sent = 0;
+	unsigned long timeout;
+
+ retry:
+	timeout = ADMIN_TIMEOUT;
+	while (nr_queues > 0) {
+		if (nvme_delete_queue(&dev->queues[nr_queues], opcode))
+			break;
+		nr_queues--;
+		sent++;
+	}
+	while (sent) {
+		struct nvme_queue *nvmeq = &dev->queues[nr_queues + sent];
+
+		timeout = wait_for_completion_io_timeout(&nvmeq->delete_done,
+				timeout);
+		if (timeout == 0)
+			return false;
+
+		/* handle any remaining CQEs */
+		if (opcode == nvme_admin_delete_cq &&
+		    !test_bit(NVMEQ_DELETE_ERROR, &nvmeq->flags))
+			nvme_poll_irqdisable(nvmeq, -1);
+
+		sent--;
+		if (nr_queues)
+			goto retry;
+	}
+	return true;
+}
+
+/*
+ * return error value only when tagset allocation failed
+ */
+static int nvme_dev_add(struct nvme_dev *dev)
+{
+	int ret;
+
+	if (!dev->ctrl.tagset) {
+		dev->tagset.ops = &nvme_mq_ops;
+		dev->tagset.nr_hw_queues = dev->online_queues - 1;
+		dev->tagset.nr_maps = 2; /* default + read */
+		if (dev->io_queues[HCTX_TYPE_POLL])
+			dev->tagset.nr_maps++;
+		dev->tagset.timeout = NVME_IO_TIMEOUT;
+		dev->tagset.numa_node = dev_to_node(dev->dev);
+		dev->tagset.queue_depth =
+				min_t(int, dev->q_depth, BLK_MQ_MAX_DEPTH) - 1;
+		dev->tagset.cmd_size = nvme_pci_cmd_size(dev, false);
+		if ((dev->ctrl.sgls & ((1 << 0) | (1 << 1))) && sgl_threshold) {
+			dev->tagset.cmd_size = max(dev->tagset.cmd_size,
+					nvme_pci_cmd_size(dev, true));
+		}
+		dev->tagset.flags = BLK_MQ_F_SHOULD_MERGE;
+		dev->tagset.driver_data = dev;
+
+		ret = blk_mq_alloc_tag_set(&dev->tagset);
+		if (ret) {
+			dev_warn(dev->ctrl.device,
+				"IO queues tagset allocation failed %d\n", ret);
+			return ret;
+		}
+		dev->ctrl.tagset = &dev->tagset;
+
+		nvme_dbbuf_set(dev);
+	} else {
+		blk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);
+
+		/* Free previously allocated queues that are no longer usable */
+		nvme_free_queues(dev, dev->online_queues);
+	}
+
+	return 0;
+}
+
+static int nvme_pci_enable(struct nvme_dev *dev)
+{
+	int result = -ENOMEM;
+	struct pci_dev *pdev = to_pci_dev(dev->dev);
+
+	if (pci_enable_device_mem(pdev))
+		return result;
+
+	pci_set_master(pdev);
+
+	if (dma_set_mask_and_coherent(dev->dev, DMA_BIT_MASK(64)) &&
+	    dma_set_mask_and_coherent(dev->dev, DMA_BIT_MASK(32)))
+		goto disable;
+
+	if (readl(dev->bar + NVME_REG_CSTS) == -1) {
+		result = -ENODEV;
+		goto disable;
+	}
+
+	/*
+	 * Some devices and/or platforms don't advertise or work with INTx
+	 * interrupts. Pre-enable a single MSIX or MSI vec for setup. We'll
+	 * adjust this later.
+	 */
+	result = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_ALL_TYPES);
+	if (result < 0)
+		return result;
+
+	dev->ctrl.cap = lo_hi_readq(dev->bar + NVME_REG_CAP);
+
+	dev->q_depth = min_t(int, NVME_CAP_MQES(dev->ctrl.cap) + 1,
+				io_queue_depth);
+	dev->db_stride = 1 << NVME_CAP_STRIDE(dev->ctrl.cap);
+	dev->dbs = dev->bar + 4096;
+
+	/*
+	 * Temporary fix for the Apple controller found in the MacBook8,1 and
+	 * some MacBook7,1 to avoid controller resets and data loss.
+	 */
+	if (pdev->vendor == PCI_VENDOR_ID_APPLE && pdev->device == 0x2001) {
+		dev->q_depth = 2;
+		dev_warn(dev->ctrl.device, "detected Apple NVMe controller, "
+			"set queue depth=%u to work around controller resets\n",
+			dev->q_depth);
+	} else if (pdev->vendor == PCI_VENDOR_ID_SAMSUNG &&
+		   (pdev->device == 0xa821 || pdev->device == 0xa822) &&
+		   NVME_CAP_MQES(dev->ctrl.cap) == 0) {
+		dev->q_depth = 64;
+		dev_err(dev->ctrl.device, "detected PM1725 NVMe controller, "
+                        "set queue depth=%u\n", dev->q_depth);
+	}
+
+	nvme_map_cmb(dev);
+
+	pci_enable_pcie_error_reporting(pdev);
+	pci_save_state(pdev);
+	return 0;
+
+ disable:
+	pci_disable_device(pdev);
+	return result;
+}
+
+static void nvme_dev_unmap(struct nvme_dev *dev)
+{
+	if (dev->bar)
+		iounmap(dev->bar);
+	pci_release_mem_regions(to_pci_dev(dev->dev));
+}
+
+static void nvme_pci_disable(struct nvme_dev *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev->dev);
+
+	pci_free_irq_vectors(pdev);
+
+	if (pci_is_enabled(pdev)) {
+		pci_disable_pcie_error_reporting(pdev);
+		pci_disable_device(pdev);
+	}
+}
+
+static void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)
+{
+	bool dead = true;
+	struct pci_dev *pdev = to_pci_dev(dev->dev);
+
+	mutex_lock(&dev->shutdown_lock);
+	if (pci_is_enabled(pdev)) {
+		u32 csts = readl(dev->bar + NVME_REG_CSTS);
+
+		if (dev->ctrl.state == NVME_CTRL_LIVE ||
+		    dev->ctrl.state == NVME_CTRL_RESETTING)
+			nvme_start_freeze(&dev->ctrl);
+		dead = !!((csts & NVME_CSTS_CFS) || !(csts & NVME_CSTS_RDY) ||
+			pdev->error_state  != pci_channel_io_normal);
+	}
+
+	/*
+	 * Give the controller a chance to complete all entered requests if
+	 * doing a safe shutdown.
+	 */
+	if (!dead) {
+		if (shutdown)
+			nvme_wait_freeze_timeout(&dev->ctrl, NVME_IO_TIMEOUT);
+	}
+
+	nvme_stop_queues(&dev->ctrl);
+
+	if (!dead && dev->ctrl.queue_count > 0) {
+		nvme_disable_io_queues(dev);
+		nvme_disable_admin_queue(dev, shutdown);
+	}
+	nvme_suspend_io_queues(dev);
+	nvme_suspend_queue(&dev->queues[0]);
+	nvme_pci_disable(dev);
+
+	blk_mq_tagset_busy_iter(&dev->tagset, nvme_cancel_request, &dev->ctrl);
+	blk_mq_tagset_busy_iter(&dev->admin_tagset, nvme_cancel_request, &dev->ctrl);
+
+	/*
+	 * The driver will not be starting up queues again if shutting down so
+	 * must flush all entered requests to their failed completion to avoid
+	 * deadlocking blk-mq hot-cpu notifier.
+	 */
+	if (shutdown)
+		nvme_start_queues(&dev->ctrl);
+	mutex_unlock(&dev->shutdown_lock);
+}
+
+static int nvme_setup_prp_pools(struct nvme_dev *dev)
+{
+	dev->prp_page_pool = dma_pool_create("prp list page", dev->dev,
+						PAGE_SIZE, PAGE_SIZE, 0);
+	if (!dev->prp_page_pool)
+		return -ENOMEM;
+
+	/* Optimisation for I/Os between 4k and 128k */
+	dev->prp_small_pool = dma_pool_create("prp list 256", dev->dev,
+						256, 256, 0);
+	if (!dev->prp_small_pool) {
+		dma_pool_destroy(dev->prp_page_pool);
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+static void nvme_release_prp_pools(struct nvme_dev *dev)
+{
+	dma_pool_destroy(dev->prp_page_pool);
+	dma_pool_destroy(dev->prp_small_pool);
+}
+
+static void nvme_pci_free_ctrl(struct nvme_ctrl *ctrl)
+{
+	struct nvme_dev *dev = to_nvme_dev(ctrl);
+
+	nvme_dbbuf_dma_free(dev);
+	put_device(dev->dev);
+	if (dev->tagset.tags)
+		blk_mq_free_tag_set(&dev->tagset);
+	if (dev->ctrl.admin_q)
+		blk_put_queue(dev->ctrl.admin_q);
+	kfree(dev->queues);
+	free_opal_dev(dev->ctrl.opal_dev);
+	mempool_destroy(dev->iod_mempool);
+	kfree(dev);
+}
+
+static void nvme_remove_dead_ctrl(struct nvme_dev *dev, int status)
+{
+	dev_warn(dev->ctrl.device, "Removing after probe failure status: %d\n", status);
+
+	nvme_get_ctrl(&dev->ctrl);
+	nvme_dev_disable(dev, false);
+	nvme_kill_queues(&dev->ctrl);
+	if (!queue_work(nvme_wq, &dev->remove_work))
+		nvme_put_ctrl(&dev->ctrl);
+}
+
+static void nvme_reset_work(struct work_struct *work)
+{
+	struct nvme_dev *dev =
+		container_of(work, struct nvme_dev, ctrl.reset_work);
+	bool was_suspend = !!(dev->ctrl.ctrl_config & NVME_CC_SHN_NORMAL);
+	int result = -ENODEV;
+	enum nvme_ctrl_state new_state = NVME_CTRL_LIVE;
+
+	if (WARN_ON(dev->ctrl.state != NVME_CTRL_RESETTING))
+		goto out;
+
+	/*
+	 * If we're called to reset a live controller first shut it down before
+	 * moving on.
+	 */
+	if (dev->ctrl.ctrl_config & NVME_CC_ENABLE)
+		nvme_dev_disable(dev, false);
+
+	mutex_lock(&dev->shutdown_lock);
+	result = nvme_pci_enable(dev);
+	if (result)
+		goto out_unlock;
+
+	result = nvme_pci_configure_admin_queue(dev);
+	if (result)
+		goto out_unlock;
+
+	result = nvme_alloc_admin_tags(dev);
+	if (result)
+		goto out_unlock;
+
+	/*
+	 * Limit the max command size to prevent iod->sg allocations going
+	 * over a single page.
+	 */
+	dev->ctrl.max_hw_sectors = NVME_MAX_KB_SZ << 1;
+	dev->ctrl.max_segments = NVME_MAX_SEGS;
+	mutex_unlock(&dev->shutdown_lock);
+
+	/*
+	 * Introduce CONNECTING state from nvme-fc/rdma transports to mark the
+	 * initializing procedure here.
+	 */
+	if (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_CONNECTING)) {
+		dev_warn(dev->ctrl.device,
+			"failed to mark controller CONNECTING\n");
+		goto out;
+	}
+
+	result = nvme_init_identify(&dev->ctrl);
+	if (result)
+		goto out;
+
+	if (dev->ctrl.oacs & NVME_CTRL_OACS_SEC_SUPP) {
+		if (!dev->ctrl.opal_dev)
+			dev->ctrl.opal_dev =
+				init_opal_dev(&dev->ctrl, &nvme_sec_submit);
+		else if (was_suspend)
+			opal_unlock_from_suspend(dev->ctrl.opal_dev);
+	} else {
+		free_opal_dev(dev->ctrl.opal_dev);
+		dev->ctrl.opal_dev = NULL;
+	}
+
+	if (dev->ctrl.oacs & NVME_CTRL_OACS_DBBUF_SUPP) {
+		result = nvme_dbbuf_dma_alloc(dev);
+		if (result)
+			dev_warn(dev->dev,
+				 "unable to allocate dma for dbbuf\n");
+	}
+
+	if (dev->ctrl.hmpre) {
+		result = nvme_setup_host_mem(dev);
+		if (result < 0)
+			goto out;
+	}
+
+	result = nvme_setup_io_queues(dev);
+	if (result)
+		goto out;
+
+	/*
+	 * Keep the controller around but remove all namespaces if we don't have
+	 * any working I/O queue.
+	 */
+	if (dev->online_queues < 2) {
+		dev_warn(dev->ctrl.device, "IO queues not created\n");
+		nvme_kill_queues(&dev->ctrl);
+		nvme_remove_namespaces(&dev->ctrl);
+		new_state = NVME_CTRL_ADMIN_ONLY;
+	} else {
+		nvme_start_queues(&dev->ctrl);
+		nvme_wait_freeze(&dev->ctrl);
+		/* hit this only when allocate tagset fails */
+		if (nvme_dev_add(dev))
+			new_state = NVME_CTRL_ADMIN_ONLY;
+		nvme_unfreeze(&dev->ctrl);
+	}
+
+	/*
+	 * If only admin queue live, keep it to do further investigation or
+	 * recovery.
+	 */
+	if (!nvme_change_ctrl_state(&dev->ctrl, new_state)) {
+		dev_warn(dev->ctrl.device,
+			"failed to mark controller state %d\n", new_state);
+		goto out;
+	}
+
+	nvme_start_ctrl(&dev->ctrl);
+	return;
+
+ out_unlock:
+	mutex_unlock(&dev->shutdown_lock);
+ out:
+	nvme_remove_dead_ctrl(dev, result);
+}
+
+static void nvme_remove_dead_ctrl_work(struct work_struct *work)
+{
+	struct nvme_dev *dev = container_of(work, struct nvme_dev, remove_work);
+	struct pci_dev *pdev = to_pci_dev(dev->dev);
+
+	if (pci_get_drvdata(pdev))
+		device_release_driver(&pdev->dev);
+	nvme_put_ctrl(&dev->ctrl);
+}
+
+static int nvme_pci_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val)
+{
+	*val = readl(to_nvme_dev(ctrl)->bar + off);
+	return 0;
+}
+
+static int nvme_pci_reg_write32(struct nvme_ctrl *ctrl, u32 off, u32 val)
+{
+	writel(val, to_nvme_dev(ctrl)->bar + off);
+	return 0;
+}
+
+static int nvme_pci_reg_read64(struct nvme_ctrl *ctrl, u32 off, u64 *val)
+{
+	*val = readq(to_nvme_dev(ctrl)->bar + off);
+	return 0;
+}
+
+static int nvme_pci_get_address(struct nvme_ctrl *ctrl, char *buf, int size)
+{
+	struct pci_dev *pdev = to_pci_dev(to_nvme_dev(ctrl)->dev);
+
+	return snprintf(buf, size, "%s", dev_name(&pdev->dev));
+}
+
+static const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {
+	.name			= "pcie",
+	.module			= THIS_MODULE,
+	.flags			= NVME_F_METADATA_SUPPORTED |
+				  NVME_F_PCI_P2PDMA,
+	.reg_read32		= nvme_pci_reg_read32,
+	.reg_write32		= nvme_pci_reg_write32,
+	.reg_read64		= nvme_pci_reg_read64,
+	.free_ctrl		= nvme_pci_free_ctrl,
+	.submit_async_event	= nvme_pci_submit_async_event,
+	.get_address		= nvme_pci_get_address,
+};
+
+static int nvme_dev_map(struct nvme_dev *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev->dev);
+
+	if (pci_request_mem_regions(pdev, "nvme"))
+		return -ENODEV;
+
+	if (nvme_remap_bar(dev, NVME_REG_DBS + 4096))
+		goto release;
+
+	return 0;
+  release:
+	pci_release_mem_regions(pdev);
+	return -ENODEV;
+}
+
+static unsigned long check_vendor_combination_bug(struct pci_dev *pdev)
+{
+	if (pdev->vendor == 0x144d && pdev->device == 0xa802) {
+		/*
+		 * Several Samsung devices seem to drop off the PCIe bus
+		 * randomly when APST is on and uses the deepest sleep state.
+		 * This has been observed on a Samsung "SM951 NVMe SAMSUNG
+		 * 256GB", a "PM951 NVMe SAMSUNG 512GB", and a "Samsung SSD
+		 * 950 PRO 256GB", but it seems to be restricted to two Dell
+		 * laptops.
+		 */
+		if (dmi_match(DMI_SYS_VENDOR, "Dell Inc.") &&
+		    (dmi_match(DMI_PRODUCT_NAME, "XPS 15 9550") ||
+		     dmi_match(DMI_PRODUCT_NAME, "Precision 5510")))
+			return NVME_QUIRK_NO_DEEPEST_PS;
+	} else if (pdev->vendor == 0x144d && pdev->device == 0xa804) {
+		/*
+		 * Samsung SSD 960 EVO drops off the PCIe bus after system
+		 * suspend on a Ryzen board, ASUS PRIME B350M-A, as well as
+		 * within few minutes after bootup on a Coffee Lake board -
+		 * ASUS PRIME Z370-A
+		 */
+		if (dmi_match(DMI_BOARD_VENDOR, "ASUSTeK COMPUTER INC.") &&
+		    (dmi_match(DMI_BOARD_NAME, "PRIME B350M-A") ||
+		     dmi_match(DMI_BOARD_NAME, "PRIME Z370-A")))
+			return NVME_QUIRK_NO_APST;
+	}
+
+	return 0;
+}
+
+static void nvme_async_probe(void *data, async_cookie_t cookie)
+{
+	struct nvme_dev *dev = data;
+
+	nvme_reset_ctrl_sync(&dev->ctrl);
+	flush_work(&dev->ctrl.scan_work);
+	nvme_put_ctrl(&dev->ctrl);
+}
+
+static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
+{
+	int node, result = -ENOMEM;
+	struct nvme_dev *dev;
+	unsigned long quirks = id->driver_data;
+	size_t alloc_size;
+
+	node = dev_to_node(&pdev->dev);
+	if (node == NUMA_NO_NODE)
+		set_dev_node(&pdev->dev, first_memory_node);
+
+	dev = kzalloc_node(sizeof(*dev), GFP_KERNEL, node);
+	if (!dev)
+		return -ENOMEM;
+
+	dev->queues = kcalloc_node(max_queue_count(), sizeof(struct nvme_queue),
+					GFP_KERNEL, node);
+	if (!dev->queues)
+		goto free;
+
+	dev->dev = get_device(&pdev->dev);
+	pci_set_drvdata(pdev, dev);
+
+	result = nvme_dev_map(dev);
+	if (result)
+		goto put_pci;
+
+	INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
+	INIT_WORK(&dev->remove_work, nvme_remove_dead_ctrl_work);
+	mutex_init(&dev->shutdown_lock);
+
+	result = nvme_setup_prp_pools(dev);
+	if (result)
+		goto unmap;
+
+	quirks |= check_vendor_combination_bug(pdev);
+
+	/*
+	 * Double check that our mempool alloc size will cover the biggest
+	 * command we support.
+	 */
+	alloc_size = nvme_pci_iod_alloc_size(dev, NVME_MAX_KB_SZ,
+						NVME_MAX_SEGS, true);
+	WARN_ON_ONCE(alloc_size > PAGE_SIZE);
+
+	dev->iod_mempool = mempool_create_node(1, mempool_kmalloc,
+						mempool_kfree,
+						(void *) alloc_size,
+						GFP_KERNEL, node);
+	if (!dev->iod_mempool) {
+		result = -ENOMEM;
+		goto release_pools;
+	}
+
+	result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
+			quirks);
+	if (result)
+		goto release_mempool;
+
+	dev_info(dev->ctrl.device, "pci function %s\n", dev_name(&pdev->dev));
+
+	nvme_get_ctrl(&dev->ctrl);
+	async_schedule(nvme_async_probe, dev);
+
+	return 0;
+
+ release_mempool:
+	mempool_destroy(dev->iod_mempool);
+ release_pools:
+	nvme_release_prp_pools(dev);
+ unmap:
+	nvme_dev_unmap(dev);
+ put_pci:
+	put_device(dev->dev);
+ free:
+	kfree(dev->queues);
+	kfree(dev);
+	return result;
+}
+
+static void nvme_reset_prepare(struct pci_dev *pdev)
+{
+	struct nvme_dev *dev = pci_get_drvdata(pdev);
+	nvme_dev_disable(dev, false);
+}
+
+static void nvme_reset_done(struct pci_dev *pdev)
+{
+	struct nvme_dev *dev = pci_get_drvdata(pdev);
+	nvme_reset_ctrl_sync(&dev->ctrl);
+}
+
+static void nvme_shutdown(struct pci_dev *pdev)
+{
+	struct nvme_dev *dev = pci_get_drvdata(pdev);
+	nvme_dev_disable(dev, true);
+}
+
+/*
+ * The driver's remove may be called on a device in a partially initialized
+ * state. This function must not have any dependencies on the device state in
+ * order to proceed.
+ */
+static void nvme_remove(struct pci_dev *pdev)
+{
+	struct nvme_dev *dev = pci_get_drvdata(pdev);
+
+	nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DELETING);
+	pci_set_drvdata(pdev, NULL);
+
+	if (!pci_device_is_present(pdev)) {
+		nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DEAD);
+		nvme_dev_disable(dev, true);
+		nvme_dev_remove_admin(dev);
+	}
+
+	flush_work(&dev->ctrl.reset_work);
+	nvme_stop_ctrl(&dev->ctrl);
+	nvme_remove_namespaces(&dev->ctrl);
+	nvme_dev_disable(dev, true);
+	nvme_release_cmb(dev);
+	nvme_free_host_mem(dev);
+	nvme_dev_remove_admin(dev);
+	nvme_free_queues(dev, 0);
+	nvme_uninit_ctrl(&dev->ctrl);
+	nvme_release_prp_pools(dev);
+	nvme_dev_unmap(dev);
+	nvme_put_ctrl(&dev->ctrl);
+}
+
+#ifdef CONFIG_PM_SLEEP
+static int nvme_suspend(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct nvme_dev *ndev = pci_get_drvdata(pdev);
+
+	nvme_dev_disable(ndev, true);
+	return 0;
+}
+
+static int nvme_resume(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct nvme_dev *ndev = pci_get_drvdata(pdev);
+
+	nvme_reset_ctrl(&ndev->ctrl);
+	return 0;
+}
+#endif
+
+static SIMPLE_DEV_PM_OPS(nvme_dev_pm_ops, nvme_suspend, nvme_resume);
+
+static pci_ers_result_t nvme_error_detected(struct pci_dev *pdev,
+						pci_channel_state_t state)
+{
+	struct nvme_dev *dev = pci_get_drvdata(pdev);
+
+	/*
+	 * A frozen channel requires a reset. When detected, this method will
+	 * shutdown the controller to quiesce. The controller will be restarted
+	 * after the slot reset through driver's slot_reset callback.
+	 */
+	switch (state) {
+	case pci_channel_io_normal:
+		return PCI_ERS_RESULT_CAN_RECOVER;
+	case pci_channel_io_frozen:
+		dev_warn(dev->ctrl.device,
+			"frozen state error detected, reset controller\n");
+		nvme_dev_disable(dev, false);
+		return PCI_ERS_RESULT_NEED_RESET;
+	case pci_channel_io_perm_failure:
+		dev_warn(dev->ctrl.device,
+			"failure state error detected, request disconnect\n");
+		return PCI_ERS_RESULT_DISCONNECT;
+	}
+	return PCI_ERS_RESULT_NEED_RESET;
+}
+
+static pci_ers_result_t nvme_slot_reset(struct pci_dev *pdev)
+{
+	struct nvme_dev *dev = pci_get_drvdata(pdev);
+
+	dev_info(dev->ctrl.device, "restart after slot reset\n");
+	pci_restore_state(pdev);
+	nvme_reset_ctrl(&dev->ctrl);
+	return PCI_ERS_RESULT_RECOVERED;
+}
+
+static void nvme_error_resume(struct pci_dev *pdev)
+{
+	struct nvme_dev *dev = pci_get_drvdata(pdev);
+
+	flush_work(&dev->ctrl.reset_work);
+}
+
+static const struct pci_error_handlers nvme_err_handler = {
+	.error_detected	= nvme_error_detected,
+	.slot_reset	= nvme_slot_reset,
+	.resume		= nvme_error_resume,
+	.reset_prepare	= nvme_reset_prepare,
+	.reset_done	= nvme_reset_done,
+};
+
+static const struct pci_device_id nvme_id_table[] = {
+	{ PCI_VDEVICE(INTEL, 0x0953),
+		.driver_data = NVME_QUIRK_STRIPE_SIZE |
+				NVME_QUIRK_DEALLOCATE_ZEROES, },
+	{ PCI_VDEVICE(INTEL, 0x0a53),
+		.driver_data = NVME_QUIRK_STRIPE_SIZE |
+				NVME_QUIRK_DEALLOCATE_ZEROES, },
+	{ PCI_VDEVICE(INTEL, 0x0a54),
+		.driver_data = NVME_QUIRK_STRIPE_SIZE |
+				NVME_QUIRK_DEALLOCATE_ZEROES, },
+	{ PCI_VDEVICE(INTEL, 0x0a55),
+		.driver_data = NVME_QUIRK_STRIPE_SIZE |
+				NVME_QUIRK_DEALLOCATE_ZEROES, },
+	{ PCI_VDEVICE(INTEL, 0xf1a5),	/* Intel 600P/P3100 */
+		.driver_data = NVME_QUIRK_NO_DEEPEST_PS |
+				NVME_QUIRK_MEDIUM_PRIO_SQ },
+	{ PCI_VDEVICE(INTEL, 0xf1a6),	/* Intel 760p/Pro 7600p */
+		.driver_data = NVME_QUIRK_IGNORE_DEV_SUBNQN, },
+	{ PCI_VDEVICE(INTEL, 0x5845),	/* Qemu emulated controller */
+		.driver_data = NVME_QUIRK_IDENTIFY_CNS |
+				NVME_QUIRK_DISABLE_WRITE_ZEROES, },
+	{ PCI_DEVICE(0x1bb1, 0x0100),   /* Seagate Nytro Flash Storage */
+		.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },
+	{ PCI_DEVICE(0x1c58, 0x0003),	/* HGST adapter */
+		.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },
+	{ PCI_DEVICE(0x1c58, 0x0023),	/* WDC SN200 adapter */
+		.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },
+	{ PCI_DEVICE(0x1c5f, 0x0540),	/* Memblaze Pblaze4 adapter */
+		.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },
+	{ PCI_DEVICE(0x144d, 0xa821),   /* Samsung PM1725 */
+		.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },
+	{ PCI_DEVICE(0x144d, 0xa822),   /* Samsung PM1725a */
+		.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },
+	{ PCI_DEVICE(0x1d1d, 0x1f1f),	/* LighNVM qemu device */
+		.driver_data = NVME_QUIRK_LIGHTNVM, },
+	{ PCI_DEVICE(0x1d1d, 0x2807),	/* CNEX WL */
+		.driver_data = NVME_QUIRK_LIGHTNVM, },
+	{ PCI_DEVICE(0x1d1d, 0x2601),	/* CNEX Granby */
+		.driver_data = NVME_QUIRK_LIGHTNVM, },
+	{ PCI_DEVICE_CLASS(PCI_CLASS_STORAGE_EXPRESS, 0xffffff) },
+	{ PCI_DEVICE(PCI_VENDOR_ID_APPLE, 0x2001) },
+	{ PCI_DEVICE(PCI_VENDOR_ID_APPLE, 0x2003) },
+	{ 0, }
+};
+MODULE_DEVICE_TABLE(pci, nvme_id_table);
+
+static struct pci_driver nvme_driver = {
+	.name		= "nvme",
+	.id_table	= nvme_id_table,
+	.probe		= nvme_probe,
+	.remove		= nvme_remove,
+	.shutdown	= nvme_shutdown,
+	.driver		= {
+		.pm	= &nvme_dev_pm_ops,
+	},
+	.sriov_configure = pci_sriov_configure_simple,
+	.err_handler	= &nvme_err_handler,
+};
+
+static int __init nvme_init(void)
+{
+	BUILD_BUG_ON(IRQ_AFFINITY_MAX_SETS < 2);
+	return pci_register_driver(&nvme_driver);
+}
+
+static void __exit nvme_exit(void)
+{
+	pci_unregister_driver(&nvme_driver);
+	flush_workqueue(nvme_wq);
+	_nvme_check_size();
+}
+
+MODULE_AUTHOR("Matthew Wilcox <willy@linux.intel.com>");
+MODULE_LICENSE("GPL");
+MODULE_VERSION("1.0");
+module_init(nvme_init);
+module_exit(nvme_exit);
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/rdma.c b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/rdma.c
new file mode 100644
index 0000000..06e4152
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/rdma.c
@@ -0,0 +1,2481 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * NVMe over Fabrics RDMA host code.
+ * Copyright (c) 2015-2016 HGST, a Western Digital Company.
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <rdma/mr_pool.h>
+#include <linux/err.h>
+#include <linux/string.h>
+#include <linux/atomic.h>
+#include <linux/blk-mq.h>
+#include <linux/blk-mq-rdma.h>
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/scatterlist.h>
+#include <linux/nvme.h>
+#include <asm/unaligned.h>
+
+#include <rdma/ib_verbs.h>
+#include <rdma/rdma_cm.h>
+#include <linux/nvme-rdma.h>
+
+#include "nvme.h"
+#include "fabrics.h"
+
+
+#define NVME_RDMA_CONNECT_TIMEOUT_MS	3000		/* 3 second */
+
+/* The limits seems to be rather arbitrary. Bump to
+ * 257 to accomondate mapping 1MB from user space */
+#define NVME_RDMA_MAX_SEGMENTS		257
+
+#define NVME_RDMA_MAX_INLINE_SEGMENTS	4
+
+struct nvme_rdma_device {
+	struct ib_device	*dev;
+	struct ib_pd		*pd;
+	struct kref		ref;
+	struct list_head	entry;
+	unsigned int		num_inline_segments;
+};
+
+struct nvme_rdma_qe {
+	struct ib_cqe		cqe;
+	void			*data;
+	u64			dma;
+};
+
+struct nvme_rdma_queue;
+struct nvme_rdma_request {
+	//struct nvme_request	req;
+	struct nvme_io_param    param;
+	struct ib_mr		*mr;
+	struct ib_mr		*key_mr;
+	struct nvme_rdma_qe	sqe;
+	union nvme_result	result;
+	__le16			status;
+	refcount_t		ref;
+	struct ib_sge		sge[1 + NVME_RDMA_MAX_INLINE_SEGMENTS];
+	u32			num_sge;
+	int			nents;
+	struct ib_reg_wr	reg_wr;
+	struct ib_cqe		reg_cqe;
+	struct ib_reg_wr	key_reg_wr;
+	struct ib_cqe		key_cqe;
+	struct nvme_rdma_queue  *queue;
+	struct sg_table		sg_table;
+	struct scatterlist	*key_sgl;
+	int			key_sgl_nents;
+	struct scatterlist	first_sgl[];
+};
+
+enum nvme_rdma_queue_flags {
+	NVME_RDMA_Q_ALLOCATED		= 0,
+	NVME_RDMA_Q_LIVE		= 1,
+	NVME_RDMA_Q_TR_READY		= 2,
+};
+
+struct nvme_rdma_queue {
+	struct nvme_rdma_qe	*rsp_ring;
+	int			queue_size;
+	size_t			cmnd_capsule_len;
+	struct nvme_rdma_ctrl	*ctrl;
+	struct nvme_rdma_device	*device;
+	struct ib_cq		*ib_cq;
+	struct ib_qp		*qp;
+
+	unsigned long		flags;
+	struct rdma_cm_id	*cm_id;
+	int			cm_error;
+	struct completion	cm_done;
+};
+
+struct nvme_rdma_ctrl {
+	/* read only in the hot path */
+	struct nvme_rdma_queue	*queues;
+
+	/* other member variables */
+	struct blk_mq_tag_set	tag_set;
+	struct work_struct	err_work;
+
+	struct nvme_rdma_qe	async_event_sqe;
+
+	struct delayed_work	reconnect_work;
+
+	struct list_head	list;
+
+	struct blk_mq_tag_set	admin_tag_set;
+	struct nvme_rdma_device	*device;
+
+	u32			max_fr_pages;
+
+	struct sockaddr_storage addr;
+	struct sockaddr_storage src_addr;
+
+	struct nvme_ctrl	ctrl;
+	bool			use_inline_data;
+	u32			io_queues[HCTX_MAX_TYPES];
+};
+
+static inline struct nvme_rdma_ctrl *to_rdma_ctrl(struct nvme_ctrl *ctrl)
+{
+	return container_of(ctrl, struct nvme_rdma_ctrl, ctrl);
+}
+
+static LIST_HEAD(device_list);
+static DEFINE_MUTEX(device_list_mutex);
+
+static LIST_HEAD(nvme_rdma_ctrl_list);
+static DEFINE_MUTEX(nvme_rdma_ctrl_mutex);
+
+/*
+ * Disabling this option makes small I/O goes faster, but is fundamentally
+ * unsafe.  With it turned off we will have to register a global rkey that
+ * allows read and write access to all physical memory.
+ */
+static bool register_always = true;
+module_param(register_always, bool, 0444);
+MODULE_PARM_DESC(register_always,
+	 "Use memory registration even for contiguous memory regions");
+
+static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
+		struct rdma_cm_event *event);
+static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc);
+
+static const struct blk_mq_ops nvme_rdma_mq_ops;
+static const struct blk_mq_ops nvme_rdma_admin_mq_ops;
+
+/* XXX: really should move to a generic header sooner or later.. */
+static inline void put_unaligned_le24(u32 val, u8 *p)
+{
+	*p++ = val;
+	*p++ = val >> 8;
+	*p++ = val >> 16;
+}
+
+static inline int nvme_rdma_queue_idx(struct nvme_rdma_queue *queue)
+{
+	return queue - queue->ctrl->queues;
+}
+
+static bool nvme_rdma_poll_queue(struct nvme_rdma_queue *queue)
+{
+	return nvme_rdma_queue_idx(queue) >
+		queue->ctrl->io_queues[HCTX_TYPE_DEFAULT] +
+		queue->ctrl->io_queues[HCTX_TYPE_READ];
+}
+
+static inline size_t nvme_rdma_inline_data_size(struct nvme_rdma_queue *queue)
+{
+	return queue->cmnd_capsule_len - sizeof(struct nvme_command);
+}
+
+static void nvme_rdma_free_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
+		size_t capsule_size, enum dma_data_direction dir)
+{
+	ib_dma_unmap_single(ibdev, qe->dma, capsule_size, dir);
+	kfree(qe->data);
+}
+
+static int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,
+		size_t capsule_size, enum dma_data_direction dir)
+{
+
+	qe->data = kzalloc(capsule_size, GFP_KERNEL);
+	if (!qe->data) {
+		pr_err("%s: qe %px failed to alloc data\n", __FUNCTION__, qe);
+		return -ENOMEM;
+	}
+
+	qe->dma = ib_dma_map_single(ibdev, qe->data, capsule_size, dir);
+	if (ib_dma_mapping_error(ibdev, qe->dma)) {
+		kfree(qe->data);
+		qe->data = NULL;
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static void nvme_rdma_free_ring(struct ib_device *ibdev,
+		struct nvme_rdma_qe *ring, size_t ib_queue_size,
+		size_t capsule_size, enum dma_data_direction dir)
+{
+	int i;
+
+	for (i = 0; i < ib_queue_size; i++)
+		nvme_rdma_free_qe(ibdev, &ring[i], capsule_size, dir);
+	kfree(ring);
+}
+
+static struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,
+		size_t ib_queue_size, size_t capsule_size,
+		enum dma_data_direction dir)
+{
+	struct nvme_rdma_qe *ring;
+	int i;
+
+	ring = kcalloc(ib_queue_size, sizeof(struct nvme_rdma_qe), GFP_KERNEL);
+	if (!ring)
+		return NULL;
+
+	for (i = 0; i < ib_queue_size; i++) {
+		if (nvme_rdma_alloc_qe(ibdev, &ring[i], capsule_size, dir))
+			goto out_free_ring;
+	}
+
+	return ring;
+
+out_free_ring:
+	nvme_rdma_free_ring(ibdev, ring, i, capsule_size, dir);
+	return NULL;
+}
+
+static void nvme_rdma_qp_event(struct ib_event *event, void *context)
+{
+	pr_debug("QP event %s (%d)\n",
+		 ib_event_msg(event->event), event->event);
+}
+
+static int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)
+{
+	int ret;
+
+	ret = wait_for_completion_interruptible_timeout(&queue->cm_done,
+			msecs_to_jiffies(NVME_RDMA_CONNECT_TIMEOUT_MS) + 1);
+	if (ret < 0)
+		return ret;
+	if (ret == 0)
+		return -ETIMEDOUT;
+	WARN_ON_ONCE(queue->cm_error > 0);
+	return queue->cm_error;
+}
+
+static int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)
+{
+	struct nvme_rdma_device *dev = queue->device;
+	struct ib_qp_init_attr init_attr;
+	int ret;
+
+	memset(&init_attr, 0, sizeof(init_attr));
+	init_attr.event_handler = nvme_rdma_qp_event;
+	/* +1 for drain */
+	init_attr.cap.max_send_wr = factor * queue->queue_size + 1;
+	/* +1 for drain */
+	init_attr.cap.max_recv_wr = queue->queue_size + 1;
+	init_attr.cap.max_recv_sge = 1;
+	init_attr.cap.max_send_sge = 1 + dev->num_inline_segments;
+	init_attr.sq_sig_type = IB_SIGNAL_REQ_WR;
+	init_attr.qp_type = IB_QPT_RC;
+	init_attr.send_cq = queue->ib_cq;
+	init_attr.recv_cq = queue->ib_cq;
+
+	ret = rdma_create_qp(queue->cm_id, dev->pd, &init_attr);
+
+	queue->qp = queue->cm_id->qp;
+	return ret;
+}
+
+static void nvme_rdma_exit_request(struct blk_mq_tag_set *set,
+		struct request *rq, unsigned int hctx_idx)
+{
+	struct nvme_rdma_ctrl *ctrl = set->driver_data;
+	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
+	int queue_idx = (set == &ctrl->tag_set) ? hctx_idx + 1 : 0;
+	struct nvme_rdma_queue *queue = &ctrl->queues[queue_idx];
+	struct nvme_rdma_device *dev = queue->device;
+
+	nvme_rdma_free_qe(dev->dev, &req->sqe, sizeof(struct nvme_command),
+			DMA_TO_DEVICE);
+}
+
+static int nvme_rdma_init_request(struct blk_mq_tag_set *set,
+		struct request *rq, unsigned int hctx_idx,
+		unsigned int numa_node)
+{
+	struct nvme_rdma_ctrl *ctrl = set->driver_data;
+	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
+	int queue_idx = (set == &ctrl->tag_set) ? hctx_idx + 1 : 0;
+	struct nvme_rdma_queue *queue = &ctrl->queues[queue_idx];
+	struct nvme_rdma_device *dev = queue->device;
+	struct ib_device *ibdev = dev->dev;
+	int ret;
+
+	nvme_req(rq)->ctrl = &ctrl->ctrl;
+
+	pr_debug("%s: rq %px\n", __FUNCTION__, req);
+	ret = nvme_rdma_alloc_qe(ibdev, &req->sqe, sizeof(struct nvme_command),
+			DMA_TO_DEVICE);
+	if (ret)
+		return ret;
+
+	req->queue = queue;
+
+	return 0;
+}
+
+static int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
+		unsigned int hctx_idx)
+{
+	struct nvme_rdma_ctrl *ctrl = data;
+	struct nvme_rdma_queue *queue = &ctrl->queues[hctx_idx + 1];
+
+	BUG_ON(hctx_idx >= ctrl->ctrl.queue_count);
+
+	hctx->driver_data = queue;
+	return 0;
+}
+
+static int nvme_rdma_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
+		unsigned int hctx_idx)
+{
+	struct nvme_rdma_ctrl *ctrl = data;
+	struct nvme_rdma_queue *queue = &ctrl->queues[0];
+
+	BUG_ON(hctx_idx != 0);
+
+	hctx->driver_data = queue;
+	return 0;
+}
+
+static void nvme_rdma_free_dev(struct kref *ref)
+{
+	struct nvme_rdma_device *ndev =
+		container_of(ref, struct nvme_rdma_device, ref);
+
+	mutex_lock(&device_list_mutex);
+	list_del(&ndev->entry);
+	mutex_unlock(&device_list_mutex);
+
+	ib_dealloc_pd(ndev->pd);
+	kfree(ndev);
+}
+
+static void nvme_rdma_dev_put(struct nvme_rdma_device *dev)
+{
+	kref_put(&dev->ref, nvme_rdma_free_dev);
+}
+
+static int nvme_rdma_dev_get(struct nvme_rdma_device *dev)
+{
+	return kref_get_unless_zero(&dev->ref);
+}
+
+static struct nvme_rdma_device *
+nvme_rdma_find_get_device(struct rdma_cm_id *cm_id)
+{
+	struct nvme_rdma_device *ndev;
+
+	mutex_lock(&device_list_mutex);
+	list_for_each_entry(ndev, &device_list, entry) {
+		if (ndev->dev->node_guid == cm_id->device->node_guid &&
+		    nvme_rdma_dev_get(ndev))
+			goto out_unlock;
+	}
+
+	ndev = kzalloc(sizeof(*ndev), GFP_KERNEL);
+	if (!ndev)
+		goto out_err;
+
+	ndev->dev = cm_id->device;
+	kref_init(&ndev->ref);
+
+	ndev->pd = ib_alloc_pd(ndev->dev,
+		register_always ? 0 : IB_PD_UNSAFE_GLOBAL_RKEY);
+	if (IS_ERR(ndev->pd))
+		goto out_free_dev;
+
+	if (!(ndev->dev->attrs.device_cap_flags &
+	      IB_DEVICE_MEM_MGT_EXTENSIONS)) {
+		dev_err(&ndev->dev->dev,
+			"Memory registrations not supported.\n");
+		goto out_free_pd;
+	}
+
+	ndev->num_inline_segments = min(NVME_RDMA_MAX_INLINE_SEGMENTS,
+					ndev->dev->attrs.max_send_sge - 1);
+	list_add(&ndev->entry, &device_list);
+out_unlock:
+	mutex_unlock(&device_list_mutex);
+	return ndev;
+
+out_free_pd:
+	ib_dealloc_pd(ndev->pd);
+out_free_dev:
+	kfree(ndev);
+out_err:
+	mutex_unlock(&device_list_mutex);
+	return NULL;
+}
+
+static void nvme_rdma_destroy_queue_ib(struct nvme_rdma_queue *queue)
+{
+	struct nvme_rdma_device *dev;
+	struct ib_device *ibdev;
+
+	if (!test_and_clear_bit(NVME_RDMA_Q_TR_READY, &queue->flags))
+		return;
+
+	dev = queue->device;
+	ibdev = dev->dev;
+
+	ib_mr_pool_destroy(queue->qp, &queue->qp->rdma_mrs);
+
+	/*
+	 * The cm_id object might have been destroyed during RDMA connection
+	 * establishment error flow to avoid getting other cma events, thus
+	 * the destruction of the QP shouldn't use rdma_cm API.
+	 */
+	ib_destroy_qp(queue->qp);
+	ib_free_cq(queue->ib_cq);
+
+	nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+			sizeof(struct nvme_completion), DMA_FROM_DEVICE);
+
+	nvme_rdma_dev_put(dev);
+}
+
+static int nvme_rdma_get_max_fr_pages(struct ib_device *ibdev)
+{
+	return min_t(u32, NVME_RDMA_MAX_SEGMENTS,
+		     ibdev->attrs.max_fast_reg_page_list_len - 1);
+}
+
+static int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)
+{
+	struct ib_device *ibdev;
+	/* key MR, MR, SEND, INV and much more to prevent WR exhaust */
+	const int send_wr_factor = 10;
+	const int cq_factor = send_wr_factor + 1;	/* + RECV */
+	int comp_vector, idx = nvme_rdma_queue_idx(queue);
+	enum ib_poll_context poll_ctx;
+	int ret, pages_per_mr;
+
+	queue->device = nvme_rdma_find_get_device(queue->cm_id);
+	if (!queue->device) {
+		dev_err(queue->cm_id->device->dev.parent,
+			"no client data found!\n");
+		return -ECONNREFUSED;
+	}
+	ibdev = queue->device->dev;
+
+        dev_dbg(queue->ctrl->ctrl.device,
+		"max_fast_reg_page_list_len %d\n",
+	        ibdev->attrs.max_fast_reg_page_list_len);
+	/*
+	 * Spread I/O queues completion vectors according their queue index.
+	 * Admin queues can always go on completion vector 0.
+	 */
+	comp_vector = idx == 0 ? idx : idx - 1;
+
+	/* Polling queues need direct cq polling context */
+	if (nvme_rdma_poll_queue(queue))
+		poll_ctx = IB_POLL_DIRECT;
+	else
+		poll_ctx = IB_POLL_SOFTIRQ;
+
+	/* +1 for ib_stop_cq */
+	queue->ib_cq = ib_alloc_cq(ibdev, queue,
+				cq_factor * queue->queue_size + 1,
+				comp_vector, poll_ctx);
+	if (IS_ERR(queue->ib_cq)) {
+		ret = PTR_ERR(queue->ib_cq);
+		goto out_put_dev;
+	}
+
+	ret = nvme_rdma_create_qp(queue, send_wr_factor);
+	if (ret)
+		goto out_destroy_ib_cq;
+
+	queue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,
+			sizeof(struct nvme_completion), DMA_FROM_DEVICE);
+	if (!queue->rsp_ring) {
+		ret = -ENOMEM;
+		goto out_destroy_qp;
+	}
+
+	/*
+	 * Currently we don't use SG_GAPS MR's so if the first entry is
+	 * misaligned we'll end up using two entries for a single data page,
+	 * so one additional entry is required.
+	 */
+	pages_per_mr = nvme_rdma_get_max_fr_pages(ibdev) + 1;
+	ret = ib_mr_pool_init(queue->qp, &queue->qp->rdma_mrs,
+			      queue->queue_size * 2,
+			      IB_MR_TYPE_MEM_REG,
+			      //nvme_rdma_get_max_fr_pages(ibdev));
+			      pages_per_mr);
+	if (ret) {
+		dev_err(queue->ctrl->ctrl.device,
+			"failed to initialize MR pool sized %d for QID %d\n",
+			queue->queue_size, idx);
+		goto out_destroy_ring;
+	}
+
+	set_bit(NVME_RDMA_Q_TR_READY, &queue->flags);
+
+	return 0;
+
+out_destroy_ring:
+	nvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,
+			    sizeof(struct nvme_completion), DMA_FROM_DEVICE);
+out_destroy_qp:
+	rdma_destroy_qp(queue->cm_id);
+out_destroy_ib_cq:
+	ib_free_cq(queue->ib_cq);
+out_put_dev:
+	nvme_rdma_dev_put(queue->device);
+	return ret;
+}
+
+static int nvme_rdma_alloc_queue(struct nvme_rdma_ctrl *ctrl,
+		int idx, size_t queue_size)
+{
+	struct nvme_rdma_queue *queue;
+	struct sockaddr *src_addr = NULL;
+	int ret;
+
+	pr_debug("%s: qid %u qdepth %zu\n", __FUNCTION__, idx, queue_size);
+
+	queue = &ctrl->queues[idx];
+	queue->ctrl = ctrl;
+	init_completion(&queue->cm_done);
+
+	if (idx > 0)
+		queue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;
+	else
+		queue->cmnd_capsule_len = sizeof(struct nvme_command);
+
+	queue->queue_size = queue_size;
+
+	queue->cm_id = rdma_create_id(&init_net, nvme_rdma_cm_handler, queue,
+			RDMA_PS_TCP, IB_QPT_RC);
+	if (IS_ERR(queue->cm_id)) {
+		dev_info(ctrl->ctrl.device,
+			"failed to create CM ID: %ld\n", PTR_ERR(queue->cm_id));
+		return PTR_ERR(queue->cm_id);
+	}
+
+	if (ctrl->ctrl.opts->mask & NVMF_OPT_HOST_TRADDR)
+		src_addr = (struct sockaddr *)&ctrl->src_addr;
+
+	queue->cm_error = -ETIMEDOUT;
+	ret = rdma_resolve_addr(queue->cm_id, src_addr,
+			(struct sockaddr *)&ctrl->addr,
+			NVME_RDMA_CONNECT_TIMEOUT_MS);
+	if (ret) {
+		dev_info(ctrl->ctrl.device,
+			"rdma_resolve_addr failed (%d).\n", ret);
+		goto out_destroy_cm_id;
+	}
+
+	ret = nvme_rdma_wait_for_cm(queue);
+	if (ret) {
+		dev_info(ctrl->ctrl.device,
+			"rdma connection establishment failed (%d)\n", ret);
+		goto out_destroy_cm_id;
+	}
+
+	set_bit(NVME_RDMA_Q_ALLOCATED, &queue->flags);
+
+	return 0;
+
+out_destroy_cm_id:
+	rdma_destroy_id(queue->cm_id);
+	nvme_rdma_destroy_queue_ib(queue);
+	return ret;
+}
+
+static void nvme_rdma_stop_queue(struct nvme_rdma_queue *queue)
+{
+	if (!test_and_clear_bit(NVME_RDMA_Q_LIVE, &queue->flags))
+		return;
+
+	rdma_disconnect(queue->cm_id);
+	ib_drain_qp(queue->qp);
+}
+
+static void nvme_rdma_free_queue(struct nvme_rdma_queue *queue)
+{
+	if (!test_and_clear_bit(NVME_RDMA_Q_ALLOCATED, &queue->flags))
+		return;
+
+	nvme_rdma_destroy_queue_ib(queue);
+	rdma_destroy_id(queue->cm_id);
+}
+
+static void nvme_rdma_free_io_queues(struct nvme_rdma_ctrl *ctrl)
+{
+	int i;
+
+	for (i = 1; i < ctrl->ctrl.queue_count; i++)
+		nvme_rdma_free_queue(&ctrl->queues[i]);
+}
+
+static void nvme_rdma_stop_io_queues(struct nvme_rdma_ctrl *ctrl)
+{
+	int i;
+
+	for (i = 1; i < ctrl->ctrl.queue_count; i++)
+		nvme_rdma_stop_queue(&ctrl->queues[i]);
+}
+
+static int nvme_rdma_start_queue(struct nvme_rdma_ctrl *ctrl, int idx)
+{
+	struct nvme_rdma_queue *queue = &ctrl->queues[idx];
+	bool poll = nvme_rdma_poll_queue(queue);
+	int ret;
+
+	if (idx)
+		ret = nvmf_connect_io_queue(&ctrl->ctrl, idx, poll);
+	else
+		ret = nvmf_connect_admin_queue(&ctrl->ctrl);
+
+	if (!ret)
+		set_bit(NVME_RDMA_Q_LIVE, &queue->flags);
+	else
+		dev_info(ctrl->ctrl.device,
+			"failed to connect queue: %d ret=%d\n", idx, ret);
+	return ret;
+}
+
+static int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl)
+{
+	int i, ret = 0;
+
+	for (i = 1; i < ctrl->ctrl.queue_count; i++) {
+		ret = nvme_rdma_start_queue(ctrl, i);
+		if (ret)
+			goto out_stop_queues;
+	}
+
+	return 0;
+
+out_stop_queues:
+	for (i--; i >= 1; i--)
+		nvme_rdma_stop_queue(&ctrl->queues[i]);
+	return ret;
+}
+
+static int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)
+{
+	struct nvmf_ctrl_options *opts = ctrl->ctrl.opts;
+	struct ib_device *ibdev = ctrl->device->dev;
+	unsigned int nr_io_queues;
+	int i, ret;
+
+	nr_io_queues = min(opts->nr_io_queues, num_online_cpus());
+
+	/*
+	 * we map queues according to the device irq vectors for
+	 * optimal locality so we don't need more queues than
+	 * completion vectors.
+	 */
+	nr_io_queues = min_t(unsigned int, nr_io_queues,
+				ibdev->num_comp_vectors);
+
+        dev_info(ctrl->ctrl.device, "%s is capable of %u of comp_vectors\n",
+                ibdev->node_desc, ibdev->num_comp_vectors);
+
+	if (opts->nr_write_queues) {
+		ctrl->io_queues[HCTX_TYPE_DEFAULT] =
+				min(opts->nr_write_queues, nr_io_queues);
+		nr_io_queues += ctrl->io_queues[HCTX_TYPE_DEFAULT];
+	} else {
+		ctrl->io_queues[HCTX_TYPE_DEFAULT] = nr_io_queues;
+	}
+
+	ctrl->io_queues[HCTX_TYPE_READ] = nr_io_queues;
+
+	if (opts->nr_poll_queues) {
+		ctrl->io_queues[HCTX_TYPE_POLL] =
+			min(opts->nr_poll_queues, num_online_cpus());
+		nr_io_queues += ctrl->io_queues[HCTX_TYPE_POLL];
+	}
+
+        pr_err("%s: Asking target for %u qpairs\n", __FUNCTION__, nr_io_queues);
+	ret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);
+	if (ret)
+		return ret;
+
+	ctrl->ctrl.queue_count = nr_io_queues + 1;
+	if (ctrl->ctrl.queue_count < 2)
+		return 0;
+
+	for (i = 1; i < ctrl->ctrl.queue_count; i++) {
+		ret = nvme_rdma_alloc_queue(ctrl, i,
+				ctrl->ctrl.sqsize + 1);
+		if (ret)
+			goto out_free_queues;
+	}
+
+	dev_err(ctrl->ctrl.device,
+		"creating %d I/O queues with qdepth %u.\n", nr_io_queues, ctrl->ctrl.sqsize + 1);
+
+	return 0;
+
+out_free_queues:
+	for (i--; i >= 1; i--)
+		nvme_rdma_free_queue(&ctrl->queues[i]);
+
+	return ret;
+}
+
+static void nvme_rdma_free_tagset(struct nvme_ctrl *nctrl,
+		struct blk_mq_tag_set *set)
+{
+	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+
+	blk_mq_free_tag_set(set);
+	nvme_rdma_dev_put(ctrl->device);
+}
+
+static struct blk_mq_tag_set *nvme_rdma_alloc_tagset(struct nvme_ctrl *nctrl,
+		bool admin)
+{
+	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+	struct blk_mq_tag_set *set;
+	int ret;
+
+	if (admin) {
+		set = &ctrl->admin_tag_set;
+		memset(set, 0, sizeof(*set));
+		set->ops = &nvme_rdma_admin_mq_ops;
+		set->queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+		set->reserved_tags = 2; /* connect + keep-alive */
+		set->numa_node = nctrl->numa_node;
+		set->cmd_size = sizeof(struct nvme_rdma_request) +
+			SG_CHUNK_SIZE * sizeof(struct scatterlist);
+		set->driver_data = ctrl;
+		set->nr_hw_queues = 1;
+		set->timeout = ADMIN_TIMEOUT;
+		set->flags = BLK_MQ_F_NO_SCHED;
+	} else {
+		set = &ctrl->tag_set;
+		memset(set, 0, sizeof(*set));
+		set->ops = &nvme_rdma_mq_ops;
+		set->queue_depth = nctrl->sqsize + 1;
+		set->reserved_tags = 1; /* fabric connect */
+		set->numa_node = nctrl->numa_node;
+		set->flags = BLK_MQ_F_SHOULD_MERGE;
+		set->cmd_size = sizeof(struct nvme_rdma_request) +
+			SG_CHUNK_SIZE * sizeof(struct scatterlist);
+		set->driver_data = ctrl;
+		set->nr_hw_queues = nctrl->queue_count - 1;
+		set->timeout = NVME_IO_TIMEOUT;
+		set->nr_maps = nctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2;
+	}
+
+	ret = blk_mq_alloc_tag_set(set);
+	if (ret)
+		goto out;
+
+	/*
+	 * We need a reference on the device as long as the tag_set is alive,
+	 * as the MRs in the request structures need a valid ib_device.
+	 */
+	ret = nvme_rdma_dev_get(ctrl->device);
+	if (!ret) {
+		ret = -EINVAL;
+		goto out_free_tagset;
+	}
+
+	return set;
+
+out_free_tagset:
+	blk_mq_free_tag_set(set);
+out:
+	return ERR_PTR(ret);
+}
+
+static void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl,
+		bool remove)
+{
+	if (remove) {
+		blk_cleanup_queue(ctrl->ctrl.admin_q);
+		nvme_rdma_free_tagset(&ctrl->ctrl, ctrl->ctrl.admin_tagset);
+	}
+	if (ctrl->async_event_sqe.data) {
+		nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+				sizeof(struct nvme_command), DMA_TO_DEVICE);
+		ctrl->async_event_sqe.data = NULL;
+	}
+	nvme_rdma_free_queue(&ctrl->queues[0]);
+}
+
+static int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,
+		bool new)
+{
+	int error;
+
+	error = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+	if (error)
+		return error;
+
+	ctrl->device = ctrl->queues[0].device;
+	ctrl->ctrl.numa_node = dev_to_node(ctrl->device->dev->dma_device);
+
+	ctrl->max_fr_pages = nvme_rdma_get_max_fr_pages(ctrl->device->dev);
+
+	error = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+			sizeof(struct nvme_command), DMA_TO_DEVICE);
+	if (error)
+		goto out_free_queue;
+
+	if (new) {
+		ctrl->ctrl.admin_tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, true);
+		if (IS_ERR(ctrl->ctrl.admin_tagset)) {
+			error = PTR_ERR(ctrl->ctrl.admin_tagset);
+			goto out_free_async_qe;
+		}
+
+		ctrl->ctrl.admin_q = blk_mq_init_queue(&ctrl->admin_tag_set);
+		if (IS_ERR(ctrl->ctrl.admin_q)) {
+			error = PTR_ERR(ctrl->ctrl.admin_q);
+			goto out_free_tagset;
+		}
+	}
+
+	error = nvme_rdma_start_queue(ctrl, 0);
+	if (error)
+		goto out_cleanup_queue;
+
+	error = ctrl->ctrl.ops->reg_read64(&ctrl->ctrl, NVME_REG_CAP,
+			&ctrl->ctrl.cap);
+	if (error) {
+		dev_err(ctrl->ctrl.device,
+			"prop_get NVME_REG_CAP failed\n");
+		goto out_stop_queue;
+	}
+
+	ctrl->ctrl.sqsize =
+		min_t(int, NVME_CAP_MQES(ctrl->ctrl.cap), ctrl->ctrl.sqsize);
+
+	error = nvme_enable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
+	if (error)
+		goto out_stop_queue;
+
+	ctrl->ctrl.max_hw_sectors =
+		(ctrl->max_fr_pages - 1) << (ilog2(SZ_4K) - 9);
+
+	ctrl->ctrl.max_segments = ctrl->max_fr_pages;
+	ctrl->ctrl.max_hw_sectors = ctrl->max_fr_pages << (ilog2(SZ_4K) - 9);
+
+	error = nvme_init_identify(&ctrl->ctrl);
+	if (error)
+		goto out_stop_queue;
+
+	return 0;
+
+out_stop_queue:
+	nvme_rdma_stop_queue(&ctrl->queues[0]);
+out_cleanup_queue:
+	if (new)
+		blk_cleanup_queue(ctrl->ctrl.admin_q);
+out_free_tagset:
+	if (new)
+		nvme_rdma_free_tagset(&ctrl->ctrl, ctrl->ctrl.admin_tagset);
+out_free_async_qe:
+	nvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,
+		sizeof(struct nvme_command), DMA_TO_DEVICE);
+	ctrl->async_event_sqe.data = NULL;
+out_free_queue:
+	nvme_rdma_free_queue(&ctrl->queues[0]);
+	return error;
+}
+
+static void nvme_rdma_destroy_io_queues(struct nvme_rdma_ctrl *ctrl,
+		bool remove)
+{
+	if (remove) {
+		blk_cleanup_queue(ctrl->ctrl.connect_q);
+		nvme_rdma_free_tagset(&ctrl->ctrl, ctrl->ctrl.tagset);
+	}
+	nvme_rdma_free_io_queues(ctrl);
+}
+
+static int nvme_rdma_configure_io_queues(struct nvme_rdma_ctrl *ctrl, bool new)
+{
+	int ret;
+        //unsigned cpu;
+
+	ret = nvme_rdma_alloc_io_queues(ctrl);
+	if (ret)
+		return ret;
+
+	if (new) {
+		ctrl->ctrl.tagset = nvme_rdma_alloc_tagset(&ctrl->ctrl, false);
+		if (IS_ERR(ctrl->ctrl.tagset)) {
+			ret = PTR_ERR(ctrl->ctrl.tagset);
+			goto out_free_io_queues;
+		}
+
+		ctrl->ctrl.connect_q = blk_mq_init_queue(&ctrl->tag_set);
+		if (IS_ERR(ctrl->ctrl.connect_q)) {
+			ret = PTR_ERR(ctrl->ctrl.connect_q);
+			goto out_free_tag_set;
+		}
+	} else {
+		blk_mq_update_nr_hw_queues(&ctrl->tag_set,
+			ctrl->ctrl.queue_count - 1);
+	}
+/*
+	for_each_possible_cpu(cpu) {
+                uint32_t qid = ctrl->tag_set.map[HCTX_TYPE_DEFAULT].mq_map[cpu];
+                dev_err(ctrl->ctrl.device, "cpu %u numa %u qid %u\n",
+                        cpu, local_memory_node(cpu_to_node(cpu)), qid);
+        }
+*/
+	ret = nvme_rdma_start_io_queues(ctrl);
+	if (ret)
+		goto out_cleanup_connect_q;
+
+	return 0;
+
+out_cleanup_connect_q:
+	if (new)
+		blk_cleanup_queue(ctrl->ctrl.connect_q);
+out_free_tag_set:
+	if (new)
+		nvme_rdma_free_tagset(&ctrl->ctrl, ctrl->ctrl.tagset);
+out_free_io_queues:
+	nvme_rdma_free_io_queues(ctrl);
+	return ret;
+}
+
+static void nvme_rdma_teardown_admin_queue(struct nvme_rdma_ctrl *ctrl,
+		bool remove)
+{
+	blk_mq_quiesce_queue(ctrl->ctrl.admin_q);
+	nvme_rdma_stop_queue(&ctrl->queues[0]);
+	blk_mq_tagset_busy_iter(&ctrl->admin_tag_set, nvme_cancel_request,
+			&ctrl->ctrl);
+	blk_mq_unquiesce_queue(ctrl->ctrl.admin_q);
+	nvme_rdma_destroy_admin_queue(ctrl, remove);
+}
+
+static void nvme_rdma_teardown_io_queues(struct nvme_rdma_ctrl *ctrl,
+		bool remove)
+{
+	if (ctrl->ctrl.queue_count > 1) {
+		nvme_stop_queues(&ctrl->ctrl);
+		nvme_rdma_stop_io_queues(ctrl);
+		blk_mq_tagset_busy_iter(&ctrl->tag_set, nvme_cancel_request,
+				&ctrl->ctrl);
+		if (remove)
+			nvme_start_queues(&ctrl->ctrl);
+		nvme_rdma_destroy_io_queues(ctrl, remove);
+	}
+}
+
+static void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)
+{
+	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);
+
+	if (list_empty(&ctrl->list))
+		goto free_ctrl;
+
+	mutex_lock(&nvme_rdma_ctrl_mutex);
+	list_del(&ctrl->list);
+	mutex_unlock(&nvme_rdma_ctrl_mutex);
+
+	nvmf_free_options(nctrl->opts);
+free_ctrl:
+	kfree(ctrl->queues);
+	kfree(ctrl);
+}
+
+static void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)
+{
+	/* If we are resetting/deleting then do nothing */
+	if (ctrl->ctrl.state != NVME_CTRL_CONNECTING) {
+		WARN_ON_ONCE(ctrl->ctrl.state == NVME_CTRL_NEW ||
+			ctrl->ctrl.state == NVME_CTRL_LIVE);
+		return;
+	}
+
+	if (nvmf_should_reconnect(&ctrl->ctrl)) {
+		dev_info(ctrl->ctrl.device, "Reconnecting in %d seconds...\n",
+			ctrl->ctrl.opts->reconnect_delay);
+		queue_delayed_work(nvme_wq, &ctrl->reconnect_work,
+				ctrl->ctrl.opts->reconnect_delay * HZ);
+	} else {
+		nvme_delete_ctrl(&ctrl->ctrl);
+	}
+}
+
+static int nvme_rdma_setup_ctrl(struct nvme_rdma_ctrl *ctrl, bool new)
+{
+	int ret = -EINVAL;
+	bool changed;
+
+	ret = nvme_rdma_configure_admin_queue(ctrl, new);
+	if (ret)
+		return ret;
+
+	if (ctrl->ctrl.icdoff) {
+		dev_err(ctrl->ctrl.device, "icdoff is not supported!\n");
+		goto destroy_admin;
+	}
+
+	if (!(ctrl->ctrl.sgls & (1 << 2))) {
+		dev_err(ctrl->ctrl.device,
+			"Mandatory keyed sgls are not supported!\n");
+		goto destroy_admin;
+	}
+
+	if (ctrl->ctrl.opts->queue_size > ctrl->ctrl.sqsize + 1) {
+		dev_warn(ctrl->ctrl.device,
+			"queue_size %zu > ctrl sqsize %u, clamping down\n",
+			ctrl->ctrl.opts->queue_size, ctrl->ctrl.sqsize + 1);
+	}
+
+	if (ctrl->ctrl.sqsize + 1 > ctrl->ctrl.maxcmd) {
+		dev_warn(ctrl->ctrl.device,
+			"sqsize %u > ctrl maxcmd %u, clamping down\n",
+			ctrl->ctrl.sqsize + 1, ctrl->ctrl.maxcmd);
+		ctrl->ctrl.sqsize = ctrl->ctrl.maxcmd - 1;
+	}
+
+	if (ctrl->ctrl.sgls & (1 << 20))
+		ctrl->use_inline_data = true;
+
+	if (ctrl->ctrl.queue_count > 1) {
+		ret = nvme_rdma_configure_io_queues(ctrl, new);
+		if (ret)
+			goto destroy_admin;
+	}
+
+	changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);
+	if (!changed) {
+		/* state change failure is ok if we're in DELETING state */
+		WARN_ON_ONCE(ctrl->ctrl.state != NVME_CTRL_DELETING);
+		ret = -EINVAL;
+		goto destroy_io;
+	}
+
+	nvme_start_ctrl(&ctrl->ctrl);
+	return 0;
+
+destroy_io:
+	if (ctrl->ctrl.queue_count > 1)
+		nvme_rdma_destroy_io_queues(ctrl, new);
+destroy_admin:
+	nvme_rdma_stop_queue(&ctrl->queues[0]);
+	nvme_rdma_destroy_admin_queue(ctrl, new);
+	return ret;
+}
+
+static void nvme_rdma_reconnect_ctrl_work(struct work_struct *work)
+{
+	struct nvme_rdma_ctrl *ctrl = container_of(to_delayed_work(work),
+			struct nvme_rdma_ctrl, reconnect_work);
+
+	++ctrl->ctrl.nr_reconnects;
+
+	if (nvme_rdma_setup_ctrl(ctrl, false))
+		goto requeue;
+
+	dev_info(ctrl->ctrl.device, "Successfully reconnected (%d attempts)\n",
+			ctrl->ctrl.nr_reconnects);
+
+	ctrl->ctrl.nr_reconnects = 0;
+
+	return;
+
+requeue:
+	dev_info(ctrl->ctrl.device, "Failed reconnect attempt %d\n",
+			ctrl->ctrl.nr_reconnects);
+	nvme_rdma_reconnect_or_remove(ctrl);
+}
+
+static void nvme_rdma_error_recovery_work(struct work_struct *work)
+{
+	struct nvme_rdma_ctrl *ctrl = container_of(work,
+			struct nvme_rdma_ctrl, err_work);
+
+	nvme_stop_keep_alive(&ctrl->ctrl);
+	nvme_rdma_teardown_io_queues(ctrl, false);
+	nvme_start_queues(&ctrl->ctrl);
+	nvme_rdma_teardown_admin_queue(ctrl, false);
+
+	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {
+		/* state change failure is ok if we're in DELETING state */
+		WARN_ON_ONCE(ctrl->ctrl.state != NVME_CTRL_DELETING);
+		return;
+	}
+
+	nvme_rdma_reconnect_or_remove(ctrl);
+}
+
+static void nvme_rdma_error_recovery(struct nvme_rdma_ctrl *ctrl)
+{
+	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RESETTING))
+		return;
+
+	queue_work(nvme_wq, &ctrl->err_work);
+}
+
+static void nvme_rdma_wr_error(struct ib_cq *cq, struct ib_wc *wc,
+		const char *op)
+{
+	struct nvme_rdma_queue *queue = cq->cq_context;
+	struct nvme_rdma_ctrl *ctrl = queue->ctrl;
+
+	if (ctrl->ctrl.state == NVME_CTRL_LIVE)
+		dev_info(ctrl->ctrl.device,
+			     "%s for CQE 0x%p failed with status %s (%d)\n",
+			     op, wc->wr_cqe,
+			     ib_wc_status_msg(wc->status), wc->status);
+	nvme_rdma_error_recovery(ctrl);
+}
+
+static void nvme_rdma_key_xfer_done(struct ib_cq *cq, struct ib_wc *wc) {
+    if (unlikely(wc->status != IB_WC_SUCCESS))
+	    nvme_rdma_wr_error(cq, wc, "Key MEMREG");
+}
+
+static void nvme_rdma_memreg_done(struct ib_cq *cq, struct ib_wc *wc)
+{
+	if (unlikely(wc->status != IB_WC_SUCCESS))
+		nvme_rdma_wr_error(cq, wc, "MEMREG");
+}
+
+static void nvme_rdma_inv_key_rkey_done(struct ib_cq *cq, struct ib_wc *wc)
+{
+	struct nvme_rdma_request *req =
+		container_of(wc->wr_cqe, struct nvme_rdma_request, key_cqe);
+	struct request *rq = blk_mq_rq_from_pdu(req);
+
+	pr_debug("%s: ref %u\n", __FUNCTION__, refcount_read(&req->ref));
+
+	if (unlikely(wc->status != IB_WC_SUCCESS)) {
+		nvme_rdma_wr_error(cq, wc, "LOCAL_INV");
+		return;
+	}
+
+	if (refcount_dec_and_test(&req->ref))
+		nvme_end_request(rq, req->status, req->result);
+}
+
+static void nvme_rdma_inv_rkey_done(struct ib_cq *cq, struct ib_wc *wc)
+{
+	struct nvme_rdma_request *req =
+		container_of(wc->wr_cqe, struct nvme_rdma_request, reg_cqe);
+	struct request *rq = blk_mq_rq_from_pdu(req);
+
+	if (unlikely(wc->status != IB_WC_SUCCESS)) {
+		nvme_rdma_wr_error(cq, wc, "LOCAL_INV");
+		return;
+	}
+
+	if (refcount_dec_and_test(&req->ref))
+		nvme_end_request(rq, req->status, req->result);
+}
+
+static int nvme_rdma_inv_key_rkey(struct nvme_rdma_queue *queue,
+		struct nvme_rdma_request *req)
+{
+	struct ib_send_wr wr = {
+		.opcode		    = IB_WR_LOCAL_INV,
+		.next		    = NULL,
+		.num_sge	    = 0,
+		.send_flags	    = IB_SEND_SIGNALED,
+		.ex.invalidate_rkey = req->key_mr->rkey,
+	};
+
+	req->key_cqe.done = nvme_rdma_inv_key_rkey_done;
+	wr.wr_cqe = &req->key_cqe;
+
+	return ib_post_send(queue->qp, &wr, NULL);
+}
+
+static int nvme_rdma_inv_rkey(struct nvme_rdma_queue *queue,
+		struct nvme_rdma_request *req)
+{
+	struct ib_send_wr wr = {
+		.opcode		    = IB_WR_LOCAL_INV,
+		.next		    = NULL,
+		.num_sge	    = 0,
+		.send_flags	    = IB_SEND_SIGNALED,
+		.ex.invalidate_rkey = req->mr->rkey,
+	};
+
+	req->reg_cqe.done = nvme_rdma_inv_rkey_done;
+	wr.wr_cqe = &req->reg_cqe;
+
+	return ib_post_send(queue->qp, &wr, NULL);
+}
+
+static void nvme_rdma_unmap_data(struct nvme_rdma_queue *queue,
+		struct request *rq)
+{
+	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
+	struct nvme_rdma_device *dev = queue->device;
+	struct ib_device *ibdev = dev->dev;
+
+	if (req->key_mr) {
+		pr_debug("%s: put key_mr cid %u\n", __FUNCTION__, rq->tag);
+		ib_mr_pool_put(queue->qp, &queue->qp->rdma_mrs, req->key_mr);
+		req->key_mr = NULL;
+	}
+
+	if (req->key_sgl) {
+		ib_dma_unmap_sg(ibdev, req->key_sgl, req->key_sgl_nents, DMA_TO_DEVICE);
+		kfree(req->key_sgl);
+		req->key_sgl = NULL;
+	}
+
+	if(kv_req(rq)->kv_rddpt == 1) {
+		return;
+	}
+
+	if (!blk_rq_nr_phys_segments(rq))
+		return;
+
+	if (req->mr) {
+		ib_mr_pool_put(queue->qp, &queue->qp->rdma_mrs, req->mr);
+		req->mr = NULL;
+	}
+
+	ib_dma_unmap_sg(ibdev, req->sg_table.sgl,
+			req->nents, rq_data_dir(rq) ==
+				    WRITE ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
+
+	nvme_cleanup_cmd(rq);
+	sg_free_table_chained(&req->sg_table, true);
+}
+
+static int nvme_rdma_set_sg_null(struct nvme_command *c)
+{
+	struct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;
+
+	sg->addr = 0;
+	put_unaligned_le24(0, sg->length);
+	put_unaligned_le32(0, sg->key);
+	sg->type = NVME_KEY_SGL_FMT_DATA_DESC << 4;
+	return 0;
+}
+
+static int nvme_rdma_map_sg_inline(struct nvme_rdma_queue *queue,
+		struct nvme_rdma_request *req, struct nvme_command *c,
+		int count)
+{
+	struct nvme_sgl_desc *sg = &c->common.dptr.sgl;
+	struct scatterlist *sgl = req->sg_table.sgl;
+	struct ib_sge *sge = &req->sge[1];
+	u32 len = 0;
+	int i;
+
+	for (i = 0; i < count; i++, sgl++, sge++) {
+		sge->addr = sg_dma_address(sgl);
+		sge->length = sg_dma_len(sgl);
+		sge->lkey = queue->device->pd->local_dma_lkey;
+		len += sge->length;
+	}
+
+	sg->addr = cpu_to_le64(queue->ctrl->ctrl.icdoff);
+	sg->length = cpu_to_le32(len);
+	sg->type = (NVME_SGL_FMT_DATA_DESC << 4) | NVME_SGL_FMT_OFFSET;
+
+	if ((c->common.opcode == nvme_cmd_kv_store) ||
+	    (c->common.opcode == nvme_cmd_kv_retrieve))
+		pr_debug("%s: cid %u val length %u\n",
+			 __FUNCTION__, c->common.command_id, len);
+
+	req->num_sge += count;
+	return 0;
+}
+
+static int nvme_rdma_map_sg_single(struct nvme_rdma_queue *queue,
+		struct nvme_rdma_request *req, struct nvme_command *c)
+{
+	struct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;
+
+	sg->addr = cpu_to_le64(sg_dma_address(req->sg_table.sgl));
+	put_unaligned_le24(sg_dma_len(req->sg_table.sgl), sg->length);
+	put_unaligned_le32(queue->device->pd->unsafe_global_rkey, sg->key);
+	sg->type = NVME_KEY_SGL_FMT_DATA_DESC << 4;
+	return 0;
+}
+
+static int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,
+		struct nvme_rdma_request *req, struct nvme_command *c,
+		int count)
+{
+	struct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;
+	int nr;
+
+	req->mr = ib_mr_pool_get(queue->qp, &queue->qp->rdma_mrs);
+	if (WARN_ON_ONCE(!req->mr))
+		return -EAGAIN;
+
+	refcount_inc(&req->ref);
+
+	/*
+	 * Align the MR to a 4K page size to match the ctrl page size and
+	 * the block virtual boundary.
+	 */
+	nr = ib_map_mr_sg(req->mr, req->sg_table.sgl, count, NULL, SZ_4K);
+	if (unlikely(nr < count)) {
+		ib_mr_pool_put(queue->qp, &queue->qp->rdma_mrs, req->mr);
+		req->mr = NULL;
+		if (nr < 0)
+			return nr;
+		return -EINVAL;
+	}
+
+	ib_update_fast_reg_key(req->mr, ib_inc_rkey(req->mr->rkey));
+
+	req->reg_cqe.done = nvme_rdma_memreg_done;
+	memset(&req->reg_wr, 0, sizeof(req->reg_wr));
+	req->reg_wr.wr.opcode = IB_WR_REG_MR;
+	req->reg_wr.wr.wr_cqe = &req->reg_cqe;
+	req->reg_wr.wr.num_sge = 0;
+	req->reg_wr.mr = req->mr;
+	req->reg_wr.key = req->mr->rkey;
+	req->reg_wr.access = IB_ACCESS_LOCAL_WRITE |
+			     IB_ACCESS_REMOTE_READ |
+			     IB_ACCESS_REMOTE_WRITE;
+
+	sg->addr = cpu_to_le64(req->mr->iova);
+	put_unaligned_le24(req->mr->length, sg->length);
+	put_unaligned_le32(req->mr->rkey, sg->key);
+	sg->type = (NVME_KEY_SGL_FMT_DATA_DESC << 4) |
+			NVME_SGL_FMT_INVALIDATE;
+
+	if ((c->common.opcode == nvme_cmd_kv_store) ||
+	    (c->common.opcode == nvme_cmd_kv_retrieve))
+		pr_debug("%s: cid %u val length %llu\n",
+			 __FUNCTION__, c->common.command_id, req->mr->length);
+
+	return 0;
+}
+
+static inline bool nvme_rdma_biovec_phys_mergeable(struct request_queue *q,
+		struct bio_vec *vec1, struct bio_vec *vec2)
+{
+	unsigned long mask = queue_segment_boundary(q);
+	phys_addr_t addr1 = page_to_phys(vec1->bv_page) + vec1->bv_offset;
+	phys_addr_t addr2 = page_to_phys(vec2->bv_page) + vec2->bv_offset;
+
+	if (addr1 + vec1->bv_len != addr2)
+		return false;
+	if ((addr1 | mask) != ((addr2 + vec2->bv_len - 1) | mask))
+		return false;
+	return true;
+}
+
+
+static inline struct scatterlist *nvme_rdma_next_sg(struct scatterlist **sg,
+		struct scatterlist *sglist)
+{
+	if (!*sg)
+		return sglist;
+
+	/*
+	 * If the driver previously mapped a shorter list, we could see a
+	 * termination bit prematurely unless it fully inits the sg table
+	 * on each mapping. We KNOW that there must be more entries here
+	 * or the driver would be buggy, so force clear the termination bit
+	 * to avoid doing a full sg_init_table() in drivers for each command.
+	 */
+	sg_unmark_end(*sg);
+	return sg_next(*sg);
+}
+
+
+static unsigned get_max_segment_size(struct request_queue *q,
+				     unsigned offset)
+{
+	unsigned long mask = queue_segment_boundary(q);
+
+	/* default segment boundary mask means no boundary limit */
+	if (mask == BLK_SEG_BOUNDARY_MASK)
+		return queue_max_segment_size(q);
+
+	return min_t(unsigned long, mask - (mask & offset) + 1,
+		     queue_max_segment_size(q));
+}
+
+
+static unsigned nvme_rdma_bvec_map_sg(struct request_queue *q,
+		struct bio_vec *bvec, struct scatterlist *sglist,
+		struct scatterlist **sg)
+{
+	unsigned nbytes = bvec->bv_len;
+	unsigned nsegs = 0, total = 0, offset = 0;
+
+	while (nbytes > 0) {
+		unsigned seg_size;
+		struct page *pg;
+		unsigned idx;
+
+		*sg = nvme_rdma_next_sg(sg, sglist);
+
+		seg_size = get_max_segment_size(q, bvec->bv_offset + total);
+		seg_size = min(nbytes, seg_size);
+
+		offset = (total + bvec->bv_offset) % PAGE_SIZE;
+		idx = (total + bvec->bv_offset) / PAGE_SIZE;
+		pg = bvec_nth_page(bvec->bv_page, idx);
+
+		sg_set_page(*sg, pg, seg_size, offset);
+
+		total += seg_size;
+		nbytes -= seg_size;
+		nsegs++;
+	}
+
+	return nsegs;
+}
+
+
+static inline void
+nvme_rdma_segment_map_sg(struct request_queue *q, struct bio_vec *bvec,
+		     struct scatterlist *sglist, struct bio_vec *bvprv,
+		     struct scatterlist **sg, int *nsegs)
+{
+
+	int nbytes = bvec->bv_len;
+
+	if (*sg) {
+		if ((*sg)->length + nbytes > queue_max_segment_size(q))
+			goto new_segment;
+		if (!nvme_rdma_biovec_phys_mergeable(q, bvprv, bvec))
+			goto new_segment;
+
+		(*sg)->length += nbytes;
+	} else {
+new_segment:
+		if (bvec->bv_offset + bvec->bv_len <= PAGE_SIZE) {
+			*sg = nvme_rdma_next_sg(sg, sglist);
+			sg_set_page(*sg, bvec->bv_page, nbytes, bvec->bv_offset);
+			(*nsegs) += 1;
+		} else
+			(*nsegs) += nvme_rdma_bvec_map_sg(q, bvec, sglist, sg);
+	}
+	*bvprv = *bvec;
+}
+
+
+/*
+ * map a key bio to scatterlist, return number of sg entries setup
+ */
+int nvme_rdma_map_key(struct request_queue *q, struct request *rq,
+		      struct scatterlist *sglist)
+{
+	struct bio *bio = kv_req(rq)->key_bio;
+	struct scatterlist *sg = NULL;
+        struct bio_vec bvec, bvprv = { NULL };
+        struct bvec_iter iter;
+        int nsegs = 0;
+
+        bio_for_each_bvec(bvec, bio, iter)
+		nvme_rdma_segment_map_sg(q, &bvec, sglist, &bvprv, &sg,
+					&nsegs);
+
+	//TODO: last sg is extended, how about the 1st sg?
+	// Probably useless, since if the key is copied, it
+	// will not straddle pages
+	if (!bio_flagged(bio, BIO_USER_MAPPED) &&
+	    (key_len(rq) & q->dma_pad_mask)) {
+		unsigned int pad_len =
+			(q->dma_pad_mask & ~key_len(rq)) + 1;
+
+		sg->length += pad_len;
+	}
+
+	pr_debug("%s: nsegs %u sglist len %u dma_pad_mask 0x%x dma alignment %x\n",
+		__FUNCTION__, nsegs, sglist->length, q->dma_pad_mask, q->dma_alignment);
+
+	if (q->dma_drain_size && q->dma_drain_needed(rq)) {
+		if (op_is_write(req_op(rq)))
+			memset(q->dma_drain_buffer, 0, q->dma_drain_size);
+
+		sg_unmark_end(sg);
+		sg = sg_next(sg);
+		sg_set_page(sg, virt_to_page(q->dma_drain_buffer),
+			    q->dma_drain_size,
+			    ((unsigned long)q->dma_drain_buffer) &
+			    (PAGE_SIZE - 1));
+		nsegs++;
+		rq->extra_len += q->dma_drain_size;
+	}
+
+	if (sg)
+		sg_mark_end(sg);
+
+	return nsegs;
+}
+
+static int nvme_rdma_map_data(struct nvme_rdma_queue *queue,
+		struct request *rq, struct nvme_command *c)
+{
+	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
+	struct nvme_rdma_device *dev = queue->device;
+	struct ib_device *ibdev = dev->dev;
+	int count, ret;
+
+	req->num_sge = 1;
+	refcount_set(&req->ref, 2); /* send and recv completions */
+
+	c->common.flags |= NVME_CMD_SGL_METABUF;
+
+	if (kv_req(rq)->key_bio) {
+		int nr, nents;
+		struct nvme_keyed_sgl_desc *sgl = (void *)&c->common.cdw12;
+		struct scatterlist *sglist = kzalloc(2 * sizeof (struct scatterlist), GFP_KERNEL);
+		if (!sglist) {
+			pr_err("kzalloc failed\n");
+			return -ENOMEM;
+		}
+		sg_init_table(sglist, 1);
+
+		nents = nvme_rdma_map_key(rq->q, rq, sglist);
+		nents = ib_dma_map_sg(ibdev, sglist, nents, DMA_TO_DEVICE);
+		if (unlikely(nents <= 0)) {
+			return -EIO;
+		}
+
+		/* Max key len is 255. It could only straddle 2 pages */
+		if (nents > 2) {
+			pr_err("%s: key sgl is %u\n", __FUNCTION__, nents);
+			return -EIO;
+		}
+		req->key_mr = ib_mr_pool_get(queue->qp, &queue->qp->rdma_mrs);
+		if (WARN_ON_ONCE(!req->key_mr)) {
+			pr_err("%s: failed to allocate\n", __FUNCTION__);
+			return -EIO;
+			//return -EAGAIN;
+		}
+
+		refcount_inc(&req->ref);
+
+		pr_debug("%s: key_mr %px\n", __FUNCTION__, req->key_mr);
+
+		nr = ib_map_mr_sg(req->key_mr, sglist, nents, NULL, SZ_4K);
+		if (unlikely(nr < nents)) {
+			ib_mr_pool_put(queue->qp, &queue->qp->rdma_mrs, req->key_mr);
+			if (nr < 0)
+				return nr;
+			return -EINVAL;
+		}
+
+		req->key_sgl = sglist;
+		req->key_sgl_nents = nents;
+
+		ib_update_fast_reg_key(req->key_mr, ib_inc_rkey(req->key_mr->rkey));
+
+		req->key_cqe.done = nvme_rdma_key_xfer_done;
+		memset(&req->key_reg_wr, 0, sizeof(req->key_reg_wr));
+		req->key_reg_wr.wr.opcode = IB_WR_REG_MR;
+		req->key_reg_wr.wr.wr_cqe = &req->key_cqe;
+		req->key_reg_wr.wr.num_sge = 0;
+		req->key_reg_wr.mr = req->key_mr;
+		req->key_reg_wr.key = req->key_mr->rkey;
+		req->key_reg_wr.access = IB_ACCESS_LOCAL_WRITE |
+				     IB_ACCESS_REMOTE_READ |
+				     IB_ACCESS_REMOTE_WRITE;
+
+		sgl->addr = cpu_to_le64(req->key_mr->iova);
+		put_unaligned_le24(req->key_mr->length, sgl->length);
+		put_unaligned_le32(req->key_mr->rkey, sgl->key);
+		sgl->type = (NVME_KEY_SGL_FMT_DATA_DESC << 4) |
+			    NVME_SGL_FMT_ADDRESS;/* NVME_SGL_FMT_INVALIDATE; */
+	}
+
+	if(kv_req(rq)->kv_rddpt == 1) {
+		return 0;
+	}
+
+	if (!blk_rq_nr_phys_segments(rq))
+		return nvme_rdma_set_sg_null(c);
+
+	req->sg_table.sgl = req->first_sgl;
+	ret = sg_alloc_table_chained(&req->sg_table,
+			blk_rq_nr_phys_segments(rq), req->sg_table.sgl);
+	if (ret)
+		return -ENOMEM;
+
+
+	req->nents = blk_rq_map_sg(rq->q, rq, req->sg_table.sgl);
+
+	count = ib_dma_map_sg(ibdev, req->sg_table.sgl, req->nents,
+		    rq_data_dir(rq) == WRITE ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
+	if (unlikely(count <= 0)) {
+		ret = -EIO;
+		goto out_free_table;
+	}
+
+	if (count <= dev->num_inline_segments) {
+		if (rq_data_dir(rq) == WRITE && nvme_rdma_queue_idx(queue) &&
+		    queue->ctrl->use_inline_data &&
+		    blk_rq_payload_bytes(rq) <=
+				nvme_rdma_inline_data_size(queue)) {
+			ret = nvme_rdma_map_sg_inline(queue, req, c, count);
+			goto out;
+		}
+
+		if (count == 1 && dev->pd->flags & IB_PD_UNSAFE_GLOBAL_RKEY) {
+			ret = nvme_rdma_map_sg_single(queue, req, c);
+			goto out;
+		}
+	}
+
+	ret = nvme_rdma_map_sg_fr(queue, req, c, count);
+out:
+	if (unlikely(ret))
+		goto out_unmap_sg;
+
+	return 0;
+
+out_unmap_sg:
+	ib_dma_unmap_sg(ibdev, req->sg_table.sgl,
+			req->nents, rq_data_dir(rq) ==
+			WRITE ? DMA_TO_DEVICE : DMA_FROM_DEVICE);
+out_free_table:
+	sg_free_table_chained(&req->sg_table, true);
+	return ret;
+}
+
+static void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)
+{
+	struct nvme_rdma_qe *qe =
+		container_of(wc->wr_cqe, struct nvme_rdma_qe, cqe);
+	struct nvme_rdma_request *req =
+		container_of(qe, struct nvme_rdma_request, sqe);
+	struct request *rq = blk_mq_rq_from_pdu(req);
+
+	if (unlikely(wc->status != IB_WC_SUCCESS)) {
+		nvme_rdma_wr_error(cq, wc, "SEND");
+		return;
+	}
+
+	if (refcount_dec_and_test(&req->ref))
+		nvme_end_request(rq, req->status, req->result);
+}
+
+static int nvme_rdma_post_send(struct nvme_rdma_queue *queue,
+		struct nvme_rdma_qe *qe, struct ib_sge *sge, u32 num_sge,
+		struct ib_send_wr *first)
+{
+	struct ib_send_wr wr, *tail_wr;
+	int ret;
+
+	sge->addr   = qe->dma;
+	sge->length = sizeof(struct nvme_command),
+	sge->lkey   = queue->device->pd->local_dma_lkey;
+
+	wr.next       = NULL;
+	wr.wr_cqe     = &qe->cqe;
+	wr.sg_list    = sge;
+	wr.num_sge    = num_sge;
+	wr.opcode     = IB_WR_SEND;
+	wr.send_flags = IB_SEND_SIGNALED;
+
+	if (first) {
+		tail_wr = first;
+		while (tail_wr->next)
+			tail_wr = tail_wr->next;
+		tail_wr->next = &wr;
+	} else {
+		first = &wr;
+	}
+
+	pr_debug("%s: first %px\n", __FUNCTION__, first);
+	if (first->next) {
+		pr_debug("%s: next %px\n", __FUNCTION__, first->next);
+		if (first->next->next)
+			pr_debug("%s: next next* %px\n", __FUNCTION__, first->next->next);
+	}
+
+	ret = ib_post_send(queue->qp, first, NULL);
+	if (unlikely(ret)) {
+		dev_err(queue->ctrl->ctrl.device,
+			     "%s failed with error code %d\n", __func__, ret);
+	}
+	return ret;
+}
+
+static int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,
+		struct nvme_rdma_qe *qe)
+{
+	struct ib_recv_wr wr;
+	struct ib_sge list;
+	int ret;
+
+	list.addr   = qe->dma;
+	list.length = sizeof(struct nvme_completion);
+	list.lkey   = queue->device->pd->local_dma_lkey;
+
+	qe->cqe.done = nvme_rdma_recv_done;
+
+	wr.next     = NULL;
+	wr.wr_cqe   = &qe->cqe;
+	wr.sg_list  = &list;
+	wr.num_sge  = 1;
+
+	ret = ib_post_recv(queue->qp, &wr, NULL);
+	if (unlikely(ret)) {
+		dev_err(queue->ctrl->ctrl.device,
+			"%s failed with error code %d\n", __func__, ret);
+	}
+	return ret;
+}
+
+static struct blk_mq_tags *nvme_rdma_tagset(struct nvme_rdma_queue *queue)
+{
+	u32 queue_idx = nvme_rdma_queue_idx(queue);
+
+	if (queue_idx == 0)
+		return queue->ctrl->admin_tag_set.tags[queue_idx];
+	return queue->ctrl->tag_set.tags[queue_idx - 1];
+}
+
+static void nvme_rdma_async_done(struct ib_cq *cq, struct ib_wc *wc)
+{
+	if (unlikely(wc->status != IB_WC_SUCCESS))
+		nvme_rdma_wr_error(cq, wc, "ASYNC");
+}
+
+static void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)
+{
+	struct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);
+	struct nvme_rdma_queue *queue = &ctrl->queues[0];
+	struct ib_device *dev = queue->device->dev;
+	struct nvme_rdma_qe *sqe = &ctrl->async_event_sqe;
+	struct nvme_command *cmd = sqe->data;
+	struct ib_sge sge;
+	int ret;
+
+	ib_dma_sync_single_for_cpu(dev, sqe->dma, sizeof(*cmd), DMA_TO_DEVICE);
+
+	memset(cmd, 0, sizeof(*cmd));
+	cmd->common.opcode = nvme_admin_async_event;
+	cmd->common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+	cmd->common.flags |= NVME_CMD_SGL_METABUF;
+	nvme_rdma_set_sg_null(cmd);
+
+	sqe->cqe.done = nvme_rdma_async_done;
+
+	ib_dma_sync_single_for_device(dev, sqe->dma, sizeof(*cmd),
+			DMA_TO_DEVICE);
+
+	ret = nvme_rdma_post_send(queue, sqe, &sge, 1, NULL);
+	WARN_ON_ONCE(ret);
+}
+
+static void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,
+		struct nvme_completion *cqe, struct ib_wc *wc)
+{
+	struct request *rq;
+	struct nvme_rdma_request *req;
+
+	pr_debug("%s: cid %u status %u result 0x%llx\n",
+		 __FUNCTION__, cqe->command_id, cqe->status, cqe->result.u64);
+
+	rq = blk_mq_tag_to_rq(nvme_rdma_tagset(queue), cqe->command_id);
+	if (!rq) {
+		dev_err(queue->ctrl->ctrl.device,
+			"tag 0x%x on QP %#x not found\n",
+			cqe->command_id, queue->qp->qp_num);
+		nvme_rdma_error_recovery(queue->ctrl);
+		return;
+	}
+	req = blk_mq_rq_to_pdu(rq);
+	req->status = cqe->status;
+	req->result = cqe->result;
+
+	pr_debug("%s: ref %u\n", __FUNCTION__, refcount_read(&req->ref));
+
+	if (wc->wc_flags & IB_WC_WITH_INVALIDATE) {
+		if (unlikely(wc->ex.invalidate_rkey != req->mr->rkey)) {
+			dev_err(queue->ctrl->ctrl.device,
+				"Bogus remote invalidation for rkey %#x\n",
+				req->mr->rkey);
+			nvme_rdma_error_recovery(queue->ctrl);
+		}
+                refcount_set(&req->ref, 1);
+	} else {
+		if (req->key_mr) {
+			int ret;
+
+			ret = nvme_rdma_inv_key_rkey(queue, req);
+			if (unlikely(ret < 0)) {
+				dev_err(queue->ctrl->ctrl.device,
+					"Queueing INV WR for key rkey %#x failed (%d)\n",
+					req->key_mr->rkey, ret);
+				nvme_rdma_error_recovery(queue->ctrl);
+			}
+		}
+
+		if (req->mr) {
+			int ret;
+
+			ret = nvme_rdma_inv_rkey(queue, req);
+			if (unlikely(ret < 0)) {
+				dev_err(queue->ctrl->ctrl.device,
+					"Queueing INV WR for rkey %#x failed (%d)\n",
+					req->mr->rkey, ret);
+				nvme_rdma_error_recovery(queue->ctrl);
+			}
+		}
+
+		/* the local invalidation completion will end the request */
+		//return;
+	}
+
+	if (refcount_dec_and_test(&req->ref))
+		nvme_end_request(rq, req->status, req->result);
+}
+
+static void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)
+{
+	struct nvme_rdma_qe *qe =
+		container_of(wc->wr_cqe, struct nvme_rdma_qe, cqe);
+	struct nvme_rdma_queue *queue = cq->cq_context;
+	struct ib_device *ibdev = queue->device->dev;
+	struct nvme_completion *cqe = qe->data;
+	const size_t len = sizeof(struct nvme_completion);
+
+	if (unlikely(wc->status != IB_WC_SUCCESS)) {
+		nvme_rdma_wr_error(cq, wc, "RECV");
+		return;
+	}
+
+	ib_dma_sync_single_for_cpu(ibdev, qe->dma, len, DMA_FROM_DEVICE);
+	/*
+	 * AEN requests are special as they don't time out and can
+	 * survive any kind of queue freeze and often don't respond to
+	 * aborts.  We don't even bother to allocate a struct request
+	 * for them but rather special case them here.
+	 */
+	if (unlikely(nvme_rdma_queue_idx(queue) == 0 &&
+			cqe->command_id >= NVME_AQ_BLK_MQ_DEPTH))
+		nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
+				&cqe->result);
+	else
+		nvme_rdma_process_nvme_rsp(queue, cqe, wc);
+	ib_dma_sync_single_for_device(ibdev, qe->dma, len, DMA_FROM_DEVICE);
+
+	nvme_rdma_post_recv(queue, qe);
+}
+
+static int nvme_rdma_conn_established(struct nvme_rdma_queue *queue)
+{
+	int ret, i;
+
+	for (i = 0; i < queue->queue_size; i++) {
+		ret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);
+		if (ret)
+			goto out_destroy_queue_ib;
+	}
+
+	return 0;
+
+out_destroy_queue_ib:
+	nvme_rdma_destroy_queue_ib(queue);
+	return ret;
+}
+
+static int nvme_rdma_conn_rejected(struct nvme_rdma_queue *queue,
+		struct rdma_cm_event *ev)
+{
+	struct rdma_cm_id *cm_id = queue->cm_id;
+	int status = ev->status;
+	const char *rej_msg;
+	const struct nvme_rdma_cm_rej *rej_data;
+	u8 rej_data_len;
+
+	rej_msg = rdma_reject_msg(cm_id, status);
+	rej_data = rdma_consumer_reject_data(cm_id, ev, &rej_data_len);
+
+	if (rej_data && rej_data_len >= sizeof(u16)) {
+		u16 sts = le16_to_cpu(rej_data->sts);
+
+		dev_err(queue->ctrl->ctrl.device,
+		      "Connect rejected: status %d (%s) nvme status %d (%s).\n",
+		      status, rej_msg, sts, nvme_rdma_cm_msg(sts));
+	} else {
+		dev_err(queue->ctrl->ctrl.device,
+			"Connect rejected: status %d (%s).\n", status, rej_msg);
+	}
+
+	return -ECONNRESET;
+}
+
+static int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)
+{
+	int ret;
+
+	ret = nvme_rdma_create_queue_ib(queue);
+	if (ret)
+		return ret;
+
+	ret = rdma_resolve_route(queue->cm_id, NVME_RDMA_CONNECT_TIMEOUT_MS);
+	if (ret) {
+		dev_err(queue->ctrl->ctrl.device,
+			"rdma_resolve_route failed (%d).\n",
+			queue->cm_error);
+		goto out_destroy_queue;
+	}
+
+	return 0;
+
+out_destroy_queue:
+	nvme_rdma_destroy_queue_ib(queue);
+	return ret;
+}
+
+static int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)
+{
+	struct nvme_rdma_ctrl *ctrl = queue->ctrl;
+	struct rdma_conn_param param = { };
+	struct nvme_rdma_cm_req priv = { };
+	int ret;
+
+	param.qp_num = queue->qp->qp_num;
+	param.flow_control = 1;
+
+	param.responder_resources = queue->device->dev->attrs.max_qp_rd_atom;
+	/* maximum retry count */
+	param.retry_count = 7;
+	param.rnr_retry_count = 7;
+	param.private_data = &priv;
+	param.private_data_len = sizeof(priv);
+
+	priv.recfmt = cpu_to_le16(NVME_RDMA_CM_FMT_1_0);
+	priv.qid = cpu_to_le16(nvme_rdma_queue_idx(queue));
+	/*
+	 * set the admin queue depth to the minimum size
+	 * specified by the Fabrics standard.
+	 */
+	if (priv.qid == 0) {
+		priv.hrqsize = cpu_to_le16(NVME_AQ_DEPTH);
+		priv.hsqsize = cpu_to_le16(NVME_AQ_DEPTH - 1);
+	} else {
+		/*
+		 * current interpretation of the fabrics spec
+		 * is at minimum you make hrqsize sqsize+1, or a
+		 * 1's based representation of sqsize.
+		 */
+		priv.hrqsize = cpu_to_le16(queue->queue_size);
+		priv.hsqsize = cpu_to_le16(queue->ctrl->ctrl.sqsize);
+	}
+
+	ret = rdma_connect(queue->cm_id, &param);
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"rdma_connect failed (%d).\n", ret);
+		goto out_destroy_queue_ib;
+	}
+
+	return 0;
+
+out_destroy_queue_ib:
+	nvme_rdma_destroy_queue_ib(queue);
+	return ret;
+}
+
+static int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,
+		struct rdma_cm_event *ev)
+{
+	struct nvme_rdma_queue *queue = cm_id->context;
+	int cm_error = 0;
+
+	dev_dbg(queue->ctrl->ctrl.device, "%s (%d): status %d id %p\n",
+		rdma_event_msg(ev->event), ev->event,
+		ev->status, cm_id);
+
+	switch (ev->event) {
+	case RDMA_CM_EVENT_ADDR_RESOLVED:
+		cm_error = nvme_rdma_addr_resolved(queue);
+		break;
+	case RDMA_CM_EVENT_ROUTE_RESOLVED:
+		cm_error = nvme_rdma_route_resolved(queue);
+		break;
+	case RDMA_CM_EVENT_ESTABLISHED:
+		queue->cm_error = nvme_rdma_conn_established(queue);
+		/* complete cm_done regardless of success/failure */
+		complete(&queue->cm_done);
+		return 0;
+	case RDMA_CM_EVENT_REJECTED:
+		nvme_rdma_destroy_queue_ib(queue);
+		cm_error = nvme_rdma_conn_rejected(queue, ev);
+		break;
+	case RDMA_CM_EVENT_ROUTE_ERROR:
+	case RDMA_CM_EVENT_CONNECT_ERROR:
+	case RDMA_CM_EVENT_UNREACHABLE:
+		nvme_rdma_destroy_queue_ib(queue);
+		/* fall through */
+	case RDMA_CM_EVENT_ADDR_ERROR:
+		dev_err(queue->ctrl->ctrl.device,
+			"CM error event %d\n", ev->event);
+		cm_error = -ECONNRESET;
+		break;
+	case RDMA_CM_EVENT_DISCONNECTED:
+	case RDMA_CM_EVENT_ADDR_CHANGE:
+	case RDMA_CM_EVENT_TIMEWAIT_EXIT:
+		dev_dbg(queue->ctrl->ctrl.device,
+			"disconnect received - connection closed\n");
+		nvme_rdma_error_recovery(queue->ctrl);
+		break;
+	case RDMA_CM_EVENT_DEVICE_REMOVAL:
+		/* device removal is handled via the ib_client API */
+		break;
+	default:
+		dev_err(queue->ctrl->ctrl.device,
+			"Unexpected RDMA CM event (%d)\n", ev->event);
+		nvme_rdma_error_recovery(queue->ctrl);
+		break;
+	}
+
+	if (cm_error) {
+		queue->cm_error = cm_error;
+		complete(&queue->cm_done);
+	}
+
+	return 0;
+}
+
+static enum blk_eh_timer_return
+nvme_rdma_timeout(struct request *rq, bool reserved)
+{
+	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
+	struct nvme_rdma_queue *queue = req->queue;
+	struct nvme_rdma_ctrl *ctrl = queue->ctrl;
+
+	dev_warn(ctrl->ctrl.device, "I/O %d QID %d timeout\n",
+		 rq->tag, nvme_rdma_queue_idx(queue));
+
+	if (ctrl->ctrl.state == NVME_CTRL_CONNECTING) {
+                dev_warn(ctrl->ctrl.device, "nvmf connect timed-out\n");
+                blk_mq_tagset_busy_iter(&ctrl->admin_tag_set, nvme_cancel_request,
+		                        &ctrl->ctrl);
+		return BLK_EH_DONE;
+	}
+
+	if (ctrl->ctrl.state != NVME_CTRL_LIVE) {
+		/*
+		 * Teardown immediately if controller times out while starting
+		 * or we are already started error recovery. all outstanding
+		 * requests are completed on shutdown, so we return BLK_EH_DONE.
+		 */
+		flush_work(&ctrl->err_work);
+		nvme_rdma_teardown_io_queues(ctrl, false);
+		nvme_rdma_teardown_admin_queue(ctrl, false);
+		return BLK_EH_DONE;
+	}
+
+	dev_warn(ctrl->ctrl.device, "starting error recovery\n");
+	nvme_rdma_error_recovery(ctrl);
+
+	return BLK_EH_RESET_TIMER;
+}
+
+static blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,
+		const struct blk_mq_queue_data *bd)
+{
+	struct nvme_ns *ns = hctx->queue->queuedata;
+	struct nvme_rdma_queue *queue = hctx->driver_data;
+	struct request *rq = bd->rq;
+	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
+	struct nvme_rdma_qe *sqe = &req->sqe;
+	struct nvme_command *c = sqe->data;
+	struct ib_device *dev;
+	bool queue_ready = test_bit(NVME_RDMA_Q_LIVE, &queue->flags);
+	blk_status_t ret;
+	int err;
+	struct ib_send_wr *head_wr = NULL;
+
+	WARN_ON_ONCE(rq->tag < 0);
+
+	if (!nvmf_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
+		return nvmf_fail_nonready_command(&queue->ctrl->ctrl, rq);
+
+	dev = queue->device->dev;
+	ib_dma_sync_single_for_cpu(dev, sqe->dma,
+			sizeof(struct nvme_command), DMA_TO_DEVICE);
+
+	ret = nvme_setup_cmd(ns, rq, c);
+	if (ret)
+		return ret;
+
+	if ((c->common.opcode == nvme_cmd_kv_store) ||
+	    (c->common.opcode == nvme_cmd_kv_retrieve))
+		pr_debug("%s: nvme_rdma_request 0x%px cid %u op 0x%x\n",
+			 __FUNCTION__, req, rq->tag, c->common.opcode);
+
+	blk_mq_start_request(rq);
+
+	err = nvme_rdma_map_data(queue, rq, c);
+	if (unlikely(err < 0)) {
+		dev_err(queue->ctrl->ctrl.device,
+			     "Failed to map data (%d)\n", err);
+		nvme_cleanup_cmd(rq);
+		goto err;
+	}
+
+	sqe->cqe.done = nvme_rdma_send_done;
+
+	if (req->mr) {
+		pr_debug("%s value reg_wr %px\n", __FUNCTION__, &req->reg_wr);
+		req->reg_wr.wr.next = NULL;
+		head_wr = &req->reg_wr.wr;
+	}
+	if (req->key_mr) {
+		pr_debug("%s key key_reg_wr %px\n", __FUNCTION__, &req->key_reg_wr);
+		req->key_reg_wr.wr.next = head_wr;
+		head_wr = &req->key_reg_wr.wr;
+	}
+
+	ib_dma_sync_single_for_device(dev, sqe->dma,
+			sizeof(struct nvme_command), DMA_TO_DEVICE);
+
+	err = nvme_rdma_post_send(queue, sqe, req->sge, req->num_sge, head_wr);
+	if (unlikely(err)) {
+		nvme_rdma_unmap_data(queue, rq);
+		goto err;
+	}
+
+	return BLK_STS_OK;
+err:
+	if (err == -ENOMEM || err == -EAGAIN)
+		return BLK_STS_RESOURCE;
+	return BLK_STS_IOERR;
+}
+
+static int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx)
+{
+	struct nvme_rdma_queue *queue = hctx->driver_data;
+
+	return ib_process_cq_direct(queue->ib_cq, -1);
+}
+
+static void nvme_rdma_complete_rq(struct request *rq)
+{
+	struct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);
+
+	nvme_rdma_unmap_data(req->queue, rq);
+	nvme_complete_rq(rq);
+}
+
+static int nvme_rdma_map_queues(struct blk_mq_tag_set *set)
+{
+	struct nvme_rdma_ctrl *ctrl = set->driver_data;
+
+	set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	set->map[HCTX_TYPE_DEFAULT].nr_queues =
+			ctrl->io_queues[HCTX_TYPE_DEFAULT];
+	set->map[HCTX_TYPE_READ].nr_queues = ctrl->io_queues[HCTX_TYPE_READ];
+	if (ctrl->ctrl.opts->nr_write_queues) {
+		/* separate read/write queues */
+		set->map[HCTX_TYPE_READ].queue_offset =
+				ctrl->io_queues[HCTX_TYPE_DEFAULT];
+	} else {
+		/* mixed read/write queues */
+		set->map[HCTX_TYPE_READ].queue_offset = 0;
+	}
+	blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_DEFAULT],
+			ctrl->device->dev, 0);
+	blk_mq_rdma_map_queues(&set->map[HCTX_TYPE_READ],
+			ctrl->device->dev, 0);
+
+	if (ctrl->ctrl.opts->nr_poll_queues) {
+		set->map[HCTX_TYPE_POLL].nr_queues =
+				ctrl->io_queues[HCTX_TYPE_POLL];
+		set->map[HCTX_TYPE_POLL].queue_offset =
+				ctrl->io_queues[HCTX_TYPE_DEFAULT];
+		if (ctrl->ctrl.opts->nr_write_queues)
+			set->map[HCTX_TYPE_POLL].queue_offset +=
+				ctrl->io_queues[HCTX_TYPE_READ];
+		blk_mq_map_queues(&set->map[HCTX_TYPE_POLL]);
+	}
+	return 0;
+}
+
+static const struct blk_mq_ops nvme_rdma_mq_ops = {
+	.queue_rq	= nvme_rdma_queue_rq,
+	.complete	= nvme_rdma_complete_rq,
+	.init_request	= nvme_rdma_init_request,
+	.exit_request	= nvme_rdma_exit_request,
+	.init_hctx	= nvme_rdma_init_hctx,
+	.timeout	= nvme_rdma_timeout,
+	.map_queues	= nvme_rdma_map_queues,
+	.poll		= nvme_rdma_poll,
+};
+
+static const struct blk_mq_ops nvme_rdma_admin_mq_ops = {
+	.queue_rq	= nvme_rdma_queue_rq,
+	.complete	= nvme_rdma_complete_rq,
+	.init_request	= nvme_rdma_init_request,
+	.exit_request	= nvme_rdma_exit_request,
+	.init_hctx	= nvme_rdma_init_admin_hctx,
+	.timeout	= nvme_rdma_timeout,
+};
+
+static void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)
+{
+	cancel_work_sync(&ctrl->err_work);
+	cancel_delayed_work_sync(&ctrl->reconnect_work);
+
+	nvme_rdma_teardown_io_queues(ctrl, shutdown);
+	if (shutdown)
+		nvme_shutdown_ctrl(&ctrl->ctrl);
+	else
+		nvme_disable_ctrl(&ctrl->ctrl, ctrl->ctrl.cap);
+	nvme_rdma_teardown_admin_queue(ctrl, shutdown);
+}
+
+static void nvme_rdma_delete_ctrl(struct nvme_ctrl *ctrl)
+{
+	nvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);
+}
+
+static void nvme_rdma_reset_ctrl_work(struct work_struct *work)
+{
+	struct nvme_rdma_ctrl *ctrl =
+		container_of(work, struct nvme_rdma_ctrl, ctrl.reset_work);
+
+	nvme_stop_ctrl(&ctrl->ctrl);
+	nvme_rdma_shutdown_ctrl(ctrl, false);
+
+	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {
+		/* state change failure should never happen */
+		WARN_ON_ONCE(1);
+		return;
+	}
+
+	if (nvme_rdma_setup_ctrl(ctrl, false))
+		goto out_fail;
+
+	return;
+
+out_fail:
+	++ctrl->ctrl.nr_reconnects;
+	nvme_rdma_reconnect_or_remove(ctrl);
+}
+
+static const struct nvme_ctrl_ops nvme_rdma_ctrl_ops = {
+	.name			= "rdma",
+	.module			= THIS_MODULE,
+	.flags			= NVME_F_FABRICS,
+	.reg_read32		= nvmf_reg_read32,
+	.reg_read64		= nvmf_reg_read64,
+	.reg_write32		= nvmf_reg_write32,
+	.free_ctrl		= nvme_rdma_free_ctrl,
+	.submit_async_event	= nvme_rdma_submit_async_event,
+	.delete_ctrl		= nvme_rdma_delete_ctrl,
+	.get_address		= nvmf_get_address,
+};
+
+/*
+ * Fails a connection request if it matches an existing controller
+ * (association) with the same tuple:
+ * <Host NQN, Host ID, local address, remote address, remote port, SUBSYS NQN>
+ *
+ * if local address is not specified in the request, it will match an
+ * existing controller with all the other parameters the same and no
+ * local port address specified as well.
+ *
+ * The ports don't need to be compared as they are intrinsically
+ * already matched by the port pointers supplied.
+ */
+static bool
+nvme_rdma_existing_controller(struct nvmf_ctrl_options *opts)
+{
+	struct nvme_rdma_ctrl *ctrl;
+	bool found = false;
+
+	mutex_lock(&nvme_rdma_ctrl_mutex);
+	list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+		found = nvmf_ip_options_match(&ctrl->ctrl, opts);
+		if (found)
+			break;
+	}
+	mutex_unlock(&nvme_rdma_ctrl_mutex);
+
+	return found;
+}
+
+static struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,
+		struct nvmf_ctrl_options *opts)
+{
+	struct nvme_rdma_ctrl *ctrl;
+	int ret;
+	bool changed;
+
+	ctrl = kzalloc(sizeof(*ctrl), GFP_KERNEL);
+	if (!ctrl)
+		return ERR_PTR(-ENOMEM);
+	ctrl->ctrl.opts = opts;
+	INIT_LIST_HEAD(&ctrl->list);
+
+	if (!(opts->mask & NVMF_OPT_TRSVCID)) {
+		opts->trsvcid =
+			kstrdup(__stringify(NVME_RDMA_IP_PORT), GFP_KERNEL);
+		if (!opts->trsvcid) {
+			ret = -ENOMEM;
+			goto out_free_ctrl;
+		}
+		opts->mask |= NVMF_OPT_TRSVCID;
+	}
+
+	ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
+			opts->traddr, opts->trsvcid, &ctrl->addr);
+	if (ret) {
+		pr_err("malformed address passed: %s:%s\n",
+			opts->traddr, opts->trsvcid);
+		goto out_free_ctrl;
+	}
+
+	if (opts->mask & NVMF_OPT_HOST_TRADDR) {
+		ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
+			opts->host_traddr, NULL, &ctrl->src_addr);
+		if (ret) {
+			pr_err("malformed src address passed: %s\n",
+			       opts->host_traddr);
+			goto out_free_ctrl;
+		}
+	}
+
+	if (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {
+		ret = -EALREADY;
+		goto out_free_ctrl;
+	}
+
+	INIT_DELAYED_WORK(&ctrl->reconnect_work,
+			nvme_rdma_reconnect_ctrl_work);
+	INIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);
+	INIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);
+
+	ctrl->ctrl.queue_count = opts->nr_io_queues + opts->nr_write_queues +
+				opts->nr_poll_queues + 1;
+	ctrl->ctrl.sqsize = opts->queue_size - 1;
+	ctrl->ctrl.kato = opts->kato;
+
+	ret = -ENOMEM;
+	ctrl->queues = kcalloc(ctrl->ctrl.queue_count, sizeof(*ctrl->queues),
+				GFP_KERNEL);
+	if (!ctrl->queues)
+		goto out_free_ctrl;
+
+	ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,
+				0 /* no quirks, we're perfect! */);
+	if (ret)
+		goto out_kfree_queues;
+
+	changed = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING);
+	WARN_ON_ONCE(!changed);
+
+	ret = nvme_rdma_setup_ctrl(ctrl, true);
+	if (ret)
+		goto out_uninit_ctrl;
+
+	dev_info(ctrl->ctrl.device, "new ctrl: NQN \"%s\", addr %pISpcs "
+                 "ib_dev %s guid 0x%lx\n",
+		 ctrl->ctrl.opts->subsysnqn, &ctrl->addr,
+                 ctrl->device->dev->node_desc,
+                 (unsigned long) ctrl->device->dev->node_guid);
+
+	nvme_get_ctrl(&ctrl->ctrl);
+
+	mutex_lock(&nvme_rdma_ctrl_mutex);
+	list_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);
+	mutex_unlock(&nvme_rdma_ctrl_mutex);
+
+	return &ctrl->ctrl;
+
+out_uninit_ctrl:
+	nvme_uninit_ctrl(&ctrl->ctrl);
+	nvme_put_ctrl(&ctrl->ctrl);
+	if (ret > 0)
+		ret = -EIO;
+	return ERR_PTR(ret);
+out_kfree_queues:
+	kfree(ctrl->queues);
+out_free_ctrl:
+	kfree(ctrl);
+	return ERR_PTR(ret);
+}
+
+static struct nvmf_transport_ops nvme_rdma_transport = {
+	.name		= "rdma",
+	.module		= THIS_MODULE,
+	.required_opts	= NVMF_OPT_TRADDR,
+	.allowed_opts	= NVMF_OPT_TRSVCID | NVMF_OPT_RECONNECT_DELAY |
+			  NVMF_OPT_HOST_TRADDR | NVMF_OPT_CTRL_LOSS_TMO |
+			  NVMF_OPT_NR_WRITE_QUEUES | NVMF_OPT_NR_POLL_QUEUES,
+	.create_ctrl	= nvme_rdma_create_ctrl,
+};
+
+static void nvme_rdma_remove_one(struct ib_device *ib_device, void *client_data)
+{
+	struct nvme_rdma_ctrl *ctrl;
+	struct nvme_rdma_device *ndev;
+	bool found = false;
+
+	mutex_lock(&device_list_mutex);
+	list_for_each_entry(ndev, &device_list, entry) {
+		if (ndev->dev == ib_device) {
+			found = true;
+			break;
+		}
+	}
+	mutex_unlock(&device_list_mutex);
+
+	if (!found)
+		return;
+
+	/* Delete all controllers using this device */
+	mutex_lock(&nvme_rdma_ctrl_mutex);
+	list_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {
+		if (ctrl->device->dev != ib_device)
+			continue;
+		nvme_delete_ctrl(&ctrl->ctrl);
+	}
+	mutex_unlock(&nvme_rdma_ctrl_mutex);
+
+	flush_workqueue(nvme_delete_wq);
+}
+
+static struct ib_client nvme_rdma_ib_client = {
+	.name   = "nvme_rdma",
+	.remove = nvme_rdma_remove_one
+};
+
+static int __init nvme_rdma_init_module(void)
+{
+	int ret;
+
+	ret = ib_register_client(&nvme_rdma_ib_client);
+	if (ret)
+		return ret;
+
+	ret = nvmf_register_transport(&nvme_rdma_transport);
+	if (ret)
+		goto err_unreg_client;
+
+	return 0;
+
+err_unreg_client:
+	ib_unregister_client(&nvme_rdma_ib_client);
+	return ret;
+}
+
+static void __exit nvme_rdma_cleanup_module(void)
+{
+	nvmf_unregister_transport(&nvme_rdma_transport);
+	ib_unregister_client(&nvme_rdma_ib_client);
+}
+
+module_init(nvme_rdma_init_module);
+module_exit(nvme_rdma_cleanup_module);
+
+MODULE_LICENSE("GPL v2");
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/re_insmod.sh b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/re_insmod.sh
new file mode 100755
index 0000000..11e507e
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/re_insmod.sh
@@ -0,0 +1,17 @@
+#!/bin/bash
+
+echo "Disconnect all remote device ..."
+nvme disconnect-all
+
+echo "Remove driver modules ..."
+sudo rmmod nvme-tcp.ko
+sudo rmmod nvme-fabrics.ko
+sudo rmmod nvme
+sudo rmmod nvme_core
+
+echo "Insert driver module ..."
+sudo insmod nvme-core.ko
+sudo insmod nvme.ko
+sudo insmod nvme-fabrics.ko
+sudo insmod nvme-tcp.ko
+
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/tcp.c b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/tcp.c
new file mode 100644
index 0000000..69667f7
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/tcp.c
@@ -0,0 +1,2522 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * NVMe over Fabrics TCP host.
+ * Copyright (c) 2018 Lightbits Labs. All rights reserved.
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/nvme-tcp.h>
+#include <net/sock.h>
+#include <net/tcp.h>
+#include <linux/blk-mq.h>
+#include <crypto/hash.h>
+
+#include "nvme.h"
+#include "fabrics.h"
+
+struct nvme_tcp_queue;
+
+enum nvme_tcp_send_state {
+	NVME_TCP_SEND_CMD_PDU = 0,
+	NVME_TCP_SEND_H2C_PDU,
+	NVME_TCP_SEND_KEY,
+	NVME_TCP_SEND_DATA,
+	NVME_TCP_SEND_DDGST,
+};
+
+struct nvme_tcp_request {
+	//struct nvme_request	req;
+	struct nvme_io_param	param;
+	void			*pdu;
+	struct nvme_tcp_queue	*queue;
+	u32			data_len;
+	u32			pdu_len;
+	u32			pdu_sent;
+	u16			ttag;
+	struct list_head	entry;
+	__le32			ddgst;
+
+	struct bio		*curr_bio;
+	struct iov_iter		iter;
+
+	/* send state */
+	size_t			offset;
+	size_t			data_sent;
+	enum nvme_tcp_send_state state;
+};
+
+enum nvme_tcp_queue_flags {
+	NVME_TCP_Q_ALLOCATED	= 0,
+	NVME_TCP_Q_LIVE		= 1,
+};
+
+enum nvme_tcp_recv_state {
+	NVME_TCP_RECV_PDU = 0,
+	NVME_TCP_RECV_DATA,
+	NVME_TCP_RECV_DDGST,
+};
+
+struct nvme_tcp_ctrl;
+struct nvme_tcp_queue {
+	struct socket		*sock;
+	struct work_struct	io_work;
+	int			io_cpu;
+
+	spinlock_t		lock;
+	struct list_head	send_list;
+
+	/* recv state */
+	void			*pdu;
+	int			pdu_remaining;
+	int			pdu_offset;
+	size_t			data_remaining;
+	size_t			ddgst_remaining;
+
+	/* send state */
+	struct nvme_tcp_request *request;
+
+	int			queue_size;
+	size_t			cmnd_capsule_len;
+	struct nvme_tcp_ctrl	*ctrl;
+	unsigned long		flags;
+	bool			rd_enabled;
+
+	bool			hdr_digest;
+	bool			data_digest;
+	struct ahash_request	*rcv_hash;
+	struct ahash_request	*snd_hash;
+	__le32			exp_ddgst;
+	__le32			recv_ddgst;
+
+	struct page_frag_cache	pf_cache;
+
+	void (*state_change)(struct sock *);
+	void (*data_ready)(struct sock *);
+	void (*write_space)(struct sock *);
+};
+
+struct nvme_tcp_ctrl {
+	/* read only in the hot path */
+	struct nvme_tcp_queue	*queues;
+	struct blk_mq_tag_set	tag_set;
+
+	/* other member variables */
+	struct list_head	list;
+	struct blk_mq_tag_set	admin_tag_set;
+	struct sockaddr_storage addr;
+	struct sockaddr_storage src_addr;
+	struct nvme_ctrl	ctrl;
+
+	struct work_struct	err_work;
+	struct delayed_work	connect_work;
+	struct nvme_tcp_request async_req;
+};
+
+static LIST_HEAD(nvme_tcp_ctrl_list);
+static DEFINE_MUTEX(nvme_tcp_ctrl_mutex);
+static struct workqueue_struct *nvme_tcp_wq;
+static struct blk_mq_ops nvme_tcp_mq_ops;
+static struct blk_mq_ops nvme_tcp_admin_mq_ops;
+
+static inline struct nvme_tcp_ctrl *to_tcp_ctrl(struct nvme_ctrl *ctrl)
+{
+	return container_of(ctrl, struct nvme_tcp_ctrl, ctrl);
+}
+
+static inline int nvme_tcp_queue_id(struct nvme_tcp_queue *queue)
+{
+	return queue - queue->ctrl->queues;
+}
+
+static inline struct blk_mq_tags *nvme_tcp_tagset(struct nvme_tcp_queue *queue)
+{
+	u32 queue_idx = nvme_tcp_queue_id(queue);
+
+	if (queue_idx == 0)
+		return queue->ctrl->admin_tag_set.tags[queue_idx];
+	return queue->ctrl->tag_set.tags[queue_idx - 1];
+}
+
+static inline u8 nvme_tcp_hdgst_len(struct nvme_tcp_queue *queue)
+{
+	return queue->hdr_digest ? NVME_TCP_DIGEST_LENGTH : 0;
+}
+
+static inline u8 nvme_tcp_ddgst_len(struct nvme_tcp_queue *queue)
+{
+	return queue->data_digest ? NVME_TCP_DIGEST_LENGTH : 0;
+}
+
+static inline size_t nvme_tcp_inline_data_size(struct nvme_tcp_queue *queue)
+{
+	return queue->cmnd_capsule_len - sizeof(struct nvme_command);
+}
+
+static inline bool nvme_tcp_async_req(struct nvme_tcp_request *req)
+{
+	return req == &req->queue->ctrl->async_req;
+}
+
+static inline bool nvme_tcp_has_inline_data(struct nvme_tcp_request *req)
+{
+	struct request *rq;
+	unsigned int bytes;
+
+	if (unlikely(nvme_tcp_async_req(req)))
+		return false; /* async events don't have a request */
+
+	rq = blk_mq_rq_from_pdu(req);
+	bytes = blk_rq_payload_bytes(rq);
+
+	return rq_data_dir(rq) == WRITE && bytes &&
+		bytes <= nvme_tcp_inline_data_size(req->queue);
+}
+
+static inline bool nvme_tcp_has_large_key(struct nvme_tcp_request *req)
+{
+	struct request *rq;
+
+	if (unlikely(nvme_tcp_async_req(req)))
+		return false; /* async events don't have a request */
+
+	rq = blk_mq_rq_from_pdu(req);
+
+	return kv_req(rq)->key_bio;
+}
+
+
+static inline struct page *nvme_tcp_req_cur_page(struct nvme_tcp_request *req)
+{
+	return req->iter.bvec->bv_page;
+}
+
+static inline size_t nvme_tcp_req_cur_offset(struct nvme_tcp_request *req)
+{
+	return req->iter.bvec->bv_offset + req->iter.iov_offset;
+}
+
+static inline size_t nvme_tcp_req_cur_length(struct nvme_tcp_request *req)
+{
+	return min_t(size_t, req->iter.bvec->bv_len - req->iter.iov_offset,
+			req->pdu_len - req->pdu_sent);
+}
+
+static inline size_t nvme_tcp_req_offset(struct nvme_tcp_request *req)
+{
+	return req->iter.iov_offset;
+}
+
+static inline size_t nvme_tcp_pdu_data_left(struct nvme_tcp_request *req)
+{
+	return rq_data_dir(blk_mq_rq_from_pdu(req)) == WRITE ?
+			req->pdu_len - req->pdu_sent : 0;
+}
+
+static inline size_t nvme_tcp_pdu_last_send(struct nvme_tcp_request *req,
+		int len)
+{
+	return nvme_tcp_pdu_data_left(req) <= len;
+}
+
+static void nvme_tcp_init_iter(struct nvme_tcp_request *req,
+		unsigned int dir)
+{
+	struct request *rq = blk_mq_rq_from_pdu(req);
+	struct bio_vec *vec;
+	unsigned int size;
+	int nsegs;
+	size_t offset;
+
+	if (rq->rq_flags & RQF_SPECIAL_PAYLOAD) {
+		vec = &rq->special_vec;
+		nsegs = 1;
+		size = blk_rq_payload_bytes(rq);
+		offset = 0;
+	} else {
+		struct bio *bio = req->curr_bio;
+
+		vec = __bvec_iter_bvec(bio->bi_io_vec, bio->bi_iter);
+		nsegs = bio_segments(bio);
+		size = bio->bi_iter.bi_size;
+		offset = bio->bi_iter.bi_bvec_done;
+	}
+
+	iov_iter_bvec(&req->iter, dir, vec, nsegs, size);
+	req->iter.iov_offset = offset;
+}
+
+static inline void nvme_tcp_advance_req(struct nvme_tcp_request *req,
+		int len)
+{
+	req->data_sent += len;
+	req->pdu_sent += len;
+	iov_iter_advance(&req->iter, len);
+	if (!iov_iter_count(&req->iter) &&
+	    req->data_sent < req->data_len) {
+		req->curr_bio = req->curr_bio->bi_next;
+		nvme_tcp_init_iter(req, WRITE);
+	}
+}
+
+static inline void nvme_tcp_queue_request(struct nvme_tcp_request *req)
+{
+	struct nvme_tcp_queue *queue = req->queue;
+
+	spin_lock(&queue->lock);
+	list_add_tail(&req->entry, &queue->send_list);
+	spin_unlock(&queue->lock);
+
+	queue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);
+}
+
+static inline struct nvme_tcp_request *
+nvme_tcp_fetch_request(struct nvme_tcp_queue *queue)
+{
+	struct nvme_tcp_request *req;
+
+	spin_lock(&queue->lock);
+	req = list_first_entry_or_null(&queue->send_list,
+			struct nvme_tcp_request, entry);
+	if (req)
+		list_del(&req->entry);
+	spin_unlock(&queue->lock);
+
+	return req;
+}
+
+static inline void nvme_tcp_ddgst_final(struct ahash_request *hash,
+		__le32 *dgst)
+{
+	ahash_request_set_crypt(hash, NULL, (u8 *)dgst, 0);
+	crypto_ahash_final(hash);
+}
+
+static inline void nvme_tcp_ddgst_update(struct ahash_request *hash,
+		struct page *page, off_t off, size_t len)
+{
+	struct scatterlist sg;
+
+	sg_init_marker(&sg, 1);
+	sg_set_page(&sg, page, len, off);
+	ahash_request_set_crypt(hash, &sg, NULL, len);
+	crypto_ahash_update(hash);
+}
+
+static inline void nvme_tcp_hdgst(struct ahash_request *hash,
+		void *pdu, size_t len)
+{
+	struct scatterlist sg;
+
+	sg_init_one(&sg, pdu, len);
+	ahash_request_set_crypt(hash, &sg, pdu + len, len);
+	crypto_ahash_digest(hash);
+}
+
+static int nvme_tcp_verify_hdgst(struct nvme_tcp_queue *queue,
+		void *pdu, size_t pdu_len)
+{
+	struct nvme_tcp_hdr *hdr = pdu;
+	__le32 recv_digest;
+	__le32 exp_digest;
+
+	if (unlikely(!(hdr->flags & NVME_TCP_F_HDGST))) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d: header digest flag is cleared\n",
+			nvme_tcp_queue_id(queue));
+		return -EPROTO;
+	}
+
+	recv_digest = *(__le32 *)(pdu + hdr->hlen);
+	nvme_tcp_hdgst(queue->rcv_hash, pdu, pdu_len);
+	exp_digest = *(__le32 *)(pdu + hdr->hlen);
+	if (recv_digest != exp_digest) {
+		dev_err(queue->ctrl->ctrl.device,
+			"header digest error: recv %#x expected %#x\n",
+			le32_to_cpu(recv_digest), le32_to_cpu(exp_digest));
+		return -EIO;
+	}
+
+	return 0;
+}
+
+static int nvme_tcp_check_ddgst(struct nvme_tcp_queue *queue, void *pdu)
+{
+	struct nvme_tcp_hdr *hdr = pdu;
+	u8 digest_len = nvme_tcp_hdgst_len(queue);
+	u32 len;
+
+	len = le32_to_cpu(hdr->plen) - hdr->hlen -
+		((hdr->flags & NVME_TCP_F_HDGST) ? digest_len : 0);
+
+	if (unlikely(len && !(hdr->flags & NVME_TCP_F_DDGST))) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d: data digest flag is cleared\n",
+		nvme_tcp_queue_id(queue));
+		return -EPROTO;
+	}
+	crypto_ahash_init(queue->rcv_hash);
+
+	return 0;
+}
+
+static void nvme_tcp_exit_request(struct blk_mq_tag_set *set,
+		struct request *rq, unsigned int hctx_idx)
+{
+	struct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);
+
+	page_frag_free(req->pdu);
+}
+
+static int nvme_tcp_init_request(struct blk_mq_tag_set *set,
+		struct request *rq, unsigned int hctx_idx,
+		unsigned int numa_node)
+{
+	struct nvme_tcp_ctrl *ctrl = set->driver_data;
+	struct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);
+	int queue_idx = (set == &ctrl->tag_set) ? hctx_idx + 1 : 0;
+	struct nvme_tcp_queue *queue = &ctrl->queues[queue_idx];
+	u8 hdgst = nvme_tcp_hdgst_len(queue);
+
+	req->pdu = page_frag_alloc(&queue->pf_cache,
+		sizeof(struct nvme_tcp_cmd_pdu) + hdgst,
+		GFP_KERNEL | __GFP_ZERO);
+	if (!req->pdu)
+		return -ENOMEM;
+
+	req->queue = queue;
+	nvme_req(rq)->ctrl = &ctrl->ctrl;
+
+	return 0;
+}
+
+static int nvme_tcp_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,
+		unsigned int hctx_idx)
+{
+	struct nvme_tcp_ctrl *ctrl = data;
+	struct nvme_tcp_queue *queue = &ctrl->queues[hctx_idx + 1];
+
+	hctx->driver_data = queue;
+	return 0;
+}
+
+static int nvme_tcp_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,
+		unsigned int hctx_idx)
+{
+	struct nvme_tcp_ctrl *ctrl = data;
+	struct nvme_tcp_queue *queue = &ctrl->queues[0];
+
+	hctx->driver_data = queue;
+	return 0;
+}
+
+static enum nvme_tcp_recv_state
+nvme_tcp_recv_state(struct nvme_tcp_queue *queue)
+{
+	return  (queue->pdu_remaining) ? NVME_TCP_RECV_PDU :
+		(queue->ddgst_remaining) ? NVME_TCP_RECV_DDGST :
+		NVME_TCP_RECV_DATA;
+}
+
+static void nvme_tcp_init_recv_ctx(struct nvme_tcp_queue *queue)
+{
+	queue->pdu_remaining = sizeof(struct nvme_tcp_rsp_pdu) +
+				nvme_tcp_hdgst_len(queue);
+	queue->pdu_offset = 0;
+	queue->data_remaining = -1;
+	queue->ddgst_remaining = 0;
+}
+
+static void nvme_tcp_error_recovery(struct nvme_ctrl *ctrl)
+{
+	pr_err("%s: ctrl %px\n", __FUNCTION__, ctrl);
+
+	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING))
+		return;
+
+	queue_work(nvme_wq, &to_tcp_ctrl(ctrl)->err_work);
+}
+
+static int nvme_tcp_process_nvme_cqe(struct nvme_tcp_queue *queue,
+		struct nvme_completion *cqe)
+{
+	struct request *rq;
+
+	rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), cqe->command_id);
+	if (!rq) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d tag 0x%x not found\n",
+			nvme_tcp_queue_id(queue), cqe->command_id);
+		nvme_tcp_error_recovery(&queue->ctrl->ctrl);
+		return -EINVAL;
+	}
+
+	pr_debug("%s: rq(%px) state %u cid %u bio 0x%px\n",
+		 __FUNCTION__, rq, rq->state, cqe->command_id, rq->bio);
+
+	nvme_end_request(rq, cqe->status, cqe->result);
+
+	return 0;
+}
+
+static int nvme_tcp_handle_c2h_data(struct nvme_tcp_queue *queue,
+		struct nvme_tcp_data_pdu *pdu)
+{
+	struct request *rq;
+
+	pr_debug("%s: c2h cid %u datao %u datal %u",
+	       __FUNCTION__, pdu->command_id, pdu->data_offset, pdu->data_length);
+
+	rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+	if (!rq) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d tag %#x not found\n",
+			nvme_tcp_queue_id(queue), pdu->command_id);
+		return -ENOENT;
+	}
+
+	if (!blk_rq_payload_bytes(rq)) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d tag %#x unexpected data\n",
+			nvme_tcp_queue_id(queue), rq->tag);
+		return -EIO;
+	}
+
+	queue->data_remaining = le32_to_cpu(pdu->data_length);
+
+	if (pdu->hdr.flags & NVME_TCP_F_DATA_SUCCESS &&
+	    unlikely(!(pdu->hdr.flags & NVME_TCP_F_DATA_LAST))) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d tag %#x SUCCESS set but not last PDU\n",
+			nvme_tcp_queue_id(queue), rq->tag);
+		nvme_tcp_error_recovery(&queue->ctrl->ctrl);
+		return -EPROTO;
+	}
+
+	return 0;
+
+}
+
+static int nvme_tcp_handle_comp(struct nvme_tcp_queue *queue,
+		struct nvme_tcp_rsp_pdu *pdu)
+{
+	struct nvme_completion *cqe = &pdu->cqe;
+	int ret = 0;
+
+	pr_debug("%s: result u64 %llx sq_head %x sq_id %x cid %x status %x\n",
+			__FUNCTION__, cqe->result.u64, cqe->sq_head,
+			cqe->sq_id, cqe->command_id, cqe->status);
+	/*
+	 * AEN requests are special as they don't time out and can
+	 * survive any kind of queue freeze and often don't respond to
+	 * aborts.  We don't even bother to allocate a struct request
+	 * for them but rather special case them here.
+	 */
+	if (unlikely(nvme_tcp_queue_id(queue) == 0 &&
+	    cqe->command_id >= NVME_AQ_BLK_MQ_DEPTH))
+		nvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,
+				&cqe->result);
+	else
+		ret = nvme_tcp_process_nvme_cqe(queue, cqe);
+
+	return ret;
+}
+
+static int nvme_tcp_setup_h2c_data_pdu(struct nvme_tcp_request *req,
+		struct nvme_tcp_r2t_pdu *pdu)
+{
+	struct nvme_tcp_data_pdu *data = req->pdu;
+	struct nvme_tcp_queue *queue = req->queue;
+	struct request *rq = blk_mq_rq_from_pdu(req);
+	u8 hdgst = nvme_tcp_hdgst_len(queue);
+	u8 ddgst = nvme_tcp_ddgst_len(queue);
+
+	req->pdu_len = le32_to_cpu(pdu->r2t_length);
+	req->pdu_sent = 0;
+
+	if (unlikely(req->data_sent + req->pdu_len > req->data_len)) {
+		dev_err(queue->ctrl->ctrl.device,
+			"req %d r2t len %u exceeded data len %u (%zu sent)\n",
+			rq->tag, req->pdu_len, req->data_len,
+			req->data_sent);
+		return -EPROTO;
+	}
+
+	if (unlikely(le32_to_cpu(pdu->r2t_offset) < req->data_sent)) {
+		dev_err(queue->ctrl->ctrl.device,
+			"req %d unexpected r2t offset %u (expected %zu)\n",
+			rq->tag, le32_to_cpu(pdu->r2t_offset),
+			req->data_sent);
+		return -EPROTO;
+	}
+
+	pr_debug("%s: cid %u sent %u len %u\n", __FUNCTION__,
+			rq->tag, req->pdu_sent, req->pdu_len);
+
+	memset(data, 0, sizeof(*data));
+	data->hdr.type = nvme_tcp_h2c_data;
+	data->hdr.flags = NVME_TCP_F_DATA_LAST;
+	if (queue->hdr_digest)
+		data->hdr.flags |= NVME_TCP_F_HDGST;
+	if (queue->data_digest)
+		data->hdr.flags |= NVME_TCP_F_DDGST;
+	data->hdr.hlen = sizeof(*data);
+	data->hdr.pdo = data->hdr.hlen + hdgst;
+	data->hdr.plen =
+		cpu_to_le32(data->hdr.hlen + hdgst + req->pdu_len + ddgst);
+	data->ttag = pdu->ttag;
+	data->command_id = rq->tag;
+	data->data_offset = cpu_to_le32(req->data_sent);
+	data->data_length = cpu_to_le32(req->pdu_len);
+	return 0;
+}
+
+static int nvme_tcp_handle_r2t(struct nvme_tcp_queue *queue,
+		struct nvme_tcp_r2t_pdu *pdu)
+{
+	struct nvme_tcp_request *req;
+	struct request *rq;
+	int ret;
+
+	rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+	if (!rq) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d tag %#x not found\n",
+			nvme_tcp_queue_id(queue), pdu->command_id);
+		return -ENOENT;
+	}
+	req = blk_mq_rq_to_pdu(rq);
+
+	ret = nvme_tcp_setup_h2c_data_pdu(req, pdu);
+	if (unlikely(ret))
+		return ret;
+
+	req->state = NVME_TCP_SEND_H2C_PDU;
+	req->offset = 0;
+
+	nvme_tcp_queue_request(req);
+
+	return 0;
+}
+
+static char *pdu_type_to_str[0x9 + 1] = {
+	[7] = "TYPE_C2H",
+	[5] = "TYPE_RSP",
+	[9] = "TYPE_R2T",
+};
+
+static int nvme_tcp_recv_pdu(struct nvme_tcp_queue *queue, struct sk_buff *skb,
+		unsigned int *offset, size_t *len)
+{
+	struct nvme_tcp_hdr *hdr;
+	char *pdu = queue->pdu;
+	size_t rcv_len = min_t(size_t, *len, queue->pdu_remaining);
+	int ret;
+	struct nvme_tcp_term_pdu *term_pdu;
+	int qid;
+
+	ret = skb_copy_bits(skb, *offset,
+		&pdu[queue->pdu_offset], rcv_len);
+	if (unlikely(ret))
+		return ret;
+
+	pr_debug("%s(%u): q %px pdu_off %u len %zu rcv_len %zu remaining %u\n",
+	       __FUNCTION__, smp_processor_id(), queue, queue->pdu_offset, *len,
+	       rcv_len, queue->pdu_remaining);
+
+	queue->pdu_remaining -= rcv_len;
+	queue->pdu_offset += rcv_len;
+	*offset += rcv_len;
+	*len -= rcv_len;
+	if (queue->pdu_remaining)
+		return 0;
+
+	hdr = queue->pdu;
+	if (queue->hdr_digest) {
+		ret = nvme_tcp_verify_hdgst(queue, queue->pdu, hdr->hlen);
+		if (unlikely(ret))
+			return ret;
+	}
+
+	if (queue->data_digest) {
+		ret = nvme_tcp_check_ddgst(queue, queue->pdu);
+		if (unlikely(ret))
+			return ret;
+	}
+
+	pr_debug("%s: pdu:%s hlen/plen %u/%u\n",
+	       __FUNCTION__, pdu_type_to_str[hdr->type], hdr->hlen, hdr->plen);
+
+	switch (hdr->type) {
+	case nvme_tcp_c2h_data:
+		ret = nvme_tcp_handle_c2h_data(queue, (void *)queue->pdu);
+		break;
+	case nvme_tcp_rsp:
+		nvme_tcp_init_recv_ctx(queue);
+		ret = nvme_tcp_handle_comp(queue, (void *)queue->pdu);
+		break;
+	case nvme_tcp_r2t:
+		nvme_tcp_init_recv_ctx(queue);
+		ret = nvme_tcp_handle_r2t(queue, (void *)queue->pdu);
+		break;
+	case nvme_tcp_c2h_term:
+		term_pdu = (void *)pdu;
+		qid = queue - queue->ctrl->queues;
+		dev_err(queue->ctrl->ctrl.device,
+			"Rcvd c2h termination for qid %d fes %0x fei %x\n",
+			qid, term_pdu->fes, term_pdu->fei);
+	default:
+		dev_err(queue->ctrl->ctrl.device,
+			"unsupported pdu type (%d)\n", hdr->type);
+		return -EINVAL;
+	}
+
+	return ret;
+}
+
+static inline void nvme_tcp_end_request(struct request *rq, u16 status)
+{
+	union nvme_result res = {};
+
+	nvme_end_request(rq, cpu_to_le16(status << 1), res);
+}
+
+
+static int nvme_tcp_recv_data(struct nvme_tcp_queue *queue, struct sk_buff *skb,
+			      unsigned int *offset, size_t *len)
+{
+	struct nvme_tcp_data_pdu *pdu = (void *)queue->pdu;
+	struct nvme_tcp_request *req;
+	struct request *rq;
+
+	rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue), pdu->command_id);
+	if (!rq) {
+		dev_err(queue->ctrl->ctrl.device,
+			"queue %d tag %#x not found\n",
+			nvme_tcp_queue_id(queue), pdu->command_id);
+		return -ENOENT;
+	}
+	req = blk_mq_rq_to_pdu(rq);
+
+	while (true) {
+		int recv_len, ret;
+
+		recv_len = min_t(size_t, *len, queue->data_remaining);
+		if (!recv_len)
+			break;
+
+		if (!iov_iter_count(&req->iter)) {
+			req->curr_bio = req->curr_bio->bi_next;
+
+			/*
+			 * If we don`t have any bios it means that controller
+			 * sent more data than we requested, hence error
+			 */
+			if (!req->curr_bio) {
+				dev_err(queue->ctrl->ctrl.device,
+					"queue %d no space in request %#x",
+					nvme_tcp_queue_id(queue), rq->tag);
+				nvme_tcp_init_recv_ctx(queue);
+				return -EIO;
+			}
+			nvme_tcp_init_iter(req, READ);
+		}
+
+		/* we can read only from what is left in this bio */
+		recv_len = min_t(size_t, recv_len,
+				iov_iter_count(&req->iter));
+
+		if (queue->data_digest)
+			ret = skb_copy_and_hash_datagram_iter(skb, *offset,
+				&req->iter, recv_len, queue->rcv_hash);
+		else
+			ret = skb_copy_datagram_iter(skb, *offset,
+					&req->iter, recv_len);
+		if (ret) {
+			dev_err(queue->ctrl->ctrl.device,
+				"queue %d failed to copy request %#x data",
+				nvme_tcp_queue_id(queue), rq->tag);
+			return ret;
+		}
+
+		{
+			struct bio *bio = req->curr_bio;
+			struct bio_vec *vec = __bvec_iter_bvec(bio->bi_io_vec, bio->bi_iter);
+			char *p = kmap(vec->bv_page);
+			unsigned off = vec->bv_offset;
+
+			pr_debug("%s: off cid %u data_remaining %zu page %px 0x%x value:%s\n",
+		        	 __FUNCTION__, pdu->command_id, queue->data_remaining,
+			 	vec->bv_page, off, p+off);
+			kunmap(vec->bv_page);
+		}
+
+		*len -= recv_len;
+		*offset += recv_len;
+		queue->data_remaining -= recv_len;
+	}
+
+	if (!queue->data_remaining) {
+		if (queue->data_digest) {
+			nvme_tcp_ddgst_final(queue->rcv_hash, &queue->exp_ddgst);
+			queue->ddgst_remaining = NVME_TCP_DIGEST_LENGTH;
+		} else {
+			if (pdu->hdr.flags & NVME_TCP_F_DATA_SUCCESS)
+				nvme_tcp_end_request(rq, NVME_SC_SUCCESS);
+			nvme_tcp_init_recv_ctx(queue);
+		}
+	}
+
+	return 0;
+}
+
+static int nvme_tcp_recv_ddgst(struct nvme_tcp_queue *queue,
+		struct sk_buff *skb, unsigned int *offset, size_t *len)
+{
+	struct nvme_tcp_data_pdu *pdu = (void *)queue->pdu;
+	char *ddgst = (char *)&queue->recv_ddgst;
+	size_t recv_len = min_t(size_t, *len, queue->ddgst_remaining);
+	off_t off = NVME_TCP_DIGEST_LENGTH - queue->ddgst_remaining;
+	int ret;
+
+	ret = skb_copy_bits(skb, *offset, &ddgst[off], recv_len);
+	if (unlikely(ret))
+		return ret;
+
+	queue->ddgst_remaining -= recv_len;
+	*offset += recv_len;
+	*len -= recv_len;
+	if (queue->ddgst_remaining)
+		return 0;
+
+	if (queue->recv_ddgst != queue->exp_ddgst) {
+		dev_err(queue->ctrl->ctrl.device,
+			"data digest error: recv %#x expected %#x\n",
+			le32_to_cpu(queue->recv_ddgst),
+			le32_to_cpu(queue->exp_ddgst));
+		return -EIO;
+	}
+
+	if (pdu->hdr.flags & NVME_TCP_F_DATA_SUCCESS) {
+		struct request *rq = blk_mq_tag_to_rq(nvme_tcp_tagset(queue),
+						pdu->command_id);
+
+		nvme_tcp_end_request(rq, NVME_SC_SUCCESS);
+	}
+
+	nvme_tcp_init_recv_ctx(queue);
+	return 0;
+}
+
+static int nvme_tcp_recv_skb(read_descriptor_t *desc, struct sk_buff *skb,
+			     unsigned int offset, size_t len)
+{
+	struct nvme_tcp_queue *queue = desc->arg.data;
+	size_t consumed = len;
+	int result;
+
+	while (len) {
+		switch (nvme_tcp_recv_state(queue)) {
+		case NVME_TCP_RECV_PDU:
+			result = nvme_tcp_recv_pdu(queue, skb, &offset, &len);
+			break;
+		case NVME_TCP_RECV_DATA:
+			result = nvme_tcp_recv_data(queue, skb, &offset, &len);
+			break;
+		case NVME_TCP_RECV_DDGST:
+			result = nvme_tcp_recv_ddgst(queue, skb, &offset, &len);
+			break;
+		default:
+			result = -EFAULT;
+		}
+		if (result) {
+			dev_err(queue->ctrl->ctrl.device,
+				"receive failed:  %d\n", result);
+			queue->rd_enabled = false;
+			nvme_tcp_error_recovery(&queue->ctrl->ctrl);
+			return result;
+		}
+	}
+
+	return consumed;
+}
+
+static void nvme_tcp_data_ready(struct sock *sk)
+{
+	struct nvme_tcp_queue *queue;
+
+	read_lock(&sk->sk_callback_lock);
+	queue = sk->sk_user_data;
+	if (likely(queue && queue->rd_enabled))
+		queue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);
+	read_unlock(&sk->sk_callback_lock);
+}
+
+static void nvme_tcp_write_space(struct sock *sk)
+{
+	struct nvme_tcp_queue *queue;
+
+	read_lock_bh(&sk->sk_callback_lock);
+	queue = sk->sk_user_data;
+	if (likely(queue && sk_stream_is_writeable(sk))) {
+		clear_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
+		queue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);
+	}
+	read_unlock_bh(&sk->sk_callback_lock);
+}
+
+static void nvme_tcp_state_change(struct sock *sk)
+{
+	struct nvme_tcp_queue *queue;
+
+	read_lock(&sk->sk_callback_lock);
+	queue = sk->sk_user_data;
+	if (!queue)
+		goto done;
+
+	switch (sk->sk_state) {
+	case TCP_CLOSE:
+	case TCP_CLOSE_WAIT:
+	case TCP_LAST_ACK:
+	case TCP_FIN_WAIT1:
+	case TCP_FIN_WAIT2:
+		/* fallthrough */
+		pr_err("%s: sk_state %u\n", __FUNCTION__, sk->sk_state);
+		nvme_tcp_error_recovery(&queue->ctrl->ctrl);
+		break;
+	default:
+		dev_info(queue->ctrl->ctrl.device,
+			"queue %d socket state %d\n",
+			nvme_tcp_queue_id(queue), sk->sk_state);
+	}
+
+	queue->state_change(sk);
+done:
+	read_unlock(&sk->sk_callback_lock);
+}
+
+static inline void nvme_tcp_done_send_req(struct nvme_tcp_queue *queue)
+{
+	queue->request = NULL;
+}
+
+static void nvme_tcp_fail_request(struct nvme_tcp_request *req)
+{
+	nvme_tcp_end_request(blk_mq_rq_from_pdu(req), NVME_SC_DATA_XFER_ERROR);
+}
+
+static int nvme_tcp_try_send_key(struct nvme_tcp_request *req)
+{
+	struct request *rq = blk_mq_rq_from_pdu(req);
+	struct nvme_tcp_queue *queue = req->queue;
+	struct bio *bio = kv_req(rq)->key_bio;
+
+	size_t left = key_len(rq);
+
+	int ret, flags = MSG_DONTWAIT;
+
+	struct bio_vec *bvec;
+	int i;
+	struct bvec_iter_all iter_all;
+
+	bio_for_each_segment_all(bvec, bio, i, iter_all) {
+		struct page *page = bvec->bv_page;
+		size_t off = bvec->bv_offset;
+		size_t len = min_t(size_t, left, PAGE_SIZE - off);
+
+		left -= len;
+
+		if (!left && !queue->data_digest)
+			flags |= MSG_EOR;
+		else
+			flags |= MSG_MORE;
+
+		pr_debug("Slab page ? %s\n", PageSlab(page) ? "True" : "False");
+
+		ret = kernel_sendpage(queue->sock, page, off, len, flags);
+		if (ret <= 0 || ret != len) {
+			pr_err("%s: failed to send key size %u\n",
+				__FUNCTION__, key_len(rq));
+			return ret < 0 ? ret : -EIO;
+		}
+
+		if (queue->data_digest)
+			nvme_tcp_ddgst_update(queue->snd_hash, page,
+					      off, ret);
+	}
+
+	/* fully successful last write*/
+	if (!left) {
+		if (queue->data_digest) {
+			nvme_tcp_ddgst_final(queue->snd_hash,
+					     &req->ddgst);
+			req->state = NVME_TCP_SEND_DDGST;
+			req->offset = 0;
+		} else {
+			nvme_tcp_done_send_req(queue);
+		}
+
+		return 1;
+	}
+
+	pr_err("%s: send incomplete %zu bytes left\n", __FUNCTION__, left);
+	return -EIO;
+}
+
+
+static int nvme_tcp_try_send_data(struct nvme_tcp_request *req)
+{
+	struct nvme_tcp_queue *queue = req->queue;
+
+	while (true) {
+		struct page *page = nvme_tcp_req_cur_page(req);
+		size_t offset = nvme_tcp_req_cur_offset(req);
+		size_t len = nvme_tcp_req_cur_length(req);
+		bool last = nvme_tcp_pdu_last_send(req, len);
+		int ret, flags = MSG_DONTWAIT;
+		char *p = kmap(page);
+
+		if (last && !queue->data_digest)
+			flags |= MSG_EOR;
+		else
+			flags |= MSG_MORE;
+
+		pr_debug("%s: slab=%s value:%s\n",
+				__FUNCTION__,  PageSlab(page) ? "True" : "False",
+				p+offset);
+
+		ret = kernel_sendpage(queue->sock, page, offset, len, flags);
+		if (ret <= 0)
+			return ret;
+
+		nvme_tcp_advance_req(req, ret);
+		if (queue->data_digest)
+			nvme_tcp_ddgst_update(queue->snd_hash, page,
+					offset, ret);
+
+		/* fully successful last write*/
+		if (last && ret == len) {
+			if (queue->data_digest) {
+				nvme_tcp_ddgst_final(queue->snd_hash,
+					&req->ddgst);
+				req->state = NVME_TCP_SEND_DDGST;
+				req->offset = 0;
+			} else {
+				nvme_tcp_done_send_req(queue);
+			}
+			return 1;
+		}
+	}
+	return -EAGAIN;
+}
+
+static int nvme_tcp_try_send_cmd_pdu(struct nvme_tcp_request *req)
+{
+	struct nvme_tcp_queue *queue = req->queue;
+	struct nvme_tcp_cmd_pdu *pdu = req->pdu;
+	bool inline_data = nvme_tcp_has_inline_data(req);
+	bool lkey = nvme_tcp_has_large_key(req);
+	//int flags = MSG_DONTWAIT | (inline_data ? MSG_MORE : MSG_EOR);
+	int flags = MSG_DONTWAIT | ((lkey || inline_data) ? MSG_MORE : MSG_EOR);
+	u8 hdgst = nvme_tcp_hdgst_len(queue);
+	int len = sizeof(*pdu) + hdgst - req->offset;
+	int ret;
+
+	if (queue->hdr_digest && !req->offset)
+		nvme_tcp_hdgst(queue->snd_hash, pdu, sizeof(*pdu));
+
+	ret = kernel_sendpage(queue->sock, virt_to_page(pdu),
+			offset_in_page(pdu) + req->offset, len, flags);
+	if (unlikely(ret <= 0))
+		return ret;
+
+	len -= ret;
+	if (!len) {
+		if (lkey) {
+			req->state = NVME_TCP_SEND_KEY;
+			/* Digest? */
+		} else if (inline_data) {
+				req->state = NVME_TCP_SEND_DATA;
+				if (queue->data_digest)
+					crypto_ahash_init(queue->snd_hash);
+			nvme_tcp_init_iter(req, WRITE);
+		} else {
+			nvme_tcp_done_send_req(queue);
+		}
+		return 1;
+	}
+	req->offset += ret;
+
+	return -EAGAIN;
+}
+
+static int nvme_tcp_try_send_data_pdu(struct nvme_tcp_request *req)
+{
+	struct nvme_tcp_queue *queue = req->queue;
+	struct nvme_tcp_data_pdu *pdu = req->pdu;
+	u8 hdgst = nvme_tcp_hdgst_len(queue);
+	int len = sizeof(*pdu) - req->offset + hdgst;
+	int ret;
+
+	if (queue->hdr_digest && !req->offset)
+		nvme_tcp_hdgst(queue->snd_hash, pdu, sizeof(*pdu));
+
+	ret = kernel_sendpage(queue->sock, virt_to_page(pdu),
+			offset_in_page(pdu) + req->offset, len,
+			MSG_DONTWAIT | MSG_MORE);
+	if (unlikely(ret <= 0))
+		return ret;
+
+	len -= ret;
+	if (!len) {
+		req->state = NVME_TCP_SEND_DATA;
+		if (queue->data_digest)
+			crypto_ahash_init(queue->snd_hash);
+		if (!req->data_sent)
+			nvme_tcp_init_iter(req, WRITE);
+		return 1;
+	}
+	req->offset += ret;
+
+	return -EAGAIN;
+}
+
+static int nvme_tcp_try_send_ddgst(struct nvme_tcp_request *req)
+{
+	struct nvme_tcp_queue *queue = req->queue;
+	int ret;
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_EOR };
+	struct kvec iov = {
+		.iov_base = &req->ddgst + req->offset,
+		.iov_len = NVME_TCP_DIGEST_LENGTH - req->offset
+	};
+
+	ret = kernel_sendmsg(queue->sock, &msg, &iov, 1, iov.iov_len);
+	if (unlikely(ret <= 0))
+		return ret;
+
+	if (req->offset + ret == NVME_TCP_DIGEST_LENGTH) {
+		nvme_tcp_done_send_req(queue);
+		return 1;
+	}
+
+	req->offset += ret;
+	return -EAGAIN;
+}
+
+static int nvme_tcp_try_send(struct nvme_tcp_queue *queue)
+{
+	struct nvme_tcp_request *req;
+	int ret = 1;
+
+	if (!queue->request) {
+		queue->request = nvme_tcp_fetch_request(queue);
+		if (!queue->request)
+			return 0;
+	}
+	req = queue->request;
+
+	pr_debug("%s(%u)\n", __FUNCTION__, smp_processor_id());
+
+	/* Does a pipeline table generate more efficient code? */
+	if (req->state == NVME_TCP_SEND_CMD_PDU) {
+		ret = nvme_tcp_try_send_cmd_pdu(req);
+		if (ret <= 0)
+			goto done;
+		if (!nvme_tcp_has_large_key(req) &&
+		    nvme_tcp_has_inline_data(req))
+			return ret;
+	}
+
+	/* It's relative location is crucial */
+	if (req->state == NVME_TCP_SEND_KEY) {
+		ret = nvme_tcp_try_send_key(req);
+		if (ret <= 0)
+			goto done;
+	}
+
+	if (req->state == NVME_TCP_SEND_H2C_PDU) {
+		ret = nvme_tcp_try_send_data_pdu(req);
+		if (ret <= 0)
+			goto done;
+	}
+
+	if (req->state == NVME_TCP_SEND_DATA) {
+		ret = nvme_tcp_try_send_data(req);
+		if (ret <= 0)
+			goto done;
+	}
+
+	if (req->state == NVME_TCP_SEND_DDGST)
+		ret = nvme_tcp_try_send_ddgst(req);
+done:
+	if (ret == -EAGAIN)
+		ret = 0;
+	return ret;
+}
+
+static int nvme_tcp_try_recv(struct nvme_tcp_queue *queue)
+{
+	struct sock *sk = queue->sock->sk;
+	read_descriptor_t rd_desc;
+	int consumed;
+
+	rd_desc.arg.data = queue;
+	rd_desc.count = 1;
+	lock_sock(sk);
+	consumed = tcp_read_sock(sk, &rd_desc, nvme_tcp_recv_skb);
+	release_sock(sk);
+	return consumed;
+}
+
+static void nvme_tcp_io_work(struct work_struct *w)
+{
+	struct nvme_tcp_queue *queue =
+		container_of(w, struct nvme_tcp_queue, io_work);
+	unsigned long start = jiffies + msecs_to_jiffies(1);
+
+	do {
+		bool pending = false;
+		int result;
+
+		result = nvme_tcp_try_send(queue);
+		if (result > 0) {
+			pending = true;
+		} else if (unlikely(result < 0)) {
+			dev_err(queue->ctrl->ctrl.device,
+				"failed to send request %d\n", result);
+			if (result != -EPIPE)
+				nvme_tcp_fail_request(queue->request);
+			nvme_tcp_done_send_req(queue);
+			return;
+		}
+
+		result = nvme_tcp_try_recv(queue);
+		if (result > 0)
+			pending = true;
+
+		if (!pending)
+			return;
+
+	} while (time_after(jiffies, start)); /* quota is exhausted */
+
+	queue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);
+}
+
+static void nvme_tcp_free_crypto(struct nvme_tcp_queue *queue)
+{
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(queue->rcv_hash);
+
+	ahash_request_free(queue->rcv_hash);
+	ahash_request_free(queue->snd_hash);
+	crypto_free_ahash(tfm);
+}
+
+static int nvme_tcp_alloc_crypto(struct nvme_tcp_queue *queue)
+{
+	struct crypto_ahash *tfm;
+
+	tfm = crypto_alloc_ahash("crc32c", 0, CRYPTO_ALG_ASYNC);
+	if (IS_ERR(tfm))
+		return PTR_ERR(tfm);
+
+	queue->snd_hash = ahash_request_alloc(tfm, GFP_KERNEL);
+	if (!queue->snd_hash)
+		goto free_tfm;
+	ahash_request_set_callback(queue->snd_hash, 0, NULL, NULL);
+
+	queue->rcv_hash = ahash_request_alloc(tfm, GFP_KERNEL);
+	if (!queue->rcv_hash)
+		goto free_snd_hash;
+	ahash_request_set_callback(queue->rcv_hash, 0, NULL, NULL);
+
+	return 0;
+free_snd_hash:
+	ahash_request_free(queue->snd_hash);
+free_tfm:
+	crypto_free_ahash(tfm);
+	return -ENOMEM;
+}
+
+static void nvme_tcp_free_async_req(struct nvme_tcp_ctrl *ctrl)
+{
+	struct nvme_tcp_request *async = &ctrl->async_req;
+
+	page_frag_free(async->pdu);
+}
+
+static int nvme_tcp_alloc_async_req(struct nvme_tcp_ctrl *ctrl)
+{
+	struct nvme_tcp_queue *queue = &ctrl->queues[0];
+	struct nvme_tcp_request *async = &ctrl->async_req;
+	u8 hdgst = nvme_tcp_hdgst_len(queue);
+
+	async->pdu = page_frag_alloc(&queue->pf_cache,
+		sizeof(struct nvme_tcp_cmd_pdu) + hdgst,
+		GFP_KERNEL | __GFP_ZERO);
+	if (!async->pdu)
+		return -ENOMEM;
+
+	async->queue = &ctrl->queues[0];
+	return 0;
+}
+
+static void nvme_tcp_free_queue(struct nvme_ctrl *nctrl, int qid)
+{
+	struct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(nctrl);
+	struct nvme_tcp_queue *queue = &ctrl->queues[qid];
+
+	if (!test_and_clear_bit(NVME_TCP_Q_ALLOCATED, &queue->flags))
+		return;
+
+	if (queue->hdr_digest || queue->data_digest)
+		nvme_tcp_free_crypto(queue);
+
+	sock_release(queue->sock);
+	kfree(queue->pdu);
+}
+
+static int nvme_tcp_init_connection(struct nvme_tcp_queue *queue)
+{
+	struct nvme_tcp_icreq_pdu *icreq;
+	struct nvme_tcp_icresp_pdu *icresp;
+	struct msghdr msg = {};
+	struct kvec iov;
+	bool ctrl_hdgst, ctrl_ddgst;
+	int ret;
+
+	icreq = kzalloc(sizeof(*icreq), GFP_KERNEL);
+	if (!icreq)
+		return -ENOMEM;
+
+	icresp = kzalloc(sizeof(*icresp), GFP_KERNEL);
+	if (!icresp) {
+		ret = -ENOMEM;
+		goto free_icreq;
+	}
+
+	icreq->hdr.type = nvme_tcp_icreq;
+	icreq->hdr.hlen = sizeof(*icreq);
+	icreq->hdr.pdo = 0;
+	icreq->hdr.plen = cpu_to_le32(icreq->hdr.hlen);
+	icreq->pfv = cpu_to_le16(NVME_TCP_PFV_1_0);
+	icreq->maxr2t = 0; /* single inflight r2t supported */
+	icreq->hpda = 0; /* no alignment constraint */
+	if (queue->hdr_digest)
+		icreq->digest |= NVME_TCP_HDR_DIGEST_ENABLE;
+	if (queue->data_digest)
+		icreq->digest |= NVME_TCP_DATA_DIGEST_ENABLE;
+
+	iov.iov_base = icreq;
+	iov.iov_len = sizeof(*icreq);
+	ret = kernel_sendmsg(queue->sock, &msg, &iov, 1, iov.iov_len);
+	if (ret < 0)
+		goto free_icresp;
+
+	memset(&msg, 0, sizeof(msg));
+	iov.iov_base = icresp;
+	iov.iov_len = sizeof(*icresp);
+	ret = kernel_recvmsg(queue->sock, &msg, &iov, 1,
+			iov.iov_len, msg.msg_flags);
+	if (ret < 0)
+		goto free_icresp;
+
+	ret = -EINVAL;
+	if (icresp->hdr.type != nvme_tcp_icresp) {
+		pr_err("queue %d: bad type returned %d\n",
+			nvme_tcp_queue_id(queue), icresp->hdr.type);
+		goto free_icresp;
+	}
+
+	if (le32_to_cpu(icresp->hdr.plen) != sizeof(*icresp)) {
+		pr_err("queue %d: bad pdu length returned %d\n",
+			nvme_tcp_queue_id(queue), icresp->hdr.plen);
+		goto free_icresp;
+	}
+
+	if (icresp->pfv != NVME_TCP_PFV_1_0) {
+		pr_err("queue %d: bad pfv returned %d\n",
+			nvme_tcp_queue_id(queue), icresp->pfv);
+		goto free_icresp;
+	}
+
+	ctrl_ddgst = !!(icresp->digest & NVME_TCP_DATA_DIGEST_ENABLE);
+	if ((queue->data_digest && !ctrl_ddgst) ||
+	    (!queue->data_digest && ctrl_ddgst)) {
+		pr_err("queue %d: data digest mismatch host: %s ctrl: %s\n",
+			nvme_tcp_queue_id(queue),
+			queue->data_digest ? "enabled" : "disabled",
+			ctrl_ddgst ? "enabled" : "disabled");
+		goto free_icresp;
+	}
+
+	ctrl_hdgst = !!(icresp->digest & NVME_TCP_HDR_DIGEST_ENABLE);
+	if ((queue->hdr_digest && !ctrl_hdgst) ||
+	    (!queue->hdr_digest && ctrl_hdgst)) {
+		pr_err("queue %d: header digest mismatch host: %s ctrl: %s\n",
+			nvme_tcp_queue_id(queue),
+			queue->hdr_digest ? "enabled" : "disabled",
+			ctrl_hdgst ? "enabled" : "disabled");
+		goto free_icresp;
+	}
+
+	if (icresp->cpda != 0) {
+		pr_err("queue %d: unsupported cpda returned %d\n",
+			nvme_tcp_queue_id(queue), icresp->cpda);
+		goto free_icresp;
+	}
+
+	ret = 0;
+free_icresp:
+	kfree(icresp);
+free_icreq:
+	kfree(icreq);
+	return ret;
+}
+
+static int nvme_tcp_alloc_queue(struct nvme_ctrl *nctrl,
+		int qid, size_t queue_size)
+{
+	struct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(nctrl);
+	struct nvme_tcp_queue *queue = &ctrl->queues[qid];
+	struct linger sol = { .l_onoff = 1, .l_linger = 0 };
+	int sod = 1;
+	unsigned xcvbuf = 6291456;
+	int ret, opt, rcv_pdu_size, n;
+
+	queue->ctrl = ctrl;
+	INIT_LIST_HEAD(&queue->send_list);
+	spin_lock_init(&queue->lock);
+	INIT_WORK(&queue->io_work, nvme_tcp_io_work);
+	queue->queue_size = queue_size;
+
+	if (qid > 0)
+		queue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;
+	else
+		queue->cmnd_capsule_len = sizeof(struct nvme_command) +
+						NVME_TCP_ADMIN_CCSZ;
+
+	ret = sock_create(ctrl->addr.ss_family, SOCK_STREAM,
+			IPPROTO_TCP, &queue->sock);
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"failed to create socket: %d\n", ret);
+		return ret;
+	}
+
+	ret = kernel_setsockopt(queue->sock, SOL_SOCKET, SO_DEBUG,
+			(char *)&sod, sizeof(sod));
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"failed to set SO_DEBUG sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+
+	/* Single syn retry */
+	opt = 1;
+	ret = kernel_setsockopt(queue->sock, IPPROTO_TCP, TCP_SYNCNT,
+			(char *)&opt, sizeof(opt));
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"failed to set TCP_SYNCNT sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	/* Set TCP no delay */
+	opt = 1;
+	ret = kernel_setsockopt(queue->sock, IPPROTO_TCP,
+			TCP_NODELAY, (char *)&opt, sizeof(opt));
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"failed to set TCP_NODELAY sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	/*
+	 * Cleanup whatever is sitting in the TCP transmit queue on socket
+	 * close. This is done to prevent stale data from being sent should
+	 * the network connection be restored before TCP times out.
+	 */
+	ret = kernel_setsockopt(queue->sock, SOL_SOCKET, SO_LINGER,
+			(char *)&sol, sizeof(sol));
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"failed to set SO_LINGER sock opt %d\n", ret);
+		goto err_sock;
+	}
+
+	ret = kernel_setsockopt(queue->sock, SOL_SOCKET, SO_RCVBUF,
+			(char *)&xcvbuf, sizeof(xcvbuf));
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"failed to set SO_RCVBUF sock opt to %u ret=%d\n", xcvbuf, ret);
+		goto err_sock;
+	}
+
+
+	ret = kernel_setsockopt(queue->sock, SOL_SOCKET, SO_SNDBUF,
+			(char *)&xcvbuf, sizeof(xcvbuf));
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"failed to set SO_RCVBUF sock opt to %u ret=%d\n", xcvbuf, ret);
+		goto err_sock;
+	}
+
+
+	queue->sock->sk->sk_allocation = GFP_ATOMIC;
+	if (!qid)
+		n = 0;
+	else
+		n = (qid - 1) % num_online_cpus();
+	queue->io_cpu = cpumask_next_wrap(n - 1, cpu_online_mask, -1, false);
+	queue->request = NULL;
+	queue->data_remaining = 0;
+	queue->ddgst_remaining = 0;
+	queue->pdu_remaining = 0;
+	queue->pdu_offset = 0;
+	sk_set_memalloc(queue->sock->sk);
+
+	if (ctrl->ctrl.opts->mask & NVMF_OPT_HOST_TRADDR) {
+		ret = kernel_bind(queue->sock, (struct sockaddr *)&ctrl->src_addr,
+			sizeof(ctrl->src_addr));
+		if (ret) {
+			dev_err(ctrl->ctrl.device,
+				"failed to bind queue %d socket %d\n",
+				qid, ret);
+			goto err_sock;
+		}
+	}
+
+	queue->hdr_digest = nctrl->opts->hdr_digest;
+	queue->data_digest = nctrl->opts->data_digest;
+	if (queue->hdr_digest || queue->data_digest) {
+		ret = nvme_tcp_alloc_crypto(queue);
+		if (ret) {
+			dev_err(ctrl->ctrl.device,
+				"failed to allocate queue %d crypto\n", qid);
+			goto err_sock;
+		}
+	}
+
+	rcv_pdu_size = sizeof(struct nvme_tcp_rsp_pdu) +
+			nvme_tcp_hdgst_len(queue);
+	queue->pdu = kmalloc(rcv_pdu_size, GFP_KERNEL);
+	if (!queue->pdu) {
+		ret = -ENOMEM;
+		goto err_crypto;
+	}
+
+	dev_dbg(ctrl->ctrl.device, "connecting queue %d\n",
+			nvme_tcp_queue_id(queue));
+
+	ret = kernel_connect(queue->sock, (struct sockaddr *)&ctrl->addr,
+		sizeof(ctrl->addr), 0);
+	if (ret) {
+		dev_err(ctrl->ctrl.device,
+			"failed to connect socket: %d\n", ret);
+		goto err_rcv_pdu;
+	}
+
+	{
+		struct sockaddr_storage	sockaddr;
+
+		ret = kernel_getsockname(queue->sock, (struct sockaddr *)&sockaddr);
+		if (ret < 0)
+			pr_err("getsockname");
+		else {
+			struct sockaddr_in *sin = (struct sockaddr_in *)&sockaddr;
+			pr_err("qid %2d port # %d\n", qid, ntohs(sin->sin_port));
+		}
+	}
+
+	ret = nvme_tcp_init_connection(queue);
+	if (ret)
+		goto err_init_connect;
+
+	queue->rd_enabled = true;
+	set_bit(NVME_TCP_Q_ALLOCATED, &queue->flags);
+	nvme_tcp_init_recv_ctx(queue);
+
+	write_lock_bh(&queue->sock->sk->sk_callback_lock);
+	queue->sock->sk->sk_user_data = queue;
+	queue->state_change = queue->sock->sk->sk_state_change;
+	queue->data_ready = queue->sock->sk->sk_data_ready;
+	queue->write_space = queue->sock->sk->sk_write_space;
+	queue->sock->sk->sk_data_ready = nvme_tcp_data_ready;
+	queue->sock->sk->sk_state_change = nvme_tcp_state_change;
+	queue->sock->sk->sk_write_space = nvme_tcp_write_space;
+	write_unlock_bh(&queue->sock->sk->sk_callback_lock);
+
+	return 0;
+
+err_init_connect:
+	kernel_sock_shutdown(queue->sock, SHUT_RDWR);
+err_rcv_pdu:
+	kfree(queue->pdu);
+err_crypto:
+	if (queue->hdr_digest || queue->data_digest)
+		nvme_tcp_free_crypto(queue);
+err_sock:
+	sock_release(queue->sock);
+	queue->sock = NULL;
+	return ret;
+}
+
+static void nvme_tcp_restore_sock_calls(struct nvme_tcp_queue *queue)
+{
+	struct socket *sock = queue->sock;
+
+	write_lock_bh(&sock->sk->sk_callback_lock);
+	sock->sk->sk_user_data  = NULL;
+	sock->sk->sk_data_ready = queue->data_ready;
+	sock->sk->sk_state_change = queue->state_change;
+	sock->sk->sk_write_space  = queue->write_space;
+	write_unlock_bh(&sock->sk->sk_callback_lock);
+}
+
+static void __nvme_tcp_stop_queue(struct nvme_tcp_queue *queue)
+{
+	kernel_sock_shutdown(queue->sock, SHUT_RDWR);
+	nvme_tcp_restore_sock_calls(queue);
+	cancel_work_sync(&queue->io_work);
+}
+
+static void nvme_tcp_stop_queue(struct nvme_ctrl *nctrl, int qid)
+{
+	struct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(nctrl);
+	struct nvme_tcp_queue *queue = &ctrl->queues[qid];
+
+	if (!test_and_clear_bit(NVME_TCP_Q_LIVE, &queue->flags))
+		return;
+
+	__nvme_tcp_stop_queue(queue);
+}
+
+static int nvme_tcp_start_queue(struct nvme_ctrl *nctrl, int idx)
+{
+	struct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(nctrl);
+	int ret;
+
+	if (idx)
+		ret = nvmf_connect_io_queue(nctrl, idx, false);
+	else
+		ret = nvmf_connect_admin_queue(nctrl);
+
+	if (!ret) {
+		set_bit(NVME_TCP_Q_LIVE, &ctrl->queues[idx].flags);
+	} else {
+		__nvme_tcp_stop_queue(&ctrl->queues[idx]);
+		dev_err(nctrl->device,
+			"failed to connect queue: %d ret=%d\n", idx, ret);
+	}
+	return ret;
+}
+
+static struct blk_mq_tag_set *nvme_tcp_alloc_tagset(struct nvme_ctrl *nctrl,
+		bool admin)
+{
+	struct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(nctrl);
+	struct blk_mq_tag_set *set;
+	int ret;
+
+	if (admin) {
+		set = &ctrl->admin_tag_set;
+		memset(set, 0, sizeof(*set));
+		set->ops = &nvme_tcp_admin_mq_ops;
+		set->queue_depth = NVME_AQ_MQ_TAG_DEPTH;
+		set->reserved_tags = 2; /* connect + keep-alive */
+		set->numa_node = NUMA_NO_NODE;
+		set->cmd_size = sizeof(struct nvme_tcp_request);
+		set->driver_data = ctrl;
+		set->nr_hw_queues = 1;
+		set->timeout = ADMIN_TIMEOUT;
+	} else {
+		set = &ctrl->tag_set;
+		memset(set, 0, sizeof(*set));
+		set->ops = &nvme_tcp_mq_ops;
+		set->queue_depth = nctrl->sqsize + 1;
+		set->reserved_tags = 1; /* fabric connect */
+		set->numa_node = NUMA_NO_NODE;
+		set->flags = BLK_MQ_F_SHOULD_MERGE;
+		set->cmd_size = sizeof(struct nvme_tcp_request);
+		set->driver_data = ctrl;
+		set->nr_hw_queues = nctrl->queue_count - 1;
+		set->timeout = NVME_IO_TIMEOUT;
+		set->nr_maps = 2 /* default + read */;
+	}
+
+	ret = blk_mq_alloc_tag_set(set);
+	if (ret)
+		return ERR_PTR(ret);
+
+	return set;
+}
+
+static void nvme_tcp_free_admin_queue(struct nvme_ctrl *ctrl)
+{
+	if (to_tcp_ctrl(ctrl)->async_req.pdu) {
+		nvme_tcp_free_async_req(to_tcp_ctrl(ctrl));
+		to_tcp_ctrl(ctrl)->async_req.pdu = NULL;
+	}
+
+	nvme_tcp_free_queue(ctrl, 0);
+}
+
+static void nvme_tcp_free_io_queues(struct nvme_ctrl *ctrl)
+{
+	int i;
+
+	for (i = 1; i < ctrl->queue_count; i++)
+		nvme_tcp_free_queue(ctrl, i);
+}
+
+static void nvme_tcp_stop_io_queues(struct nvme_ctrl *ctrl)
+{
+	int i;
+
+	for (i = 1; i < ctrl->queue_count; i++)
+		nvme_tcp_stop_queue(ctrl, i);
+}
+
+static int nvme_tcp_start_io_queues(struct nvme_ctrl *ctrl)
+{
+	int i, ret = 0;
+
+	for (i = 1; i < ctrl->queue_count; i++) {
+		ret = nvme_tcp_start_queue(ctrl, i);
+		if (ret)
+			goto out_stop_queues;
+	}
+
+	return 0;
+
+out_stop_queues:
+	for (i--; i >= 1; i--)
+		nvme_tcp_stop_queue(ctrl, i);
+	return ret;
+}
+
+static int nvme_tcp_alloc_admin_queue(struct nvme_ctrl *ctrl)
+{
+	int ret;
+
+	ret = nvme_tcp_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);
+	if (ret)
+		return ret;
+
+	ret = nvme_tcp_alloc_async_req(to_tcp_ctrl(ctrl));
+	if (ret)
+		goto out_free_queue;
+
+	return 0;
+
+out_free_queue:
+	nvme_tcp_free_queue(ctrl, 0);
+	return ret;
+}
+
+static int nvme_tcp_alloc_io_queues(struct nvme_ctrl *ctrl)
+{
+	int i, ret;
+
+	for (i = 1; i < ctrl->queue_count; i++) {
+		ret = nvme_tcp_alloc_queue(ctrl, i,
+				ctrl->sqsize + 1);
+		if (ret)
+			goto out_free_queues;
+	}
+
+	return 0;
+
+out_free_queues:
+	for (i--; i >= 1; i--)
+		nvme_tcp_free_queue(ctrl, i);
+
+	return ret;
+}
+
+static unsigned int nvme_tcp_nr_io_queues(struct nvme_ctrl *ctrl)
+{
+	unsigned int nr_io_queues;
+
+	nr_io_queues = min(ctrl->opts->nr_io_queues, num_online_cpus());
+	nr_io_queues += min(ctrl->opts->nr_write_queues, num_online_cpus());
+
+	return nr_io_queues;
+}
+
+static int nvme_alloc_io_queues(struct nvme_ctrl *ctrl)
+{
+	unsigned int nr_io_queues;
+	int ret;
+
+	nr_io_queues = nvme_tcp_nr_io_queues(ctrl);
+	ret = nvme_set_queue_count(ctrl, &nr_io_queues);
+	if (ret)
+		return ret;
+
+	ctrl->queue_count = nr_io_queues + 1;
+	if (ctrl->queue_count < 2)
+		return 0;
+
+	dev_info(ctrl->device,
+		"creating %d I/O queues.\n", nr_io_queues);
+
+	return nvme_tcp_alloc_io_queues(ctrl);
+}
+
+static void nvme_tcp_destroy_io_queues(struct nvme_ctrl *ctrl, bool remove)
+{
+	nvme_tcp_stop_io_queues(ctrl);
+	if (remove) {
+		blk_cleanup_queue(ctrl->connect_q);
+		blk_mq_free_tag_set(ctrl->tagset);
+	}
+	nvme_tcp_free_io_queues(ctrl);
+}
+
+static int nvme_tcp_configure_io_queues(struct nvme_ctrl *ctrl, bool new)
+{
+	int ret;
+
+	ret = nvme_alloc_io_queues(ctrl);
+	if (ret)
+		return ret;
+
+	if (new) {
+		ctrl->tagset = nvme_tcp_alloc_tagset(ctrl, false);
+		if (IS_ERR(ctrl->tagset)) {
+			ret = PTR_ERR(ctrl->tagset);
+			goto out_free_io_queues;
+		}
+
+		ctrl->connect_q = blk_mq_init_queue(ctrl->tagset);
+		if (IS_ERR(ctrl->connect_q)) {
+			ret = PTR_ERR(ctrl->connect_q);
+			goto out_free_tag_set;
+		}
+	} else {
+		blk_mq_update_nr_hw_queues(ctrl->tagset,
+			ctrl->queue_count - 1);
+	}
+
+	ret = nvme_tcp_start_io_queues(ctrl);
+	if (ret)
+		goto out_cleanup_connect_q;
+
+	return 0;
+
+out_cleanup_connect_q:
+	if (new)
+		blk_cleanup_queue(ctrl->connect_q);
+out_free_tag_set:
+	if (new)
+		blk_mq_free_tag_set(ctrl->tagset);
+out_free_io_queues:
+	nvme_tcp_free_io_queues(ctrl);
+	return ret;
+}
+
+static void nvme_tcp_destroy_admin_queue(struct nvme_ctrl *ctrl, bool remove)
+{
+	nvme_tcp_stop_queue(ctrl, 0);
+	if (remove) {
+		blk_cleanup_queue(ctrl->admin_q);
+		blk_mq_free_tag_set(ctrl->admin_tagset);
+	}
+	nvme_tcp_free_admin_queue(ctrl);
+}
+
+static int nvme_tcp_configure_admin_queue(struct nvme_ctrl *ctrl, bool new)
+{
+	int error;
+
+	error = nvme_tcp_alloc_admin_queue(ctrl);
+	if (error)
+		return error;
+
+	if (new) {
+		ctrl->admin_tagset = nvme_tcp_alloc_tagset(ctrl, true);
+		if (IS_ERR(ctrl->admin_tagset)) {
+			error = PTR_ERR(ctrl->admin_tagset);
+			goto out_free_queue;
+		}
+
+		ctrl->admin_q = blk_mq_init_queue(ctrl->admin_tagset);
+		if (IS_ERR(ctrl->admin_q)) {
+			error = PTR_ERR(ctrl->admin_q);
+			goto out_free_tagset;
+		}
+	}
+
+	error = nvme_tcp_start_queue(ctrl, 0);
+	if (error)
+		goto out_cleanup_queue;
+
+	error = ctrl->ops->reg_read64(ctrl, NVME_REG_CAP, &ctrl->cap);
+	if (error) {
+		dev_err(ctrl->device,
+			"prop_get NVME_REG_CAP failed\n");
+		goto out_stop_queue;
+	}
+
+	ctrl->sqsize = min_t(int, NVME_CAP_MQES(ctrl->cap), ctrl->sqsize);
+	pr_err("target supported sqsize %u\n", ctrl->sqsize);
+
+	error = nvme_enable_ctrl(ctrl, ctrl->cap);
+	if (error)
+		goto out_stop_queue;
+
+	error = nvme_init_identify(ctrl);
+	if (error)
+		goto out_stop_queue;
+
+	return 0;
+
+out_stop_queue:
+	nvme_tcp_stop_queue(ctrl, 0);
+out_cleanup_queue:
+	if (new)
+		blk_cleanup_queue(ctrl->admin_q);
+out_free_tagset:
+	if (new)
+		blk_mq_free_tag_set(ctrl->admin_tagset);
+out_free_queue:
+	nvme_tcp_free_admin_queue(ctrl);
+	return error;
+}
+
+static void nvme_tcp_teardown_admin_queue(struct nvme_ctrl *ctrl,
+		bool remove)
+{
+	blk_mq_quiesce_queue(ctrl->admin_q);
+	nvme_tcp_stop_queue(ctrl, 0);
+	blk_mq_tagset_busy_iter(ctrl->admin_tagset, nvme_cancel_request, ctrl);
+	blk_mq_unquiesce_queue(ctrl->admin_q);
+	nvme_tcp_destroy_admin_queue(ctrl, remove);
+}
+
+static void nvme_tcp_teardown_io_queues(struct nvme_ctrl *ctrl,
+		bool remove)
+{
+	if (ctrl->queue_count <= 1)
+		return;
+	nvme_stop_queues(ctrl);
+	nvme_tcp_stop_io_queues(ctrl);
+	blk_mq_tagset_busy_iter(ctrl->tagset, nvme_cancel_request, ctrl);
+	if (remove)
+		nvme_start_queues(ctrl);
+	nvme_tcp_destroy_io_queues(ctrl, remove);
+}
+
+static void nvme_tcp_reconnect_or_remove(struct nvme_ctrl *ctrl)
+{
+	/* If we are resetting/deleting then do nothing */
+	if (ctrl->state != NVME_CTRL_CONNECTING) {
+		WARN_ON_ONCE(ctrl->state == NVME_CTRL_NEW ||
+			ctrl->state == NVME_CTRL_LIVE);
+		return;
+	}
+
+	if (nvmf_should_reconnect(ctrl)) {
+		dev_info(ctrl->device, "Reconnecting in %d seconds...\n",
+			ctrl->opts->reconnect_delay);
+//		queue_delayed_work(nvme_wq, &to_tcp_ctrl(ctrl)->connect_work,
+//				ctrl->opts->reconnect_delay * HZ);
+		pr_err("%s: ctrl %px\n", __FUNCTION__, ctrl);
+		queue_delayed_work(nvme_wq, &to_tcp_ctrl(ctrl)->connect_work,
+				5 * HZ);
+
+	} else {
+		dev_info(ctrl->device, "Removing controller...\n");
+		nvme_delete_ctrl(ctrl);
+	}
+}
+
+static int nvme_tcp_setup_ctrl(struct nvme_ctrl *ctrl, bool new)
+{
+	struct nvmf_ctrl_options *opts = ctrl->opts;
+	int ret = -EINVAL;
+
+	ret = nvme_tcp_configure_admin_queue(ctrl, new);
+	if (ret)
+		return ret;
+
+	if (ctrl->icdoff) {
+		dev_err(ctrl->device, "icdoff is not supported!\n");
+		goto destroy_admin;
+	}
+
+	if (opts->queue_size > ctrl->sqsize + 1)
+		dev_warn(ctrl->device,
+			"queue_size %zu > ctrl sqsize %u, clamping down\n",
+			opts->queue_size, ctrl->sqsize + 1);
+
+	if (ctrl->sqsize + 1 > ctrl->maxcmd) {
+		dev_warn(ctrl->device,
+			"sqsize %u > ctrl maxcmd %u, clamping down\n",
+			ctrl->sqsize + 1, ctrl->maxcmd);
+		ctrl->sqsize = ctrl->maxcmd - 1;
+	}
+
+	if (ctrl->queue_count > 1) {
+		ret = nvme_tcp_configure_io_queues(ctrl, new);
+		if (ret)
+			goto destroy_admin;
+	}
+
+	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_LIVE)) {
+		/* state change failure is ok if we're in DELETING state */
+		WARN_ON_ONCE(ctrl->state != NVME_CTRL_DELETING);
+		ret = -EINVAL;
+		goto destroy_io;
+	}
+
+	nvme_start_ctrl(ctrl);
+	return 0;
+
+destroy_io:
+	if (ctrl->queue_count > 1)
+		nvme_tcp_destroy_io_queues(ctrl, new);
+destroy_admin:
+	nvme_tcp_stop_queue(ctrl, 0);
+	nvme_tcp_destroy_admin_queue(ctrl, new);
+	return ret;
+}
+
+static void nvme_tcp_reconnect_ctrl_work(struct work_struct *work)
+{
+	struct nvme_tcp_ctrl *tcp_ctrl = container_of(to_delayed_work(work),
+			struct nvme_tcp_ctrl, connect_work);
+	struct nvme_ctrl *ctrl = &tcp_ctrl->ctrl;
+
+	pr_debug("%s\n", __FUNCTION__);
+
+	++ctrl->nr_reconnects;
+
+	if (nvme_tcp_setup_ctrl(ctrl, false))
+		goto requeue;
+
+	dev_info(ctrl->device, "Successfully reconnected (%d attempt)\n",
+			ctrl->nr_reconnects);
+
+	ctrl->nr_reconnects = 0;
+
+	return;
+
+requeue:
+	dev_info(ctrl->device, "Failed reconnect attempt %d\n",
+			ctrl->nr_reconnects);
+	nvme_tcp_reconnect_or_remove(ctrl);
+}
+
+static void nvme_tcp_error_recovery_work(struct work_struct *work)
+{
+	struct nvme_tcp_ctrl *tcp_ctrl = container_of(work,
+				struct nvme_tcp_ctrl, err_work);
+	struct nvme_ctrl *ctrl = &tcp_ctrl->ctrl;
+
+	pr_debug("%s\n", __FUNCTION__);
+	nvme_stop_keep_alive(ctrl);
+	nvme_tcp_teardown_io_queues(ctrl, false);
+	/* unquiesce to fail fast pending requests */
+	nvme_start_queues(ctrl);
+	nvme_tcp_teardown_admin_queue(ctrl, false);
+
+	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_CONNECTING)) {
+		/* state change failure is ok if we're in DELETING state */
+		WARN_ON_ONCE(ctrl->state != NVME_CTRL_DELETING);
+		return;
+	}
+
+	nvme_tcp_reconnect_or_remove(ctrl);
+}
+
+static void nvme_tcp_teardown_ctrl(struct nvme_ctrl *ctrl, bool shutdown)
+{
+	cancel_work_sync(&to_tcp_ctrl(ctrl)->err_work);
+	cancel_delayed_work_sync(&to_tcp_ctrl(ctrl)->connect_work);
+
+	nvme_tcp_teardown_io_queues(ctrl, shutdown);
+	if (shutdown)
+		nvme_shutdown_ctrl(ctrl);
+	else
+		nvme_disable_ctrl(ctrl, ctrl->cap);
+	nvme_tcp_teardown_admin_queue(ctrl, shutdown);
+}
+
+static void nvme_tcp_delete_ctrl(struct nvme_ctrl *ctrl)
+{
+	nvme_tcp_teardown_ctrl(ctrl, true);
+}
+
+static void nvme_reset_ctrl_work(struct work_struct *work)
+{
+	struct nvme_ctrl *ctrl =
+		container_of(work, struct nvme_ctrl, reset_work);
+
+	pr_debug("%s\n", __FUNCTION__);
+
+	nvme_stop_ctrl(ctrl);
+	nvme_tcp_teardown_ctrl(ctrl, false);
+
+	if (!nvme_change_ctrl_state(ctrl, NVME_CTRL_CONNECTING)) {
+		/* state change failure is ok if we're in DELETING state */
+		WARN_ON_ONCE(ctrl->state != NVME_CTRL_DELETING);
+		return;
+	}
+
+	if (nvme_tcp_setup_ctrl(ctrl, false))
+		goto out_fail;
+
+	return;
+
+out_fail:
+	++ctrl->nr_reconnects;
+	nvme_tcp_reconnect_or_remove(ctrl);
+}
+
+static void nvme_tcp_free_ctrl(struct nvme_ctrl *nctrl)
+{
+	struct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(nctrl);
+
+	if (list_empty(&ctrl->list))
+		goto free_ctrl;
+
+	mutex_lock(&nvme_tcp_ctrl_mutex);
+	list_del(&ctrl->list);
+	mutex_unlock(&nvme_tcp_ctrl_mutex);
+
+	nvmf_free_options(nctrl->opts);
+free_ctrl:
+	kfree(ctrl->queues);
+	kfree(ctrl);
+}
+
+static void nvme_tcp_set_sg_null(struct nvme_command *c)
+{
+	struct nvme_sgl_desc *sg = &c->common.dptr.sgl;
+
+	sg->addr = 0;
+	sg->length = 0;
+	sg->type = (NVME_TRANSPORT_SGL_DATA_DESC << 4) |
+			NVME_SGL_FMT_TRANSPORT_A;
+}
+
+static void nvme_tcp_set_kv_sg_inline(struct nvme_tcp_queue *queue,
+			struct nvme_command *c, u32 key_len, u32 val_len)
+{
+	struct nvme_sgl_desc *key_sg = &c->kv_tcp.kptr.sgl;
+	struct nvme_sgl_desc *val_sg = &c->kv_tcp.dptr.sgl;
+
+	key_sg->addr = cpu_to_le64(queue->ctrl->ctrl.icdoff);
+	key_sg->length = cpu_to_le32(key_len);
+	key_sg->type = (NVME_SGL_FMT_DATA_DESC << 4) | NVME_SGL_FMT_OFFSET;
+
+	val_sg->addr = cpu_to_le64(queue->ctrl->ctrl.icdoff);
+	val_sg->length = cpu_to_le32(val_len);
+	val_sg->type = (NVME_TRANSPORT_SGL_DATA_DESC << 4) | NVME_SGL_FMT_TRANSPORT_A;
+}
+
+
+static void nvme_tcp_set_sg_inline(struct nvme_tcp_queue *queue,
+		struct nvme_command *c, u32 data_len)
+{
+	struct nvme_sgl_desc *sg = &c->common.dptr.sgl;
+
+	sg->addr = cpu_to_le64(queue->ctrl->ctrl.icdoff);
+	sg->length = cpu_to_le32(data_len);
+	sg->type = (NVME_SGL_FMT_DATA_DESC << 4) | NVME_SGL_FMT_OFFSET;
+}
+
+static void nvme_tcp_set_sg_host_data(struct nvme_command *c,
+		u32 data_len)
+{
+	struct nvme_sgl_desc *sg = &c->common.dptr.sgl;
+
+	sg->addr = 0;
+	sg->length = cpu_to_le32(data_len);
+	sg->type = (NVME_TRANSPORT_SGL_DATA_DESC << 4) |
+			NVME_SGL_FMT_TRANSPORT_A;
+}
+
+static void nvme_tcp_submit_async_event(struct nvme_ctrl *arg)
+{
+	struct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(arg);
+	struct nvme_tcp_queue *queue = &ctrl->queues[0];
+	struct nvme_tcp_cmd_pdu *pdu = ctrl->async_req.pdu;
+	struct nvme_command *cmd = &pdu->cmd;
+	u8 hdgst = nvme_tcp_hdgst_len(queue);
+
+	memset(pdu, 0, sizeof(*pdu));
+	pdu->hdr.type = nvme_tcp_cmd;
+	if (queue->hdr_digest)
+		pdu->hdr.flags |= NVME_TCP_F_HDGST;
+	pdu->hdr.hlen = sizeof(*pdu);
+	pdu->hdr.plen = cpu_to_le32(pdu->hdr.hlen + hdgst);
+
+	cmd->common.opcode = nvme_admin_async_event;
+	cmd->common.command_id = NVME_AQ_BLK_MQ_DEPTH;
+	cmd->common.flags |= NVME_CMD_SGL_METABUF;
+	nvme_tcp_set_sg_null(cmd);
+
+	ctrl->async_req.state = NVME_TCP_SEND_CMD_PDU;
+	ctrl->async_req.offset = 0;
+	ctrl->async_req.curr_bio = NULL;
+	ctrl->async_req.data_len = 0;
+
+	nvme_tcp_queue_request(&ctrl->async_req);
+}
+
+static enum blk_eh_timer_return
+nvme_tcp_timeout(struct request *rq, bool reserved)
+{
+	struct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);
+	struct nvme_tcp_ctrl *ctrl = req->queue->ctrl;
+	struct nvme_tcp_cmd_pdu *pdu = req->pdu;
+
+	pr_err("%s: rq %px reserved %d\n", __FUNCTION__, rq, reserved);
+
+	dev_warn(ctrl->ctrl.device,
+		"queue %d: timeout request %#x type %d\n",
+		nvme_tcp_queue_id(req->queue), rq->tag, pdu->hdr.type);
+
+	if (ctrl->ctrl.state != NVME_CTRL_LIVE) {
+		/*
+		 * Teardown immediately if controller times out while starting
+		 * or we are already started error recovery. all outstanding
+		 * requests are completed on shutdown, so we return BLK_EH_DONE.
+		 */
+		flush_work(&ctrl->err_work);
+		nvme_tcp_teardown_io_queues(&ctrl->ctrl, false);
+		nvme_tcp_teardown_admin_queue(&ctrl->ctrl, false);
+		return BLK_EH_DONE;
+	}
+
+	dev_warn(ctrl->ctrl.device, "starting error recovery\n");
+	nvme_tcp_error_recovery(&ctrl->ctrl);
+
+	return BLK_EH_RESET_TIMER;
+}
+
+static blk_status_t nvme_tcp_map_data(struct nvme_tcp_queue *queue,
+			struct request *rq)
+{
+	struct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);
+	struct nvme_tcp_cmd_pdu *pdu = req->pdu;
+	struct nvme_command *c = &pdu->cmd;
+
+	c->common.flags |= NVME_CMD_SGL_METABUF;
+
+	if (kv_req(rq)->key_bio)
+		nvme_tcp_set_kv_sg_inline(queue, c, key_len(rq), req->data_len);
+	else if (rq_data_dir(rq) == WRITE && req->data_len &&
+	    req->data_len <= nvme_tcp_inline_data_size(queue))
+		nvme_tcp_set_sg_inline(queue, c, req->data_len);
+	else
+		nvme_tcp_set_sg_host_data(c, req->data_len);
+
+	return 0;
+}
+
+static blk_status_t nvme_tcp_setup_cmd_pdu(struct nvme_ns *ns, struct request *rq)
+{
+	struct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);
+	struct nvme_tcp_cmd_pdu *pdu = req->pdu;
+	struct nvme_tcp_queue *queue = req->queue;
+	u8 hdgst = nvme_tcp_hdgst_len(queue), ddgst = 0;
+	blk_status_t ret;
+
+	ret = nvme_setup_cmd(ns, rq, &pdu->cmd);
+	if (ret)
+		return ret;
+
+	pr_debug("%s: rq %px cid %u opcode %x\n",
+		 __FUNCTION__, rq,
+		 pdu->cmd.common.command_id,
+		 pdu->cmd.common.opcode);
+
+	req->state = NVME_TCP_SEND_CMD_PDU;
+	req->offset = 0;
+	req->data_sent = 0;
+	req->pdu_len = 0;
+	req->pdu_sent = 0;
+	req->data_len = blk_rq_payload_bytes(rq);
+	req->curr_bio = rq->bio;
+
+	if (kv_req(rq)->key_bio) {
+		pr_debug("%s: key len %u\n", __FUNCTION__, key_len(rq));
+		req->pdu_len = key_len(rq);
+		if (rq_data_dir(rq) == READ)
+			nvme_tcp_init_iter(req, READ);
+	} else if (rq_data_dir(rq) == WRITE &&
+		req->data_len <= nvme_tcp_inline_data_size(queue))
+		req->pdu_len = req->data_len;
+	else if (req->curr_bio)
+		nvme_tcp_init_iter(req, READ);
+
+	pdu->hdr.type = nvme_tcp_cmd;
+	pdu->hdr.flags = 0;
+	if (queue->hdr_digest)
+		pdu->hdr.flags |= NVME_TCP_F_HDGST;
+	if (queue->data_digest && req->pdu_len) {
+		pdu->hdr.flags |= NVME_TCP_F_DDGST;
+		ddgst = nvme_tcp_ddgst_len(queue);
+	}
+	pdu->hdr.hlen = sizeof(*pdu);
+	pdu->hdr.pdo = req->pdu_len ? pdu->hdr.hlen + hdgst : 0;
+	pdu->hdr.plen =
+		cpu_to_le32(pdu->hdr.hlen + hdgst + req->pdu_len + ddgst);
+
+	ret = nvme_tcp_map_data(queue, rq);
+	if (unlikely(ret)) {
+		dev_err(queue->ctrl->ctrl.device,
+			"Failed to map data (%d)\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+static blk_status_t nvme_tcp_queue_rq(struct blk_mq_hw_ctx *hctx,
+		const struct blk_mq_queue_data *bd)
+{
+	struct nvme_ns *ns = hctx->queue->queuedata;
+	struct nvme_tcp_queue *queue = hctx->driver_data;
+	struct request *rq = bd->rq;
+	struct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);
+	bool queue_ready = test_bit(NVME_TCP_Q_LIVE, &queue->flags);
+	blk_status_t ret;
+
+	if (!nvmf_check_ready(&queue->ctrl->ctrl, rq, queue_ready))
+		return nvmf_fail_nonready_command(&queue->ctrl->ctrl, rq);
+
+	ret = nvme_tcp_setup_cmd_pdu(ns, rq);
+	if (unlikely(ret))
+		return ret;
+
+	blk_mq_start_request(rq);
+
+	nvme_tcp_queue_request(req);
+
+	return BLK_STS_OK;
+}
+
+static int nvme_tcp_map_queues(struct blk_mq_tag_set *set)
+{
+	struct nvme_tcp_ctrl *ctrl = set->driver_data;
+
+	set->map[HCTX_TYPE_DEFAULT].queue_offset = 0;
+	set->map[HCTX_TYPE_READ].nr_queues = ctrl->ctrl.queue_count - 1;
+
+	if (ctrl->ctrl.opts->nr_write_queues) {
+		/* separate read/write queues */
+		set->map[HCTX_TYPE_DEFAULT].nr_queues =
+				ctrl->ctrl.opts->nr_write_queues;
+		set->map[HCTX_TYPE_READ].queue_offset =
+				ctrl->ctrl.opts->nr_write_queues;
+	} else {
+		/* mixed read/write queues */
+		set->map[HCTX_TYPE_DEFAULT].nr_queues =
+					ctrl->ctrl.queue_count - 1;
+		set->map[HCTX_TYPE_READ].queue_offset = 0;
+	}
+	blk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);
+	blk_mq_map_queues(&set->map[HCTX_TYPE_READ]);
+	return 0;
+}
+
+static struct blk_mq_ops nvme_tcp_mq_ops = {
+	.queue_rq	= nvme_tcp_queue_rq,
+	.complete	= nvme_complete_rq,
+	.init_request	= nvme_tcp_init_request,
+	.exit_request	= nvme_tcp_exit_request,
+	.init_hctx	= nvme_tcp_init_hctx,
+	.timeout	= nvme_tcp_timeout,
+	.map_queues	= nvme_tcp_map_queues,
+};
+
+static struct blk_mq_ops nvme_tcp_admin_mq_ops = {
+	.queue_rq	= nvme_tcp_queue_rq,
+	.complete	= nvme_complete_rq,
+	.init_request	= nvme_tcp_init_request,
+	.exit_request	= nvme_tcp_exit_request,
+	.init_hctx	= nvme_tcp_init_admin_hctx,
+	.timeout	= nvme_tcp_timeout,
+};
+
+static const struct nvme_ctrl_ops nvme_tcp_ctrl_ops = {
+	.name			= "tcp",
+	.module			= THIS_MODULE,
+	.flags			= NVME_F_FABRICS,
+	.reg_read32		= nvmf_reg_read32,
+	.reg_read64		= nvmf_reg_read64,
+	.reg_write32		= nvmf_reg_write32,
+	.free_ctrl		= nvme_tcp_free_ctrl,
+	.submit_async_event	= nvme_tcp_submit_async_event,
+	.delete_ctrl		= nvme_tcp_delete_ctrl,
+	.get_address		= nvmf_get_address,
+};
+
+static bool
+nvme_tcp_existing_controller(struct nvmf_ctrl_options *opts)
+{
+	struct nvme_tcp_ctrl *ctrl;
+	bool found = false;
+
+	mutex_lock(&nvme_tcp_ctrl_mutex);
+	list_for_each_entry(ctrl, &nvme_tcp_ctrl_list, list) {
+		found = nvmf_ip_options_match(&ctrl->ctrl, opts);
+		if (found)
+			break;
+	}
+	mutex_unlock(&nvme_tcp_ctrl_mutex);
+
+	return found;
+}
+
+static struct nvme_ctrl *nvme_tcp_create_ctrl(struct device *dev,
+		struct nvmf_ctrl_options *opts)
+{
+	struct nvme_tcp_ctrl *ctrl;
+	int ret;
+
+	ctrl = kzalloc(sizeof(*ctrl), GFP_KERNEL);
+	if (!ctrl)
+		return ERR_PTR(-ENOMEM);
+
+	pr_err("%s: tcp ctrl %px\n", __FUNCTION__, ctrl);
+
+	INIT_LIST_HEAD(&ctrl->list);
+	ctrl->ctrl.opts = opts;
+	ctrl->ctrl.queue_count = opts->nr_io_queues + opts->nr_write_queues + 1;
+	ctrl->ctrl.sqsize = opts->queue_size - 1;
+	ctrl->ctrl.kato = opts->kato;
+
+	INIT_DELAYED_WORK(&ctrl->connect_work,
+			nvme_tcp_reconnect_ctrl_work);
+	INIT_WORK(&ctrl->err_work, nvme_tcp_error_recovery_work);
+	INIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);
+
+	if (!(opts->mask & NVMF_OPT_TRSVCID)) {
+		opts->trsvcid =
+			kstrdup(__stringify(NVME_TCP_DISC_PORT), GFP_KERNEL);
+		if (!opts->trsvcid) {
+			ret = -ENOMEM;
+			goto out_free_ctrl;
+		}
+		opts->mask |= NVMF_OPT_TRSVCID;
+	}
+
+	ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
+			opts->traddr, opts->trsvcid, &ctrl->addr);
+	if (ret) {
+		pr_err("malformed address passed: %s:%s\n",
+			opts->traddr, opts->trsvcid);
+		goto out_free_ctrl;
+	}
+
+	if (opts->mask & NVMF_OPT_HOST_TRADDR) {
+		ret = inet_pton_with_scope(&init_net, AF_UNSPEC,
+			opts->host_traddr, NULL, &ctrl->src_addr);
+		if (ret) {
+			pr_err("malformed src address passed: %s\n",
+			       opts->host_traddr);
+			goto out_free_ctrl;
+		}
+	}
+
+	if (!opts->duplicate_connect && nvme_tcp_existing_controller(opts)) {
+		ret = -EALREADY;
+		goto out_free_ctrl;
+	}
+
+	ctrl->queues = kcalloc(ctrl->ctrl.queue_count, sizeof(*ctrl->queues),
+				GFP_KERNEL);
+	if (!ctrl->queues) {
+		ret = -ENOMEM;
+		goto out_free_ctrl;
+	}
+
+	ret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_tcp_ctrl_ops, 0);
+	if (ret)
+		goto out_kfree_queues;
+
+	if (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {
+		WARN_ON_ONCE(1);
+		ret = -EINTR;
+		goto out_uninit_ctrl;
+	}
+
+	ret = nvme_tcp_setup_ctrl(&ctrl->ctrl, true);
+	if (ret)
+		goto out_uninit_ctrl;
+
+	dev_info(ctrl->ctrl.device, "new ctrl: NQN \"%s\", addr %pISp\n",
+		ctrl->ctrl.opts->subsysnqn, &ctrl->addr);
+
+	nvme_get_ctrl(&ctrl->ctrl);
+
+	mutex_lock(&nvme_tcp_ctrl_mutex);
+	list_add_tail(&ctrl->list, &nvme_tcp_ctrl_list);
+	mutex_unlock(&nvme_tcp_ctrl_mutex);
+
+	return &ctrl->ctrl;
+
+out_uninit_ctrl:
+	nvme_uninit_ctrl(&ctrl->ctrl);
+	nvme_put_ctrl(&ctrl->ctrl);
+	if (ret > 0)
+		ret = -EIO;
+	return ERR_PTR(ret);
+out_kfree_queues:
+	kfree(ctrl->queues);
+out_free_ctrl:
+	kfree(ctrl);
+	return ERR_PTR(ret);
+}
+
+static struct nvmf_transport_ops nvme_tcp_transport = {
+	.name		= "tcp",
+	.module		= THIS_MODULE,
+	.required_opts	= NVMF_OPT_TRADDR,
+	.allowed_opts	= NVMF_OPT_TRSVCID | NVMF_OPT_RECONNECT_DELAY |
+			  NVMF_OPT_HOST_TRADDR | NVMF_OPT_CTRL_LOSS_TMO |
+			  NVMF_OPT_HDR_DIGEST | NVMF_OPT_DATA_DIGEST |
+			  NVMF_OPT_NR_WRITE_QUEUES,
+	.create_ctrl	= nvme_tcp_create_ctrl,
+};
+
+static int __init nvme_tcp_init_module(void)
+{
+	nvme_tcp_wq = alloc_workqueue("nvme_tcp_wq",
+			WQ_MEM_RECLAIM | WQ_HIGHPRI, 0);
+	if (!nvme_tcp_wq)
+		return -ENOMEM;
+
+	nvmf_register_transport(&nvme_tcp_transport);
+	return 0;
+}
+
+static void __exit nvme_tcp_cleanup_module(void)
+{
+	struct nvme_tcp_ctrl *ctrl;
+
+	nvmf_unregister_transport(&nvme_tcp_transport);
+
+	mutex_lock(&nvme_tcp_ctrl_mutex);
+	list_for_each_entry(ctrl, &nvme_tcp_ctrl_list, list)
+		nvme_delete_ctrl(&ctrl->ctrl);
+	mutex_unlock(&nvme_tcp_ctrl_mutex);
+	flush_workqueue(nvme_delete_wq);
+
+	destroy_workqueue(nvme_tcp_wq);
+}
+
+module_init(nvme_tcp_init_module);
+module_exit(nvme_tcp_cleanup_module);
+
+MODULE_LICENSE("GPL v2");
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/trace.c b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/trace.c
new file mode 100644
index 0000000..5f24ea7
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/trace.c
@@ -0,0 +1,150 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * NVM Express device driver tracepoints
+ * Copyright (c) 2018 Johannes Thumshirn, SUSE Linux GmbH
+ */
+
+#include <asm/unaligned.h>
+#include "trace.h"
+
+static const char *nvme_trace_create_sq(struct trace_seq *p, u8 *cdw10)
+{
+	const char *ret = trace_seq_buffer_ptr(p);
+	u16 sqid = get_unaligned_le16(cdw10);
+	u16 qsize = get_unaligned_le16(cdw10 + 2);
+	u16 sq_flags = get_unaligned_le16(cdw10 + 4);
+	u16 cqid = get_unaligned_le16(cdw10 + 6);
+
+
+	trace_seq_printf(p, "sqid=%u, qsize=%u, sq_flags=0x%x, cqid=%u",
+			 sqid, qsize, sq_flags, cqid);
+	trace_seq_putc(p, 0);
+
+	return ret;
+}
+
+static const char *nvme_trace_create_cq(struct trace_seq *p, u8 *cdw10)
+{
+	const char *ret = trace_seq_buffer_ptr(p);
+	u16 cqid = get_unaligned_le16(cdw10);
+	u16 qsize = get_unaligned_le16(cdw10 + 2);
+	u16 cq_flags = get_unaligned_le16(cdw10 + 4);
+	u16 irq_vector = get_unaligned_le16(cdw10 + 6);
+
+	trace_seq_printf(p, "cqid=%u, qsize=%u, cq_flags=0x%x, irq_vector=%u",
+			 cqid, qsize, cq_flags, irq_vector);
+	trace_seq_putc(p, 0);
+
+	return ret;
+}
+
+static const char *nvme_trace_admin_identify(struct trace_seq *p, u8 *cdw10)
+{
+	const char *ret = trace_seq_buffer_ptr(p);
+	u8 cns = cdw10[0];
+	u16 ctrlid = get_unaligned_le16(cdw10 + 2);
+
+	trace_seq_printf(p, "cns=%u, ctrlid=%u", cns, ctrlid);
+	trace_seq_putc(p, 0);
+
+	return ret;
+}
+
+static const char *nvme_trace_admin_get_features(struct trace_seq *p,
+						 u8 *cdw10)
+{
+	const char *ret = trace_seq_buffer_ptr(p);
+	u8 fid = cdw10[0];
+	u8 sel = cdw10[1] & 0x7;
+	u32 cdw11 = get_unaligned_le32(cdw10 + 4);
+
+	trace_seq_printf(p, "fid=0x%x sel=0x%x cdw11=0x%x", fid, sel, cdw11);
+	trace_seq_putc(p, 0);
+
+	return ret;
+}
+
+static const char *nvme_trace_read_write(struct trace_seq *p, u8 *cdw10)
+{
+	const char *ret = trace_seq_buffer_ptr(p);
+	u64 slba = get_unaligned_le64(cdw10);
+	u16 length = get_unaligned_le16(cdw10 + 8);
+	u16 control = get_unaligned_le16(cdw10 + 10);
+	u32 dsmgmt = get_unaligned_le32(cdw10 + 12);
+	u32 reftag = get_unaligned_le32(cdw10 +  16);
+
+	trace_seq_printf(p,
+			 "slba=%llu, len=%u, ctrl=0x%x, dsmgmt=%u, reftag=%u",
+			 slba, length, control, dsmgmt, reftag);
+	trace_seq_putc(p, 0);
+
+	return ret;
+}
+
+static const char *nvme_trace_dsm(struct trace_seq *p, u8 *cdw10)
+{
+	const char *ret = trace_seq_buffer_ptr(p);
+
+	trace_seq_printf(p, "nr=%u, attributes=%u",
+			 get_unaligned_le32(cdw10),
+			 get_unaligned_le32(cdw10 + 4));
+	trace_seq_putc(p, 0);
+
+	return ret;
+}
+
+static const char *nvme_trace_common(struct trace_seq *p, u8 *cdw10)
+{
+	const char *ret = trace_seq_buffer_ptr(p);
+
+	trace_seq_printf(p, "cdw10=%*ph", 24, cdw10);
+	trace_seq_putc(p, 0);
+
+	return ret;
+}
+
+const char *nvme_trace_parse_admin_cmd(struct trace_seq *p,
+				       u8 opcode, u8 *cdw10)
+{
+	switch (opcode) {
+	case nvme_admin_create_sq:
+		return nvme_trace_create_sq(p, cdw10);
+	case nvme_admin_create_cq:
+		return nvme_trace_create_cq(p, cdw10);
+	case nvme_admin_identify:
+		return nvme_trace_admin_identify(p, cdw10);
+	case nvme_admin_get_features:
+		return nvme_trace_admin_get_features(p, cdw10);
+	default:
+		return nvme_trace_common(p, cdw10);
+	}
+}
+
+const char *nvme_trace_parse_nvm_cmd(struct trace_seq *p,
+				     u8 opcode, u8 *cdw10)
+{
+	switch (opcode) {
+	case nvme_cmd_read:
+	case nvme_cmd_write:
+	case nvme_cmd_write_zeroes:
+		return nvme_trace_read_write(p, cdw10);
+	case nvme_cmd_dsm:
+		return nvme_trace_dsm(p, cdw10);
+	default:
+		return nvme_trace_common(p, cdw10);
+	}
+}
+
+const char *nvme_trace_disk_name(struct trace_seq *p, char *name)
+{
+	const char *ret = trace_seq_buffer_ptr(p);
+
+	if (*name)
+		trace_seq_printf(p, "disk=%s, ", name);
+	trace_seq_putc(p, 0);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(nvme_trace_disk_name);
+
+EXPORT_TRACEPOINT_SYMBOL_GPL(nvme_sq);
diff --git a/PDK/driver/PCIe/kernel_driver/kernel_v5.1/trace.h b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/trace.h
new file mode 100644
index 0000000..97d3c77
--- /dev/null
+++ b/PDK/driver/PCIe/kernel_driver/kernel_v5.1/trace.h
@@ -0,0 +1,210 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * NVM Express device driver tracepoints
+ * Copyright (c) 2018 Johannes Thumshirn, SUSE Linux GmbH
+ */
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM nvme
+
+#if !defined(_TRACE_NVME_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_NVME_H
+
+#include <linux/nvme.h>
+#include <linux/tracepoint.h>
+#include <linux/trace_seq.h>
+
+#include "nvme.h"
+
+#define nvme_admin_opcode_name(opcode)	{ opcode, #opcode }
+#define show_admin_opcode_name(val)					\
+	__print_symbolic(val,						\
+		nvme_admin_opcode_name(nvme_admin_delete_sq),		\
+		nvme_admin_opcode_name(nvme_admin_create_sq),		\
+		nvme_admin_opcode_name(nvme_admin_get_log_page),	\
+		nvme_admin_opcode_name(nvme_admin_delete_cq),		\
+		nvme_admin_opcode_name(nvme_admin_create_cq),		\
+		nvme_admin_opcode_name(nvme_admin_identify),		\
+		nvme_admin_opcode_name(nvme_admin_abort_cmd),		\
+		nvme_admin_opcode_name(nvme_admin_set_features),	\
+		nvme_admin_opcode_name(nvme_admin_get_features),	\
+		nvme_admin_opcode_name(nvme_admin_async_event),		\
+		nvme_admin_opcode_name(nvme_admin_ns_mgmt),		\
+		nvme_admin_opcode_name(nvme_admin_activate_fw),		\
+		nvme_admin_opcode_name(nvme_admin_download_fw),		\
+		nvme_admin_opcode_name(nvme_admin_ns_attach),		\
+		nvme_admin_opcode_name(nvme_admin_keep_alive),		\
+		nvme_admin_opcode_name(nvme_admin_directive_send),	\
+		nvme_admin_opcode_name(nvme_admin_directive_recv),	\
+		nvme_admin_opcode_name(nvme_admin_dbbuf),		\
+		nvme_admin_opcode_name(nvme_admin_format_nvm),		\
+		nvme_admin_opcode_name(nvme_admin_security_send),	\
+		nvme_admin_opcode_name(nvme_admin_security_recv),	\
+		nvme_admin_opcode_name(nvme_admin_sanitize_nvm))
+
+#define nvme_opcode_name(opcode)	{ opcode, #opcode }
+#define show_nvm_opcode_name(val)				\
+	__print_symbolic(val,					\
+		nvme_opcode_name(nvme_cmd_flush),		\
+		nvme_opcode_name(nvme_cmd_write),		\
+		nvme_opcode_name(nvme_cmd_read),		\
+		nvme_opcode_name(nvme_cmd_write_uncor),		\
+		nvme_opcode_name(nvme_cmd_compare),		\
+		nvme_opcode_name(nvme_cmd_write_zeroes),	\
+		nvme_opcode_name(nvme_cmd_dsm),			\
+		nvme_opcode_name(nvme_cmd_resv_register),	\
+		nvme_opcode_name(nvme_cmd_resv_report),		\
+		nvme_opcode_name(nvme_cmd_resv_acquire),	\
+		nvme_opcode_name(nvme_cmd_resv_release))
+
+#define show_opcode_name(qid, opcode)					\
+	(qid ? show_nvm_opcode_name(opcode) : show_admin_opcode_name(opcode))
+
+const char *nvme_trace_parse_admin_cmd(struct trace_seq *p, u8 opcode,
+		u8 *cdw10);
+const char *nvme_trace_parse_nvm_cmd(struct trace_seq *p, u8 opcode,
+		u8 *cdw10);
+
+#define parse_nvme_cmd(qid, opcode, cdw10) 			\
+	(qid ?							\
+	 nvme_trace_parse_nvm_cmd(p, opcode, cdw10) : 		\
+	 nvme_trace_parse_admin_cmd(p, opcode, cdw10))
+
+const char *nvme_trace_disk_name(struct trace_seq *p, char *name);
+#define __print_disk_name(name)				\
+	nvme_trace_disk_name(p, name)
+
+#ifndef TRACE_HEADER_MULTI_READ
+static inline void __assign_disk_name(char *name, struct gendisk *disk)
+{
+	if (disk)
+		memcpy(name, disk->disk_name, DISK_NAME_LEN);
+	else
+		memset(name, 0, DISK_NAME_LEN);
+}
+#endif
+
+TRACE_EVENT(nvme_setup_cmd,
+	    TP_PROTO(struct request *req, struct nvme_command *cmd),
+	    TP_ARGS(req, cmd),
+	    TP_STRUCT__entry(
+		__array(char, disk, DISK_NAME_LEN)
+		__field(int, ctrl_id)
+		__field(int, qid)
+		__field(u8, opcode)
+		__field(u8, flags)
+		__field(u16, cid)
+		__field(u32, nsid)
+		__field(u64, metadata)
+		__array(u8, cdw10, 24)
+	    ),
+	    TP_fast_assign(
+		__entry->ctrl_id = nvme_req(req)->ctrl->instance;
+		__entry->qid = nvme_req_qid(req);
+		__entry->opcode = cmd->common.opcode;
+		__entry->flags = cmd->common.flags;
+		__entry->cid = cmd->common.command_id;
+		__entry->nsid = le32_to_cpu(cmd->common.nsid);
+		__entry->metadata = le64_to_cpu(cmd->common.metadata);
+		__assign_disk_name(__entry->disk, req->rq_disk);
+		memcpy(__entry->cdw10, &cmd->common.cdw10,
+			sizeof(__entry->cdw10));
+	    ),
+	    TP_printk("nvme%d: %sqid=%d, cmdid=%u, nsid=%u, flags=0x%x, meta=0x%llx, cmd=(%s %s)",
+		      __entry->ctrl_id, __print_disk_name(__entry->disk),
+		      __entry->qid, __entry->cid, __entry->nsid,
+		      __entry->flags, __entry->metadata,
+		      show_opcode_name(__entry->qid, __entry->opcode),
+		      parse_nvme_cmd(__entry->qid, __entry->opcode, __entry->cdw10))
+);
+
+TRACE_EVENT(nvme_complete_rq,
+	    TP_PROTO(struct request *req),
+	    TP_ARGS(req),
+	    TP_STRUCT__entry(
+		__array(char, disk, DISK_NAME_LEN)
+		__field(int, ctrl_id)
+		__field(int, qid)
+		__field(int, cid)
+		__field(u64, result)
+		__field(u8, retries)
+		__field(u8, flags)
+		__field(u16, status)
+	    ),
+	    TP_fast_assign(
+		__entry->ctrl_id = nvme_req(req)->ctrl->instance;
+		__entry->qid = nvme_req_qid(req);
+		__entry->cid = req->tag;
+		__entry->result = le64_to_cpu(nvme_req(req)->result.u64);
+		__entry->retries = nvme_req(req)->retries;
+		__entry->flags = nvme_req(req)->flags;
+		__entry->status = nvme_req(req)->status;
+		__assign_disk_name(__entry->disk, req->rq_disk);
+	    ),
+	    TP_printk("nvme%d: %sqid=%d, cmdid=%u, res=%llu, retries=%u, flags=0x%x, status=%u",
+		      __entry->ctrl_id, __print_disk_name(__entry->disk),
+		      __entry->qid, __entry->cid, __entry->result,
+		      __entry->retries, __entry->flags, __entry->status)
+
+);
+
+#define aer_name(aer) { aer, #aer }
+
+TRACE_EVENT(nvme_async_event,
+	TP_PROTO(struct nvme_ctrl *ctrl, u32 result),
+	TP_ARGS(ctrl, result),
+	TP_STRUCT__entry(
+		__field(int, ctrl_id)
+		__field(u32, result)
+	),
+	TP_fast_assign(
+		__entry->ctrl_id = ctrl->instance;
+		__entry->result = result;
+	),
+	TP_printk("nvme%d: NVME_AEN=%#08x [%s]",
+		__entry->ctrl_id, __entry->result,
+		__print_symbolic(__entry->result,
+		aer_name(NVME_AER_NOTICE_NS_CHANGED),
+		aer_name(NVME_AER_NOTICE_ANA),
+		aer_name(NVME_AER_NOTICE_FW_ACT_STARTING),
+		aer_name(NVME_AER_ERROR),
+		aer_name(NVME_AER_SMART),
+		aer_name(NVME_AER_CSS),
+		aer_name(NVME_AER_VS))
+	)
+);
+
+#undef aer_name
+
+TRACE_EVENT(nvme_sq,
+	TP_PROTO(struct request *req, __le16 sq_head, int sq_tail),
+	TP_ARGS(req, sq_head, sq_tail),
+	TP_STRUCT__entry(
+		__field(int, ctrl_id)
+		__array(char, disk, DISK_NAME_LEN)
+		__field(int, qid)
+		__field(u16, sq_head)
+		__field(u16, sq_tail)
+	),
+	TP_fast_assign(
+		__entry->ctrl_id = nvme_req(req)->ctrl->instance;
+		__assign_disk_name(__entry->disk, req->rq_disk);
+		__entry->qid = nvme_req_qid(req);
+		__entry->sq_head = le16_to_cpu(sq_head);
+		__entry->sq_tail = sq_tail;
+	),
+	TP_printk("nvme%d: %sqid=%d, head=%u, tail=%u",
+		__entry->ctrl_id, __print_disk_name(__entry->disk),
+		__entry->qid, __entry->sq_head, __entry->sq_tail
+	)
+);
+
+#endif /* _TRACE_NVME_H */
+
+#undef TRACE_INCLUDE_PATH
+#define TRACE_INCLUDE_PATH .
+#undef TRACE_INCLUDE_FILE
+#define TRACE_INCLUDE_FILE trace
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/README.md b/README.md
index c022f0d..390e69f 100644
--- a/README.md
+++ b/README.md
@@ -41,7 +41,7 @@ KVSSD
    |- PDK (Platform Development Kit)
    |    |- core
    |    |    |- src
-   |    |        |- api : SAMSUNG KV API (Application Programming Interface) & SNIA KV Storage API
+   |    |        |- api : KV API (Application Programming Interface) & SNIA KV Storage API
    |    |        |- device abstract layer
    |    |             |- emulator : KV SSD emulator
    |    |             |- kernel_driver_adapater : Kernel driver adapter
diff --git a/RELEASE_HISTORY b/RELEASE_HISTORY
index a0ca2a9..4876869 100644
--- a/RELEASE_HISTORY
+++ b/RELEASE_HISTORY
@@ -1,16 +1,5 @@
 
 Release History
----   v1.4.0
-     Add container support
-     Add support for CentOS 6.6 kernel driver v2.6.32, Centos7.1 kernel driver v3.10.0-229, kernel driver v4.14.81, and Centos7.7 kernel driver v3.10.0-1062
-     Add partial retrieve support
-     New key generator in kv_bench to guarantees the keys to be read existed
-     Multiple bug fix
-     Remove the sample code and test
-
----   v1.3.0
-   Support SNIA KV API
-   Bug fix
 
 ---   v1.2.0
    Modify the computing method to adapt the changes of Namespace Utilization (NUSE) field of NVMe specification.
diff --git a/application/kvbench/README b/application/kvbench/README
index 8d25480..80bf774 100644
--- a/application/kvbench/README
+++ b/application/kvbench/README
@@ -7,9 +7,6 @@ BUILD ==========================================================================
    0.1  sudo apt-get install libsnappy-dev libev-dev libbz2-dev liblz4-dev libzstd-dev libjemalloc-dev libnuma-dev libgflags-dev libssl-dev libz-dev
    0.2 (CentOS) sudo yum install snappy-devel libev-devel bzip2-devel lz4-devel gflags-devel jemalloc-devel openssl-devel zlib-devel
 
-   You can download and install zstd library from the following link:
-      https://github.com/facebook/zstd
-
 1. RocksDB (Linux filesystem)
   1.1 Build rocksdb from source code
     Download rocksdb source code: https://github.com/facebook/rocksdb
@@ -130,9 +127,6 @@ Step 4. run benchmark
     sudo LD_LIBRARY_PATH=<YOUR_AEROSPIKE_LIB_DIR> ./as_bench -f bench_config.ini
 
     # If you see a "Double allocated SPDK thread" message when running RocksDB on SPDK, it is harmless and can be ignored
-    # If you see a "ERR IO error: While open a file for appending: XXXXXX: Too many open files" message when running RocksDB,
-      it needs to use ulimit command to set the limitation of opening files.
-                  ulimit -Sn 204800
 
 CONFIGURATION =====================================================================
 0. Two phases during each run:
diff --git a/application/kvbench/bench/couch_bench.cc b/application/kvbench/bench/couch_bench.cc
index 7629a22..8f6ee70 100644
--- a/application/kvbench/bench/couch_bench.cc
+++ b/application/kvbench/bench/couch_bench.cc
@@ -11,10 +11,6 @@
 #include <malloc.h>
 #endif
 #include <signal.h>
-#include <limits.h>
-#include <string>
-#include <regex>
-
 #include <dirent.h>
 
 #include "libcouchstore/couch_db.h"
@@ -47,15 +43,6 @@
 #include "thpool.h"
 #endif
 
-bool is_valide_path_name(char* str){
-  if(strnlen(str, PATH_MAX+1)>PATH_MAX){
-    return false;
-  }
-  std::ostringstream pattern;
-  pattern << "^(/[a-zA-Z0-9_.]{1," << NAME_MAX << "})+/?$";
-  return std::regex_match(std::string(str),
-                          std::regex(pattern.str()));
-}
 
 couchstore_error_t couchstore_close_db(Db *db);
 
@@ -202,7 +189,6 @@ struct bench_info {
 
     // synchronous write
     uint8_t sync_write;
-    uint8_t key_existing;
 };
 
 #define MIN(a,b) (((a)<(b))?(a):(b))
@@ -210,7 +196,6 @@ struct bench_info {
 static uint32_t rnd_seed;
 static int print_term_ms = 100;
 static int filesize_chk_term = 4;
-static std::atomic<std::uint64_t> max_key_id;
 
 FILE *log_fp = NULL;
 FILE *insert_latency_fp = NULL;
@@ -318,10 +303,6 @@ uint64_t print_proc_io_stat(char *buf, int print)
     unsigned long temp;
     uint64_t val=0;
     FILE *fp = fopen(buf, "r");
-    if(fp == NULL) {
-      fprintf(stderr, "open %s failed\n", buf);
-      return 0;
-    }
     while(!feof(fp)) {
         ret = fscanf(fp, "%s %lu", str, &temp);
         if (!strcmp(str, "write_bytes:")) {
@@ -351,18 +332,18 @@ int get_value_size_by_ratio(struct bench_info *binfo, size_t idx) {
 
   int value_size = 0;
   if(idx % 100 < binfo->value_size_ratio[0]){
-    value_size = binfo->value_size[0];
+    value_size = binfo->value_size[0] == 0 ? 4096 : binfo->value_size[0];
   } else if (idx % 100 <  binfo->value_size_ratio[0] + binfo->value_size_ratio[1] ) {
-    value_size  = binfo->value_size[1];
+    value_size  = binfo->value_size[1] == 0 ? 4096 : binfo->value_size[1];
   } else if (idx % 100 <  binfo->value_size_ratio[0] + binfo->value_size_ratio[1] + binfo->value_size_ratio[2]){
-    value_size  = binfo->value_size[2];
+    value_size  = binfo->value_size[2] == 0 ? 4096 : binfo->value_size[2];
   } else if (idx % 100 <  binfo->value_size_ratio[0] + binfo->value_size_ratio[1] + binfo->value_size_ratio[2] + binfo->value_size_ratio[3]) {
-    value_size  = binfo->value_size[3];
+    value_size  = binfo->value_size[3] == 0 ? 4096 : binfo->value_size[3];
   } else {
-    value_size  = binfo->value_size[4];
+    value_size  = binfo->value_size[4] == 0 ? 4096 : binfo->value_size[4];
   }
   
-  return value_size; 
+  return value_size == 0 ? 4096 : value_size; 
 }
 
 void _create_doc(struct bench_info *binfo,
@@ -395,10 +376,7 @@ void _create_doc(struct bench_info *binfo,
       //doc->id.buf = (char *)malloc(16);
     }
 
-    if(doc->id.buf == NULL){
-      fprintf(stderr, "No pool elem available for key - %d\n", kp->num_freeblocks);
-      exit(1);
-    }
+    if(doc->id.buf == NULL){ fprintf(stderr, "No pool elem available for key - %d\n", kp->num_freeblocks); exit(1);}
     
     if (binfo->keyfile) {
       doc->id.size = keyloader_get_key(&binfo->kl, idx, doc->id.buf);
@@ -425,11 +403,7 @@ void _create_doc(struct bench_info *binfo,
 	
 	doc->data.buf = (char *)Allocate(vp);
 	//doc->data.buf = (char *)malloc(4096);
-  if(doc->data.buf == NULL) { 
-    fprintf(stderr,
-            "No pool elem available for value - %d\n", vp->num_freeblocks);
-    exit(1);
-  }
+	if(doc->data.buf == NULL){ fprintf(stderr, "No pool elem available for value - %d\n", vp->num_freeblocks); exit(1);}
 	//doc->data.buf = (char*)malloc(4096);
 	//memcpy(doc->data.buf + doc->data.size - 5, (void*)"<end>", 5);
       }
@@ -830,12 +804,8 @@ void * pop_print_time(void *voidargs)
 	  elapsed_ms = (uint64_t)tv.tv_sec * 1000 +
 	    (uint64_t)tv.tv_usec / 1000;
 	  remain_sec = (binfo->ndocs * binfo->nfiles - counter);
-    if(elapsed_ms != 0){
-      remain_sec = remain_sec / MAX(1, (counter / elapsed_ms));
-      remain_sec = remain_sec / 1000;
-    }else{
-      remain_sec = 0;
-    }
+	  remain_sec = remain_sec / MAX(1, (counter / elapsed_ms));
+	  remain_sec = remain_sec / 1000;
 	}
 
 	iops = (double)counter / (tv.tv_sec + tv.tv_usec/1000000.0);
@@ -1048,7 +1018,7 @@ void population(Db **db, struct bench_info *binfo)
     //thpool_wait(thpool);
     thpool_destroy(thpool);
 #endif
-    max_key_id = binfo->ndocs;
+    
     fprintf(stdout, "population done \n");
 
 }
@@ -1158,10 +1128,6 @@ void _bench_result_print(struct bench_result *result)
          sizeof(struct bench_result_hit), _bench_result_cmp);
 
     fp = fopen("result.txt", "w");
-    if(fp == NULL) {
-      fprintf(stderr, "open %s failed\n", "result.txt");
-      return;
-    }
     fprintf(fp, "== files ==\n");
     cum = 0;
     for (i=0;i<result->nfiles;++i){
@@ -1498,21 +1464,7 @@ void * bench_thread(void *voidargs)
   int keylen = (binfo->keylen.type == RND_FIXED)? binfo->keylen.a : 0;
   long int total_entries = 0;
   int iterator_send = 0;
-  uint64_t max_key_index = 0;
-  int singledb_thread_num = binfo->nreaders + binfo->niterators + binfo->nwriters + binfo->ndeleters;
-  uint64_t key_offset = 0;
-
-  if (binfo->key_existing) {
-    if (args->mode == 0) {
-      max_key_index = binfo->ndocs;
-      if ((max_key_index % singledb_thread_num) > (args->id % singledb_thread_num)) {
-        max_key_index = max_key_index / singledb_thread_num + 1;
-      } else {
-        max_key_index = max_key_index / singledb_thread_num;
-      }
-      key_offset = args->id % singledb_thread_num;
-    }
-  }
+  
 #if defined(__BLOBFS_ROCKS_BENCH)
   // Set up SPDK-specific stuff for this thread
   rocksdb::SpdkInitializeThread();
@@ -1527,6 +1479,7 @@ void * bench_thread(void *voidargs)
 #if defined __KV_BENCH 
   db_idx = args->id % binfo->nfiles; // for kvssd, # files = # devices, single thread pre device
 #elif defined __BLOBFS_ROCKS_BENCH
+  int singledb_thread_num = binfo->nreaders + binfo->niterators + binfo->nwriters + binfo->ndeleters;
   db_idx = args->id / singledb_thread_num;
 #else
   db_idx = 0; 
@@ -1564,7 +1517,7 @@ void * bench_thread(void *voidargs)
   //int use_udd;
   if(binfo->with_iterator > 0) {
     if(binfo->kv_write_mode == 1) {
-      fprintf(stdout, "\nWARN: Only support iterator under ASYNC mode\n");
+      fprintf(stdout, "WARN: Only support iterator under ASYNC mode\n");
       exit(0);
     }
     couchstore_iterator_open(db[db_idx], binfo->iterator_mode);
@@ -1655,8 +1608,6 @@ void * bench_thread(void *voidargs)
       } else {
 	if(cur_op_idx % 100 < binfo->ratio[1] + binfo->ratio[2]){
 	  write_mode = 1; // write: update/insert
-      if (binfo->key_existing && cur_op_idx % 100 < binfo->ratio[1])
-        write_mode = 5; // update
 	} else if(cur_op_idx % 100 < binfo->ratio[0] + binfo->ratio[1] + binfo->ratio[2]){
 	  write_mode = 2; // read
 	} else {
@@ -1665,22 +1616,7 @@ void * bench_thread(void *voidargs)
 	cur_op_idx++;
       }
       
-      if(write_mode == 1 || write_mode == 5) { // write
-        if (binfo->key_existing) {
-          if (args->mode == 0) {
-            if (write_mode == 5 && max_key_index != 0) { //update
-              r = r % max_key_index;
-            } else { //insert
-              r = max_key_index++;
-            }
-            r = r * singledb_thread_num + key_offset;
-            write_mode = 1;
-          } else {
-             if (r > max_key_id) {
-               r = max_key_id++;
-             }
-          }
-        }
+      if(write_mode == 1) { // write
 	_create_doc(binfo, r, &rq_doc, NULL, binfo->seq_fill,
 		    args->socketid, args->keypool, args->valuepool);
 
@@ -1729,26 +1665,10 @@ void * bench_thread(void *voidargs)
       } else if (write_mode == 2) { // read
 
 #if defined __ROCKS_BENCH || defined(__KVDB_BENCH)
-	if(rq_id.buf == NULL) {
-    rq_id.buf = (char *)Allocate(args->keypool);
-    if(rq_id.buf == NULL){
-       fprintf(stderr,
-       "Allocate buffer from  keypool failed. "
-       "Please set 'key_pool_size' bigger\n");
-       exit(1);
-    }
-  }
+	if(rq_id.buf == NULL) rq_id.buf = (char *)Allocate(args->keypool);
 	if(binfo->keyfile) {
 	  rq_id.size = keyloader_get_key(&binfo->kl, r, rq_doc->id.buf);
 	}else{
-      if (binfo->key_existing) {
-        if (args->mode == 0 && max_key_index != 0) {
-          r = r % max_key_index;
-          r = r * singledb_thread_num + key_offset;
-        } else if (args->mode > 0 && max_key_id != 0) {
-          r = r % max_key_id;
-        }
-      }
 	  if (binfo->seq_fill){
 	    rq_id.size = binfo->keylen.a;
 	    keygen_seqfill(r, rq_id.buf, binfo->keylen.a);
@@ -1765,14 +1685,6 @@ void * bench_thread(void *voidargs)
 	if (binfo->keyfile) {
 	  rq_doc->id.size = keyloader_get_key(&binfo->kl, r, rq_doc->id.buf);
 	} else {
-      if (binfo->key_existing) {
-        if (args->mode == 0 && max_key_index != 0) {
-          r = r % max_key_index;
-          r = r * singledb_thread_num + key_offset;
-        } else if (args->mode > 0 && max_key_id != 0) {
-          r = r % max_key_id;
-        }
-      }
 	  if (binfo->seq_fill){
 	    rq_doc->id.size = binfo->keylen.a;
 	    keygen_seqfill(r, rq_doc->id.buf, binfo->keylen.a);
@@ -1848,23 +1760,12 @@ void * bench_thread(void *voidargs)
 	}
 	
       } else { // delete
-	if(rq_doc == NULL) {
-	  rq_doc = (Doc *)malloc(sizeof(Doc));
-	  memset(rq_doc, 0, sizeof(Doc));
-	}
+	if(rq_doc == NULL) rq_doc = (Doc *)malloc(sizeof(Doc));
 	if(rq_doc->id.buf == NULL)
 	  rq_doc->id.buf = (char *)Allocate(args->keypool);
 	if (binfo->keyfile) {
 	  rq_doc->id.size = keyloader_get_key(&binfo->kl, r, rq_doc->id.buf);
 	} else {
-          if (binfo->key_existing) {
-            if (args->mode == 0 && max_key_index != 0) {
-              r = --max_key_index;
-              r = r * singledb_thread_num + key_offset;
-            } else if (args->mode > 0 && max_key_id != 0) {
-              r = --max_key_id;
-            }
-          }
 	  if (binfo->seq_fill){
 	    rq_doc->id.size = binfo->keylen.a;
 	    keygen_seqfill(r, rq_doc->id.buf, binfo->keylen.a);
@@ -1973,16 +1874,16 @@ void * bench_thread(void *voidargs)
 	if(binfo->with_iterator == 1 && iterator_send == 0 && args->cur_qdepth < binfo->queue_depth - 1) {
 	  // Do one iterator operation first if (curr qdepth + 1 < max qdepth) 
 	  if (!couchstore_iterator_check_status(db[db_idx])) {
-	    if(couchstore_iterator_has_finish(db[db_idx])) {
+	    if(couchstore_iterator_has_finish) {
 	      couchstore_iterator_next(db[db_idx]);
 	      args->cur_qdepth++;
 	      iterator_send = 1;
 	    }
-	  }  /*else {
+	  } else {
 	    fprintf(stdout, "Iteration Done \n");
 	    got_signal = 1;
 	    break;
-	  }*/
+	  }
 	}
 #endif
 	
@@ -1991,8 +1892,6 @@ void * bench_thread(void *voidargs)
 	} else {
 	  if(cur_op_idx % 100 < binfo->ratio[1] + binfo->ratio[2]){
 	    write_mode = 1; // write: update/insert
-	    if (binfo->key_existing && cur_op_idx % 100 < binfo->ratio[1])
-          write_mode = 5;
 	  } else if(cur_op_idx % 100 < binfo->ratio[0] + binfo->ratio[1] + binfo->ratio[2]){
 	    write_mode = 2; // read
 	  } else {
@@ -2001,21 +1900,7 @@ void * bench_thread(void *voidargs)
 	  cur_op_idx++;
 	}
 	
-	if(write_mode == 1 || write_mode == 5) { // write
-      if (binfo->key_existing) {
-        if (args->mode == 0) {
-          if (write_mode == 5 && max_key_index != 0) {
-            r = r % max_key_index;
-          } else {
-            r = max_key_index++;
-          }
-          r = r * singledb_thread_num + key_offset;
-          write_mode = 1;
-        } else if (args->mode > 0) {
-          if (r > max_key_id)
-            r = max_key_id++;
-        }
-      }
+	if(write_mode == 1) { // write
 	  if(rq_doc == NULL) rq_doc = (Doc *)malloc(sizeof(Doc));
 	  _create_doc(binfo, r, &rq_doc, NULL, binfo->seq_fill,
 		      args->socketid, args->keypool, args->valuepool);
@@ -2043,14 +1928,6 @@ void * bench_thread(void *voidargs)
 	  if (binfo->keyfile) {
 	    rq_doc->id.size = keyloader_get_key(&binfo->kl, r, rq_doc->id.buf);
 	  } else {
-        if (binfo->key_existing) {
-          if (args->mode == 0 && max_key_index != 0) {
-            r = r % max_key_index;
-            r = r * singledb_thread_num + key_offset;
-          } else if (args->mode > 0 && max_key_id != 0) {
-            r = r % max_key_id;
-          }
-        }
 	    if (binfo->seq_fill){
 	      rq_doc->id.size = binfo->keylen.a;
 	      keygen_seqfill(r, rq_doc->id.buf, binfo->keylen.a);
@@ -2092,14 +1969,6 @@ void * bench_thread(void *voidargs)
 	  if (binfo->keyfile) {
 	    rq_doc->id.size = keyloader_get_key(&binfo->kl, r, rq_doc->id.buf);
 	  } else {
-        if (binfo->key_existing) {
-          if (args->mode == 0 && max_key_index != 0) {
-            r = --max_key_index;
-            r = r * singledb_thread_num + key_offset;
-          } else if (args->mode > 0 && max_key_id != 0) {
-            r = --max_key_id;
-          }
-        }
 	    if (binfo->seq_fill){
 	      rq_doc->id.size = binfo->keylen.a;
 	      keygen_seqfill(r, rq_doc->id.buf, binfo->keylen.a);
@@ -2155,11 +2024,11 @@ void * bench_thread(void *voidargs)
 	  total_entries += couchstore_iterator_get_numentries(db[db_idx]);
 	  if(couchstore_iterator_has_finish(db[db_idx]))
 	    iterator_send = 0;
-	  /*
+
 	  if(couchstore_iterator_check_status(db[db_idx])){
 	    got_signal = 1;
 	    break;
-	  }*/
+	  }
 	}
 #endif
       }
@@ -2247,10 +2116,6 @@ void _wait_leveldb_compaction(struct bench_info *binfo, Db **db)
 
     while(1) {
         fp = fopen(buf, "r");
-        if(fp == NULL) {
-          fprintf(stderr, "open %s failed\n", buf);
-          return;
-        }
         while(!feof(fp)) {
             ret = fscanf(fp, "%s %lu", str, &temp);
             if (!strcmp(str, "write_bytes:")) {
@@ -2311,7 +2176,6 @@ couchstore_error_t couchstore_kvs_set_aio_option(int queue_depth, char *core_mas
 couchstore_error_t couchstore_kvs_set_aiothreads(int aio_threads);
 couchstore_error_t couchstore_kvs_set_coremask(char *core_ids);
 couchstore_error_t couchstore_kvs_get_aiocompletion(int32_t *count);
-couchstore_error_t couchstore_kvs_set_max_sample(uint32_t sample_num);
 
 static int _does_file_exist(char *filename) {
     struct stat st;
@@ -2536,7 +2400,6 @@ void db_env_setup(struct bench_info *binfo){
     couchstore_kvs_set_aio_option(binfo->queue_depth, binfo->core_ids, binfo->cq_thread_ids, binfo->mem_size_mb);
     couchstore_kvs_set_aiothreads(binfo->aiothreads_per_device);
     couchstore_kvs_set_coremask(binfo->core_ids);
-    couchstore_kvs_set_max_sample(binfo->latency_max);
     //}
     couchstore_setup_device(binfo->kv_device_path, NULL, binfo->kv_emul_configfile, binfo->nfiles, binfo->kv_write_mode, 0/*binfo->is_polling*/);
 #endif
@@ -2930,9 +2793,11 @@ void do_bench(struct bench_info *binfo)
 
     b_args[i].keypool = (mempool_t *)malloc(sizeof(mempool_t));
     b_args[i].keypool->base = b_args[i].keypool->nextfreeblock = NULL;
+    pool_setup(&info_key, b_args[i].keypool, nodeid);
 
     b_args[i].valuepool = (mempool_t *)malloc(sizeof(mempool_t));
     b_args[i].valuepool->base = b_args[i].valuepool->nextfreeblock = NULL;
+    pool_setup(&info_value, b_args[i].valuepool, nodeid);
     
     //open db instances
 #if defined(__FDB_BENCH) || defined(__COUCH_BENCH) || defined(__WT_BENCH)
@@ -2994,9 +2859,6 @@ void do_bench(struct bench_info *binfo)
     //nodeid = 0;
     pthread_attr_init(&attr[i]);
     b_args[i].socketid = nodeid;
-    // initialize key/value pool when nodeid is valid value
-    pool_setup(&info_key, b_args[i].keypool, nodeid);
-    pool_setup(&info_value, b_args[i].valuepool, nodeid);
 
     int coreid[64] = { 0 };
     int core_count = 0;
@@ -3372,12 +3234,12 @@ void do_bench(struct bench_info *binfo)
 	    (double)op_count_delete / gap_double,
 	    gap_double * 1000000 / op_count_delete);
   }
-  if (binfo->with_iterator != 2) {
-    lprintf("total %" _F64 " operations performed\n",
+
+  lprintf("total %" _F64 " operations performed\n",
 	  op_count_read + op_count_write + op_count_delete);
 
-    lprintf("Throughput(Benchmark) %.2f ops/sec\n", (double)(op_count_read + op_count_write + op_count_delete) / gap_double);
-  }
+  lprintf("Throughput(Benchmark) %.2f ops/sec\n", (double)(op_count_read + op_count_write + op_count_delete) / gap_double);
+
   if(op_count_iter_key > 0) {
     lprintf("Throughput(Iterator)  %.2f keys/sec; total %ld keys\n", (double)(op_count_iter_key) / gap_double, op_count_iter_key);
   }
@@ -4067,20 +3929,10 @@ struct bench_info get_benchinfo(char* bench_config_filename, int config_only)
     binfo.kp_numunits = iniparser_getint(cfg, (char*)"system:key_pool_size", 128);
     binfo.kp_unitsize = iniparser_getint(cfg, (char*)"system:key_pool_unit", 16);
     binfo.kp_alignment = iniparser_getint(cfg, (char*)"system:key_pool_alignment", 4096);
-    if(binfo.kp_alignment <= 0) {
-      fprintf(stderr,
-      "ERROR: system:key_pool_alignment must be greater than 0\n");
-      exit(1);
-    }
-
     binfo.vp_numunits = iniparser_getint(cfg, (char*)"system:value_pool_size", 128);
     binfo.vp_unitsize = iniparser_getint(cfg, (char*)"system:value_pool_unit", 4096);
     binfo.vp_alignment = iniparser_getint(cfg, (char*)"system:value_pool_alignment", 4096);
-    if(binfo.vp_alignment <= 0) {
-      fprintf(stderr,
-      "ERROR: system:value_pool_alignment must be greater than 0\n");
-      exit(1);
-    }
+
     str = iniparser_getstring(cfg, (char*)"kvs:write_mode", (char*)"sync");
     binfo.kv_write_mode = (str[0]=='s')?(1):(0);
 
@@ -4092,18 +3944,15 @@ struct bench_info get_benchinfo(char* bench_config_filename, int config_only)
     strcpy(binfo.device_path, str);
 
     binfo.device_name = (char **)malloc(sizeof(char*) * 256);
-#ifndef __KV_BENCH
     i = 0;
     pt = strtok (str, ",");
     while(pt != NULL){// && i < binfo.nfiles) {
       binfo.device_name[i] = (char *)malloc(256);
       devname_ret = _get_filename_pos(pt);
-      //strcpy(binfo.device_name[i++], devname_ret ? devname_ret : pt);
-      snprintf(binfo.device_name[i++], 256, "%s", devname_ret ? devname_ret : pt);
+      strcpy(binfo.device_name[i++], devname_ret ? devname_ret : pt);
       pt = strtok(NULL, ",");
     }
     binfo.nfiles = i;
-#endif
 
 #if defined(__KV_BENCH) || defined (__AS_BENCH) || defined (__KVROCKS_BENCH)
     str = iniparser_getstring(cfg, (char*)"kvs:store_option", (char*)"post");
@@ -4118,7 +3967,7 @@ struct bench_info get_benchinfo(char* bench_config_filename, int config_only)
 	iniparser_free(cfg);
 	exit(1);
       } else if (binfo.queue_depth > binfo.kp_numunits || binfo.queue_depth > binfo.vp_numunits) {
-	fprintf(stderr, "WARN: Please set key/value pool size equal to or greater than the queue_depth: %d\n", binfo.queue_depth);
+	fprintf(stderr, "WARN: Please set key/value pool unit equal to or greater than the queue_depth: %d\n", binfo.queue_depth);
 	iniparser_free(cfg);
 	exit(1);
       }
@@ -4135,18 +3984,7 @@ struct bench_info get_benchinfo(char* bench_config_filename, int config_only)
     binfo.mem_size_mb = iniparser_getint(cfg, (char*)"kvs:mem_size_mb", 1024);
     str = iniparser_getstring(cfg, (char*)"kvs:device_path", (char*)"");
     strcpy(binfo.kv_device_path, str);
-#ifdef __KV_BENCH
-    i = 0;
-    pt = strtok (str, ",");
-    while(pt != NULL){
-      binfo.device_name[i] = (char *)malloc(256);
-      devname_ret = _get_filename_pos(pt);
-      //strcpy(binfo.device_name[i++], devname_ret ? devname_ret : pt);
-      snprintf(binfo.device_name[i++], 256, "%s", devname_ret ? devname_ret : pt);
-      pt = strtok(NULL, ",");
-    }
-    binfo.nfiles = i;
-#endif
+    
     str = iniparser_getstring(cfg, (char*)"kvs:emul_configfile", (char*)"/tmp/kvemul.conf");
     strcpy(binfo.kv_emul_configfile, str);
     
@@ -4281,29 +4119,6 @@ struct bench_info get_benchinfo(char* bench_config_filename, int config_only)
 
     str = iniparser_getstring(cfg, (char*)"db_file:filename",
                                    (char*)"./dummy");
-
-#if defined(__ROCKS_BENCH)
-    if(!is_valide_path_name(str)){
-      fprintf(stderr,
-        "ERROR: filename as below is invalid.\n[db_file]\nfilename = %s\n", str);
-      exit(1);
-    }
-    if(access(str, F_OK)){
-      fprintf(stderr,
-        "ERROR: file does not exist.\n[db_file]\nfilename = %s\n", str);
-      exit(1);
-    }
-
-    struct stat st;
-    stat(str,&st);
-    if (!S_ISDIR(st.st_mode)){
-      fprintf(stderr,
-        "ERROR: filename as below must be a directory for rocksdb_bench."
-        "\n[db_file]\nfilename = %s\n", str);
-      exit(1);
-    }
-#endif
-
     char dirname[256], *dirname_ret, *filename_ret;
     dirname_ret = _get_dirname(str, dirname);
     filename_ret = _get_filename_pos(str);
@@ -4533,7 +4348,7 @@ struct bench_info get_benchinfo(char* bench_config_filename, int config_only)
       exit(1);
     }
     if(binfo.vp_unitsize == 0 || binfo.kp_unitsize == 0){
-      fprintf(stderr, "WARN: Invalid memory pool size, should be greater than 0\n");
+      fprintf(stderr, "WARN: Invalide memory pool size, should be greater than 0\n");
       iniparser_free(cfg);
       exit(1);
     }
@@ -4709,7 +4524,6 @@ struct bench_info get_benchinfo(char* bench_config_filename, int config_only)
     str = iniparser_getstring(cfg, (char*)"operation:write_type",
                                    (char*)"sync");
     binfo.sync_write = (str[0]=='s')?(1):(0);
-    binfo.key_existing = iniparser_getboolean(cfg, (char*)"operation:key_existing", false);
 
     binfo.compact_thres =
         iniparser_getint(cfg, (char*)"compaction:threshold", 30);
@@ -4740,20 +4554,10 @@ struct bench_info get_benchinfo(char* bench_config_filename, int config_only)
     }
     binfo.latency_max =
         iniparser_getint(cfg, (char*)"latency_monitor:max_samples", 1000000);
-    if (!binfo.latency_max) {
-      printf("WARN: max_sample cannot be 0\n");
-      iniparser_free(cfg);
-      exit(0);
-    }
+
     print_term_ms =
         iniparser_getint(cfg, (char*)"latency_monitor:print_term_ms", 100);
-    if (!print_term_ms) {
-      printf("WARN: print_term_ms cannot be 0\n");
-      iniparser_free(cfg);
-      exit(0);
-    }
-    if (binfo.bench_secs != 0 && print_term_ms > binfo.bench_secs * 1000)
-      print_term_ms = binfo.bench_secs * 1000;
+
     iniparser_free(cfg);
     return binfo;
 }
diff --git a/application/kvbench/bench_config.ini b/application/kvbench/bench_config.ini
index f97611a..ab6e5fa 100644
--- a/application/kvbench/bench_config.ini
+++ b/application/kvbench/bench_config.ini
@@ -123,7 +123,6 @@ write_batchsize_upper_bound = 1
 
 read_write_insert_delete = 50:50:0:0
 write_type = sync
-key_existing = true
 
 [compaction]
 threshold = 50
diff --git a/application/kvbench/spdk/rocksdb/buckifier/rocks_test_runner.sh b/application/kvbench/spdk/rocksdb/buckifier/rocks_test_runner.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/build_tools/amalgamate.py b/application/kvbench/spdk/rocksdb/build_tools/amalgamate.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/build_tools/build_detect_platform b/application/kvbench/spdk/rocksdb/build_tools/build_detect_platform
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/build_tools/cont_integration.sh b/application/kvbench/spdk/rocksdb/build_tools/cont_integration.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/build_tools/dockerbuild.sh b/application/kvbench/spdk/rocksdb/build_tools/dockerbuild.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/build_tools/fb_compile_mongo.sh b/application/kvbench/spdk/rocksdb/build_tools/fb_compile_mongo.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/build_tools/format-diff.sh b/application/kvbench/spdk/rocksdb/build_tools/format-diff.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/build_tools/gnu_parallel b/application/kvbench/spdk/rocksdb/build_tools/gnu_parallel
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/build_tools/make_package.sh b/application/kvbench/spdk/rocksdb/build_tools/make_package.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/build_tools/precommit_checker.py b/application/kvbench/spdk/rocksdb/build_tools/precommit_checker.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/build_tools/regression_build_test.sh b/application/kvbench/spdk/rocksdb/build_tools/regression_build_test.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/build_tools/rocksdb-lego-determinator b/application/kvbench/spdk/rocksdb/build_tools/rocksdb-lego-determinator
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/build_tools/update_dependencies.sh b/application/kvbench/spdk/rocksdb/build_tools/update_dependencies.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/build_tools/version.sh b/application/kvbench/spdk/rocksdb/build_tools/version.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/coverage/coverage_test.sh b/application/kvbench/spdk/rocksdb/coverage/coverage_test.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/java/crossbuild/build-linux-centos.sh b/application/kvbench/spdk/rocksdb/java/crossbuild/build-linux-centos.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/java/crossbuild/build-linux.sh b/application/kvbench/spdk/rocksdb/java/crossbuild/build-linux.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/java/crossbuild/docker-build-linux-centos.sh b/application/kvbench/spdk/rocksdb/java/crossbuild/docker-build-linux-centos.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/java/jdb_bench.sh b/application/kvbench/spdk/rocksdb/java/jdb_bench.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/tools/auto_sanity_test.sh b/application/kvbench/spdk/rocksdb/tools/auto_sanity_test.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/tools/benchmark.sh b/application/kvbench/spdk/rocksdb/tools/benchmark.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/tools/benchmark_leveldb.sh b/application/kvbench/spdk/rocksdb/tools/benchmark_leveldb.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/tools/check_format_compatible.sh b/application/kvbench/spdk/rocksdb/tools/check_format_compatible.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/tools/dbench_monitor b/application/kvbench/spdk/rocksdb/tools/dbench_monitor
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/tools/generate_random_db.sh b/application/kvbench/spdk/rocksdb/tools/generate_random_db.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/tools/pflag b/application/kvbench/spdk/rocksdb/tools/pflag
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/tools/rdb/rdb b/application/kvbench/spdk/rocksdb/tools/rdb/rdb
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/tools/regression_test.sh b/application/kvbench/spdk/rocksdb/tools/regression_test.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/tools/rocksdb_dump_test.sh b/application/kvbench/spdk/rocksdb/tools/rocksdb_dump_test.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/tools/run_flash_bench.sh b/application/kvbench/spdk/rocksdb/tools/run_flash_bench.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/tools/run_leveldb.sh b/application/kvbench/spdk/rocksdb/tools/run_leveldb.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/rocksdb/tools/verify_random_db.sh b/application/kvbench/spdk/rocksdb/tools/verify_random_db.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/autobuild.sh b/application/kvbench/spdk/spdk/autobuild.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/autopackage.sh b/application/kvbench/spdk/spdk/autopackage.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/autorun.sh b/application/kvbench/spdk/spdk/autorun.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/autorun_post.py b/application/kvbench/spdk/spdk/autorun_post.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/autotest.sh b/application/kvbench/spdk/spdk/autotest.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/configure b/application/kvbench/spdk/spdk/configure
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/buildtools/auto-config-h.sh b/application/kvbench/spdk/spdk/dpdk/buildtools/auto-config-h.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/buildtools/gen-build-mk.sh b/application/kvbench/spdk/spdk/dpdk/buildtools/gen-build-mk.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/buildtools/gen-config-h.sh b/application/kvbench/spdk/spdk/dpdk/buildtools/gen-config-h.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/buildtools/relpath.sh b/application/kvbench/spdk/spdk/dpdk/buildtools/relpath.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/devtools/build-tags.sh b/application/kvbench/spdk/spdk/dpdk/devtools/build-tags.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/devtools/check-git-log.sh b/application/kvbench/spdk/spdk/dpdk/devtools/check-git-log.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/devtools/check-includes.sh b/application/kvbench/spdk/spdk/dpdk/devtools/check-includes.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/devtools/check-maintainers.sh b/application/kvbench/spdk/spdk/dpdk/devtools/check-maintainers.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/devtools/checkpatches.sh b/application/kvbench/spdk/spdk/dpdk/devtools/checkpatches.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/devtools/cocci.sh b/application/kvbench/spdk/spdk/dpdk/devtools/cocci.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/devtools/git-log-fixes.sh b/application/kvbench/spdk/spdk/dpdk/devtools/git-log-fixes.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/devtools/test-build.sh b/application/kvbench/spdk/spdk/dpdk/devtools/test-build.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/devtools/test-null.sh b/application/kvbench/spdk/spdk/dpdk/devtools/test-null.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/devtools/validate-abi.sh b/application/kvbench/spdk/spdk/dpdk/devtools/validate-abi.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/doc/api/doxy-html-custom.sh b/application/kvbench/spdk/spdk/dpdk/doc/api/doxy-html-custom.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/examples/ip_pipeline/config/diagram-generator.py b/application/kvbench/spdk/spdk/dpdk/examples/ip_pipeline/config/diagram-generator.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/examples/ip_pipeline/config/pipeline-to-core-mapping.py b/application/kvbench/spdk/spdk/dpdk/examples/ip_pipeline/config/pipeline-to-core-mapping.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/examples/performance-thread/l3fwd-thread/test.sh b/application/kvbench/spdk/spdk/dpdk/examples/performance-thread/l3fwd-thread/test.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/test/cmdline_test/cmdline_test.py b/application/kvbench/spdk/spdk/dpdk/test/cmdline_test/cmdline_test.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/dpdk/usertools/cpu_layout.py b/application/kvbench/spdk/spdk/dpdk/usertools/cpu_layout.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/autotest_common.sh b/application/kvbench/spdk/spdk/scripts/autotest_common.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/build_kmod.sh b/application/kvbench/spdk/spdk/scripts/build_kmod.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/ceph/start.sh b/application/kvbench/spdk/spdk/scripts/ceph/start.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/ceph/stop.sh b/application/kvbench/spdk/spdk/scripts/ceph/stop.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/check_format.sh b/application/kvbench/spdk/spdk/scripts/check_format.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/detect_cc.sh b/application/kvbench/spdk/spdk/scripts/detect_cc.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/eofnl b/application/kvbench/spdk/spdk/scripts/eofnl
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/fio.py b/application/kvbench/spdk/spdk/scripts/fio.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/gen_nvme.sh b/application/kvbench/spdk/spdk/scripts/gen_nvme.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/genconfig.py b/application/kvbench/spdk/spdk/scripts/genconfig.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/perf/nvme/run_fio_test.py b/application/kvbench/spdk/spdk/scripts/perf/nvme/run_fio_test.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/perf/nvme/run_fio_test.sh b/application/kvbench/spdk/spdk/scripts/perf/nvme/run_fio_test.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/pkgdep.sh b/application/kvbench/spdk/spdk/scripts/pkgdep.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/prep_benchmarks.sh b/application/kvbench/spdk/spdk/scripts/prep_benchmarks.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/rpc.py b/application/kvbench/spdk/spdk/scripts/rpc.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/setup.sh b/application/kvbench/spdk/spdk/scripts/setup.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/scripts/vagrant/build.sh b/application/kvbench/spdk/spdk/scripts/vagrant/build.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/blobfs/rocksdb/postprocess.py b/application/kvbench/spdk/spdk/test/blobfs/rocksdb/postprocess.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/blobfs/rocksdb/rocksdb.sh b/application/kvbench/spdk/spdk/test/blobfs/rocksdb/rocksdb.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/blobfs/rocksdb/run_tests.sh b/application/kvbench/spdk/spdk/test/blobfs/rocksdb/run_tests.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/blobfs/rocksdb/run_tests1.sh b/application/kvbench/spdk/spdk/test/blobfs/rocksdb/run_tests1.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/calsoft/calsoft.py b/application/kvbench/spdk/spdk/test/iscsi_tgt/calsoft/calsoft.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/calsoft/calsoft.sh b/application/kvbench/spdk/spdk/test/iscsi_tgt/calsoft/calsoft.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/ext4test/ext4test.sh b/application/kvbench/spdk/spdk/test/iscsi_tgt/ext4test/ext4test.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/filesystem/filesystem.sh b/application/kvbench/spdk/spdk/test/iscsi_tgt/filesystem/filesystem.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/fio/fio.sh b/application/kvbench/spdk/spdk/test/iscsi_tgt/fio/fio.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/fio/running_config.sh b/application/kvbench/spdk/spdk/test/iscsi_tgt/fio/running_config.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/idle_migration/build_configuration.sh b/application/kvbench/spdk/spdk/test/iscsi_tgt/idle_migration/build_configuration.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/idle_migration/connection_status.py b/application/kvbench/spdk/spdk/test/iscsi_tgt/idle_migration/connection_status.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/idle_migration/idle_migration.sh b/application/kvbench/spdk/spdk/test/iscsi_tgt/idle_migration/idle_migration.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/ip_migration/ip_migration.sh b/application/kvbench/spdk/spdk/test/iscsi_tgt/ip_migration/ip_migration.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/iscsi_tgt.sh b/application/kvbench/spdk/spdk/test/iscsi_tgt/iscsi_tgt.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/nvme_remote/fio_remote_nvme.sh b/application/kvbench/spdk/spdk/test/iscsi_tgt/nvme_remote/fio_remote_nvme.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/rbd/rbd.sh b/application/kvbench/spdk/spdk/test/iscsi_tgt/rbd/rbd.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/reset/reset.sh b/application/kvbench/spdk/spdk/test/iscsi_tgt/reset/reset.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/rpc_config/rpc_config.py b/application/kvbench/spdk/spdk/test/iscsi_tgt/rpc_config/rpc_config.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/iscsi_tgt/rpc_config/rpc_config.sh b/application/kvbench/spdk/spdk/test/iscsi_tgt/rpc_config/rpc_config.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/lib/bdev/blockdev.sh b/application/kvbench/spdk/spdk/test/lib/bdev/blockdev.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/lib/env/env.sh b/application/kvbench/spdk/spdk/test/lib/env/env.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/lib/event/event.sh b/application/kvbench/spdk/spdk/test/lib/event/event.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/lib/ioat/ioat.sh b/application/kvbench/spdk/spdk/test/lib/ioat/ioat.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/lib/nvme/hotplug.sh b/application/kvbench/spdk/spdk/test/lib/nvme/hotplug.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/lib/nvme/nvme.sh b/application/kvbench/spdk/spdk/test/lib/nvme/nvme.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/common.sh b/application/kvbench/spdk/spdk/test/nvmf/common.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/discovery/discovery.sh b/application/kvbench/spdk/spdk/test/nvmf/discovery/discovery.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/filesystem/filesystem.sh b/application/kvbench/spdk/spdk/test/nvmf/filesystem/filesystem.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/fio/fio.sh b/application/kvbench/spdk/spdk/test/nvmf/fio/fio.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/fio/nvmf_fio.py b/application/kvbench/spdk/spdk/test/nvmf/fio/nvmf_fio.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/host/aer.sh b/application/kvbench/spdk/spdk/test/nvmf/host/aer.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/host/fio.sh b/application/kvbench/spdk/spdk/test/nvmf/host/fio.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/host/identify.sh b/application/kvbench/spdk/spdk/test/nvmf/host/identify.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/host/identify_kernel_nvmf.sh b/application/kvbench/spdk/spdk/test/nvmf/host/identify_kernel_nvmf.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/host/perf.sh b/application/kvbench/spdk/spdk/test/nvmf/host/perf.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/multiconnection/multiconnection.sh b/application/kvbench/spdk/spdk/test/nvmf/multiconnection/multiconnection.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/nvme_cli/nvme_cli.sh b/application/kvbench/spdk/spdk/test/nvmf/nvme_cli/nvme_cli.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/nvmf.sh b/application/kvbench/spdk/spdk/test/nvmf/nvmf.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/rpc/rpc.sh b/application/kvbench/spdk/spdk/test/nvmf/rpc/rpc.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/nvmf/shutdown/shutdown.sh b/application/kvbench/spdk/spdk/test/nvmf/shutdown/shutdown.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/vhost/common/run_fio.py b/application/kvbench/spdk/spdk/test/vhost/common/run_fio.py
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/vhost/common/run_vhost.sh b/application/kvbench/spdk/spdk/test/vhost/common/run_vhost.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/vhost/common/vm_run.sh b/application/kvbench/spdk/spdk/test/vhost/common/vm_run.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/vhost/common/vm_setup.sh b/application/kvbench/spdk/spdk/test/vhost/common/vm_setup.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/vhost/common/vm_shutdown.sh b/application/kvbench/spdk/spdk/test/vhost/common/vm_shutdown.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/vhost/common/vm_ssh.sh b/application/kvbench/spdk/spdk/test/vhost/common/vm_ssh.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/vhost/fiotest/autotest.sh b/application/kvbench/spdk/spdk/test/vhost/fiotest/autotest.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/vhost/integrity/integrity_start.sh b/application/kvbench/spdk/spdk/test/vhost/integrity/integrity_start.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/vhost/integrity/integrity_vm.sh b/application/kvbench/spdk/spdk/test/vhost/integrity/integrity_vm.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/test/vhost/spdk_vhost.sh b/application/kvbench/spdk/spdk/test/vhost/spdk_vhost.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/spdk/spdk/unittest.sh b/application/kvbench/spdk/spdk/unittest.sh
old mode 100644
new mode 100755
diff --git a/application/kvbench/utils/memory.cc b/application/kvbench/utils/memory.cc
index 3c602c2..d81dc51 100644
--- a/application/kvbench/utils/memory.cc
+++ b/application/kvbench/utils/memory.cc
@@ -133,52 +133,51 @@ uint32_t IndexFromAddr(const unsigned char* p, mempool_t *pool)
   return (((uint32_t )(p - pool->base)) / pool->block_size);
 }
 
-void *Allocate(mempool_t *pool)
+void* Allocate(mempool_t *pool)
 {
 
-  if (pool->num_initializedblocks < pool->num_blocks)
+  if (pool->num_initializedblocks < pool->num_blocks )
   {
-    uint32_t *p = (uint32_t *)AddrFromIndex(pool->num_initializedblocks, pool);
-    *p = pool->num_initializedblocks + 1;
-    pool->num_initializedblocks++;
+      uint32_t * p = (uint32_t *)AddrFromIndex(pool->num_initializedblocks, pool);
+      *p = pool->num_initializedblocks + 1;
+      pool->num_initializedblocks++;
   }
-  void *ret = NULL;
-  if (pool->num_freeblocks > 0)
-  {
-    ret = (void *)pool->nextfreeblock;
-
-    --(pool->num_freeblocks);
-    if (pool->num_freeblocks != 0)
+  void* ret = NULL;
+  if ( pool->num_freeblocks > 0 )
     {
-      pool->nextfreeblock = AddrFromIndex(*((uint32_t *)pool->nextfreeblock), pool);
+      ret = (void*)pool->nextfreeblock;
+
+      --(pool->num_freeblocks);
+      if (pool->num_freeblocks!=0)
+	{
+	  pool->nextfreeblock = AddrFromIndex(*((uint32_t *)pool->nextfreeblock), pool);
+	}
+      else
+	{
+	  pool->nextfreeblock = NULL;
+	}
     }
-    else
-    {
-      pool->nextfreeblock = NULL;
-    }
-  }
 
   return ret;
 }
 
 void DeAllocate(void* p, mempool_t *pool)
 {
-  if (p == NULL)
-    return;
+  if(p == NULL) return;
 
   if (pool->nextfreeblock != NULL)
-  {
-    (*(uint32_t *)p) = IndexFromAddr(pool->nextfreeblock, pool);
-    pool->nextfreeblock = (unsigned char *)p;
-  }
+    {
+      (*(uint32_t *)p) = IndexFromAddr( pool->nextfreeblock, pool );
+      pool->nextfreeblock = (unsigned char*)p;
+    }
   else
-  {
-    *((uint32_t *)p) = pool->num_blocks;
-    pool->nextfreeblock = (unsigned char *)p;
-  }
-
-  ++(pool->num_freeblocks);
+    {
+      *((uint32_t *)p) = pool->num_blocks;
+      pool->nextfreeblock = (unsigned char*)p;
+    }
   
+  ++(pool->num_freeblocks);
+
 }
 
 void *allocate_mem_numa(int alignment, uint64_t size, int nodeid);
@@ -207,12 +206,7 @@ void pool_setup(pool_info_t *info, mempool_t *pool, int nodeid)
 #endif
 
     if (memory == NULL) {
-      fprintf(stderr,
-      "Allocate memory failed. "
-      "Pool alignment is invalid: pool alignment = %d "
-      "or Not enough memory on Socket %d : request size = %ld\n",
-      info->alignment, nodeid, size );
-      exit(0);
+      fprintf(stderr, "Not enough memory on Socket %d : request size = %ld\n", nodeid, size );
     }
 
     initialize(memory, info, pool);
diff --git a/application/kvbench/wrappers/couch_kv.cc b/application/kvbench/wrappers/couch_kv.cc
index 061eb7e..5fdb7dd 100644
--- a/application/kvbench/wrappers/couch_kv.cc
+++ b/application/kvbench/wrappers/couch_kv.cc
@@ -22,8 +22,7 @@
 
 #define workload_check (0)
 #define LATENCY_CHECK  // only for async IO completion latency
-//#define MAX_SAMPLES 1000000
-static uint32_t max_sample = 1000000;
+#define MAX_SAMPLES 1000000
 static int use_udd = 0;
 static int kdd_is_polling = 1;
 #define GB_SIZE (1024*1024*1024)
@@ -31,9 +30,6 @@ static int kdd_is_polling = 1;
 int couch_kv_min_key_len = KVS_MIN_KEY_LENGTH;
 int couch_kv_max_key_len = KVS_MAX_KEY_LENGTH;
 
-const char* g_container_name = "container1";
-const int g_max_iterator_count = 16;
-
 struct _db {
   int id;
   kvs_device_handle dev;
@@ -87,7 +83,7 @@ static const char *kv_conf_path = "../env_init.conf";
 static kvs_option_iterator g_iter_mode;
 static std::map<kvs_key*, kv_bench_data*> kvdata_map[3];
 static std::mutex kvdata_lock[3];
-static std::map<kvs_key_space_handle, kv_bench_data*> kviter_map;
+static std::map<kvs_iterator_handle, kv_bench_data*> kviter_map;
 static std::mutex kviter_lock;
 #endif
 
@@ -352,9 +348,9 @@ void on_io_complete(kvs_callback_context* ioctx) {
     end = t11.tv_sec * 1000000000L + t11.tv_nsec;
     end /= 1000L;
     start = *((unsigned long long*)ioctx->private2);
-    if (l_stat->cursor >= max_sample) {
-      l_stat->cursor = l_stat->cursor % max_sample;
-      l_stat->nsamples = max_sample;
+    if (l_stat->cursor >= MAX_SAMPLES) {
+      l_stat->cursor = l_stat->cursor % MAX_SAMPLES;
+      l_stat->nsamples = MAX_SAMPLES;
     } else {
       l_stat->nsamples = l_stat->cursor + 1; 
     }
@@ -449,7 +445,6 @@ couchstore_error_t couchstore_open_db_kvs(const char *dev_path,
   int ret;
   Db *ppdb;
   *pDb = (Db*)malloc(sizeof(Db));
-  memset(*pDb, 0, sizeof(Db));
   ppdb = *pDb;
 
   ret = kvs_open_device(dev_path, &ppdb->dev);
@@ -494,24 +489,9 @@ couchstore_error_t couchstore_open_db_kvs(const char *dev_path,
   }
     
   /* Container related op */
-  uint32_t valid_cnt = 0;
-  const uint32_t retrieve_cnt = 2;
-  kvs_container_name names[retrieve_cnt];
-  char tname[retrieve_cnt][MAX_CONT_PATH_LEN];
-  for(uint8_t idx = 0; idx < retrieve_cnt; idx++) {
-    names[idx].name_len = MAX_CONT_PATH_LEN;
-    names[idx].name = tname[idx];
-  }
-  kvs_list_containers(ppdb->dev, 1, retrieve_cnt*sizeof(kvs_container_name),
-    names, &valid_cnt);
-  for (uint8_t idx = 0; idx < valid_cnt; idx++) {
-    kvs_delete_container(ppdb->dev, names[idx].name);
-  }
-
   kvs_container_context ctx;
-  ctx.option = {KVS_KEY_ORDER_NONE};
-  kvs_create_container(ppdb->dev, g_container_name, 4, &ctx);
-  kvs_open_container(ppdb->dev, g_container_name, &ppdb->cont_hd);
+  kvs_create_container(ppdb->dev, "test", 4, &ctx);
+  kvs_open_container(ppdb->dev, "test", &ppdb->cont_hd);
   
   fprintf(stdout, "device open %s\n", dev_path);
 
@@ -552,8 +532,6 @@ couchstore_error_t couchstore_close_db(Db *db)
   }
 
   kvs_close_container(db->cont_hd);
-  
-  kvs_delete_container(db->dev, g_container_name);
   kvs_close_device(db->dev);
   free(db);
     
@@ -580,7 +558,7 @@ couchstore_error_t couchstore_iterator_open(Db *db, int iterator_mode) {
   char prefix_str[5] = "0000";
   unsigned int PREFIX_KV = 0;
   for (int i = 0; i < 4; i++){
-    PREFIX_KV |= (prefix_str[i] << (3-i)*8);
+    PREFIX_KV |= (prefix_str[i] << i*8);
   }
   iter_ctx.bit_pattern = PREFIX_KV;
 
@@ -599,7 +577,7 @@ couchstore_error_t couchstore_iterator_open(Db *db, int iterator_mode) {
 
   //kvs_close_iterator_all(db->cont_hd);
   int ret = kvs_open_iterator(db->cont_hd, &iter_ctx, &db->iter_handle);
-  if(ret && ret != KVS_ERR_ITERATOR_OPEN) {
+  if(ret) {
     fprintf(stdout, "open iter failed with err %s\n", kvs_errstr(ret));
     exit(1);
   }
@@ -728,16 +706,14 @@ couchstore_error_t kvs_store_sync(Db *db, Doc* const docs[],
 				   unsigned numdocs, couchstore_save_options options)
 {
   int i, ret;
-  kvs_store_option option;
-  option.st_type = KVS_STORE_POST;
-  option.kvs_store_compress = false;
-
+  kvs_store_option option; 
   for(i = 0; i < numdocs; i++){
     const kvs_store_context put_ctx = {option, db, NULL};
     const kvs_key kvskey = {docs[i]->id.buf, (kvs_key_t)docs[i]->id.size};
     const kvs_value kvsvalue = { docs[i]->data.buf, (uint32_t)docs[i]->data.size , 0, 0 };
 
     ret = kvs_store_tuple(db->cont_hd, &kvskey, &kvsvalue, &put_ctx);
+
     if(ret != KVS_SUCCESS) {
       fprintf(stderr, "KVBENCH: store tuple sync failed %s 0x%x\n", (char*)docs[i]->id.buf, ret);
       exit(1);
@@ -754,9 +730,7 @@ couchstore_error_t kvs_store_async(Db *db, Doc* const docs[],
   int ret;
 
   assert(numdocs == 1);
-  kvs_store_option option;
-  option.st_type = KVS_STORE_POST;
-  option.kvs_store_compress = false;
+  kvs_store_option option; 
   kvs_store_context put_ctx; //{KVS_STORE_POST , 0, db, NULL};
 
   std::unique_lock<std::mutex> lock(db->lock_k);
@@ -910,7 +884,7 @@ void on_io_complete(kvs_postprocess_context* ioctx) {
     if (*(ioctx->iter_hd) != 1)
         *ioctx->iter_hd = 1;
     std::unique_lock<std::mutex> key_lock(kviter_lock);
-    auto db_it = kviter_map.find((kvs_key_space_handle)(*ioctx->ks_hd));
+    auto db_it = kviter_map.find((kvs_iterator_handle)(*ioctx->iter_hd));
     if (db_it == kviter_map.end()) {
       fprintf(stderr, "not found keyspace handle in map %lu\n", (uint64_t)*ioctx->iter_hd);
       return;
@@ -1021,9 +995,9 @@ void on_io_complete(kvs_postprocess_context* ioctx) {
     end /= 1000L;
     start = *((unsigned long long*)kvdata->time);
     free(kvdata->time);
-    if (l_stat->cursor >= max_sample) {
-      l_stat->cursor = l_stat->cursor % max_sample;
-      l_stat->nsamples = max_sample;
+    if (l_stat->cursor >= MAX_SAMPLES) {
+      l_stat->cursor = l_stat->cursor % MAX_SAMPLES;
+      l_stat->nsamples = MAX_SAMPLES;
     } else {
       l_stat->nsamples = l_stat->cursor + 1; 
     }
@@ -1114,7 +1088,6 @@ couchstore_error_t couchstore_open_db_kvs(const char *dev_path,
   int ret;
   Db *ppdb;
   *pDb = (Db*)malloc(sizeof(Db));
-  memset(*pDb, 0, sizeof(Db));
   ppdb = *pDb;
 
   ret = kvs_open_device((char *)dev_path, &ppdb->dev);
@@ -1160,29 +1133,17 @@ couchstore_error_t couchstore_open_db_kvs(const char *dev_path,
   }
     
   /* Keyspace related op */
-  uint32_t valid_cnt = 0;
-  const uint32_t retrieve_cnt = 2;
-  kvs_key_space_name names[retrieve_cnt];
-  char tname[retrieve_cnt][MAX_CONT_PATH_LEN];
-  for(uint8_t idx = 0; idx < retrieve_cnt; idx++) {
-    names[idx].name_len = MAX_KEYSPACE_NAME_LEN;
-    names[idx].name = tname[idx];
-  }
-
-  kvs_list_key_spaces(ppdb->dev, 1, retrieve_cnt*sizeof(kvs_key_space_name),
-    names, &valid_cnt);
-  
-  for (uint8_t idx = 0; idx < valid_cnt; idx++) {
-    kvs_delete_key_space(ppdb->dev, &names[idx]);
-  }
-  
+  const char* name = "test";
   kvs_key_space_name ks_name;
-  ks_name.name = (char *)g_container_name;
-  ks_name.name_len = strlen(g_container_name);
-  kvs_option_key_space option = {KVS_KEY_ORDER_NONE};
+  kvs_option_key_space option = {KVS_KEY_ORDER_ASCEND};
+  ks_name.name = (char *)name;
+  ks_name.name_len = strlen(name);
+  
   kvs_create_key_space(ppdb->dev, &ks_name, 0, option);
-  kvs_open_key_space(ppdb->dev, (char *)g_container_name, &ppdb->cont_hd);
-
+  
+  kvs_key_space_handle ks_hd;
+  kvs_open_key_space(ppdb->dev, (char *)name, &ppdb->cont_hd);
+  
   fprintf(stdout, "device open %s\n", dev_path);
 
   return COUCHSTORE_SUCCESS;
@@ -1222,12 +1183,7 @@ couchstore_error_t couchstore_close_db(Db *db)
   }
 
   kvs_close_key_space(db->cont_hd);
-
-  kvs_key_space_name ks_name;
-  ks_name.name = (char *)g_container_name;
-  ks_name.name_len = strlen(g_container_name);
-  kvs_delete_key_space(db->dev, &ks_name);
-
+  kvs_delete_key_space(db->dev, NULL);
   kvs_close_device(db->dev);
 
   free(db);
@@ -1246,15 +1202,19 @@ couchstore_error_t couchstore_iterator_open(Db *db, int iterator_mode) {
   kvs_key_group_filter iter_ctx;
   kvs_iterator_handle iter_hd;
 
-  iter_ctx.bitmask[0] = 0xff;
-  iter_ctx.bitmask[1] = 0xff;
-  iter_ctx.bitmask[2] = 0;
-  iter_ctx.bitmask[3] = 0;
-  
-  iter_ctx.bit_pattern[0] = '0';
-  iter_ctx.bit_pattern[1] = '0';
-  iter_ctx.bit_pattern[2] = '0';
-  iter_ctx.bit_pattern[3] = '0';
+  iter_ctx.bitmask[0] = 0;
+  iter_ctx.bitmask[1] = 0;
+  iter_ctx.bitmask[2] = 0xff;
+  iter_ctx.bitmask[3] = 0xff;
+  char prefix_str[5] = "0000";
+  unsigned int PREFIX_KV = 0;
+  for (int i = 0; i < 4; i++){
+    PREFIX_KV |= (prefix_str[i] << i*8);
+  }
+  iter_ctx.bit_pattern[0] = PREFIX_KV & 0xff;
+  iter_ctx.bit_pattern[1] = PREFIX_KV & 0xff00 >> 8;
+  iter_ctx.bit_pattern[2] = PREFIX_KV & 0xff0000 >> 16;
+  iter_ctx.bit_pattern[3] = PREFIX_KV & 0xff000000 >> 24;
 
   kvs_option_iterator option;
   memset(&option, 0, sizeof(kvs_option_iterator));
@@ -1268,7 +1228,7 @@ couchstore_error_t couchstore_iterator_open(Db *db, int iterator_mode) {
 
   //kvs_close_iterator_all(db->cont_hd);
   int ret = kvs_create_iterator(db->cont_hd, &option, &iter_ctx, &db->iter_handle);
-  if(ret && ret!= KVS_ERR_ITERATOR_OPEN) {
+  if(ret) {
     fprintf(stdout, "open iter failed with err %d\n", ret);
     exit(1);
   }
@@ -1284,8 +1244,8 @@ couchstore_error_t couchstore_iterator_open(Db *db, int iterator_mode) {
   memset(kvdata, 0, sizeof(kv_bench_data));
   kvdata->db = db;
   std::unique_lock<std::mutex> k_lock(kviter_lock);
-  kviter_map.insert(std::make_pair<kvs_key_space_handle, kv_bench_data*>
-    ((kvs_key_space_handle)db->cont_hd, (kv_bench_data*)kvdata));
+  kviter_map.insert(std::make_pair<kvs_iterator_handle, kv_bench_data*>
+    ((kvs_iterator_handle)db->iter_handle, (kv_bench_data*)kvdata));
   k_lock.unlock();
   return COUCHSTORE_SUCCESS; 
 }
@@ -1300,12 +1260,12 @@ couchstore_error_t couchstore_iterator_close(Db *db) {
   }
 
   std::unique_lock<std::mutex> k_lock(kviter_lock);
-  auto it = kviter_map.find(db->cont_hd);
+  auto it = kviter_map.find(db->iter_handle);
   if (it == kviter_map.end()) {
     fprintf(stderr, "not found iterator handle %lu ", (uint64_t)db->iter_handle);
   } else {
     kv_bench_data *data = it->second;
-    kviter_map.erase(db->cont_hd);
+    kviter_map.erase(db->iter_handle);
     free(data);
   }
   k_lock.unlock();
@@ -1583,12 +1543,6 @@ couchstore_error_t couchstore_kvs_reset_aiocompletion(){
   return COUCHSTORE_SUCCESS;
 }
 
-couchstore_error_t couchstore_kvs_set_max_sample(uint32_t sample_num)
-{
-  max_sample = sample_num;
-  return COUCHSTORE_SUCCESS;
-}
-
 couchstore_error_t couchstore_close_device(int32_t dev_id)
 {
 
diff --git a/application/kvbench/wrappers/couch_rocksdb.cc b/application/kvbench/wrappers/couch_rocksdb.cc
index 9244923..e38871b 100644
--- a/application/kvbench/wrappers/couch_rocksdb.cc
+++ b/application/kvbench/wrappers/couch_rocksdb.cc
@@ -245,7 +245,6 @@ couchstore_error_t couchstore_save_documents(Db *db, Doc* const docs[], DocInfo
     status = db->db->Write(*db->write_options, &wb);
     if (!status.ok()) {
         printf("ERR %s\n", status.ToString().c_str());
-        return COUCHSTORE_ERROR_WRITE;
     }
     assert(status.ok());
 #endif
