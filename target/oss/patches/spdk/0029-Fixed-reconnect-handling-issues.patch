From ad009ee0decf7faf0c17153540223a3dd629aeff Mon Sep 17 00:00:00 2001
From: Benixon Arul dhas <benixon.a@samsung.com>
Date: Tue, 9 Feb 2021 10:25:49 -0800
Subject: [PATCH 29/76] Fixed reconnect handling issues

    * Added trace logging for key transfer states
---
 lib/nvmf/rdma.c | 45 +++++++++++++++++++++++++++++++++++++++++----
 1 file changed, 41 insertions(+), 4 deletions(-)

diff --git a/lib/nvmf/rdma.c b/lib/nvmf/rdma.c
index 562569aba..d59b3c8f9 100644
--- a/lib/nvmf/rdma.c
+++ b/lib/nvmf/rdma.c
@@ -160,6 +160,11 @@ enum spdk_nvmf_rdma_request_state {
 #define TRACE_RDMA_QP_DISCONNECT					SPDK_TPOINT_ID(TRACE_GROUP_NVMF_RDMA, 0x10)
 #define TRACE_RDMA_QP_DESTROY						SPDK_TPOINT_ID(TRACE_GROUP_NVMF_RDMA, 0x11)
 
+#define TRACE_RDMA_REQUEST_STATE_QOS_POST				SPDK_TPOINT_ID(TRACE_GROUP_NVMF_RDMA, 0x12)
+#define TRACE_RDMA_REQUEST_STATE_PENDING_GET_KEY				SPDK_TPOINT_ID(TRACE_GROUP_NVMF_RDMA, 0x13)
+#define TRACE_RDMA_REQUEST_STATE_TRANSFERRING_KEY				SPDK_TPOINT_ID(TRACE_GROUP_NVMF_RDMA, 0x14)
+#define TRACE_RDMA_REQUEST_STATE_READY_WITH_KEY				SPDK_TPOINT_ID(TRACE_GROUP_NVMF_RDMA, 0x15)
+
 SPDK_TRACE_REGISTER_FN(nvmf_trace, "nvmf_rdma", TRACE_GROUP_NVMF_RDMA)
 {
 	spdk_trace_register_object(OBJECT_NVMF_RDMA_IO, 'r');
@@ -198,6 +203,19 @@ SPDK_TRACE_REGISTER_FN(nvmf_trace, "nvmf_rdma", TRACE_GROUP_NVMF_RDMA)
 					TRACE_RDMA_REQUEST_STATE_COMPLETED,
 					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
 
+	spdk_trace_register_description("RDMA_REQ_QOS_POST",
+					TRACE_RDMA_REQUEST_STATE_QOS_POST,
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+	spdk_trace_register_description("RDMA_REQ_PENDING_GET_KEY",
+					TRACE_RDMA_REQUEST_STATE_PENDING_GET_KEY,
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+	spdk_trace_register_description("RDMA_REQ_TRANSFERRING_KEY",
+					TRACE_RDMA_REQUEST_STATE_TRANSFERRING_KEY,
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+	spdk_trace_register_description("RDMA_REQ_READY_WITH_KEY",
+					TRACE_RDMA_REQUEST_STATE_READY_WITH_KEY,
+					OWNER_NONE, OBJECT_NVMF_RDMA_IO, 0, 1, "cmid:   ");
+
 	spdk_trace_register_description("RDMA_QP_CREATE", TRACE_RDMA_QP_CREATE,
 					OWNER_NONE, OBJECT_NONE, 0, 0, "");
 	spdk_trace_register_description("RDMA_IBV_ASYNC_EVENT", TRACE_RDMA_IBV_ASYNC_EVENT,
@@ -892,7 +910,9 @@ dfly_rdma_setup_data_transfer_out(struct spdk_nvmf_rdma_qpair *rqpair, struct sp
 	data_offset = dfly_req->req_value.offset;
 	switch(rdma_req->req.cmd->nvme_cmd.opc) {
 		case SPDK_NVME_OPC_SAMSUNG_KV_RETRIEVE:
-		//assert(rdma_req->data.wr.sg_list[sge_index].length > data_offset); //jian: data_offset could be larger than for small object size
+			//assert(data_offset < 4096);//we have only 4K extra buffer allocated
+			//data_offset + data len <= sge length
+			assert(rdma_req->data.wr.sg_list[sge_index].length > data_offset + rsp->cdw0);
 			while(rem_data_length && (sge_index < rdma_req->data.wr.num_sge)) {
 				sge_len = rdma_req->data.wr.sg_list[sge_index].length;
 				if(data_offset > 0) {
@@ -1205,6 +1225,7 @@ nvmf_rdma_qpair_destroy(struct spdk_nvmf_rdma_qpair *rqpair)
 			if (req->req.qpair == qpair && req->state != RDMA_REQUEST_STATE_FREE) {
 				/* nvmf_rdma_request_process checks qpair ibv and internal state
 				 * and completes a request */
+				SPDK_NOTICELOG("Complete request %p in qp %p in state %x\n", req, qpair, req->state);
 				nvmf_rdma_request_process(rtransport, req);
 			}
 		}
@@ -2253,6 +2274,7 @@ _nvmf_rdma_request_free(struct spdk_nvmf_rdma_request *rdma_req,
 	struct spdk_nvmf_rdma_poll_group	*rgroup;
 
 	rqpair = SPDK_CONTAINEROF(rdma_req->req.qpair, struct spdk_nvmf_rdma_qpair, qpair);
+	assert(rdma_req->key.wr.sg_list[0].length == SAMSUNG_KV_MAX_FABRIC_KEY_SIZE);
 	if (rdma_req->req.data_from_pool) {
 		rgroup = rqpair->poller->group;
 
@@ -2269,7 +2291,6 @@ _nvmf_rdma_request_free(struct spdk_nvmf_rdma_request *rdma_req,
 	memset(&rdma_req->req.dif, 0, sizeof(rdma_req->req.dif));
 	rqpair->qd--;
 
-	assert(rdma_req->key.wr.sg_list[0].length == SAMSUNG_KV_MAX_FABRIC_KEY_SIZE);
 
 	STAILQ_INSERT_HEAD(&rqpair->resources->free_queue, rdma_req, state_link);
 	rdma_req->state = RDMA_REQUEST_STATE_FREE;
@@ -2305,6 +2326,10 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			STAILQ_REMOVE(&rqpair->pending_rdma_read_queue, rdma_req, spdk_nvmf_rdma_request, state_link);
 		} else if (rdma_req->state == RDMA_REQUEST_STATE_PENDING_GET_KEY) {
 			STAILQ_REMOVE(&rqpair->pending_rdma_key_read_queue, rdma_req, spdk_nvmf_rdma_request, state_link);
+			dfly_rdma_fini_key_transfer(rdma_req);
+		} else if (rdma_req->state == RDMA_REQUEST_STATE_TRANSFERRING_KEY ||
+					rdma_req->state == RDMA_REQUEST_STATE_READY_WITH_KEY){
+			dfly_rdma_fini_key_transfer(rdma_req);
 		} else if (rdma_req->state == RDMA_REQUEST_STATE_DATA_TRANSFER_TO_HOST_PENDING) {
 			STAILQ_REMOVE(&rqpair->pending_rdma_write_queue, rdma_req, spdk_nvmf_rdma_request, state_link);
 		}
@@ -2362,6 +2387,8 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 				}
 			}//else follow through without QoS
 		case RDMA_REQUEST_STATE_QOS_POST:
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_QOS_POST, 0, 0,
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
 			//END - SPDK_CONFIG_OSS_TARGET
 
 			/* The next state transition depends on the data transfer needs of this request. */
@@ -2384,8 +2411,8 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			break;
 		//SPDK_CONFIG_OSS_TARGET
 		case RDMA_REQUEST_STATE_PENDING_GET_KEY:
-			//spdk_trace_record(TRACE_RDMA_REQUEST_STATE_KEY_TRANSFER_PENDING, 0, 0,
-			//		  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_PENDING_GET_KEY, 0, 0,
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
 
 			if (rdma_req != STAILQ_FIRST(&rqpair->pending_rdma_key_read_queue)) {
 				/* This request needs to wait in line to perform RDMA */
@@ -2410,9 +2437,13 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			}
 			break;
 		case RDMA_REQUEST_STATE_TRANSFERRING_KEY:
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_TRANSFERRING_KEY, 0, 0,
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
 			//External Code should kick this in??
 			break;
 		case RDMA_REQUEST_STATE_READY_WITH_KEY:
+			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_READY_WITH_KEY, 0, 0,
+					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
 			dfly_rdma_fini_key_transfer(rdma_req);
 			/* If no data to transfer, ready to execute. */
 			if (rdma_req->req.xfer == SPDK_NVME_DATA_NONE) {
@@ -2525,6 +2556,9 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			}
 
 			rdma_req->state = RDMA_REQUEST_STATE_EXECUTING;
+			if(rdma_req->req.xfer != SPDK_NVME_DATA_NONE) {
+				assert(rdma_req->req.data != NULL);
+			}
 			spdk_nvmf_request_exec(&rdma_req->req);
 			break;
 		case RDMA_REQUEST_STATE_EXECUTING:
@@ -2627,6 +2661,8 @@ nvmf_rdma_request_process(struct spdk_nvmf_rdma_transport *rtransport,
 			spdk_trace_record(TRACE_RDMA_REQUEST_STATE_COMPLETED, 0, 0,
 					  (uintptr_t)rdma_req, (uintptr_t)rqpair->cm_id);
 
+			assert(rdma_req->key.wr.sg_list[0].length == SAMSUNG_KV_MAX_FABRIC_KEY_SIZE);
+
 			//SPDK_CONFIG_OSS_TARGET
 			if(rdma_req->req.dreq) {
 				df_lat_update_tick(rdma_req->req.dreq, DF_LAT_REQ_END);
@@ -4346,6 +4382,7 @@ nvmf_rdma_poller_poll(struct spdk_nvmf_rdma_transport *rtransport,
 				if (rdma_req->key.wr.opcode == IBV_WR_RDMA_READ) {
 					rqpair->current_read_depth--;
 					if (rdma_req->num_outstanding_key_wr == 0) {
+						dfly_rdma_fini_key_transfer(rdma_req);
 						rdma_req->state = RDMA_REQUEST_STATE_COMPLETED;
 					}
 				}
-- 
2.24.3

